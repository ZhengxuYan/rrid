Link/DOI,Publication Date,Title,Authors,Abstract
https://arxiv.org/abs/2403.08770,2024-03-13,FastMAC: Stochastic Spectral Sampling of Correspondence Graph,"['Yifei Zhang', 'Hao Zhao', 'Hongyang Li', 'Siheng Chen']","3D correspondence, i.e., a pair of 3D points, is a fundamental concept in computer vision. A set of 3D correspondences, when equipped with compatibility edges, forms a correspondence graph. This graph is a critical component in several state-of-the-art 3D point cloud registration approaches, e.g., the one based on maximal cliques (MAC). However, its properties have not been well understood. So we present the first study that introduces graph signal processing into the domain of correspondence graph. We exploit the generalized degree signal on correspondence graph and pursue sampling strategies that preserve high-frequency components of this signal. To address time-consuming singular value decomposition in deterministic sampling, we resort to a stochastic approximate sampling strategy. As such, the core of our method is the stochastic spectral sampling of correspondence graph. As an application, we build a complete 3D registration algorithm termed as FastMAC, that reaches real-time speed while leading to little to none performance drop. Through extensive experiments, we validate that FastMAC works for both indoor and outdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while maintaining high registration success rate on KITTI. Codes are publicly available at https://github.com/Forrest-110/FastMAC."
https://arxiv.org/abs/2403.08769,2024-03-13,The thermalization of $γ$-rays in radioactive expanding ejecta: A simple model and its application for Kilonovae and Ia SNe,"['Or Guttman', 'Ben Shenhar', 'Arnab Sarkar', 'Eli Waxman']","A semi-analytic approximation is derived for the time-dependent fraction $f_γ(t)$ of the energy deposited by radioactive decay $γ$-rays in a homologously expanding plasma of general structure. An analytic approximation is given for spherically symmetric plasma distributions. Applied to Kilonovae (KNe) associated with neutron stars mergers and Type Ia supernovae, our semi-analytic and analytic approximations reproduce, with a few percent and 10% accuracy, respectively, the energy deposition rates, $\dot{Q}_\text{dep}$, obtained in numeric Monte Carlo calculations. The time $t_γ$ beyond which $γ$-ray deposition is inefficient is determined by an effective frequency-independent $γ$-ray opacity $κ_{γ,\text{eff}}$, $t_γ= \sqrt{κ_{γ,\text{eff}}\langleΣ\rangle t^2}$, where $\langleΣ\rangle\propto t^{-2}$ is the average plasma column density. For $β$-decay dominated energy release, $κ_{γ,\text{eff}}$ is typically close to the effective Compton scattering opacity, $κ_{γ,\text{eff}} \approx 0.025~{\rm {cm}^{2}\,g^{-1}}$ with a weak dependence on composition. For KNe, $κ_{γ,\text{eff}}$ depends mainly on the initial electron fraction $Y_e$, $κ_{γ,\text{eff}} \approx 0.03(0.05)~{\rm {cm}^{2}\,g^{-1}}$ for $Y_e \gtrsim (\lesssim) 0.25$ (in contrast with earlier work that found $κ_{γ,\text{eff}}$ larger by 1-2 orders of magnitude for low $Y_e$), and is insensitive to the (large) nuclear physics uncertainties. Determining $t_γ$ from observations will therefore measure the ejecta $\langleΣ\rangle t^2$, providing a stringent test of models. For $\langleΣ\rangle t^2=2\times10^{11}~{\rm g\,{cm}^{-2}\,s^2}$, a typical value expected for KNe, $t_γ\approx1$ d."
https://arxiv.org/abs/2403.08768,2024-03-13,3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface,"['Linyi Jin', 'Nilesh Kulkarni', 'David Fouhey']","This paper introduces 3DFIRES, a novel system for scene-level 3D reconstruction from posed images. Designed to work with as few as one view, 3DFIRES reconstructs the complete geometry of unseen scenes, including hidden surfaces. With multiple view inputs, our method produces full reconstruction within all camera frustums. A key feature of our approach is the fusion of multi-view information at the feature level, enabling the production of coherent and comprehensive 3D reconstruction. We train our system on non-watertight scans from large-scale real scene dataset. We show it matches the efficacy of single-view reconstruction methods with only one input and surpasses existing techniques in both quantitative and qualitative measures for sparse-view 3D reconstruction."
https://arxiv.org/abs/2403.08767,2024-03-13,On the eigenvalues of the harmonic oscillator with a Gaussian perturbation,"['Paolo Amore', 'Francisco M. Fernández', 'Javier Garcia']",We test the analytical expressions for the first two eigenvalues of the harmonic oscillator with a Gaussian perturbation proposed recently. Our numerical eigenvalues show that those expressions are valid in an interval of the coupling parameter that is greater than the one estimated by the authors. We also calculate critical values of the coupling parameter and several exceptional points in the complex plane.
https://arxiv.org/abs/2403.08766,2024-03-13,MonoOcc: Digging into Monocular Semantic Occupancy Prediction,"['Yupeng Zheng', 'Xiang Li', 'Pengfei Li', 'Yuhang Zheng', 'Bu Jin', 'Chengliang Zhong', 'Xiaoxiao Long', 'Hao Zhao', 'Qichao Zhang']","Monocular Semantic Occupancy Prediction aims to infer the complete 3D geometry and semantic information of scenes from only 2D images. It has garnered significant attention, particularly due to its potential to enhance the 3D perception of autonomous vehicles. However, existing methods rely on a complex cascaded framework with relatively limited information to restore 3D scenes, including a dependency on supervision solely on the whole network's output, single-frame input, and the utilization of a small backbone. These challenges, in turn, hinder the optimization of the framework and yield inferior prediction results, particularly concerning smaller and long-tailed objects. To address these issues, we propose MonoOcc. In particular, we (i) improve the monocular occupancy prediction framework by proposing an auxiliary semantic loss as supervision to the shallow layers of the framework and an image-conditioned cross-attention module to refine voxel features with visual clues, and (ii) employ a distillation module that transfers temporal information and richer knowledge from a larger image backbone to the monocular semantic occupancy prediction framework with low cost of hardware. With these advantages, our method yields state-of-the-art performance on the camera-based SemanticKITTI Scene Completion benchmark. Codes and models can be accessed at https://github.com/ucaszyp/MonoOcc"
https://arxiv.org/abs/2403.08765,2024-03-13,An Analytic Description of Electron Thermalization in Kilonovae Ejecta,"['Ben Shenhar', 'Or Guttman', 'Eli Waxman']","A simple analytic description is provided of the rate of energy deposition by $β$-decay electrons in the homologously expanding radioactive plasma ejected in neutron star mergers, valid for a wide range of ejecta parameters -- initial entropy, electron fraction $\{s_0,Y_e\}$ and density $ρt^3$. The formulae are derived using detailed numerical calculations following the time-dependent composition and $β$-decay emission spectra (including the effect of delayed deposition). The deposition efficiency depends mainly on $ρt^3$ and only weakly on $\{s_0,Y_e\}$. The time $t_e$ at which the ratio between the rates of electron energy deposition and energy production drops to $1-e^{-1}$, is given by $t_e=t_{0e}\Big(\frac{ρt^3}{0.5(ρt^3)_0}\Big)^a$, where $(ρt^3)_0=\frac{0.05M_{\odot}}{4π(0.2c)^3}$, $t_{0e}(s_0,Y_e)\approx17$ days and $0.4\le a(s_0,Y_e)\le0.5$. The fractional uncertainty in $t_e$ due to nuclear physics uncertainties is $\approx10\%$. The result $a\le0.5$ reflects the fact that the characteristic $β$-decay electron energies do not decrease with time (largely due to ""inverted decay chains"" in which a slowly-decaying isotope decays to a rapidly-decaying isotope with higher end-point energy). We provide an analytic approximation for the time-dependent electron energy deposition rate, reproducing the numerical results to better than $50\%$ (typically $<30\%$, well within the energy production rate uncertainty due to nuclear physics uncertainties) over a 3-4 orders-of-magnitude deposition rate decrease with time. Our results may be easily incorporated in calculations of kilonovae light curves (with general density and composition structures), eliminating the need to numerically follow the time-dependent electron spectra. Identifying $t_e$, e.g. in the bolometric light curve, will constrain the (properly averaged) ejecta $ρt^3$."
https://arxiv.org/abs/2403.08764,2024-03-13,VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis,"['Enric Corona', 'Andrei Zanfir', 'Eduard Gabriel Bazavan', 'Nikos Kolotouros', 'Thiemo Alldieck', 'Cristian Sminchisescu']","We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions."
https://arxiv.org/abs/2403.08763,2024-03-13,Simple and Scalable Strategies to Continually Pre-train Large Language Models,"['Adam Ibrahim', 'Benjamin Thérien', 'Kshitij Gupta', 'Mats L. Richter', 'Quentin Anthony', 'Timothée Lesort', 'Eugene Belilovsky', 'Irina Rish']","Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget."
https://arxiv.org/abs/2403.08762,2024-03-13,Multi-axis inertial sensing with 2D arrays of Bose Einstein Condensates,"['K. Stolzenberg', 'C. Struckmann', 'S. Bode', 'R. Li', 'A. Herbst', 'V. Vollenkemper', 'D. Thomas', 'E. M. Rasel', 'N. Gaaloul', 'D. Schlippert']","Atom interferometers are an exquisite measurement tool for inertial forces. However, they are commonly limited to one single sensitive axis, allowing high-precision multi-dimensional sensing only through subsequent or postcorrected measurements. Here, we introduce a novel 2D-array-arrangement of Bose-Einstein Condensates (BEC) initialized utilizing time-averaged optical potentials for simultaneous multi-axis inertial sensing. Deploying a 3 x 3 BEC array covering 1.6 mm^2, we perform measurements of angular velocity and acceleration of a rotating reference mirror, as well as a linear acceleration, e.g., induced by gravity, gradients, and higher order derivatives. We anticipate increased sensitivity of our method in interferometers with large scale factors in long-baseline or satellite atom interferometry. Our work paves the way for simple high-precision multi-axis inertial sensing and we envision further applications, e.g., for three-dimensional wave front characterization."
https://arxiv.org/abs/2403.08761,2024-03-13,"Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches","['Yun Xin Teoh', 'Alice Othmani', 'Siew Li Goh', 'Juliana Usman', 'Khin Wee Lai']","Knee osteoarthritis is a degenerative joint disease that induces chronic pain and disability. Bone morphological analysis is a promising tool to understand the mechanical aspect of this disorder. This study proposes a 2D bone morphological analysis using manually segmented bones to explore morphological features related to distinct pain conditions. Furthermore, six semantic segmentation algorithms are assessed for extracting femur and tibia bones from X-ray images. Our analysis reveals that the morphology of the femur undergoes significant changes in instances where pain worsens. Conversely, improvements in pain may not manifest pronounced alterations in bone shape. The few-shot-learning-based algorithm, UniverSeg, demonstrated superior segmentation results with Dice scores of 99.69% for femur and 99.60% for tibia. Regarding pain condition classification, the zero-shot-learning-based algorithm, CP-SAM, achieved the highest accuracy at 66% among all models. UniverSeg is recommended for automatic knee bone segmentation, while SAM models show potential with prompt encoder modifications for optimized outcomes. These findings highlight the effectiveness of few-shot learning for semantic segmentation and the potential of zero-shot learning in enhancing classification models for knee osteoarthritis diagnosis."
https://arxiv.org/abs/2403.08760,2024-03-13,MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving Representation Learning,"['Jialv Zou', 'Bencheng Liao', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang']","Learning robust and scalable visual representations from massive multi-view video data remains a challenge in computer vision and autonomous driving. Existing pre-training methods either rely on expensive supervised learning with 3D annotations, limiting the scalability, or focus on single-frame or monocular inputs, neglecting the temporal information. We propose MIM4D, a novel pre-training paradigm based on dual masked image modeling (MIM). MIM4D leverages both spatial and temporal relations by training on masked multi-view video inputs. It constructs pseudo-3D features using continuous scene flow and projects them onto 2D plane for supervision. To address the lack of dense 3D supervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable rendering to learn geometric representations. We demonstrate that MIM4D achieves state-of-the-art performance on the nuScenes dataset for visual representation learning in autonomous driving. It significantly improves existing methods on multiple downstream tasks, including BEV segmentation (8.7% IoU), 3D object detection (3.5% mAP), and HD map construction (1.4% mAP). Our work offers a new choice for learning representation at scale in autonomous driving. Code and models are released at https://github.com/hustvl/MIM4D"
https://arxiv.org/abs/2403.08759,2024-03-13,Observational tests in scale invariance I: galaxy clusters and rotation of galaxies,['Andre Maeder'],"Galaxy velocities in clusters, rotation curves of galaxies, and ""vertical"" oscillations in the Milky Way currently show too high velocities with respect to the masses thought to be involved. While these velocity excesses are currently interpreted as the consequence of dark matter, it can also be naturally explained as a consequence of scale invariant theory, which rests on a very simple first principle: the addition of a new fundamental symmetry. In the present work, the case of scale invariance, present in General Relativity and Maxwell equations for the empty space without charge and current, is considered. Cosmological models predict a rapid decrease of these effects with increasing mean density up to the critical density, where they totally disappear. Starting from the scale invariant geodesic equation by Dirac (1973), for which a demonstration by an action principle is presented, a modified Newton equation is derived. The solutions of this equation are applied to clusters of galaxies, galactic rotation at different redshifts and ""vertical"" motions in the Milky Way. In this new framework, the convergence of theoretical predictions and observations, in different gravitational systems, epochs, mass range and spatial scales, opens interesting perspectives that deserve to be explored further."
https://arxiv.org/abs/2403.08758,2024-03-13,Spatiotemporal Diffusion Model with Paired Sampling for Accelerated Cardiac Cine MRI,"['Shihan Qiu', 'Shaoyan Pan', 'Yikang Liu', 'Lin Zhao', 'Jian Xu', 'Qi Liu', 'Terrence Chen', 'Eric Z. Chen', 'Xiao Chen', 'Shanhui Sun']",Current deep learning reconstruction for accelerated cardiac cine MRI suffers from spatial and temporal blurring. We aim to improve image sharpness and motion delineation for cine MRI under high undersampling rates. A spatiotemporal diffusion enhancement model conditional on an existing deep learning reconstruction along with a novel paired sampling strategy was developed. The diffusion model provided sharper tissue boundaries and clearer motion than the original reconstruction in experts evaluation on clinical data. The innovative paired sampling strategy substantially reduced artificial noises in the generative results.
https://arxiv.org/abs/2403.08757,2024-03-13,Efficient Combinatorial Optimization via Heat Diffusion,"['Hengyuan Ma', 'Wenlian Lu', 'Jianfeng Feng']","Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing recent advancements in harnessing thermodynamics for generative artificial intelligence, our study further reveals its significant potential in advancing combinatorial optimization."
https://arxiv.org/abs/2403.08756,2024-03-13,"Point-variety incidences, unit distances and Zarankiewicz's problem for algebraic graphs","['Aleksa Milojević', 'Benny Sudakov', 'István Tomon']","In this paper we study the number of incidences between $m$ points and $n$ varieties in $\mathbb{F}^d$, where $\mathbb{F}$ is an arbitrary field, assuming the incidence graph contains no copy of $K_{s,s}$. We also consider the analogous problem for algebraically defined graphs and unit distance graphs."
https://arxiv.org/abs/2403.08755,2024-03-13,DAM: Dynamic Adapter Merging for Continual Video QA Learning,"['Feng Cheng', 'Ziyang Wang', 'Yi-Lin Sung', 'Yan-Bo Lin', 'Mohit Bansal', 'Gedas Bertasius']","We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weights into a new adapter instance tailored for that particular test sample to compute the final VidQA prediction, mitigating the impact of inaccurate router predictions and facilitating knowledge sharing across domains. Our DAM model outperforms prior state-of-the-art continual learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA datasets spanning various domains. We further extend DAM to continual image classification and image QA and outperform prior methods by a large margin. The code is publicly available at: https://github.com/klauscc/DAM"
https://arxiv.org/abs/2403.08754,2024-03-13,"Sticky-threshold diffusions, local time approximation and parameter estimation","['Alexis Anagnostakis', 'Sara Mazzonetto']","We study a class of high frequency path functionals of a diffusion with a sticky-oscillating-skew (SOS) threshold, including the case of a sticky-reflection, and establish convergence to the local time. This extends existing results on sticky, oscillating (regime-switching) and skew or reflecting diffusions in several directions. First, it considers any normalizing sequence $(u_n)_n $ that diverges slower than $n$. Second, it allows combinations of these features. Based on this, and an approximation of the occupation time, we devise consistent skew and stickiness parameter estimators."
https://arxiv.org/abs/2403.08753,2024-03-13,Invalid proxies and volatility changes,"['Giovanni Angelini', 'Luca Fanelli', 'Luca Neri']","When in proxy-SVARs the covariance matrix of VAR disturbances is subject to exogenous, permanent, nonrecurring breaks that generate target impulse response functions (IRFs) that change across volatility regimes, even strong, exogenous external instruments can result in inconsistent estimates of the dynamic causal effects of interest if the breaks are not properly accounted for. In such cases, it is essential to explicitly incorporate the shifts in unconditional volatility in order to point-identify the target structural shocks and possibly restore consistency. We demonstrate that, under a necessary and sufficient rank condition that leverages moments implied by changes in volatility, the target IRFs can be point-identified and consistently estimated. Importantly, standard asymptotic inference remains valid in this context despite (i) the covariance between the proxies and the instrumented structural shocks being local-to-zero, as in Staiger and Stock (1997), and (ii) the potential failure of instrument exogeneity. We introduce a novel identification strategy that appropriately combines external instruments with ""informative"" changes in volatility, thus obviating the need to assume proxy relevance and exogeneity in estimation. We illustrate the effectiveness of the suggested method by revisiting a fiscal proxy-SVAR previously estimated in the literature, complementing the fiscal instruments with information derived from the massive reduction in volatility observed in the transition from the Great Inflation to the Great Moderation regimes."
https://arxiv.org/abs/2403.08752,2024-03-13,A local model for the optical energy and momentum transfer in dielectric media and the microscopic origin of Abraham's force density,"['B. Anghinoni', 'M. Partanen', 'N. G. C. Astrath']","We report on the continuity equations for linear momentum and energy associated to a recently introduced electromagnetic formulation based on classical dipolar sources [Eur. Phys. J. Plus 138, 1034 (2023)]. When connected to the mass-polariton quasi-particle dynamics, these equations provide a consistent microscopic description of the local optical energy and momentum transfer inside dielectric media, called microscopic mass-polariton formulation. This procedure also unveils the true microscopic origin of the long-known Abraham optical force density as an interplay between induced dipoles and mechanical stresses generated within the material."
https://arxiv.org/abs/2403.08751,2024-03-13,Cyclotomic Factors and LRS-Degeneracy,"['John Abbott', 'Nico Mexis']","We present three new, practical algorithms for polynomials in $\mathbb{Z}[x]$: one to test if a polynomial is cyclotomic, one to determine which cyclotomic polynomials are factors, and one to determine whether the given polynomial is LRS-degenerate. A polynomial is ``LRS-degenerate'' iff it has two distinct roots $α, β$ such that $β= ζα$ for some root of unity $ζ$. All three algorithms are based on ``intelligent brute force''. The first two produce the indexes of the cyclotomic polynomials; the third produces a list of degeneracy orders. The algorithms are implemented in CoCoALib."
https://arxiv.org/abs/2403.08750,2024-03-13,Neural reproducing kernel Banach spaces and representer theorems for deep networks,"['Francesca Bartolucci', 'Ernesto De Vito', 'Lorenzo Rosasco', 'Stefano Vigogna']","Studying the function spaces defined by neural networks helps to understand the corresponding learning models and their inductive bias. While in some limits neural networks correspond to function spaces that are reproducing kernel Hilbert spaces, these regimes do not capture the properties of the networks used in practice. In contrast, in this paper we show that deep neural networks define suitable reproducing kernel Banach spaces."
https://arxiv.org/abs/2403.08749,2024-03-13,Clinically Feasible Diffusion Reconstruction for Highly-Accelerated Cardiac Cine MRI,"['Shihan Qiu', 'Shaoyan Pan', 'Yikang Liu', 'Lin Zhao', 'Jian Xu', 'Qi Liu', 'Terrence Chen', 'Eric Z. Chen', 'Xiao Chen', 'Shanhui Sun']","The currently limited quality of accelerated cardiac cine reconstruction may potentially be improved by the emerging diffusion models, but the clinically unacceptable long processing time poses a challenge. We aim to develop a clinically feasible diffusion-model-based reconstruction pipeline to improve the image quality of cine MRI. A multi-in multi-out diffusion enhancement model together with fast inference strategies were developed to be used in conjunction with a reconstruction model. The diffusion reconstruction reduced spatial and temporal blurring in prospectively undersampled clinical data, as validated by experts inspection. The 1.5s per video processing time enabled the approach to be applied in clinical scenarios."
https://arxiv.org/abs/2403.08748,2024-03-13,Real-time 3D semantic occupancy prediction for autonomous vehicles using memory-efficient sparse convolution,"['Samuel Sze', 'Lars Kunze']","In autonomous vehicles, understanding the surrounding 3D environment of the ego vehicle in real-time is essential. A compact way to represent scenes while encoding geometric distances and semantic object information is via 3D semantic occupancy maps. State of the art 3D mapping methods leverage transformers with cross-attention mechanisms to elevate 2D vision-centric camera features into the 3D domain. However, these methods encounter significant challenges in real-time applications due to their high computational demands during inference. This limitation is particularly problematic in autonomous vehicles, where GPU resources must be shared with other tasks such as localization and planning. In this paper, we introduce an approach that extracts features from front-view 2D camera images and LiDAR scans, then employs a sparse convolution network (Minkowski Engine), for 3D semantic occupancy prediction. Given that outdoor scenes in autonomous driving scenarios are inherently sparse, the utilization of sparse convolution is particularly apt. By jointly solving the problems of 3D scene completion of sparse scenes and 3D semantic segmentation, we provide a more efficient learning framework suitable for real-time applications in autonomous vehicles. We also demonstrate competitive accuracy on the nuScenes dataset."
https://arxiv.org/abs/2403.08747,2024-03-13,Poisson suspensions without roots,['Valery V. Ryzhikov'],"We construct rigid Poisson suspensions without roots. The discrete rational component in spectrum of an ergodic automorphism S prevents some roots from existing. If S is tensorly multiplied by an ergodic automorphism of the space with a sigma-finite measure, discrete spectrum disappears in this product, but like the smile of Cheshire Cat, the memory of it can remain in the form of the absence of roots. In additional conditions, this effect is inherited by the Poisson suspension over the above product. Starting from this idea, but without using the tensor product, we describe simple rank-one constructions for which the Poisson suspensions are rigid and have no roots."
https://arxiv.org/abs/2403.08746,2024-03-13,iCONTRA: Toward Thematic Collection Design Via Interactive Concept Transfer,"['Dinh-Khoi Vo', 'Duy-Nam Ly', 'Khanh-Duy Le', 'Tam V. Nguyen', 'Minh-Triet Tran', 'Trung-Nghia Le']","Creating thematic collections in industries demands innovative designs and cohesive concepts. Designers may face challenges in maintaining thematic consistency when drawing inspiration from existing objects, landscapes, or artifacts. While AI-powered graphic design tools offer help, they often fail to generate cohesive sets based on specific thematic concepts. In response, we introduce iCONTRA, an interactive CONcept TRAnsfer system. With a user-friendly interface, iCONTRA enables both experienced designers and novices to effortlessly explore creative design concepts and efficiently generate thematic collections. We also propose a zero-shot image editing algorithm, eliminating the need for fine-tuning models, which gradually integrates information from initial objects, ensuring consistency in the generation process without influencing the background. A pilot study suggests iCONTRA's potential to reduce designers' efforts. Experimental results demonstrate its effectiveness in producing consistent and high-quality object concept transfers. iCONTRA stands as a promising tool for innovation and creative exploration in thematic collection design. The source code will be available at: https://github.com/vdkhoi20/iCONTRA."
https://arxiv.org/abs/2403.08745,2024-03-13,Boundary controllability for a fourth order degenerate parabolic equation with a singular potential,['Leandro Galo-Mendoza'],"In this paper, we prove the null controllability of a one-dimensional fourth-order degenerate parabolic equation with a singular potential. Here, we analyze cases where boundary control conditions are applied at the left endpoint. We utilize a spectral decomposition involving Bessel functions and their zeros in a convenient weighted Sobolev space for a degenerate parabolic operator with specific boundary conditions. We establish the well-posedness of the system using semigroup operator theory. Subsequently, we employ the moment method by Fattorini and Russell to obtain an upper estimate of the cost of controllability. Additionally, we derive a lower estimate of the cost of controllability using a representation theorem for analytic functions of exponential type."
https://arxiv.org/abs/2403.08744,2024-03-13,GTP before ATP: The energy currency at the origin of genes,"['Natalia Mrnjavac', 'William F. Martin']","Life is an exergonic chemical reaction. Many individual reactions in metabolism entail slightly endergonic processes that are coupled to free energy release, typically as ATP hydrolysis, in order to go forward. ATP is almost always supplied by the rotor-stator ATP synthetase (the ATPase), which harnesses chemiosmotic ion gradients. Because the ATPase is a protein, it arose after the ribosome did. Here we address two questions using comparative physiology: What was the energy currency of metabolism before the origin of the ATPase? How (and why) did ATP come to be the universal energy currency? About 27 percent of a cell's energy budget is consumed as GTP during translation. The universality of GTP-dependence in ribosome function indicates that GTP was the ancestral energy currency of protein synthesis. The use of GTP in translation and ATP in small molecule synthesis are conserved across all lineages, representing energetic compartments that arose in the last universal common ancestor, LUCA."
https://arxiv.org/abs/2403.08743,2024-03-13,Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework,"['Jingling Li', 'Zeyu Tang', 'Xiaoyu Liu', 'Peter Spirtes', 'Kun Zhang', 'Liu Leqi', 'Yang Liu']","Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework provides principled guidelines on debiasing LLM outputs even with only the black-box access."
https://arxiv.org/abs/2403.08742,2024-03-13,Multichannel quantum defect theory of strontium bound Rydberg states,"['C. L. Vaillant', 'M. P. A. Jones', 'R. M. Potvliege']","Newly calculated multichannel quantum defect theory parameters and channel fractions are presented for the singlet and triplet S, P and D series and singlet F series of strontium. These results correct those reported in Vaillant C L, Jones M P A and Potvliege R M 2014 J. Phys. B: At. Mol. Opt. Phys. 47 155001."
https://arxiv.org/abs/2403.08741,2024-03-13,Learning How to Strategically Disclose Information,"['Raj Kiriti Velicheti', 'Melih Bastopcu', 'S. Rasoul Etesami', 'Tamer Başar']","Strategic information disclosure, in its simplest form, considers a game between an information provider (sender) who has access to some private information that an information receiver is interested in. While the receiver takes an action that affects the utilities of both players, the sender can design information (or modify beliefs) of the receiver through signal commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg equilibrium for this game traditionally requires the sender to have access to the receiver's objective. In this work, we consider an online version of information design where a sender interacts with a receiver of an unknown type who is adversarially chosen at each round. Restricting attention to Gaussian prior and quadratic costs for the sender and the receiver, we show that $\mathcal{O}(\sqrt{T})$ regret is achievable with full information feedback, where $T$ is the total number of interactions between the sender and the receiver. Further, we propose a novel parametrization that allows the sender to achieve $\mathcal{O}(\sqrt{T})$ regret for a general convex utility function. We then consider the Bayesian Persuasion problem with an additional cost term in the objective function, which penalizes signaling policies that are more informative and obtain $\mathcal{O}(\log(T))$ regret. Finally, we establish a sublinear regret bound for the partial information feedback setting and provide simulations to support our theoretical results."
https://arxiv.org/abs/2403.08740,2024-03-13,Acoustic Side Channel Attack on Keyboards Based on Typing Patterns,"['Alireza Taheritajar', 'Reza Rahaeimehr']",Acoustic side-channel attacks on keyboards can bypass security measures in many systems that use keyboards as one of the input devices. These attacks aim to reveal users' sensitive information by targeting the sounds made by their keyboards as they type. Most existing approaches in this field ignore the negative impacts of typing patterns and environmental noise in their results. This paper seeks to address these shortcomings by proposing an applicable method that takes into account the user's typing pattern in a realistic environment. Our method achieved an average success rate of 43% across all our case studies when considering real-world scenarios.
https://arxiv.org/abs/2403.08739,2024-03-13,The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models,"['Carlo Nicolini', 'Jacopo Staiano', 'Bruno Lepri', 'Raffaele Marino']","A substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP. A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training. In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification."
https://arxiv.org/abs/2403.08738,2024-03-13,Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations,"['Amit Meghanani', 'Thomas Hain']","Acoustic word embeddings (AWEs) are vector representations of spoken words. An effective method for obtaining AWEs is the Correspondence Auto-Encoder (CAE). In the past, the CAE method has been associated with traditional MFCC features. Representations obtained from self-supervised learning (SSL)-based speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many downstream tasks. However, they have not been well studied in the context of learning AWEs. This work explores the effectiveness of CAE with SSL-based speech representations to obtain improved AWEs. Additionally, the capabilities of SSL-based speech models are explored in cross-lingual scenarios for obtaining AWEs. Experiments are conducted on five languages: Polish, Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the best results for word discrimination in all languages, despite Hu-BERT being pre-trained on English only. Also, the HuBERT-based CAE model works well in cross-lingual settings. It outperforms MFCC-based CAE models trained on the target languages when trained on one source language and tested on target languages."
https://arxiv.org/abs/2403.08737,2024-03-13,ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation,"['Sayar Ghosh Roy', 'Jiawei Han']","Existing Machine Learning approaches for local citation recommendation directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers. Within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited recommendation interpretability. To alleviate this, we introduce the evidence-grounded local citation recommendation task, where the target latent space comprises evidence spans for recommending specific papers. Using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ILCiteR, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature. Unlike past formulations that simply output recommendations, ILCiteR retrieves ranked lists of evidence span and recommended paper pairs. Secondly, previously proposed neural models for citation recommendation require expensive training on massive labeled data, ideally after every significant update to the pool of candidate papers. In contrast, ILCiteR relies solely on distant supervision from a dynamic evidence database and pre-trained Transformer-based Language Models without any model training. We contribute a novel dataset for the evidence-grounded local citation recommendation task and demonstrate the efficacy of our proposed conditional neural rank-ensembling approach for re-ranking evidence spans."
https://arxiv.org/abs/2403.08736,2024-03-13,Interface Design Beyond Epitaxy: Oxide Heterostructures Comprising Symmetry-forbidden Interfaces,"['Hongguang Wang', 'Varun Harbola', 'Yu-Jung Wu', 'Peter A. van Aken', 'Jochen Mannhart']","Epitaxial growth of thin-film heterostructures is generally considered the most successful procedure to obtain interfaces of excellent structural and electronic quality between three-dimensional materials. However, these interfaces can only join material systems with crystal lattices of matching symmetries and lattice constants. We present a novel category of interfaces, the fabrication of which is membrane-based and does not require epitaxial growth. These interfaces therefore overcome limitations imposed by epitaxy. Leveraging the additional degrees of freedom gained, we demonstrate atomically clean interfaces between three-fold symmetric sapphire and four-fold symmetric SrTiO3. Atomic-resolution imaging reveals structurally well-defined interfaces with a novel moiré-type reconstruction."
https://arxiv.org/abs/2403.08735,2024-03-13,"Torsion pairs, t-structures, and co-t-structures for completions of discrete cluster categories",['Sofia Franchini'],"We give a classification of torsion pairs, t-structures, and co-t-structures in the Paquette-Yildirim completion of the Igusa-Todorov discrete cluster category. We prove that the aisles of t-structures and co-t-structures are in bijection with non-crossing partitions enriched with some additional data. We also observe that recollements exist in the completion and we classify them."
https://arxiv.org/abs/2403.08734,2024-03-13,Light-driven interlayer propagation of collective-mode excitations in layered superconductors,"['Niklas Ziereis', 'Kazuaki Takasan', 'Naoto Tsuji']","Superconductors exhibit a nonlinear interaction with an applied light, which can resonantly excite the collective amplitude (Higgs) mode. Here we study light-induced dynamics of layered superconductors, where each layer is coupled to adjacent layers via the Josephson coupling and the first few layers near the surface are driven by an in-plane-polarized light. We study the system under the assumption that the interlayer Coulomb interactions are sufficiently screened out and that the phase-difference mode becomes available in the low-energy regime. We find that interlayer transport is induced via excitations of the collective amplitude and phase-difference modes, even when the applied electric field is parallel to the planes. We provide analytic calculations as well as numerical simulations of the real-time dynamics, and investigate the influence on the light-induced interlayer Josephson current and intralayer third-harmonic generation."
https://arxiv.org/abs/2403.08733,2024-03-13,GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing,"['Jing Wu', 'Jia-Wang Bian', 'Xinghui Li', 'Guangrun Wang', 'Ian Reid', 'Philip Torr', 'Victor Adrian Prisacariu']","We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS)."
https://arxiv.org/abs/2403.08732,2024-03-13,Molecular pentaquarks with hidden charm and double strangeness,"['L. Roca', 'J. Song', 'E. Oset']","We analyze theoretically the coupled-channel meson-baryon interaction with global flavor $\bar c c s s n$ and $\bar c c s s s$, where mesons are pseudoscalars or vectors and baryons have $J ^P=1/2^+$ or $3/2^+$. The aim is to explore whether the nonlinear dynamics inherent in the unitarization process within coupled channels can dynamically generate double- and triple-strange pentaquark-type states ($P_{css}$ and $P_{csss}$ respectively), for which there is no experimental evidence to date. We evaluate the s-wave scattering matrix by implementing unitarity in coupled channels, using potential kernels obtained from t-channel vector meson exchange. The required $PPV$ and $VVV$ vertices are obtained from Lagrangians derived through appropriate extensions of the local hidden gauge symmetry approach to the charm sector, while capitalizing on the symmetry of the spin and flavor wave function to evaluate the $BBV$ vertex. We find four different poles in the double strange sector, some of them degenerate in spin. For the triple-strange channel we find the meson-baryon interaction insufficient to generate a bound or resonance state through the unitary coupled-channel dynamics."
https://arxiv.org/abs/2403.08731,2024-03-13,Fault Localization in a Microfabricated Surface Ion Trap using Diamond Nitrogen-Vacancy Center Magnetometry,"['Pauli Kehayias', 'Matthew A. Delaney', 'Raymond A. Haltli', 'Susan M. Clark', 'Melissa C. Revelle', 'Andrew M. Mounce']","As quantum computing hardware becomes more complex with ongoing design innovations and growing capabilities, the quantum computing community needs increasingly powerful techniques for fabrication failure root-cause analysis. This is especially true for trapped-ion quantum computing. As trapped-ion quantum computing aims to scale to thousands of ions, the electrode numbers are growing to several hundred with likely integrated-photonic components also adding to the electrical and fabrication complexity, making faults even harder to locate. In this work, we used a high-resolution quantum magnetic imaging technique, based on nitrogen-vacancy (NV) centers in diamond, to investigate short-circuit faults in an ion trap chip. We imaged currents from these short-circuit faults to ground and compared to intentionally-created faults, finding that the root-cause of the faults was failures in the on-chip trench capacitors. This work, where we exploited the performance advantages of a quantum magnetic sensing technique to troubleshoot a piece of quantum computing hardware, is a unique example of the evolving synergy between emerging quantum technologies to achieve capabilities that were previously inaccessible."
https://arxiv.org/abs/2403.08730,2024-03-13,Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization,"['Renjie Pi', 'Tianyang Han', 'Wei Xiong', 'Jipeng Zhang', 'Runtao Liu', 'Rui Pan', 'Tong Zhang']","Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs. However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information. We treat this bias as a ""preference"" for pretraining statistics, which hinders the model's grounding in visual input. To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself. Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response. Those undesirable responses are paired with original annotated responses from the datasets to construct the preference dataset, which is subsequently utilized to perform preference learning. Our approach effectively suppresses pretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive experimentation demonstrates significant performance improvements across multiple benchmarks, advancing the state-of-the-art in multimodal conversational systems."
https://arxiv.org/abs/2403.08729,2024-03-13,Efficient and practical Hamiltonian simulation from time-dependent product formulas,"['Jan Lukas Bosse', 'Andrew M. Childs', 'Charles Derby', 'Filippo Maria Gambetta', 'Ashley Montanaro', 'Raul A. Santos']","In this work we propose an approach for implementing time-evolution of a quantum system using product formulas. The quantum algorithms we develop have provably better scaling (in terms of gate complexity and circuit depth) than a naive application of well-known Trotter formulas, for systems where the evolution is determined by a Hamiltonian with different energy scales (i.e., one part is ""large"" and another part is ""small""). Our algorithms generate a decomposition of the evolution operator into a product of simple unitaries that are directly implementable on a quantum computer. Although the theoretical scaling is suboptimal compared with state-of-the-art algorithms (e.g., quantum signal processing), the performance of the algorithms we propose is highly competitive in practice. We illustrate this via extensive numerical simulations for several models. For instance, in the strong-field regime of the 1D transverse-field Ising model, our algorithms achieve an improvement of one order of magnitude in both the system size and evolution time that can be simulated with a fixed budget of 1000 arbitrary 2-qubit gates, compared with standard Trotter formulas."
https://arxiv.org/abs/2403.08728,2024-03-13,Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data,"['Asad Aali', 'Giannis Daras', 'Brett Levac', 'Sidharth Kumar', 'Alexandros G. Dimakis', 'Jonathan I. Tamir']","We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Our method, Ambient Diffusion Posterior Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring). We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance. We further extend the Ambient Diffusion framework to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8). We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration regime than models trained on fully sampled data. We open-source our code and the trained Ambient Diffusion MRI models: https://github.com/utcsilab/ambient-diffusion-mri ."
https://arxiv.org/abs/2403.08727,2024-03-13,The q-ary Gilbert-Varshamov bound can be improved for all but finitely many positive integers q,['Xue-Bin Liang'],"For any positive integer $q\geq 2$ and any real number $δ\in(0,1)$, let $α_q(n,δn)$ denote the maximum size of a subset of $\mathbb{Z}_q^n$ with minimum Hamming distance at least $δn$, where $\mathbb{Z}_q=\{0,1,\dotsc,q-1\}$ and $n\in\mathbb{N}$. The asymptotic rate function is defined by $ R_q(δ) = \limsup_{n\rightarrow\infty}\frac{1}{n}\log_qα_q(n,δn). $ The famous $q$-ary asymptotic Gilbert-Varshamov bound, obtained in the 1950s, states that \[ R_q(δ) \geq 1 - δ\log_q(q-1)-δ\log_q\frac{1}δ-(1-δ)\log_q\frac{1}{1-δ} \stackrel{\mathrm{def}}{=}R_\mathrm{GV}(δ,q) \] for all positive integers $q\geq 2$ and $0<δ<1-q^{-1}$. In the case that $q$ is an even power of a prime with $q\geq 49$, the $q$-ary Gilbert-Varshamov bound was firstly improved by using algebraic geometry codes in the works of Tsfasman, Vladut, and Zink and of Ihara in the 1980s. The further investigation in algebraic geometry codes has shown that the $q$-ary Gilbert-Varshamov bound can also be improved in the case that $q$ is an odd power of a prime but not a prime with $q > 125$. However, it remains a long-standing open problem whether the $q$-ary Gilbert-Varshamov bound would be tight for those infinitely many integers $q$ which is a prime, except for Fermat primes not less than 257, and which is a generic positive integer not being a prime power."
https://arxiv.org/abs/2403.08726,2024-03-13,Euclid: Testing photometric selection of emission-line galaxy targets,"['M. S. Cagliari', 'B. R. Granett', 'L. Guzzo', 'M. Bethermin', 'M. Bolzonella', 'S. de la Torre', 'P. Monaco', 'M. Moresco', 'W. J. Percival', 'C. Scarlata', 'Y. Wang', 'M. Ezziati', 'O. Ilbert', 'V. Le Brun', 'A. Amara', 'S. Andreon', 'N. Auricchio', 'M. Baldi', 'S. Bardelli', 'R. Bender', 'C. Bodendorf', 'E. Branchini', 'M. Brescia', 'J. Brinchmann', 'S. Camera']","Multi-object spectroscopic galaxy surveys typically make use of photometric and colour criteria to select targets. Conversely, the Euclid NISP slitless spectrograph will record spectra for every source over its field of view. Slitless spectroscopy has the advantage of avoiding defining a priori a galaxy sample, but at the price of making the selection function harder to quantify. The Euclid Wide Survey aims at building robust statistical samples of emission-line galaxies with fluxes in the Halpha-NII complex brighter than 2e-16 erg/s/cm^2 and within 0.9<z<1.8. At faint fluxes, we expect significant contamination by wrongly measured redshifts, either due to emission-line misidentification or noise fluctuations, with the consequence of reducing the purity of the final samples. This can be significantly improved by exploiting Euclid photometric information to identify emission-line galaxies over the redshifts of interest. To this goal, we compare and quantify the performance of six machine-learning classification algorithms. We consider the case when only Euclid photometric and morphological measurements are used and when these are supplemented by ground-based photometric data. We train and test the classifiers on two mock galaxy samples, the EL-COSMOS and Euclid Flagship2 catalogues. Dense neural networks and support vector classifiers obtain the best performance, with comparable results in terms of the adopted metrics. When training on Euclid photometry alone, these can remove 87% of the sources that are fainter than the nominal flux limit or lie outside the range 0.9<z<1.8, a figure that increases to 97% when ground-based photometry is included. These results show how by using the photometric information available to Euclid it will be possible to efficiently identify and discard spurious interlopers, allowing us to build robust spectroscopic samples for cosmological investigations."
https://arxiv.org/abs/2403.08725,2024-03-13,Driving non-trivial quantum phases in conventional semiconductors with intense excitonic fields,"['Vivek Pareek', 'David R. Bacon', 'Xing Zhu', 'Yang-Hao Chan', 'Fabio Bussolotti', 'Nicholas S. Chan', 'Joel Pérez Urquizo', 'Kenji Watanabe', 'Takashi Taniguchi', 'Michael K. L. Man', 'Julien Madéo', 'Diana Y. Qiu', 'Kuan Eng Johnson Goh', 'Felipe H. da Jornada', 'Keshav M. Dani']","Inducing novel quantum phases and topologies in materials using intense light fields is a key objective of modern condensed matter physics, but nonetheless faces significant experimental challenges. Alternately, theory predicts that in the dense limit, excitons - collective excitations composed of Coulomb-bound electron-hole pairs - could also drive exotic quantum phenomena. However, the direct observation of these phenomena requires the resolution of electronic structure in momentum space in the presence of excitons, which became possible only recently. Here, using time- and angle-resolved photoemission spectroscopy of an atomically thin semiconductor in the presence of a high-density of resonantly and coherently photoexcited excitons, we observe the Bardeen-Cooper-Schrieffer (BCS) excitonic state - analogous to the Cooper pairs of superconductivity. We see the valence band transform from a conventional paraboloid into a Mexican-hat like Bogoliubov dispersion - a hallmark of the excitonic insulator phase; and we observe the recently predicted giant exciton-driven Floquet effects. Our work realizes the promise that intense bosonic fields, other than photons, can also drive novel quantum phenomena and phases in materials."
https://arxiv.org/abs/2403.08724,2024-03-13,Stabilizer Tensor Networks: universal quantum simulator on a basis of stabilizer states,"['Sergi Masot-Llima', 'Artur Garcia-Saez']","Efficient simulation of quantum computers relies on understanding and exploiting the properties of quantum states. This is the case for methods such as tensor networks, based on entanglement, and the tableau formalism, which represents stabilizer states. In this work, we integrate these two approaches to present a generalization of the tableau formalism used for Clifford circuit simulation. We explicitly prove how to update our formalism with Clifford gates, non-Clifford gates, and measurements, enabling universal circuit simulation. We also discuss how the framework allows for efficient simulation of more states, raising some interesting questions on the representation power of tensor networks and the quantum properties of resources such as entanglement and magic, and support our claims with simulations."
https://arxiv.org/abs/2403.08723,2024-03-13,Asymptotic polynomial approximation in the Bloch space,['Adem Limani'],"We investigate asymptotic polynomial approximation for a class of weighted Bloch functions in the unit disc. Our main result is a structural theorem on asymptotic polynomial approximation in the unit disc, in the flavor of the classical Plessner Theorem on asymptotic values of meromorphic functions. This provides the appropriate set up for studying metric and geometric properties of sets E on the unit circle for which the following simultaneous approximation phenomenon occurs: there exists analytic polynomials which converge uniformly to zero on E and to a non-zero function in the weighted Bloch norm. We offer a characterization completely within the realm of real-analysis, establish a connection to removable sets for analytic Sobolev functions in the complex plane, and provide several necessary conditions in terms of entropy, Hausdorff content and condenser capacity. Furthermore, we demonstrate two principal applications of our developments, which go in different directions. First, we shall deduce a rather subtle consequence in the theme of smooth approximation in de Branges-Rovnyak spaces. Secondly, we answer some questions that were raised almost a decade ago in the theory of Universal Taylor series."
https://arxiv.org/abs/2403.08722,2024-03-13,Isotope effects in supercooled H$_2$O and D$_2$O and a corresponding-states-like rescaling of the temperature and pressure,['Greg A. Kimmel'],"Water shows anomalous properties that are enhanced upon supercooling. The unusual behavior is observed in both H$_2$O and D$_2$O, however with different temperature dependences for the two isotopes. It is often noted that comparing the properties of the isotopes at two different temperatures (i.e., a temperature shift) approximately accounts for many of the observations with a temperature shift of 7.2 K in the temperature of maximum density being the most well-known example. However, the physical justification for such a shift is unclear. Motivated by recent work demonstrating a corresponding-states-like rescaling for water properties in three classical water models that all exhibit a liquid-liquid transition and critical point (B. Uralcan, et al., J. Chem. Phys. 150, 064503 (2019)), the applicability of this approach for reconciling the differences in temperature- and pressure-dependent thermodynamic properties of H$_2$O and D$_2$O is investigated here. Utilizing previously published data and equations-of-state for H$_2$O and D$_2$O, we show that the available data and models for these isotopes are consistent with such a low temperature correspondence. These observations provide support for the hypothesis that a liquid-liquid critical point, which is predicted to occur at low temperatures and high pressures, is the origin of many of water's anomalies."
https://arxiv.org/abs/2403.08721,2024-03-13,Historical Astronomical Diagrams Decomposition in Geometric Primitives,"['Syrine Kalleli', 'Scott Trigg', 'Ségolène Albouy', 'Mathieu Husson', 'Mathieu Aubry']","Automatically extracting the geometric content from the hundreds of thousands of diagrams drawn in historical manuscripts would enable historians to study the diffusion of astronomical knowledge on a global scale. However, state-of-the-art vectorization methods, often designed to tackle modern data, are not adapted to the complexity and diversity of historical astronomical diagrams. Our contribution is thus twofold. First, we introduce a unique dataset of 303 astronomical diagrams from diverse traditions, ranging from the XIIth to the XVIIIth century, annotated with more than 3000 line segments, circles and arcs. Second, we develop a model that builds on DINO-DETR to enable the prediction of multiple geometric primitives. We show that it can be trained solely on synthetic data and accurately predict primitives on our challenging dataset. Our approach widely improves over the LETR baseline, which is restricted to lines, by introducing a meaningful parametrization for multiple primitives, jointly training for detection and parameter refinement, using deformable attention and training on rich synthetic data. Our dataset and code are available on our webpage."
https://arxiv.org/abs/2403.08720,2024-03-13,Interpreting 95 GeV di-photon/$b\bar{b}$ excesses as a lightest Higgs boson of the MRSSM,"['Jan Kalinowski', 'Wojciech Kotlarski']","The Minimal R-symmetric Supersymmetric Standard Model (MRSSM) is a well motivated BSM model which can accommodate the observed 125 GeV Higgs boson in agreement with electroweak precision observables, in particular with the $W$ boson mass and $T$ parameter. In the 2016 paper we showed that the SM-like 125 GeV Higgs state can be also realised as the second-to-lightest scalar of the MRSSM, leaving room for another sub-100 GeV state. Motivated by the recent ATLAS and CMS observation of the di-photon excess at a mass of around 95 GeV we investigate the possibility whether this could be the lightest CP-even MRSSM scalar in a variation of our benchmarks presented in the 2016 work. We show that such a state can also simultaneously explain the excess in the $b\bar{b}$ final state observed around the same mass value at LEP. Due to the R-symmetric nature of the model, a light singlet-like Higgs state leads necessarily to a light bino-singlino Dirac dark matter candidate, which can give a correct relic density while evading current experimental bounds. Dark matter and LHC searches place further bounds on this scenario and point to parameter regions which are viable and of interest for the LHC Run III and upcoming dark matter experiments."
https://arxiv.org/abs/2403.08719,2024-03-13,Improved Trade-offs Between Amortization and Download Bandwidth for Linear HSS,"['Keller Blackwell', 'Mary Wootters']","A Homomorphic Secret Sharing (HSS) scheme is a secret-sharing scheme that shares a secret $x$ among $s$ servers, and additionally allows an output client to reconstruct some function $f(x)$ using information that can be locally computed by each server. A key parameter in HSS schemes is download rate, which quantifies how much information the output client needs to download from the servers. Often, download rate is improved by amortizing over $\ell$ instances of the problem, making $\ell$ also a key parameter of interest."
https://arxiv.org/abs/2403.08718,2024-03-13,Probabilistic Metaplasticity for Continual Learning with Memristors,"['Fatima Tuz Zohora', 'Vedant Karia', 'Nicholas Soures', 'Dhireesha Kudithipudi']","Crossbar architectures utilizing memristor devices hold promise to address continual learning challenges in resource-constrained edge devices. However, these nanoscale devices often exhibit low precision and high variability in conductance modulation, rendering them unsuitable for continual learning solutions that consolidate weights through precise modulation. This issue can be circumvented by accumulating weight gradients in auxiliary high-precision memory and updating memristor weights when gradients are equivalent to memristor weight resolution. However, it leads to frequent memory access, high memory overhead, and energy dissipation. In this research, we propose probabilistic metaplasticity, which consolidates weights by modulating their update probability rather than magnitude. The proposed mechanism eliminates high-precision modification to weight magnitude and consequently, high-precision memory for gradient accumulation. We demonstrate the efficacy of the proposed mechanism by integrating probabilistic metaplasticity into a spiking network trained on an error threshold with low-precision memristor weights. Evaluations of two continual learning benchmarks show that probabilistic metaplasticity consumes ~67% lower memory for additional parameters and up to two orders of magnitude lower energy during parameter updates compared to an auxiliary memory-based solution while achieving state-of-the-art performance. The proposed model shows potential for energy-efficient continual learning with low-precision emerging devices."
https://arxiv.org/abs/2403.08717,2024-03-13,A too-many dwarf galaxy satellites problem in the M83 group,"['Oliver Müller', 'Marcel S. Pawlowski', 'Yves Revaz', 'Aku Venhola', 'Marina Rejkuba', 'Michael Hilker', 'Katharina Lutz']","Dwarf galaxies in groups of galaxies provide excellent test cases for models of structure formation. This led to a so-called small-scale crisis, including the famous missing satellite and too-big-to-fail problems. It was suggested that these two problems are solved by the introduction of baryonic physics in cosmological simulations. We test for the nearby grand spiral M83 - a Milky Way sibling - whether its number of dwarf galaxy companions is compatible with today's $Λ$ + Cold Dark Matter model using two methods: with cosmological simulations that include baryons, as well as with theoretical predictions from the sub-halo mass function. By employing distance measurements we recover a list of confirmed dwarf galaxies within 330 kpc around M83 down to a magnitude of $M_V =-10$. We found that both the state-of-the-art hydrodynamical cosmological simulation Illustris-TNG50 and theoretical predictions agree with the number of confirmed satellites around M83 at the bright end of the luminosity function (>10$^8$ solar masses) but underestimate it at the faint end (down to 10$^6$ solar masses) at more than 3$σ$ and 5$σ$ levels, respectively. This indicates a too-many satellites problem in $Λ$CDM for M83. The actual degree of tension to cosmological models is underestimated, because the number of observed satellites is incomplete due to the high contamination of spurious stars and galactic cirrus."
https://arxiv.org/abs/2403.08716,2024-03-13,DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation,"['Zilin Si', 'Gu Zhang', 'Qingwei Ben', 'Branden Romero', 'Zhou Xian', 'Chao Liu', 'Chuang Gan']","We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties. Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics. The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics. Code and supplementary materials are available at the project website https://difftactile.github.io/."
https://arxiv.org/abs/2403.08715,2024-03-13,SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language Agents,"['Ruiyi Wang', 'Haofei Yu', 'Wenxin Zhang', 'Zhengyang Qi', 'Maarten Sap', 'Graham Neubig', 'Yonatan Bisk', 'Hao Zhu']","Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-$π$, improving the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark. We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction."
https://arxiv.org/abs/2403.08714,2024-03-13,Dynamic computerized tomography using inexact models and motion estimation,"['Gesa Sarnighausen', 'Anne Wald', 'Alexander Meaney']","Reconstructing a dynamic object with affine motion in computerized tomography (CT) leads to motion artifacts if the motion is not taken into account. In most cases, the actual motion is neither known nor can be determined easily. As a consequence, the respective model that describes CT is incomplete. The iterative RESESOP-Kaczmarz method can - under certain conditions and by exploiting the modeling error - reconstruct dynamic objects at different time points even if the exact motion is unknown. However, the method is very time-consuming. To speed the reconstruction process up and obtain better results, we combine the following three steps: 1. RESESOP-Kacmarz with only a few iterations is implemented to reconstruct the object at different time points. 2. The motion is estimated via landmark detection, e.g. using deep learning. 3. The estimated motion is integrated into the reconstruction process, allowing the use of dynamic filtered backprojection. We give a short review of all methods involved and present numerical results as a proof of principle."
https://arxiv.org/abs/2403.08713,2024-03-13,Phase diagram of the $J_1$-$J_2$ quantum Heisenberg model for arbitrary spin,"['Andreas Rückriegel', 'Dmytro Tarasevych', 'Peter Kopietz']","We use the spin functional renormalization group to investigate the $J_1$-$J_2$ quantum Heisenberg model on a square lattice. By incorporating sum rules associated with the fixed length of the spin operators as well as the nontrivial quantum dynamics implied by the spin algebra, we are able to compute the ground state phase diagram for arbitrary spin $S$, including the quantum paramagnetic phase at strong frustration. Our prediction for the extent of this paramagnetic region for $ S = 1/2 $ agrees well with other approaches that are computationally more expensive. We find that the quantum paramagnetic phase disappears for $ S \gtrsim 5 $ due to the suppression of quantum fluctuations with increasing $S$."
https://arxiv.org/abs/2403.08712,2024-03-13,Controlled-Joint Remote Implementation of Operators and its Possible Generalization,"['Satish Kumar', 'Nguyen Ba An', 'Anirban Pathak']","The existing notion of the shared entangled state-assisted remote preparation of unitary operator (equivalently the existing notion of quantum remote control) using local operation and classical communication is generalized to a scenario where under the control of a supervisor two users can jointly implement arbitrary unitaries (one unknown unitary operation by each or equivalently a single unitary decomposed into two unitaries of the same dimension and given to two users) on an unknown quantum state available with a geographically separated user. It is explicitly shown that the task can be performed using a four-qubit hyperentangled state, which is entangled simultaneously in both spatial and polarization degrees of freedom of photons. The proposed protocol which can be viewed as primitive for distributed photonic quantum computing is further generalized to the case that drops the restrictions on the number of controllers and the number of parties performing unitaries and allows both the numbers to be arbitrary. It is also shown that all the existing variants of quantum remote control schemes can be obtained as special cases of the present scheme."
https://arxiv.org/abs/2403.08711,2024-03-13,On the geometric phase and its role in the design of elastic topological materials,"['Mohit Kumar', 'Fabio Semperlotti']","The geometric phase provides important mathematical insights to understand the occurrence and evolution of the dynamic response in a diverse spectrum of systems ranging from quantum to classical mechanics. While the concept of geometric phase, which is an additional phase factor occurring in dynamical systems, holds the same meaning across different fields of application, its use and interpretation can acquire important nuances specific to the system of interest. In recent years, the development of the concept of quantum topological materials and its extension to classical mechanical systems have renewed the interest in the study of the geometric phase. This study reviews the concept of geometric phase and discusses, by means of either established or original results, its role in the design of elastic materials. Concepts of differential geometry and topology are put forward to provide a theoretical understanding of the geometric phase and its connection to the physical properties of the system. Then, the concept of geometric phase is applied to different types of elastic waveguides to explain how either topologically trivial or non-trivial behavior can emerge based on a proper geometric design of the waveguide."
https://arxiv.org/abs/2403.08710,2024-03-13,Boundary geometry controls topological defect transitions that determine lumen nucleation in embryonic development,"['Pamela C. Guruciaga', 'Takafumi Ichikawa', 'Takashi Hiiragi', 'Anna Erzberger']","Topological defects determine the collective properties of anisotropic materials. How their configurations are controlled is not well understood however, especially in 3D, where bulk-surface coupling can render the geometry of confining boundaries relevant. This is particularly important in living matter, where 2D topological defects have been linked to essential biological functions, whereas the role of 3D defects is unclear. Motivated by multicellular systems interacting with extracellular boundaries, we consider a polar fluid confined within curved boundaries imposing weak surface anchoring. We report a novel charge-preserving transition between different defect configurations, controlled by the boundary shape, and invariant to changes in the material parameters. We test if this geometry-driven transition occurs in confined multicellular systems and investigate the biological role of 3D polar defects in the mouse epiblast -- an embryonic tissue consisting of apico-basally polarised cells. We find that fluid-filled lumina -- structures essential for subsequent embryonic development -- tend to form near defect positions of polar fluids in embryo-like confinement geometries. Moreover, by experimentally perturbing embryo shape beyond the transition point, we trigger the formation of additional lumen nucleation sites at the predicted position. Thus, our work reveals how boundary geometry controls polar defects, and how embryos use this mechanism for shape-dependent lumen formation. Because this defect control principle is independent of specific material properties, we expect it to apply universally to systems with orientational order."
https://arxiv.org/abs/2403.08709,2024-03-13,On the Microlocal Regularity of the Gevrey Vectors for second order partial differential operators with non negative characteristic form of first kind,"['Gregorio Chinni', 'Makhlouf Derridj']",We study the microlocal regularity of the analytic/Gevrey vectors for the following class of second order partial differential equations \begin{align*}
https://arxiv.org/abs/2403.08708,2024-03-13,The importance of stretching rate in achieving true stress relaxation in the elasto-capillary thinning of dilute solutions,"['Ann Aisling', 'Renee Saraka', 'Nicolas J. Alvarez']","This work focuses on inferring the molecular state of the polymer chain required to induce elasto-capillary stress relaxation and the accurate measure of the polymer relaxation time in uniaxial stretching of dilute polymer solutions. This work is facilitated by the discovery that constant velocity applied at early times leads to initial constant extension rate before reaching the Rayleigh-Plateau instability. Such constant rate experiments are used to correlate initial stretching kinematics with the thinning dynamics in the elasto-capillary Regime. We show that there is a minimum initial strain-rate required to induce rate independent elastic effects. Below the minimum extension rate, insufficient stretching of the chain is observed before capillary instability, such that the polymer stress is comparable to the capillary stress at long times and true stress relaxation is not achieved. Above the minimum strain-rate, the chain reaches a critical stretch before instability, such that during the unstable filament thinning the polymer stress is significantly larger than the capillary stress and true stress relaxation is observed. Using a single relaxation mode Oldroyd-B model, we show that the the minimum strain rate leads to a required initial stretch of the chain before reaching the Rayleigh Plateau limit. Along with the accurate measure of relaxation time, this work introduces a characteristic dimensionless group, called the stretchability factor, that can be used to quantitatively compare different materials based on the overall material deformation/kinematic behavior, not just the relaxation time. Overall, these results demonstrate a useful methodology to study the stretching of dilute solutions using a constant velocity stretching scheme."
https://arxiv.org/abs/2403.08707,2024-03-13,Improved Randomized Approximation of Hard Universality and Emptiness Problems,"['Pantelis Andreou', 'Stavros Konstantinidis', 'Taylor J. Smith']","We build on recent research on polynomial randomized approximation (PRAX) algorithms for the hard problems of NFA universality and NFA equivalence. Loosely speaking, PRAX algorithms use sampling of infinite domains within any desired accuracy $δ$. In the spirit of experimental mathematics, we extend the concept of PRAX algorithms to be applicable to the emptiness and universality problems in any domain whose instances admit a tractable distribution as defined in this paper. A technical result here is that a linear (w.r.t. $1/δ$) number of samples is sufficient, as opposed to the quadratic number of samples in previous papers. We show how the improved and generalized PRAX algorithms apply to universality and emptiness problems in various domains: ordinary automata, tautology testing of propositions, 2D automata, and to solution sets of certain Diophantine equations."
https://arxiv.org/abs/2403.08706,2024-03-13,Optimal adaptation of surface-code decoders to local noise,['Andrew S. Darmawan'],"Information obtained from noise characterization of a quantum device can be used in classical decoding algorithms to improve the performance of quantum error-correcting codes. Focusing on the surface code under local (i.e. single-qubit) noise, we present a simple method to determine the maximum extent to which adapting a surface-code decoder to a noise feature can lead to a performance improvement. Our method is based on a tensor-network decoding algorithm, which uses the syndrome information as well as a process matrix description of the noise to compute a near-optimal correction. By selectively mischaracterizing the noise model input to the decoder and measuring the resulting loss in fidelity of the logical qubit, we can determine the relative importance of individual noise parameters for decoding. We apply this method to several physically relevant uncorrelated noise models with features such as coherence, spatial inhomogeneity and bias. While noise generally requires many parameters to describe completely, we find that to achieve near optimal decoding it appears only necessary adapt the decoder to a small number of critical parameters."
https://arxiv.org/abs/2403.08705,2024-03-13,Scalarization of isolated black holes in scalar Gauss-Bonnet theory in the fixing-the-equations approach,"['Guillermo Lara', 'Harald P. Pfeiffer', 'Nikolas A. Wittek', 'Nils L. Vu', 'Kyle C. Nelli', 'Alexander Carpenter', 'Geoffrey Lovelace', 'Mark A. Scheel', 'William Throwe']","One of the most promising avenues to perform numerical evolutions in theories beyond General Relativity is the fixing-the-equations approach, a proposal in which new ``driver'' equations are added to the evolution equations in a way that allows for stable numerical evolutions. In this direction, we extend the numerical relativity code SpECTRE to evolve a ``fixed'' version of scalar Gauss-Bonnet theory in the decoupling limit, a phenomenologically interesting theory that allows for hairy black hole solutions in vacuum. We focus on isolated black hole systems both with and without linear and angular momentum, and propose a new driver equation to improve the recovery of such stationary solutions. We demonstrate the effectiveness of the latter by numerically evolving black holes that undergo spontaneous scalarization using different driver equations. Finally, we evaluate the accuracy of the obtained solutions by comparing with the original unaltered theory."
https://arxiv.org/abs/2403.08704,2024-03-13,Limits on the OH Molecule in the Smith High Velocity Cloud,"['Anthony H. Minter', 'Felix J. Lockman', 'S. A. Balashev', 'H. Alyson Ford']","We have used the Green Bank Telescope (GBT) to search for the OH molecule at several locations in the Smith Cloud, one of the most prominent of the high-velocity clouds that surround the Milky Way. Five positions with a high HI column density were selected as targets for individual pointings, along with a square degree around a molecular cloud detected with the Planck telescope near the tip of the Smith Cloud. Gas in the Galactic disk with similar values of $N_{HI}$ has detectable OH emission. Although we found OH at velocities consistent with the foreground Aquila molecular cloud, nothing was found at the velocity of the Smith Cloud to an rms level of 0.7 mK (T$_b$) in a 1 km $s^1$ channel. The three positions that give the strictest limits on OH are analyzed in detail. Their combined data imply a $5σ$ limit on $N(H_2) / N_{HI} \leq 0.03$ scaled by a factor dependent on the OH excitation temperature and background continuum $T_{ex}/(T_{ex}-T_{bg})$. There is no evidence for far-infrared emission from dust within the Smith Cloud. These results are consistent with expectations for a low-metallicity diffuse cloud exposed to the radiation field of the Galactic halo rather than a product of a galactic fountain."
https://arxiv.org/abs/2403.08703,2024-03-13,Improved Dynamics for the Maximum Common Subgraph Problem,"['Davide Guidobene', 'Guido Cera']","The Maximum Common Subgraph (MCS) problem plays a crucial role across various domains, bridging theoretical exploration and practical applications in fields like bioinformatics and social network analysis. Despite its wide applicability, MCS is notoriously challenging and is classified as an NP-Complete (NPC) problem. This study introduces new heuristics aimed at mitigating these challenges through the reformulation of the MCS problem as the Maximum Clique and its complement, the Maximum Independent Set. Our first heuristic leverages the Motzkin-Straus theorem to reformulate the Maximum Clique Problem as a constrained optimization problem, continuing the work of Pelillo in Replicator Equations, Maximal Cliques, and Graph Isomorphism (1999) with replicator dynamics and introducing annealed imitation heuristics as in Dominant Sets and Hierarchical Clustering (Pavan and Pelillo, 2003) to improve chances of convergence to better local optima. The second technique applies heuristics drawn upon strategies for the Maximum Independent Set problem to efficiently reduce graph sizes as used by Akiwa and Iwata in 2014. This enables faster computation and, in many instances, yields near-optimal solutions. Furthermore we look at the implementation of both techniques in a single algorithm and find that it is a promising approach. Our techniques were tested on randomly generated Erdős-Rényi graph pairs. Results indicate the potential for application and substantial impact on future research directions."
https://arxiv.org/abs/2403.08702,2024-03-13,On the Stochasticity of Aerosol-Cloud Interactions within a Data-driven Framework,"['Xiang-Yu Li', 'Hailong Wang', 'TC Chakraborty', 'Armin Sorooshian', 'Luke D. Ziemba', 'Christiane Voigt', 'Kenneth Lee Thornhill']","Aerosol-cloud interactions (ACI) pose the largest uncertainty for climate projections. Among many challenges of understanding ACI, the question of whether ACI is deterministic or stochastic has not been explicitly formulated and asked. Here we attempt to answer this question by predicting cloud droplet number concentration Nc from aerosol number concentration Na and ambient conditions. We use aerosol properties, vertical velocity fluctuation w', and meteorological states (temperature T and water vapor mixing ratio q_v) from the ACTIVATE field observations (2020 to 2022) as predictor variables to estimate Nc. We show that the climatological Nc can be successfully predicted using a machine learning model despite the strongly nonlinear and multi-scale nature of ACI. However, the observation-trained machine learning model fails to predict Nc in individual cases while it successfully predicts Nc of randomly selected data points that cover a broad spatiotemporal scale, suggesting the stochastic nature of ACI at fine spatiotemporal scales."
https://arxiv.org/abs/2403.08701,2024-03-13,Review of Generative AI Methods in Cybersecurity,"['Yagmur Yigit', 'William J Buchanan', 'Madjid G Tehrani', 'Leandros Maglaras']","Large language models (LLMs) and generative artificial intelligence (GenAI) constitute paradigm shifts in cybersecurity that present hitherto unseen challenges as well as opportunities. In examining the state-of-the-art application of GenAI in cybersecurity, this work highlights how models like Google's Gemini and ChatGPT-4 potentially enhance security protocols, vulnerability assessment, and threat identification. Our research highlights the significance of a novel approach that employs LLMs to identify and eliminate sophisticated cyber threats. This paper presents a thorough assessment of LLMs' ability to produce important security insights, hence broadening the potential applications of AI-driven cybersecurity solutions. Our findings demonstrate the significance of GenAI in improving digital security. It offers recommendations for further investigations into the intricate relationship between cybersecurity requirements and artificial intelligence's potential."
https://arxiv.org/abs/2403.08700,2024-03-13,Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment,"['Paraskevas Pegios', 'Manxi Lin', 'Nina Weng', 'Morten Bo Søndergaard Svendsen', 'Zahra Bashir', 'Siavash Bigdeli', 'Anders Nymark Christensen', 'Martin Tolsgaard', 'Aasa Feragen']","Obstetric ultrasound image quality is crucial for accurate diagnosis and monitoring of fetal health. However, producing high-quality standard planes is difficult, influenced by the sonographer's expertise and factors like the maternal BMI or the fetus dynamics. In this work, we propose using diffusion-based counterfactual explainable AI to generate realistic high-quality standard planes from low-quality non-standard ones. Through quantitative and qualitative evaluation, we demonstrate the effectiveness of our method in producing plausible counterfactuals of increased quality. This shows future promise both for enhancing training of clinicians by providing visual feedback, as well as for improving image quality and, consequently, downstream diagnosis and monitoring."
https://arxiv.org/abs/2403.08699,2024-03-13,Implicit Regularization of Gradient Flow on One-Layer Softmax Attention,"['Heejune Sheen', 'Siyu Chen', 'Tianhao Wang', 'Harrison H. Zhou']","We study gradient flow on the exponential loss for a classification problem with a one-layer softmax attention model, where the key and query weight matrices are trained separately. Under a separability assumption on the data, we show that when gradient flow achieves the minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key and query weight matrices. Such implicit regularization can be described by a Support Vector Machine (SVM) problem with respect to the attention weights. This finding contrasts with prior results showing that the gradient descent induces an implicit regularization on the Frobenius norm on the product weight matrix when the key and query matrices are combined into a single weight matrix for training. For diagonal key and query matrices, our analysis builds upon the reparameterization technique and exploits approximate KKT conditions of the SVM associated with the classification data. Moreover, the results are extended to general weights configurations given proper alignment of the weight matrices' singular spaces with the data features at initialization."
https://arxiv.org/abs/2403.08698,2024-03-13,The Enigma of Gaia18cjb: a Rare Hybrid of Four and EXor?,"['Eleonora Fiorellino', 'Peter Abraham', 'Agnes Kospal', 'Maria Kun', 'Juan M. Alcala', 'Alessio Caratti o Garatti', 'Fernando Cruz-Saenz de Miera', 'David Garcia-Alvarez', 'Teresa Giannini', 'Sunkyung Park', 'Michal Siwak', 'Mate Szilagyi', 'Elvira Covino', 'Gabor Marton', 'Zsofia Nagy', 'Brunella Nisini', 'Zsofia Marianna Szabo', 'Zsofia Bora', 'Borbala Cseh', 'Csilla Kalup', 'Mate Krezinger', 'Levente Kriskovics', 'Waldemar Ogloza', 'Andras Pal', 'Adam Sodor']","Context. Gaia18cjb is one of the Gaia-alerted eruptive young star candidates which has been experiencing a slow and strong brightening during the last 13 years, similar to some FU Orionis-type objects. Aims. The aim of this work is to derive the young stellar nature of Gaia18cjb, determine its physical and accretion properties to classify its variability. Methods. We conducted monitoring observations using multi-filter optical and near-infrared photometry, as well as near-infrared spectroscopy. We present the analysis of pre-outburst and outburst optical and infrared light curves, color-magnitude diagrams in different bands, the detection of near-IR spectral lines, and estimates of both stellar and accretion parameters during the burst. Results. The optical light curve shows an unusually long (8 years) brightening event of 5 mag in the last 13 years, before reaching a plateau indicating that the burst is still on-going, suggesting a FUor-like nature. The same outburst is less strong in the infrared light curves. The near-infrared spectra, obtained during the outburst, exhibit emission lines typical of highly accreting low-intermediate mass young stars with typical EXor features. The spectral index of Gaia18cjb SED classifies it as a Class I in the pre-burst stage and a Flat Spectrum young stellar object (YSO) during the burst. Conclusions. Gaia18cjb is an eruptive YSO which shows FUor-like photometric features (in terms of brightening amplitude and length of the burst) and EXor-like spectroscopic features and accretion rate, as V350 Cep and V1647 Ori, classified as objects in between FUors and EXors"
https://arxiv.org/abs/2403.08697,2024-03-13,Sums of squares of k-term forms,"['Charu Goel', 'Bruce Reznick']","In this paper we study the cones corresponding to sums of squares of $n$-ary $d$-ic forms with at most $k$ terms. We show that these are strictly nested as $k$ increases, leading to the usual sum of squares cone. We also discuss the duals of these cones. For $n \ge 3$, we construct indefinite irreducible $n$-ary $d$-ic forms with exactly $k$ terms for $2 \le k \le \binom{n+d-1}{n-1}$."
https://arxiv.org/abs/2403.08696,2024-03-13,On the non-perturbative bulk Hilbert space of JT gravity,"['Luca V. Iliesiu', 'Adam Levine', 'Henry W. Lin', 'Henry Maxfield', 'Márk Mezei']","What is the bulk Hilbert space of quantum gravity? In this paper, we resolve this problem in 2d JT gravity, both with and without matter, providing the first example of an explicit definition of a non-perturbative Hilbert space specified in terms of metric variables. The states are wavefunctions of the length and matter state, but with a non-trivial and highly degenerate inner product. We explicitly identify the null states, and discuss their importance for defining operators non-perturbatively. To highlight the power of the formalism we developed, we study the non-perturbative effects for two bulk linear operators that may serve as proxies for the experience of an observer falling into a two-sided black hole: one captures the length of an Einstein-Rosen bridge and the other captures the center-of-mass collision energy between two particles falling from opposite sides. We track the behavior of these operators up to times of order $e^{S_\text{BH}}$, at which point the wavefunction spreads to the complete set of eigenstates of these operators. If these observables are indeed good proxies for the experience of an infalling observer, our results indicate an O(1) probability of detecting a firewall at late times that is self-averaging and universal."
https://arxiv.org/abs/2403.08695,2024-03-13,Deep Learning for In-Orbit Cloud Segmentation and Classification in Hyperspectral Satellite Data,"['Daniel Kovac', 'Jan Mucha', 'Jon Alvarez Justo', 'Jiri Mekyska', 'Zoltan Galaz', 'Krystof Novotny', 'Radoslav Pitonak', 'Jan Knezik', 'Jonas Herec', 'Tor Arne Johansen']","This article explores the latest Convolutional Neural Networks (CNNs) for cloud detection aboard hyperspectral satellites. The performance of the latest 1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and 2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed. Evaluation criteria include precision and computational efficiency for in-orbit deployment. Experiments utilize NASA's EO-1 Hyperion data, with varying spectral channel numbers after Principal Component Analysis. Results indicate that 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs, while maintaining compactness with larger spectral channel sets, albeit with increased inference times. However, the performance of 1D CNN degrades with significant channel reduction. In this context, the 2D-Justo-UNet-Simple offers the best balance for in-orbit deployment, considering precision, memory, and time costs. While nnU-net is suitable for on-ground processing, deployment of lightweight 1D-Justo-LiuNet is recommended for high-precision applications. Alternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced costs between timing and precision in orbit."
https://arxiv.org/abs/2403.08694,2024-03-13,TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning,"['Shangding Gu', 'Alois Knoll', 'Ming Jin']","The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm. In this work, we pivot to Reinforcement Learning (RL) -- but with a twist. Diverging from the typical RLHF, which refines LLMs following instruction data training, we use RL to directly generate the foundational instruction dataset that alone suffices for fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and rules, prioritizing the diversification of training datasets. It facilitates the generation of high-quality data without excessive reliance on external advanced models, paving the way for a single fine-tuning step and negating the need for subsequent RLHF stages. Our findings highlight key advantages of our approach: reduced need for human involvement and fewer model queries (only $5.73\%$ of WizardLM's total), along with enhanced capabilities of LLMs in crafting and comprehending complex instructions compared to strong baselines, and substantially improved model privacy protection."
https://arxiv.org/abs/2403.08693,2024-03-13,Do Language Models Care About Text Quality? Evaluating Web-Crawled Corpora Across 11 Languages,"['Rik van Noord', 'Taja Kuzman', 'Peter Rupnik', 'Nikola Ljubešić', 'Miquel Esplà-Gomis', 'Gema Ramírez-Sánchez', 'Antonio Toral']","Large, curated, web-crawled corpora play a vital role in training language models (LMs). They form the lion's share of the training data in virtually all recent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However, despite this importance, relatively little attention has been given to the quality of these corpora. In this paper, we compare four of the currently most relevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across eleven lower-resourced European languages. Our approach is two-fold: first, we perform an intrinsic evaluation by performing a human evaluation of the quality of samples taken from different corpora; then, we assess the practical impact of the qualitative differences by training specific LMs on each of the corpora and evaluating their performance on downstream tasks. We find that there are clear differences in quality of the corpora, with MaCoCu and OSCAR obtaining the best results. However, during the extrinsic evaluation, we actually find that the CC100 corpus achieves the highest scores. We conclude that, in our experiments, the quality of the web-crawled corpora does not seem to play a significant role when training LMs."
https://arxiv.org/abs/2403.08692,2024-03-13,An Algorithm to Parallelise Parton Showers on a GPU,"['Michael H. Seymour', 'Siddharth Sule']","The Single Instruction, Multiple Thread (SIMT) paradigm of GPU programming does not support the branching nature of a parton shower algorithm by definition. However, modern GPUs are designed to schedule threads with diverging processes independently, allowing them to handle such branches. With regular thread synchronization and careful treatment of the individual steps, one can simulate a parton shower on a GPU. We present a parallelized Sudakov veto algorithm designed to simulate parton branching on multiple events simultaneously. We also release a CUDA C++ program that generates matrix elements, showers partons, and computes jet rates and event shapes for LEP at 91.2 GeV on a GPU. To benchmark its performance, we also provide a near-identical C++ program designed to simulate events serially on a CPU. While the consequences of branching are not absent, we demonstrate that a GPU can provide the throughput of a many-core CPU. As an example, we show that the time taken to simulate 10^6 events on one NVIDIA TESLA V100 GPU is equivalent to that of 266 Intel Xeon E5-2620 CPUs."
https://arxiv.org/abs/2403.08691,2024-03-13,On the large deviation principle for Metropolis-Hastings Markov Chains: the Lyapunov function condition and examples,"['Federica Milinanni', 'Pierre Nyquist']","With an aim to analyse the performance of Markov chain Monte Carlo (MCMC) methods, in our recent work we derive a large deviation principle (LDP) for the empirical measures of Metropolis-Hastings (MH) chains on a continuous state space. One of the (sufficient) assumptions for the LDP involves the existence of a particular type of Lyapunov function, and it was left as an open question whether or not such a function exists for specific choices of MH samplers. In this paper we analyse the properties of such Lyapunov functions and investigate their existence for some of the most popular choices of MCMC samplers built on MH dynamics: Independent Metropolis Hastings, Random Walk Metropolis, and the Metropolis-adjusted Langevin algorithm. We establish under what conditions such a Lyapunov function exists, and from this obtain LDPs for some instances of the MCMC algorithms under consideration. To the best of our knowledge, these are the first large deviation results for empirical measures associated with Metropolis-Hastings chains for specific choices of proposal and target distributions."
https://arxiv.org/abs/2403.08690,2024-03-13,Controllability of continuous networks and a kernel-based learning approximation,"['Michael Herty', 'Chiara Segala', 'Giuseppe Visconti']","Residual deep neural networks are formulated as interacting particle systems leading to a description through neural differential equations, and, in the case of large input data, through mean-field neural networks. The mean-field description allows also the recast of the training processes as a controllability problem for the solution to the mean-field dynamics. We show theoretical results on the controllability of the linear microscopic and mean-field dynamics through the Hilbert Uniqueness Method and propose a computational approach based on kernel learning methods to solve numerically, and efficiently, the training problem. Further aspects of the structural properties of the mean-field equation will be reviewed."
https://arxiv.org/abs/2403.08689,2024-03-13,Exploiting Structural Consistency of Chest Anatomy for Unsupervised Anomaly Detection in Radiography Images,"['Tiange Xiang', 'Yixiao Zhang', 'Yongyi Lu', 'Alan Yuille', 'Chaoyi Zhang', 'Weidong Cai', 'Zongwei Zhou']","Radiography imaging protocols focus on particular body regions, therefore producing images of great similarity and yielding recurrent anatomical structures across patients. Exploiting this structured information could potentially ease the detection of anomalies from radiography images. To this end, we propose a Simple Space-Aware Memory Matrix for In-painting and Detecting anomalies from radiography images (abbreviated as SimSID). We formulate anomaly detection as an image reconstruction task, consisting of a space-aware memory matrix and an in-painting block in the feature space. During the training, SimSID can taxonomize the ingrained anatomical structures into recurrent visual patterns, and in the inference, it can identify anomalies (unseen/modified visual patterns) from the test image. Our SimSID surpasses the state of the arts in unsupervised anomaly detection by +8.0%, +5.0%, and +9.9% AUC scores on ZhangLab, COVIDx, and CheXpert benchmark datasets, respectively. Code: https://github.com/MrGiovanni/SimSID"
https://arxiv.org/abs/2403.08688,2024-03-13,Token Alignment via Character Matching for Subword Completion,"['Ben Athiwaratkun', 'Shiqi Wang', 'Mingyue Shang', 'Yuchen Tian', 'Zijian Wang', 'Sujan Kumar Gonugondla', 'Sanjay Krishna Gouda', 'Rob Kwiatowski', 'Ramesh Nallapati', 'Bing Xiang']","Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens. This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs. This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases. The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model's generation aligns with the prompt. This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase. The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like code completion and text autocompletion."
https://arxiv.org/abs/2403.08687,2024-03-13,Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing,"['Xiangchun Chen', 'Jiannong Cao', 'Zhixuan Liang', 'Yuvraj Sahni', 'Mingjin Zhang']","Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute microservices from end devices. Microservice offloading, a fundamentally important problem, decides when and where microservices are executed upon the arrival of services. However, the dynamic nature of the real-world CEC environment often leads to inefficient microservice offloading strategies, resulting in underutilized resources and network congestion. To address this challenge, we formulate an online joint microservice offloading and bandwidth allocation problem, JMOBA, to minimize the average completion time of services. In this paper, we introduce a novel microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement learning (DRL) and digital twin technology. Specifically, we employ digital twin techniques to predict and adapt to changing edge node loads and network conditions of CEC in real-time. Furthermore, this approach enables the generation of an efficient offloading plan, selecting the most suitable edge node for each microservice. Simulation results on real-world and synthetic datasets demonstrate that DTDRLMO outperforms heuristic and learning-based methods in average service completion time."
https://arxiv.org/abs/2403.08686,2024-03-13,Evidence of enhanced thermopower from emergent local moments in flatbands of magic-angle twisted bilayer graphene,"['Ayan Ghosh', 'Souvik Chakraborty', 'Ranit Dutta', 'Adhip Agarwala', 'K. Watanabe', 'T. Taniguchi', 'Sumilan Banerjee', 'Nandini Trivedi', 'Subroto Mukerjee', 'Anindya Das']","Recent experiments on magic-angle twisted bilayer graphene (MATBLG) indicate an unusual coexistence of heavy and light electrons, characteristic of Heavy-Fermion physics. Yet, these experiments lack direct evidence of the existence of local moments associated with the heavy electrons, a key component of Heavy-Fermion physics. Thermopower serves as a sensitive probe for measuring entropy and can, therefore, unveil the presence of local moments by assessing their impact on entropy. In this work, we have carried out comprehensive thermopower studies on MATBLG by varying temperatures ($T$) and magnetic fields ($B$). While the resistance exhibits prominent resistance peaks at integer fillings apart from the Dirac point (DP, $ν= 0$) and full band filling ($ν= \pm 4$), the thermopower remains featureless and symmetric to the DP with opposite signs, in complete violation of the Mott formula, even up to $\sim 100K$. This discrepancy indicates that resistance and thermopower are carried by different kinds of carriers. With the expected sign changes at the DP and full band filling, the thermopower demonstrates additional sign changes within the conduction and valence bands around $ν\sim \pm 1$, persisting from $5K$ to $50K$. These observations cannot be explained solely by thermopower arising from an emergent Heavy-Fermion band picture, which is highly sensitive to temperature. Instead, our data is consistent with the dominant contribution coming from the entropy of the localized moments of heavy electrons. To validate the claim, we studied the thermopower with $B_{\parallel}$ and $B_{\perp}$, revealing a $30\%$ and $50\%$ reduction, respectively, and attributed to the partial polarization of the local moments (spin/valley), resulting in decreased entropy. Our results highlight the thermopower contribution from local moments and establish the Heavy-Fermion physics in MATBLG."
https://arxiv.org/abs/2403.08685,2024-03-13,Elastic shape analysis computations for clustering left atrial appendage geometries of atrial fibrillation patients,"['Zan Ahmad', 'Minglang Yin', 'Yashil Sukurdeep', 'Noam Rotenberg', 'Eugene Kholmovski', 'Natalia A. Trayanova']","Morphological variations in the left atrial appendage (LAA) are associated with different levels of ischemic stroke risk for patients with atrial fibrillation (AF). Studying LAA morphology can elucidate mechanisms behind this association and lead to the development of advanced stroke risk stratification tools. However, current categorical descriptions of LAA morphologies are qualitative and inconsistent across studies, which impedes advancements in our understanding of stroke pathogenesis in AF. To mitigate these issues, we introduce a quantitative pipeline that combines elastic shape analysis with unsupervised learning for the categorization of LAA morphology in AF patients. As part of our pipeline, we compute pairwise elastic distances between LAA meshes from a cohort of 20 AF patients, and leverage these distances to cluster our shape data. We demonstrate that our method clusters LAA morphologies based on distinctive shape features, overcoming the innate inconsistencies of current LAA categorization systems, and paving the way for improved stroke risk metrics using objective LAA shape groups."
https://arxiv.org/abs/2403.08684,2024-03-13,Bubble breakup probability in turbulent flows,"['Aliénor Rivière', 'Stéphane Perrard']","Bubbles drive gas and chemical transfers in various industrial and geophysical context, in which flows are typically turbulent. A knowledge of the bubble size distributions is then necessary to quantify mass fluxes across interfaces. In a turbulent flow, every bubble might break, depending on both the ratio between inertial and capillary forces at its scale, namely the Weber number We. For inhomogeneous and unstationary flows, the residence time within a turbulent region will also determine the break-up probability. In this work, we use a stochastic linear model, whose parameters have been measured using direct numerical simulations, to infer the breakup probability of bubbles in turbulence as function of the Weber number and the residence time. Our model shows that bubble breakup is a memoryless process, whose breakup rate varies exponentially with $\textrm{We}^{-1}$. This linear model successfully reproduces breakup rates previously measured experimentally."
https://arxiv.org/abs/2403.08683,2024-03-13,Single file motion of robot swarms,"['Laciel Alonso-Llanes', 'Angel Garcimartín', 'Iker Zuriguel']","We present experimental results on the single file motion of a group of robots interacting with each other through position sensors. We successfully replicate the fundamental diagram typical of these systems, with a transition from free flow to congested traffic as the density of the system increases. In the latter scenario we also observe the characteristic stop-and-go waves. The unique advantages of this novel system, such as experimental stability and repeatability, allow for extended experimental runs, facilitating a comprehensive statistical analysis of the global dynamics. Above a certain density, we observe a divergence of the average jam duration and the average number of robots involved in it. This discovery enables us to precisely identify another transition: from congested intermittent flow (for intermediate densities) to a totally congested scenario for high densities. Beyond this finding, the present work demonstrates the suitability of robot swarms to model complex behaviors in many particle systems."
https://arxiv.org/abs/2403.08682,2024-03-13,OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework,"['Wanyun Li', 'Pinxue Guo', 'Xinyu Zhou', 'Lingyi Hong', 'Yangji He', 'Xiangyu Zheng', 'Wei Zhang', 'Wenqiang Zhang']","Contemporary Video Object Segmentation (VOS) approaches typically consist stages of feature extraction, matching, memory management, and multiple objects aggregation. Recent advanced models either employ a discrete modeling for these components in a sequential manner, or optimize a combined pipeline through substructure aggregation. However, these existing explicit staged approaches prevent the VOS framework from being optimized as a unified whole, leading to the limited capacity and suboptimal performance in tackling complex videos. In this paper, we propose OneVOS, a novel framework that unifies the core components of VOS with All-in-One Transformer. Specifically, to unify all aforementioned modules into a vision transformer, we model all the features of frames, masks and memory for multiple objects as transformer tokens, and integrally accomplish feature extraction, matching and memory management of multiple objects through the flexible attention mechanism. Furthermore, a Unidirectional Hybrid Attention is proposed through a double decoupling of the original attention operation, to rectify semantic errors and ambiguities of stored tokens in OneVOS framework. Finally, to alleviate the storage burden and expedite inference, we propose the Dynamic Token Selector, which unveils the working mechanism of OneVOS and naturally leads to a more efficient version of OneVOS. Extensive experiments demonstrate the superiority of OneVOS, achieving state-of-the-art performance across 7 datasets, particularly excelling in complex LVOS and MOSE datasets with 70.1% and 66.4% $J \& F$ scores, surpassing previous state-of-the-art methods by 4.2% and 7.0%, respectively. And our code will be available for reproducibility and further research."
https://arxiv.org/abs/2403.08681,2024-03-13,Towards a Consistent Calculation of the Lunar Response to Gravitational Waves,"['Han Yan', 'Xian Chen', 'Jinhai Zhang', 'Fan Zhang', 'Mengyao Wang', 'Lijing Shao']","The recent increasing interest in detecting gravitational waves (GWs) by lunar seismic measurement urges us to have a clear understanding of the response of the moon to passing GWs. In this paper, we clarify the relationship between two seemly different response functions which have been derived previously using two different methods, one taking the field-theory approach and the other using the tidal force induced by GWs. We revisit their derivation and prove, by both analytical arguments and numerical calculations, that the two response functions are equivalent. Their apparent difference can be attributed to the choice of different coordinates. Using the correct response function, we calculate the sensitivities (to GWs) of several designed lunar seismometers, and find that the sensitivity curves between $10^{-3}$ and $0.1$ Hz are much flatter than the previous calculations based on normal-mode model. Our results will help clarifying the scientific objectives of lunar GW observation, as well as provide important constraints on the design of lunar GW detectors."
https://arxiv.org/abs/2403.08680,2024-03-13,Towards the THz Networks in the 6G Era,"['Qian Ding', 'Jie Yang', 'Yang Luo', 'Chunbo Luo']","This commentary dedicates to envision what role THz is going to play in the coming human-centric 6G era. Three distinct THz network types including outdoor, indoor, and body area networks are discussed, with an emphasis on their capabilities in human body detection. Synthesizing these networks will unlock a bunch of fascinating applications across industrial, biomedical and entertainment fields, significantly enhancing the quality of human life."
https://arxiv.org/abs/2403.08679,2024-03-13,Antiferromagnetic ordering and glassy nature in NASICON type NaFe$_2$PO$_4$(SO$_4$)$_2$,"['Manish Kr. Singh', 'A. K. Bera', 'Ajay Kumar', 'S. M. Yusuf', 'R. S. Dhaka']","We investigate crystal structure and magnetic properties including spin relaxation and magnetocaloric effect in NASICON type NaFe$_2$PO$_4$(SO$_4$)$_2$ sample. The Rietveld refinement of x-ray and neutron diffraction patterns show a rhombohedral crystal structure with the R$\bar{3}$c space group. The core-level spectra confirm the desired oxidation state of constituent elements. The {\it dc}--magnetic susceptibility ($χ$) behavior in zero field-cooled (ZFC) and field-cooled (FC) modes show the ordering temperature $\approx$50~K. Interestingly, the analysis of temperature dependent neutron diffraction patterns reveal an A-type antiferromagnetic (AFM) structure with the ordered moment of 3.8 $μ_{B}$/Fe$^{3+}$ at 5~K, and a magnetostriction below $T_{\rm N}=$ 50~K. Further, the peak position in the {\it ac}--$χ$ is found to be invariant with the excitation frequency supporting the notion of dominating AFM transition. Also, the unsaturated isothermal magnetization curve supports the AFM ordering of the moments; however, the observed coercivity suggests the presence of weak ferromagnetic (FM) correlations at 5~K. On the other hand, a clear bifurcation between ZFC and FC curves of {\it dc}--$χ$ and the observed decrease in peak height of {\it ac}--$χ$ with frequency suggest for the complex magnetic interactions. The spin relaxation behavior in thermo-remanent magnetization and aging measurements indicate the glassy states at 5~K. Moreover, the Arrott plots and magnetocaloric analysis reveal the AFM--FM interactions in the sample at lower temperatures."
https://arxiv.org/abs/2403.08678,2024-03-13,Path-dependency of capital return in periodic growth processes,['Petri P. Karenlampi'],"Periodic growth processes are investigated. The expected value of the profit rate, on accrual basis, does not directly depend on divestments, neither on the capitalization path. The expected value of capitalization is path dependent. Because of the path-dependent capitalization, the return rate on capital is path-dependent, and the time-average return rate on capital differs from the expected-value return rate on capital for the growth cycle. In the absence of intermediate divestments, the internal rate of return is path-independent, thereby differing from the expected value of the rate of return on capital. It is shown that the rotation cycle length maximizing the return rate on equity is independent of market interest rate. Leveraging effect enters the microeconomics of the growth processes through a separate leveraging equation, where the leverage coefficient may reach positive or negative values. Correspondingly, from the viewpoint of wealth accumulation, the often-suggested dependency of suitable rotation length on discount rate appears to be a modeling artifact. In other words, the net present value computation is based on maximization of consumption utility, instead of a capital growth objective; borrowing is obligatory, but the leverage effect is absent."
https://arxiv.org/abs/2403.08677,2024-03-13,One-Loop Quantum Stress-Energy Tensor for the Kink and sine-Gordon Solitons,"['Noah Graham', 'Herbert Weigel']",We compute the renormalized one-loop quantum corrections to the energy density $T_{00}(x)$ and pressure $T_{11}(x)$ for solitons in the $1+1$ dimensional scalar sine-Gordon and kink models. We show how precise implementation of counterterms in dimensional regularization resolves previously identified discrepancies between the integral of $T_{00}(x)$ and the known correction to the total energy.
https://arxiv.org/abs/2403.08676,2024-03-13,Are Magnons just the van der Waals interaction in Disguise?,['Robert A. Lawrence'],"The MBD model of the van der Waals interaction is extended to also consider magnetic interactions, and it is demonstrated how this can be made to reproduce the Heisenberg Hamiltonian. It is found that this leads to a weak coupling between the charge dipole waves that are the basis for the electric-only van der Waals interaction and the spin-dipole waves (magnons) of the Heisenberg model. By applying the same level of theory to both simultaneously we demonstrate that magnons and charge-dipole waves may both be considered to be a basis for the fluctuating part of the many-body electron density."
https://arxiv.org/abs/2403.08675,2024-03-13,Axi-majoron for almost everything,"['Gabriela Barenboim', 'Pyungwon Ko', 'Wan-il Park']","The details of the minimal cosmological standard model (MCSM) proposed in Ref. [1] are discussed. The model is based on the scale-symmetry and the global Peccei-Quinn(PQ) symmetry with a key assumption that the latter is broken only in the gravity sector in a scale-invariant manner. We show that the model provides a quite simple unified framework for the unknown history of the universe from inflation to the epoch of big-bang nucleosynthesis, simultaneously addressing key puzzles of high energy theory and cosmology: (i) the origin of scales, (ii) primordial inflation, (iii) matter-antimatter asymmetry, (iv) tiny neutrino masses, (v) dark matter, and (vi) the strong CP-problem. Scale symmetry can be exact, and the Planck scale is dynamically generated. The presence of Gauss-Bonnet term may safely retain dangerous non-perturbative symmetry-breaking effects negligible, allowing large-field trans-Planckian inflation along the PQ-field. Iso-curvature perturbations of axi-majorons are suppressed. A sizable amount of PQ-number asymmetry is generated at the end of inflation, and conserved afterwards. Domain wall problem is absent due to the PQ-number asymmetry. Baryogenesis can be realized by either inflationary Affleck-Dine mechanism or spontaneous leptogenesis thanks to the PQ-number asymmetry, or by resonant leptogenesis. Dark matter can be purely cold axi-majorons from the mis-alignment contribution only with the symmetry-breaking scale of $\mathcal{O}(10^{12}) {\rm GeV}$. Hot axi-majorons from the decay of the inflaton become a natural source for a sizable amount of dark radiation. Inflationary gravitational waves are expected to have information about some masse parameters of the left-handed and the right-handed neutrinos, thanks to the presence of an early matter-domination era driven by at least a long-lived right-handed neutrino species."
https://arxiv.org/abs/2403.08674,2024-03-13,Quantum jump photodetector for narrowband photon counting with a single atom,"['Laura Zarraoa', 'Romain Veyron', 'Tomas Lamich', 'Morgan W. Mitchell']","Using a single neutral \textsuperscript{87}Rb atom held in an optical trap, and ""quantum jump"" detection of single-photon-initiated state changes, we demonstrate an intrinsically-narrowband single-photon detector, of interest for separating weak signals from strong optical background. Using novel statistical analysis, we measure quantum efficiency of \SI{2.9+-0.2e-3}{}, a record for single-pass quantum jump production, and dark counts of \SI{9+-20e-3}{counts\per\second} during passive accumulation plus \SI{1.8+-0.1e-2}{counts} per readout, orders of magnitude below those of traditional single-photon detectors. The \SI{6}{\mega\hertz} detection bandwidth is orders of magnitude narrower than existing atomic filters. Available methods can substantially improve \QJPDAcronym{} quantum efficiency, dark counts, bandwidth, and tunability."
https://arxiv.org/abs/2403.08673,2024-03-13,When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?,"['Gautham Govind Anil', 'Pascal Esser', 'Debarghya Ghoshdastidar']","Contrastive learning is a paradigm for learning representations from unlabelled data that has been highly successful for image and text data. Several recent works have examined contrastive losses to claim that contrastive models effectively learn spectral embeddings, while few works show relations between (wide) contrastive models and kernel principal component analysis (PCA). However, it is not known if trained contrastive models indeed correspond to kernel methods or PCA. In this work, we analyze the training dynamics of two-layer contrastive models, with non-linear activation, and answer when these models are close to PCA or kernel methods. It is well known in the supervised setting that neural networks are equivalent to neural tangent kernel (NTK) machines, and that the NTK of infinitely wide networks remains constant during training. We provide the first convergence results of NTK for contrastive losses, and present a nuanced picture: NTK of wide networks remains almost constant for cosine similarity based contrastive losses, but not for losses based on dot product similarity. We further study the training dynamics of contrastive models with orthogonality constraints on output layer, which is implicitly assumed in works relating contrastive learning to spectral embedding. Our deviation bounds suggest that representations learned by contrastive models are close to the principal components of a certain matrix computed from random features. We empirically show that our theoretical results possibly hold beyond two-layer networks."
https://arxiv.org/abs/2403.08672,2024-03-13,Non-linear collision-induced breakage equation: approximate solution and error estimation,"['Sanjiv Kumar Bariwal', 'Rajesh Kumar']","This article aims to provide approximate solutions for the non-linear collision-induced breakage equation using two different semi-analytical schemes, i.e., variational iteration method (VIM) and optimized decomposition method (ODM). The study also includes the detailed convergence analysis and error estimation for ODM in the case of product collisional ($K(ε,ρ)=ερ$) and breakage ($b(ε,ρ,σ)=\frac{2}ρ$) kernels with an exponential decay initial condition. By contrasting estimated concentration function and moments with exact solutions, the novelty of the suggested approaches is presented considering three numerical examples. Interestingly, in one case, VIM provides a closed-form solution, however, finite term series solutions obtained via both schemes supply a great approximation for the concentration function and moments."
https://arxiv.org/abs/2403.08671,2024-03-13,On the selection of Saffman-Taylor viscous fingers for divergent flow in a wedge,"['Cecile Andersen', 'Christopher J. Lustri', 'Scott W. McCue', 'Philippe H. Trinh']","We study self-similar viscous fingering for the case of divergent flow within a wedge-shaped Hele-Shaw cell. Previous authors have conjectured the existence of a countably-infinite number of selected solutions, each distinguished by a different value of the relative finger angle. Interestingly, the associated solution branches have been posited to merge and disappear in pairs as the surface tension decreases. We demonstrate how exponential asymptotics is used to derive the selection mechanism. In addition, asymptotic predictions of the finger-to-wedge angle are given for different sized wedges and surface-tension values. The merging of solution branches is explained; this feature is qualitatively different to the case of classic Saffman-Taylor viscous fingering in a parallel channel configuration. The phenomena of branch merging in our self-similar problem relates to tip splitting instabilities in time-dependent flows in a circular geometry, where the viscous fingers destabilise and divide in two."
https://arxiv.org/abs/2403.08670,2024-03-13,Ancilla-free measurement of out-of-time-ordered correlation functions: General measurement protocol and Rydberg atom implementation,"['Michael Kastner', 'Philip Osterholz', 'Christian Gross']","We introduce a protocol that gives access to out-of-time-ordered correlation functions in many-body quantum systems. Unlike other such protocols, our proposal, which can be applied to arbitrary initial states, neither requires ancilla degrees of freedom to the quantum system of interest, nor has the need for randomized measurements. Nontrivial experimental capabilities required to implement the protocol are single-site measurements, single-site rotations, and backwards time evolution. To exemplify the implementation of the protocol, we put forward a strategy for Hamiltonian sign inversion $H\to-H$ in arrays of Rydberg-dressed atoms. In this way, a complete and practical toolbox is obtained for the measurement of out-of-time-ordered correlations in equilibrium and nonequilibrium situations."
https://arxiv.org/abs/2403.08669,2024-03-13,"BHAC-QGP: three-dimensional MHD simulations of relativistic heavy-ion collisions, II. Application to Au-Au collisions","['Markus Mayer', 'Luciano Rezzolla', 'Hannah Elfner', 'Gabriele Inghirami', 'Dirk H. Rischke']","We present BHAC-QGP, a new numerical code to simulate the evolution of matter created in heavy-ion collisions. BHAC-QGP is based on the Black Hole Accretion Code (BHAC), which has been designed to model astrophysical processes through the solution of the equations of general-relativistic magnetohydrodynamics. Like the mother code, BHAC-QGP uses Adaptive Mesh Refinement (AMR), which allows for a dynamic adjustment of the resolution in regions of the computational domain where a particularly high accuracy is needed. We here discuss a number of applications of BHAC-QGP to Au-Au collisions at Relativistic Heavy-Ion Collider (RHIC) energies and show that the code is able to reproduce results of other simulations of these scenarios, but with much higher accuracy."
https://arxiv.org/abs/2403.08668,2024-03-13,"BHAC-QGP: three-dimensional MHD simulations of relativistic heavy-ion collisions, I. Methods and tests","['Markus Mayer', 'Luciano Rezzolla', 'Hannah Elfner', 'Gabriele Inghirami', 'Dirk H. Rischke']","We present BHAC-QGP, a new numerical code to simulate the evolution of matter created in heavy-ion collisions in the presence of electromagnetic fields. It is derived from the Black Hole Accretion Code (BHAC), which has been designed to model astrophysical processes in a general-relativistic magnetohydrodynamical description. As the original Black Hole Accretion Code, BHAC-QGP benefits from the use of Adaptive Mesh Refinement (AMR), which allows us to dynamically adjust the resolution where necessary, and makes use of time-dependent Milne coordinates and the ultrarelativistic equation of state, $P = e/3$. We demonstrate that BHAC-QGP accurately passes a number of systematic and rigorous tests."
https://arxiv.org/abs/2403.08667,2024-03-13,Surfaces and other Peano Continua with no Generic Chains,"['Gianluca Basso', 'Alessandro Codenotti', 'Andrea Vaccaro']","The space of chains on a compact connected space encodes all the different ways of continuously growing out of a point until exhausting the space. A chain is generic if its orbit under the action of the underlying homeomorphism group is comeager: a space X has a generic chain if there is essentially just one type of chain on X. Gutman, Tsankov and Zucker proved that compact manifolds of dimension at least 3 do not have a generic chain. We extend and generalize their result, covering a large class of spaces which includes all compact surfaces except for the sphere and the real projective plane - for which the question remains open - as well as all other homogeneous Peano continua, circle excluded. If the spaces are moreover strongly locally homogeneous, which is the case for any closed manifold as well as the Menger curve, we prove that chains cannot be classified up to homeomorphism by countable structures, and that the underlying homomorphism groups have non-metrizable universal minimal flows. This is in contrast with the case of 1-dimensional manifolds: their homomorphism groups have metrizable universal minimal flow and we show that their chains are classifiable by countable structures and have a generic element. The proof of the main result relies on crafting a dictionary between open sets of chains on one side, and walks on finite connected graphs on the other. We then establish a novel combinatorial necessary condition for the existence of a generic chain, an off-by-one weak amalgamation principle, and prove that it is not satisfied under the hypotheses of our theorem."
https://arxiv.org/abs/2403.08666,2024-03-13,"The neutron decay anomaly, neutron stars and dark matter","['Mar Bastero-Gil', 'Teresa Huertas-Roldan', 'Daniel Santos']","The discrepancies in different measurements of the lifetime of isolated neutrons could be resolved by considering an extra neutron decay channel into dark matter, with a branching ratio of the order of $O(1$\%). Although the decay channel into a dark fermion $χ$ plus visible matter has been already experimentally excluded, a dark decay with either a scalar or dark photon remains still a possibility. In particular, a model with a fermion mass $m_χ\approx 1$ GeV and a scalar $m_φ\approx O(\rm{MeV})$ could provide not only the required branching ratio to explain the anomaly but also a good dark matter (DM) candidate with the right thermal abundance today. Although the interaction DM-neutron will affect the formation of neutron stars, the combined effect of the dark matter self-interactions mediated by the light scalar and an effective repulsive interaction with the neutrons induced by the scalar-Higgs coupling would allow heavy enough neutron stars. The combined constraints from neutron lifetime, dark matter abundance, neutron star and Higgs physics, and Big Bang Nucleosynthesis, restrict the light scalar mass to the range $2 m_e < m_φ< 2 m_e + 0.0375$ MeV."
https://arxiv.org/abs/2403.08665,2024-03-13,A higher-dimensional Chevalley restriction theorem for classical groups in characteristic p,['Xiaopeng Xia'],"We establish a theorem concerning the commuting scheme in characteristic p. As a significant application of this theorem, we derive an explicit lower bound for the characteristic p, ensuring the validity of the higher-dimensional Chevalley restriction theorem for classical groups."
https://arxiv.org/abs/2403.08664,2024-03-13,Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records,"['Erlend Frayling', 'Jake Lever', 'Graham McDonald']","The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. This study assesses the capability of the Llama 2 LLM to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training. We focus on generating synthetic narratives for the History of Present Illness section, utilising data from the MIMIC-IV dataset for comparison. In this work introduce a novel prompting technique that leverages a chain-of-thought approach, enhancing the model's ability to generate more accurate and contextually relevant medical narratives without prior fine-tuning. Our findings suggest that this chain-of-thought prompted approach allows the zero-shot model to achieve results on par with those of fine-tuned models, based on Rouge metrics evaluation."
https://arxiv.org/abs/2403.08663,2024-03-13,QCSHQD: Quantum computing as a service for Hybrid classical-quantum software development: A Vision,"['Arif Ali Khan', 'Maryam Tavassoli Sabzevari', 'Davide Taibi', 'Matteo Esposito']","Quantum Computing (QC) is transitioning from theoretical frameworks to an indispensable powerhouse of computational capability, resulting in extensive adoption across both industrial and academic domains. QC presents exceptional advantages, including unparalleled processing speed and the potential to solve complex problems beyond the capabilities of classical computers. Nevertheless, academic researchers and industry practitioners encounter various challenges in harnessing the benefits of this technology. The limited accessibility of QC resources for classical developers, and a general lack of domain knowledge and expertise, represent insurmountable barrier, hence to address these challenges, we introduce a framework- Quantum Computing as a Service for Hybrid Classical-Quantum Software Development (QCSHQD), which leverages service-oriented strategies. Our framework comprises three principal components: an Integrated Development Environment (IDE) for user interaction, an abstraction layer dedicated to orchestrating quantum services, and a service provider responsible for executing services on quantum computer. This study presents a blueprint for QCSHQD, designed to democratize access to QC resources for classical developers who want to seamless harness QC power. The vision of QCSHQD paves the way for groundbreaking innovations by addressing key challenges of hybridization between classical and quantum computers."
https://arxiv.org/abs/2403.08662,2024-03-13,Self-Supervised Learning for Covariance Estimation,"['Tzvi Diskin', 'Ami Wiesel']","We consider the use of deep learning for covariance estimation. We propose to globally learn a neural network that will then be applied locally at inference time. Leveraging recent advancements in self-supervised foundational models, we train the network without any labeling by simply masking different samples and learning to predict their covariance given their surrounding neighbors. The architecture is based on the popular attention mechanism. Its main advantage over classical methods is the automatic exploitation of global characteristics without any distributional assumptions or regularization. It can be pre-trained as a foundation model and then be repurposed for various downstream tasks, e.g., adaptive target detection in radar or hyperspectral imagery."
https://arxiv.org/abs/2403.08661,2024-03-13,Negative pressure as a quantum effect in free-streaming in the cosmological background,"['F. Becattini', 'D. Roselli']","We present a study of energy density and pressure of a free real scalar quantum field after its decoupling from a thermal bath in the spatially flat Friedman-Lemaitre-Robertson-Walker space-time by solving the Klein-Gordon equation both analytically and numerically for different predetermined scale factor functions $a(t)$. The energy density and pressure, defined by subtracting the vacuum expectation values at the decoupling time, feature corrections with respect to the classical free-streaming solution of the relativistic Boltzmann equation. We show that if the expansion rate is comparable or larger than $mc^2/\hbar$ or $KT_0/\hbar$ where $m$ is the mass and $T_0$ the decoupling temperature, both energy density and pressure gets strong quantum corrections which substantially modify their classical dependence on the scale factor $a(t)$ and drive pressure to large negative values. For a minimally coupled field with a very low mass in an expanding de Sitter universe quantum corrections are dominant driving pressure and energy density to become asymptotically constant with an equation of state $p/\varepsilon \simeq -1$, thereby mimicking a cosmological constant. For a minimally coupled massless field, quantum corrections are asymptotically dominant for any accelerated expansion."
https://arxiv.org/abs/2403.08660,2024-03-13,"Room temperature charge density wave in a tetragonal polymorph of Gd2Os3Si5 and study of its origin in the R2T3X5 (R = Rare earth, T = transition metal, X = Si, Ge) series","['Vikash Sharma', 'Sitaram Ramakrishnan', 'S. S. Jayakrishnan', 'Surya Rohith Kotla', 'Bishal Maiti', 'Claudio Eisele', 'Harshit Agarwal', 'Leila Noohinejad', 'M. Tolkiehn', 'Dipanshu Bansal', 'Sander van Smaalen', 'Arumugam Thamizhavel']","Charge density wave (CDW) systems are proposed to exhibit application potential for electronic and optoelectronic devices. Therefore, identifying new materials that exhibit a CDW state at room temperature is crucial for the development of CDW-based devices. Here, we present a non-layered tetragonal polymorph of Gd2Os3Si5, which exhibits a CDW state at room temperature. Gd2Os3Si5 crystallizes in the U2Mn3Si5-type tetragonal crystal structure with the space group P4/mnc. Single-crystal x-ray diffraction (SXRD) analysis shows that Gd2Os3Si5 possesses an incommensurately modulated structure with modulation wave vector q = (0.53, 0, 0), while the modulation reduces the symmetry to orthorhombic Cccm(σ00)0s0. This differs in contrast to isostructural Sm2Ru3Ge5, where the modulated phase has been reported to possess the superspace symmetry Pm(α 0 γ)0. However, reinvestigation of Sm2Ru3Ge5 suggests that its modulated crystal structure can alternatively be described by Cccm(σ00)0s0, with modulations similar to Gd2Os3Si5. The magnetic susceptibility, \c{hi}(T), exhibits a maximum at low temperatures that indicates an antiferromagnetic transition at TN = 5.5 K. The \c{hi}(T) furthermore shows an anomaly at around 345 K, suggesting a CDW transition at TCDW = 345 K, that corroborates the result from high-temperature SXRD measurements. Interestingly, R2T3X5 compounds are known to crystallize either in the tetragonal Sc2Fe3Si5 type structure or in the orthorhombic U2Co3Si5 structure type. Not all of the compounds in the R2T3X5 series undergo CDW phase transitions. We find that R2T3X5 compounds will exhibit a CDW transition, if the condition : 0.526 < c/sqrt(ab) < 0.543 is satisfied. We suggest the wave vector-dependent electron-phonon coupling to be the dominant mechanism of CDW formation in the tetragonal polymorph of Gd2Os3Si5."
https://arxiv.org/abs/2403.08659,2024-03-13,Fourier Quasicrystals on $\mathbb R^n$ Preliminary Report,"['Wayne M Lawton', 'August K. Tsikh']","This paper has three aims. First, for $n \geq 1$ we construct a family of real-rooted trigonometric polynomial maps $P : \mathbb C^n \mapsto \mathbb C^n$ whose divisors are Fourier Quasicrystals (FQ). For $n = 1$ these divisors include the first nontrivial FQ with positive integer coefficients constructed by Kurasov and Sarnak \cite{kurasovsarnak}, and for $n > 1$ they overlap with Meyer's curved model sets \cite{meyer6} and two-dimensional \cite{meyer7} and multidimensional \cite{meyer8} crystalline measures. We prove that the divisors are FQ by directly computing their Fourier transforms using a formula derived in \cite{lawton}. Second, we extend the relationship between real-rootedness and amoebas, derived for $n = 1$ by Alon, Cohen and Vinzant \cite{alon}, to the case $n > 1.$ The extension uses results in \cite{bushuevatsikh} about homology of complements of amoebas of algebraic sets of codimension $> 1.$ Third, we prove that the divisors of all uniformly generic real-rooted $P$ are FQ. The proof uses the formula relating Grothendieck residues and Newton polytopes derived by Gelfond and Khovanskii \cite{gelfondkhovanskii1} . Finally, we note that Olevskii and Ulanovskii [60] have proved that all FQ are divisors of real-rooted trigonometric polynomials for $n = 1$ but that the situation for $n > 1$ remains unsolved."
https://arxiv.org/abs/2403.08658,2024-03-13,A Distributed Adaptive Algorithm for Non-Smooth Spatial Filtering Problems in Wireless Sensor Networks,"['Charles Hovine', 'Alexander Bertrand']","A wireless sensor network often relies on a fusion center to process the data collected by each of its sensing nodes. Such an approach relies on the continuous transmission of raw data to the fusion center, which typically has a major impact on the sensors' battery life. To address this issue in the particular context of spatial filtering and signal fusion problems, we recently proposed the Distributed Adaptive Signal Fusion (DASF) algorithm, which distributively computes a spatial filter expressed as the solution of a smooth optimization problem involving the network-wide sensor signal statistics. In this work, we show that the DASF algorithm can be extended to compute the filters associated with a certain class of non-smooth optimization problems. This extension makes the addition of sparsity-inducing norms to the problem's cost function possible, allowing sensor selection to be performed in a distributed fashion, alongside the filtering task of interest, thereby further reducing the network's energy consumption. We provide a description of the algorithm, prove its convergence, and validate its performance and solution tracking capabilities with numerical experiments."
https://arxiv.org/abs/2403.08657,2024-03-13,Predicting long timescale kinetics under variable experimental conditions with Kinetica.jl,"['Joe Gilkes', 'Mark Storr', 'Reinhard J. Maurer', 'Scott Habershon']","Predicting the degradation processes of molecules over long timescales is a key aspect of industrial materials design. However, it is made computationally challenging by the need to construct large networks of chemical reactions that are relevant to the experimental conditions that kinetic models must mirror, with every reaction requiring accurate kinetic data. Here we showcase Kinetica.jl, a new software package for constructing large-scale chemical reaction networks in a fully-automated fashion by exploring chemical reaction space with a kinetics-driven algorithm; coupled to efficient machine-learning models of activation energies for sampled elementary reactions, we show how this approach readily enables generation and kinetic characterization of networks containing $\sim10^{3}$ chemical species and $10^{4}$ - $10^{5}$ reactions. Symbolic-numeric modelling of the generated reaction networks is used to allow for flexible, efficient computation of kinetic profiles under experimentally-realizable conditions such as continuously-variable temperature regimes, enabling direct connection between bottom-up reaction networks and experimental observations. Highly efficient propagation of long-timescale kinetic profiles is required for automated reaction network refinement and is enabled here by a new discrete kinetic approximation. The resulting Kinetica.jl simulation package therefore enables automated generation, characterization, and long-timescale modelling of complex chemical reaction systems. We demonstrate this for hydrocarbon pyrolysis simulated over timescales of seconds, using transient temperature profiles representing those of tubular flow reactor experiments."
https://arxiv.org/abs/2403.08656,2024-03-13,Physical Memory Attacks and a Memory Safe Management System for Memory Defense,"['Alon Hillel-Tuch', 'Aspen Olmstead']","Programming errors, defective hardware components (such as hard disk spindle defects), and environmental hazards can lead to invalid memory operations. In addition, less predictable forms of environmental stress, such as radiation, thermal influence, and energy fluctuations, can induce hardware faults. Sometimes, a soft error can occur instead of a complete failure, such as a bit-flip. The 'natural' factors that can cause bit-flips are replicable through targeted attacks that result in significant compromises, including full privileged system access. Existing physical defense solutions have consistently been circumvented shortly after deployment. We will explore the concept of a novel software-based low-level layer that can protect vulnerable memory targeted by physical attack vectors related to bit-flip vulnerabilities."
https://arxiv.org/abs/2403.08655,2024-03-13,Efficient electronic cooling above 2 K by niobium-based superconducting tunnel junctions,"['J. Hätinen', 'A. Ronzani', 'R. P. Loreto', 'E. Mykkänen', 'A. Kemppinen', 'K. Viisanen', 'T. Rantanen', 'J. Geisor', 'J. Lehtinen', 'M. Ribeiro', 'J-P. Kaikkonen', 'O. Prakash', 'V. Vesterinen', 'W. Förbom', 'E. T. Mannila', 'M. Kervinen', 'J. Govenius', 'M. Prunnila']","Numerous applications, from industrial non-destructive imaging through ultra-sensitive photon counting to various implementations of solid-state quantum computers require low temperatures for their sensor and processor chips. Replacing the bulky cryo-liquid based cooling stages of cryo-enabled instruments by chip scale refrigeration is envisioned to disruptively reduce the system size similarly as microprocessors did for computers. Chip scale cooling has been demonstrated with electronic refrigerators based on tunnel junctions in the sub-1 K temperature range. Here, we extend the operation temperature to above 2 K, thereby, bridging the gap between electronic and pulse tube refrigerators. We report on scalable Al-AlOx-Nb superconducting tunnel junction cooler technology that can deliver electronic cooling power of ~ mW/mm^2, which is enough to demonstrate significant electron temperature reduction of 0.82 K against the phonon bath at 2.4 K (34% relative cooling). Our work shows that the key material of integrated superconducting circuits (niobium) enables powerful cryogenic refrigerator technology. This result is a prerequisite for practical cryogenic chip scale refrigerators and, at the same time, it introduces a new electro-thermal tool for quantum heat transport experiments."
https://arxiv.org/abs/2403.08654,2024-03-13,An Efficient End-to-End Approach to Noise Invariant Speech Features via Multi-Task Learning,"['Heitor R. Guimarães', 'Arthur Pimentel', 'Anderson R. Avila', 'Mehdi Rezagholizadeh', 'Boxing Chen', 'Tiago H. Falk']","Self-supervised speech representation learning enables the extraction of meaningful features from raw waveforms. These features can then be efficiently used across multiple downstream tasks. However, two significant issues arise when considering the deployment of such methods ``in-the-wild"": (i) Their large size, which can be prohibitive for edge applications; and (ii) their robustness to detrimental factors, such as noise and/or reverberation, that can heavily degrade the performance of such systems. In this work, we propose RobustDistiller, a novel knowledge distillation mechanism that tackles both problems jointly. Simultaneously to the distillation recipe, we apply a multi-task learning objective to encourage the network to learn noise-invariant representations by denoising the input. The proposed mechanism is evaluated on twelve different downstream tasks. It outperforms several benchmarks regardless of noise type, or noise and reverberation levels. Experimental results show that the new Student model with 23M parameters can achieve results comparable to the Teacher model with 95M parameters. Lastly, we show that the proposed recipe can be applied to other distillation methodologies, such as the recent DPWavLM. For reproducibility, code and model checkpoints will be made available at \mbox{\url{https://github.com/Hguimaraes/robustdistiller}}."
https://arxiv.org/abs/2403.08653,2024-03-13,Physics-Guided Inverse Regression for Crop Quality Assessment,"['David Shulman', 'Assaf Israeli', 'Yael Botnaro', 'Ori Margalit', 'Oved Tamir', 'Shaul Naschitz', 'Dan Gamrasni', 'Ofer M. Shir', 'Itai Dattner']","We present an innovative approach leveraging Physics-Guided Neural Networks (PGNNs) for enhancing agricultural quality assessments. Central to our methodology is the application of physics-guided inverse regression, a technique that significantly improves the model's ability to precisely predict quality metrics of crops. This approach directly addresses the challenges of scalability, speed, and practicality that traditional assessment methods face. By integrating physical principles, notably Fick`s second law of diffusion, into neural network architectures, our developed PGNN model achieves a notable advancement in enhancing both the interpretability and accuracy of assessments. Empirical validation conducted on cucumbers and mushrooms demonstrates the superior capability of our model in outperforming conventional computer vision techniques in postharvest quality evaluation. This underscores our contribution as a scalable and efficient solution to the pressing demands of global food supply challenges."
https://arxiv.org/abs/2403.08652,2024-03-13,"Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks","['Paul Ardis', 'Arjuna Flenner']","Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence. In mission critical applications, it is important to both understand associated DNN reasoning and its supporting evidence. In this paper, we propose a novel Bayesian approach to extract explanations, justifications, and uncertainty estimates from DNNs. Our approach is efficient both in terms of memory and computation, and can be applied to any black box DNN without any retraining, including applications to anomaly detection and out-of-distribution detection tasks. We validate our approach on the CIFAR-10 dataset, and show that it can significantly improve the interpretability and reliability of DNNs."
https://arxiv.org/abs/2403.08651,2024-03-13,HAIFIT: Human-Centered AI for Fashion Image Translation,"['Jianan Jiang', 'Xinglin Li', 'Weiren Yu', 'Di Wu']","In the realm of fashion design, sketches serve as the canvas for expressing an artist's distinctive drawing style and creative vision, capturing intricate details like stroke variations and texture nuances. The advent of sketch-to-image cross-modal translation technology has notably aided designers. However, existing methods often compromise these sketch details during image generation, resulting in images that deviate from the designer's intended concept. This limitation hampers the ability to offer designers a precise preview of the final output. To overcome this challenge, we introduce HAIFIT, a novel approach that transforms sketches into high-fidelity, lifelike clothing images by integrating multi-scale features and capturing extensive feature map dependencies from diverse perspectives. Through extensive qualitative and quantitative evaluations conducted on our self-collected dataset, our method demonstrates superior performance compared to existing methods in generating photorealistic clothing images. Our method excels in preserving the distinctive style and intricate details essential for fashion design applications."
https://arxiv.org/abs/2403.08650,2024-03-13,Data Augmentation in Human-Centric Vision,"['Wentao Jiang', 'Yige Zhang', 'Shaozhong Zheng', 'Si Liu', 'Shuicheng Yan']","This survey presents a comprehensive analysis of data augmentation techniques in human-centric vision tasks, a first of its kind in the field. It delves into a wide range of research areas including person ReID, human parsing, human pose estimation, and pedestrian detection, addressing the significant challenges posed by overfitting and limited training data in these domains. Our work categorizes data augmentation methods into two main types: data generation and data perturbation. Data generation covers techniques like graphic engine-based generation, generative model-based generation, and data recombination, while data perturbation is divided into image-level and human-level perturbations. Each method is tailored to the unique requirements of human-centric tasks, with some applicable across multiple areas. Our contributions include an extensive literature review, providing deep insights into the influence of these augmentation techniques in human-centric vision and highlighting the nuances of each method. We also discuss open issues and future directions, such as the integration of advanced generative models like Latent Diffusion Models, for creating more realistic and diverse training data. This survey not only encapsulates the current state of data augmentation in human-centric vision but also charts a course for future research, aiming to develop more robust, accurate, and efficient human-centric vision systems."
https://arxiv.org/abs/2403.08649,2024-03-13,A Causal Inspired Early-Branching Structure for Domain Generalization,"['Liang Chen', 'Yong Zhang', 'Yibing Song', 'Zhen Zhang', 'Lingqiao Liu']","Learning domain-invariant semantic representations is crucial for achieving domain generalization (DG), where a model is required to perform well on unseen target domains. One critical challenge is that standard training often results in entangled semantic and domain-specific features. Previous works suggest formulating the problem from a causal perspective and solving the entanglement problem by enforcing marginal independence between the causal (\ie semantic) and non-causal (\ie domain-specific) features. Despite its simplicity, the basic marginal independent-based idea alone may be insufficient to identify the causal feature. By d-separation, we observe that the causal feature can be further characterized by being independent of the domain conditioned on the object, and we propose the following two strategies as complements for the basic framework."
https://arxiv.org/abs/2403.08648,2024-03-13,Meta Reinforcement Learning for Resource Allocation in Aerial Active-RIS-assisted Networks with Rate-Splitting Multiple Access,"['Sajad Faramarzi', 'Sepideh Javadi', 'Farshad Zeinali', 'Hosein Zarini', 'Mohammad Robat Mili', 'Mehdi Bennis', 'Yonghui Li', 'Kai-Kit Wong']","Mounting a reconfigurable intelligent surface (RIS) on an unmanned aerial vehicle (UAV) holds promise for improving traditional terrestrial network performance. Unlike conventional methods deploying passive RIS on UAVs, this study delves into the efficacy of an aerial active RIS (AARIS). Specifically, the downlink transmission of an AARIS network is investigated, where the base station (BS) leverages rate-splitting multiple access (RSMA) for effective interference management and benefits from the support of an AARIS for jointly amplifying and reflecting the BS's transmit signals. Considering both the non-trivial energy consumption of the active RIS and the limited energy storage of the UAV, we propose an innovative element selection strategy for optimizing the on/off status of RIS elements, which adaptively and remarkably manages the system's power consumption. To this end, a resource management problem is formulated, aiming to maximize the system energy efficiency (EE) by jointly optimizing the transmit beamforming at the BS, the element activation, the phase shift and the amplification factor at the RIS, the RSMA common data rate at users, as well as the UAV's trajectory. Due to the dynamicity nature of UAV and user mobility, a deep reinforcement learning (DRL) algorithm is designed for resource allocation, utilizing meta-learning to adaptively handle fast time-varying system dynamics. Simulations indicate that incorporating an active RIS at the UAV leads to substantial EE gain, compared to passive RIS-aided UAV. We observe the superiority of the RSMA-based AARIS system in terms of EE, compared to existing approaches adopting non-orthogonal multiple access (NOMA)."
https://arxiv.org/abs/2403.08647,2024-03-13,Rigidity of Einstein manifolds with positive Yamabe invariant,"['Letizia Branca', 'Giovanni Catino', 'Davide Dameno', 'Paolo Mastrolia']","We provide optimal pinching results on closed Einstein manifolds with positive Yamabe invariant in any dimension, extending the optimal bound for the scalar curvature due to Gursky and LeBrun in dimension four. We also improve the known bounds of the Yamabe invariant \emph{via} the $L^{\frac{n}{2}}$-norm of the Weyl tensor for low-dimensional Einstein manifolds. Finally, we discuss some advances on an algebraic inequality involving the Weyl tensor for dimensions $5$ and $6$."
https://arxiv.org/abs/2403.08646,2024-03-13,Covariance Fitting Interferometric Phase Linking: Modular Framework and Optimization Algorithms,"['Phan Viet Hoa Vu', 'Arnaud Breloy', 'Frédéric Brigui', 'Yajing Yan', 'Guillaume Ginolhac']","Interferometric phase linking (IPL) has become a prominent technique for processing images of areas containing distributed scaterrers in SAR interferometry. Traditionally, IPL consists in estimating consistent phase differences between all pairs of SAR images in a time series from the sample covariance matrix of pixel patches on a sliding window. This paper reformulates this task as a covariance fitting problem: in this setup, IPL appears as a form of projection of an input covariance matrix so that it satisfies the phase closure property. Given this modular formulation, we propose an overview of covariance matrix estimates, regularization options, and matrix distances, that can be of interest when processing multi-temporal SAR data. In particular, we will observe that most of the existing IPL algorithms appear as special instances of this framework. We then present tools to efficiently solve related optimization problems on the torus of phase-only complex vectors: majorization-minimization and Riemannian optimization. We conclude by illustrating the merits of different options on a real-world case study."
https://arxiv.org/abs/2403.08645,2024-03-13,Fractional distortion in hyperbolic groups,"['Pallavi Dani', 'Timothy Riley']","For all integers $p>q>0$ and $k >0$, and all non-elementary torsion-free hyperbolic groups $H$, we construct a hyperbolic group $G$ in which $H$ is a subgroup, such that the distortion function of $H$ in $G$ grows like $\exp^k(n^{p/q})$. Here, $\exp^k$ denotes the $k$-fold-iterated exponential function."
https://arxiv.org/abs/2403.08644,2024-03-13,Thermodynamic Integration for Dynamically Unstable Systems Using Interatomic Force Constants without Molecular Dynamics,"['Junsoo Park', 'Zhigang Wu', 'John W. Lawson']","We demonstrate an efficient and accurate, general-purpose first-principles blueprint for calculating anharmonic vibrational free energy and predicting structural phase transition temperatures of solids. Thermodynamic integration is performed without molecular dynamics using only interatomic force constants to model analogues of the true potential and generate their thermal ensembles. By replacing \textit{ab initio} molecular dynamics (AIMD) with statistical sampling of ensemble configurations and trading density-functional theory (DFT) energy calculations on each configuration for a set of matrix operations, our approach enables a faster thermodynamic integration by 4 orders of magnitude over the traditional route via AIMD. Experimental phase transition temperatures of a variety of strongly anharmonic materials with dynamical instabilities including shape-memory alloys are recovered to largely within 25% error. Such a combination of speed and accuracy enables the method to be deployed at a large-scale for predictive mapping of phase transition temperatures."
https://arxiv.org/abs/2403.08643,2024-03-13,On the Aw-Rascle-Zhang traffic models with nonlocal look-ahead interactions,"['Thomas Hamori', 'Changhui Tan']","We present a new family of second-order traffic flow models, extending the Aw-Rascle-Zhang (ARZ) model to incorporate nonlocal interactions. Our model includes a specific nonlocal Arrhenius-type look-ahead slowdown factor. We establish both local and global well-posedness theories for these nonlocal ARZ models."
https://arxiv.org/abs/2403.08642,2024-03-13,Reweight-annealing method for calculating the value of partition function via quantum Monte Carlo,"['Yi-Ming Ding', 'Nvsen Ma', 'Gaopei Pan', 'Chen Cheng', 'Zheng Yan']","Efficient and accurate algorithm for partition function, free energy and thermal entropy calculations is of great significance in statistical physics and quantum many-body physics. Here we present an unbiased but low-technical-barrier algorithm within the quantum Monte Carlo framework, which has exceptionally high accuracy and no systemic error. Compared with the conventional specific heat integral method and Wang-Landau sampling algorithm, our method can obtain a much more accurate result of the sub-leading coefficient of the entropy. This method can be widely used in both classical and quantum Monte Carlo simulations and is easy to be parallelized on computer."
https://arxiv.org/abs/2403.08641,2024-03-13,Air-coupled ultrasound using broadband shock waves from piezoelectric spark igniters,"['K. G. Scheuer', 'R. G. DeCorby']","We used an optomechanical sensor to study the ultrasound generated by manually operated piezoelectric spark igniters. These low-energy sparks produce short-duration acoustic shock-wave pulses, with sub-microsecond rise times and frequency content extending well beyond 2 MHz in air. The same source-receiver combination was then used to demonstrate broadband characterization of solid (polymer and glass) plates in a simple setup, where single spark events yielded high-SNR data without the need for critical alignment. This setup also enabled us to estimate pressure excursions approaching 105 Pa at millimeter-scale distances from the spark. The results are in large part made possible by the small size, wide bandwidth, and high sensitivity of the optomechanical sensor, and might be of interest for air-coupled ultrasound applications in non-destructive testing."
https://arxiv.org/abs/2403.08640,2024-03-13,Refractive COLMAP: Refractive Structure-from-Motion Revisited,"['Mengkun She', 'Felix Seegräber', 'David Nakath', 'Kevin Köser']","In this paper, we present a complete refractive Structure-from-Motion (RSfM) framework for underwater 3D reconstruction using refractive camera setups (for both, flat- and dome-port underwater housings). Despite notable achievements in refractive multi-view geometry over the past decade, a robust, complete and publicly available solution for such tasks is not available at present, and often practical applications have to resort to approximating refraction effects by the intrinsic (distortion) parameters of a pinhole camera model. To fill this gap, we have integrated refraction considerations throughout the entire SfM process within the state-of-the-art, open-source SfM framework COLMAP. Numerical simulations and reconstruction results on synthetically generated but photo-realistic images with ground truth validate that enabling refraction does not compromise accuracy or robustness as compared to in-air reconstructions. Finally, we demonstrate the capability of our approach for large-scale refractive scenarios using a dataset consisting of nearly 6000 images. The implementation is released as open-source at: https://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater."
https://arxiv.org/abs/2403.08639,2024-03-13,HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction,"['Yi Zhou', 'Hui Zhang', 'Jiaqian Yu', 'Yifan Yang', 'Sangil Jung', 'Seung-In Park', 'ByungIn Yoo']","Vectorized High-Definition (HD) map construction requires predictions of the category and point coordinates of map elements (e.g. road boundary, lane divider, pedestrian crossing, etc.). State-of-the-art methods are mainly based on point-level representation learning for regressing accurate point coordinates. However, this pipeline has limitations in obtaining element-level information and handling element-level failures, e.g. erroneous element shape or entanglement between elements. To tackle the above issues, we propose a simple yet effective HybrId framework named HIMap to sufficiently learn and interact both point-level and element-level information. Concretely, we introduce a hybrid representation called HIQuery to represent all map elements, and propose a point-element interactor to interactively extract and encode the hybrid information of elements, e.g. point position and element shape, into the HIQuery. Additionally, we present a point-element consistency constraint to enhance the consistency between the point-level and element-level information. Finally, the output point-element integrated HIQuery can be directly converted into map elements' class, point coordinates, and mask. We conduct extensive experiments and consistently outperform previous methods on both nuScenes and Argoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes dataset, remarkably superior to previous SOTAs by $8.3$ mAP at least."
https://arxiv.org/abs/2403.08638,2024-03-13,Disparate Effect Of Missing Mediators On Transportability of Causal Effects,"['Vishwali Mhasawade', 'Rumi Chunara']","Transported mediation effects provide an avenue to understand how upstream interventions (such as improved neighborhood conditions like green spaces) would work differently when applied to different populations as a result of factors that mediate the effects. However, when mediators are missing in the population where the effect is to be transported, these estimates could be biased. We study this issue of missing mediators, motivated by challenges in public health, wherein mediators can be missing, not at random. We propose a sensitivity analysis framework that quantifies the impact of missing mediator data on transported mediation effects. This framework enables us to identify the settings under which the conditional transported mediation effect is rendered insignificant for the subgroup with missing mediator data. Specifically, we provide the bounds on the transported mediation effect as a function of missingness. We then apply the framework to longitudinal data from the Moving to Opportunity Study, a large-scale housing voucher experiment, to quantify the effect of missing mediators on transport effect estimates of voucher receipt, an upstream intervention on living location, in childhood on subsequent risk of mental health or substance use disorder mediated through parental health across sites. Our findings provide a tangible understanding of how much missing data can be withstood for unbiased effect estimates."
https://arxiv.org/abs/2403.08637,2024-03-13,GR as a classical spin-2 theory?,"['Niels Linnemann', 'Chris Smeenk', 'Mark Robert Baker']","The self-interaction spin-2 approach to general relativity (GR) has been extremely influential in the particle physics community. Leaving no doubt regarding its heuristic value, we argue that a view of the metric field of GR as nothing but a stand-in for a self-coupling field in flat spacetime runs into a dilemma: either the view is physically incomplete in so far as it requires recourse to GR after all, or it leads to an absurd multiplication of alternative viewpoints on GR rendering any understanding of the metric field as nothing but a spin-2 field in flat spacetime unjustified."
https://arxiv.org/abs/2403.08636,2024-03-13,Coupling of quantum-dot states via elastic-cotunneling and crossed Andreev reflection in a minimal Kitaev chain,"['Zhi-Hai Liu', 'Chuanchang Zeng', 'H. Q. Xu']","Recently, exciting progress has been made in using the superconducting nanowires coupled to gate-defined quantum dots (QDs) to mimic the Kiteav chain and realize the Majorana-bound states via a poor man's route. The essential ingredient is to balance the interdot elastic-cotunneling (ECT) and crossed Andreev reflection (CAR). As theoretically proposed, this can be mediated by the Andreev bound states (ABSs) formed in the superconducting nanowires. However, most of the gate-tuning asymmetric features observed in experiments can not be captured using the current theoretical models. To address this insufficiency, here, we consider a full model that explicitly includes all the details of both the QD states and the ABSs. Remarkable agreement is found with the recent experimental observations, where our model correctly reveals the gate-tuning asymmetry in ECTs and by which the average QD state energy can also be extracted. In contrast, CARs do not depend on the tuning of QD states. Moreover, armed with the tunability of ECTs and CARs with QD states, we also predict a controllable anisotropic superexchange interaction between electron spins in the two separated QDs."
https://arxiv.org/abs/2403.08635,2024-03-13,Human Alignment of Large Language Models through Online Preference Optimisation,"['Daniele Calandriello', 'Daniel Guo', 'Remi Munos', 'Mark Rowland', 'Yunhao Tang', 'Bernardo Avila Pires', 'Pierre Harvey Richemond', 'Charline Le Lan', 'Michal Valko', 'Tianqi Liu', 'Rishabh Joshi', 'Zeyu Zheng', 'Bilal Piot']","Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD."
https://arxiv.org/abs/2403.08634,2024-03-13,Bispectrum non-Gaussian Covariance in Redshift Space,"['Jacopo Salvalaggio', 'Lina Castiblanco', 'Jorge Noreña', 'Emiliano Sefusatti', 'Pierluigi Monaco']","We provide an analytical description of the galaxy bispectrum covariance and the power spectrum-bispectrum cross-covariance in redshift space that captures the dominant non-Gaussian contributions. The Gaussian prediction for the variance of the halo bispectrum monopole significantly underestimates numerical estimates particularly for squeezed triangles, that is bispectrum triangular configurations where one side is much smaller than the other two, whereas the effect is relatively less important when considering the quadrupole. We propose an expression for the missing non-Gaussian contribution valid in the squeezed limit that requires an accurate modeling of the bispectrum alone. We validate our model against the numerical covariance estimated from a large suite of mock catalogs and find that it accurately predicts the variance as well as the dominant off-diagonal terms. We also present an expression for the cross-covariance between power spectrum and bispectrum multipoles and likewise find it to provide a good description of the numerical results."
https://arxiv.org/abs/2403.08633,2024-03-13,Entangled Photon-pair Generation in Nonlinear Thin-films,"['Elkin A. Santos', 'Maximilian A. Weissflog', 'Thomas Pertsch', 'Frank Setzpfandt', 'Sina Saravi']","We develop a fully vectorial and non-paraxial formalism to describe spontaneous parametric down-conversion in nonlinear thin films. The formalism is capable of treating slabs with a sub-wavelength thickness, describe the associated Fabry-Pérot effects, and even treat absorptive nonlinear materials. With this formalism, we perform an in-depth study of the dynamics of entangled photon-pair generation in nonlinear thin films, to provide a needed theoretical understanding for such systems that have recently attracted much experimental attention as sources of photon pairs. As an important example, we study the far-field radiation properties of photon pairs generated from a high-refractive-index nonlinear thin-film with Zinc-Blende structure, that is deposited on a linear low-refractive-index substrate. In particular, we study the thickness-dependent effect of Fabry-Pérot interferences on the far-field radiation pattern of the photon pairs. We also pay special attention to study of entanglement generation, and find the conditions under which maximally polarization-entangled photon pairs can be generated and detected in such nonlinear thin-films."
https://arxiv.org/abs/2403.08632,2024-03-13,A Decade's Battle on Dataset Bias: Are We There Yet?,"['Zhuang Liu', 'Kaiming He']","We revisit the ""dataset classification"" experiment suggested by Torralba and Efros a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be simply explained by memorization. We hope our discovery will inspire the community to rethink the issue involving dataset bias and model capabilities."
https://arxiv.org/abs/2403.08631,2024-03-13,Evolution of cold streams in hot gaseous halos,"['WenSheng Hong', 'Weishan Zhu', 'TianRui Wang', 'Xiaohu Yang', 'LongLong Feng']","In the prevailing model of galaxy formation and evolution, the process of gas accretion onto central galaxies undergoes a transition from cold-dominated to hot-dominated modes. This shift occurs when the mass of the parent dark matter halos exceeds a critical threshold known as $M_{shock}$. Moreover, cold gas usually flows onto central galaxies through filamentary structures, currently referred to as cold streams. However, the evolution of cold streams in halos with masses around $M_{shock}$, particularly how they are disrupted, remains unclear. To address this issue, we conduct a set of idealised hydrodynamic simulations. Our simulations show that (1) for a gas metallicity $Z=0.001-0.1Z_{\odot}$, cold stream with an inflow rate $\sim 3\, \rm{M_{\odot}}/yr$ per each can persist and effectively transport cold and cool gas to the central region ($< 0.2$ virial radius) in halos with mass $10^{12}\, \rm{M_{\odot}}$, but is disrupted at a radius around $0.2$ virial radius due to compression heating for halos with mass $3 \times 10^{12}\, \rm{M_{\odot}}$. (2) At $z\sim 2$, the maximum halo mass that capable of hosting and sustaining cold streams $M_{stream}$ is between $1\times 10^{12} \rm{M_{\odot}}$ and $1.5\times 10^{12}\rm{M_{\odot}}$ for gas metallicity $Z=0.001Z_{\odot}$, while for a higher gas metallicity $Z=0.1Z_{\odot}$, this value increases to $\sim 1.5\times 10^{12}\rm{M_{\odot}}$. (3) The evolution and ultimate fate of cold streams are determined primarily by the rivalry between radiative cooling and compression. Stronger heating due to compression in halos more massive than $M_{stream}$ can surpass cooling and heat the gas in cold streams to the hot ($\geq 10^6\,$ K) phase."
https://arxiv.org/abs/2403.08630,2024-03-13,Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting,"['Guy P Nason', 'James L. Wei']","This article combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, focusing on three main contributions. Firstly, we consider the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase. Secondly, we compare the use of both the non-decimated wavelet transform and the non-decimated wavelet packet transform for computing these features, the latter providing a much larger set of potentially useful coefficient vectors. The wavelet coefficients are computed using a shifted version of the typical pyramidal algorithm to ensure no leakage of future information into these inputs. Thirdly, we evaluate the use of these wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-temporal models, and both statistical and deep learning-based methods. The latter include state-of-the-art transformer-based neural network architectures. Our experiments suggest significant benefit in replacing higher-order lagged features with wavelet features across all examined non-temporal methods for one-step-forward forecasting, and modest benefit when used as inputs for temporal deep learning-based models for long-horizon forecasting."
https://arxiv.org/abs/2403.08629,2024-03-13,Scaling Up Dynamic Human-Scene Interaction Modeling,"['Nan Jiang', 'Zhiyuan Zhang', 'Hongjie Li', 'Xiaoxuan Ma', 'Zan Wang', 'Yixin Chen', 'Tengyu Liu', 'Yixin Zhu', 'Siyuan Huang']","Confronting the challenges of data scarcity and advanced motion synthesis in human-scene interaction modeling, we introduce the TRUMANS dataset alongside a novel HSI motion synthesis method. TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes. It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact. This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model that efficiently generates HSI sequences of any length, taking into account both scene context and intended actions. In experiments, our approach shows remarkable zero-shot generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies."
https://arxiv.org/abs/2403.08628,2024-03-13,Optimal sub-Gaussian variance proxy for truncated Gaussian and exponential random variables,"['Mathias Barreto', 'Olivier Marchal', 'Julyan Arbel']","This paper establishes the optimal sub-Gaussian variance proxy for truncated Gaussian and truncated exponential random variables. The proofs rely on first characterizing the optimal variance proxy as the unique solution to a set of two equations and then observing that for these two truncated distributions, one may find explicit solutions to this set of equations. Moreover, we establish the conditions under which the optimal variance proxy coincides with the variance, thereby characterizing the strict sub-Gaussianity of the truncated random variables. Specifically, we demonstrate that truncated Gaussian variables exhibit strict sub-Gaussian behavior if and only if they are symmetric, meaning their truncation is symmetric with respect to the mean. Conversely, truncated exponential variables are shown to never exhibit strict sub-Gaussian properties. These findings contribute to the understanding of these prevalent probability distributions in statistics and machine learning, providing a valuable foundation for improved and optimal modeling and decision-making processes."
https://arxiv.org/abs/2403.08627,2024-03-13,Multifidelity linear regression for scientific machine learning from scarce data,"['Elizabeth Qian', 'Anirban Chaudhuri', 'Dayoung Kang', 'Vignesh Sella']","Machine learning (ML) methods, which fit to data the parameters of a given parameterized model class, have garnered significant interest as potential methods for learning surrogate models for complex engineering systems for which traditional simulation is expensive. However, in many scientific and engineering settings, generating high-fidelity data on which to train ML models is expensive, and the available budget for generating training data is limited. ML models trained on the resulting scarce high-fidelity data have high variance and are sensitive to vagaries of the training data set. We propose a new multifidelity training approach for scientific machine learning that exploits the scientific context where data of varying fidelities and costs are available; for example high-fidelity data may be generated by an expensive fully resolved physics simulation whereas lower-fidelity data may arise from a cheaper model based on simplifying assumptions. We use the multifidelity data to define new multifidelity Monte Carlo estimators for the unknown parameters of linear regression models, and provide theoretical analyses that guarantee the approach's accuracy and improved robustness to small training budgets. Numerical results verify the theoretical analysis and demonstrate that multifidelity learned models trained on scarce high-fidelity data and additional low-fidelity data achieve order-of-magnitude lower model variance than standard models trained on only high-fidelity data of comparable cost. This illustrates that in the scarce data regime, our multifidelity training strategy yields models with lower expected error than standard training approaches."
https://arxiv.org/abs/2403.08626,2024-03-13,Pulse-echo ultrasound attenuation tomography,"['Naiara Korta Martiartu', 'Parisa Salemi Yolgunlu', 'Martin Frenz', 'Michael Jaeger']","We present the first fully two-dimensional attenuation imaging technique developed for pulse-echo ultrasound systems. Unlike state-of-the-art techniques, which use line-by-line acquisitions, our method uses steered emissions to constrain attenuation values at each location with multiple crossing wave paths, essential to resolve the spatial variations of this tissue property. At every location, we compute normalized cross-correlations between the beamformed images that are obtained from emissions at different steering angles. We demonstrate that their log-amplitudes provide the changes between attenuation-induced amplitude losses undergone by the different incident waves. This allows us to formulate a linear tomographic problem, which we efficiently solve via a Tikhonov-regularized least-squares approach. The performance of our tomography technique is first validated in numerical examples and then experimentally demonstrated in custom-made tissue-mimicking phantoms with inclusions of varying size, echogenicity, and attenuation. We show that this technique is particularly good at resolving lateral variations in tissue attenuation and remains accurate in media with varying echogenicity. Based on a similar principle, this method can be easily combined with computed ultrasound tomography in echo mode (CUTE) for speed-of-sound imaging, paving the way towards a multi-modal ultrasound tomography framework characterizing multiple acoustic tissue properties simultaneously."
https://arxiv.org/abs/2403.08625,2024-03-13,Variance Minimisation of the Lipkin-Meshkov-Glick Model on a Quantum Computer,"['Isaac Hobday', 'Paul Stevenson', 'James Benstead']","Quantum computing can potentially provide advantages for specific computational tasks. The simulation of fermionic systems is one such task that lends itself well to quantum computation, with applications in nuclear physics and electronic systems. Here we present work in which we use a variance minimisation method to find the full spectrum of energy eigenvalues of the Lipkin-Meshkov-Glick model; an exactly-solvable nuclear shell model-type system. We perform these calculations using both quantum simulators and real quantum hardware accessed via IBM cloud-based quantum computers. Using these IBM quantum computers we are able to obtain all eigenvalues for the cases of three and seven fermions (nucleons) in the Lipkin-Meshkov-Glick model."
https://arxiv.org/abs/2403.08624,2024-03-13,Towards a Privacy and Security-Aware Framework for Ethical AI: Guiding the Development and Assessment of AI Systems,"['Daria Korobenko', 'Anastasija Nikiforova', 'Rajesh Sharma']","As artificial intelligence continues its unprecedented global expansion, accompanied by a proliferation of benefits, an increasing apprehension about the privacy and security implications of AI-enabled systems emerges. The pivotal question of effectively controlling AI development at both jurisdictional and organizational levels has become a prominent theme in contemporary discourse. While the European Parliament and Council have taken a decisive step by reaching a political agreement on the EU AI Act, the first comprehensive AI law, organizations still find it challenging to adapt to the fast-evolving AI landscape, lacking a universal tool for evaluating the privacy and security dimensions of their AI models and systems. In response to this critical challenge, this study conducts a systematic literature review spanning the years 2020 to 2023, with a primary focus on establishing a unified definition of key concepts in AI Ethics, particularly emphasizing the domains of privacy and security. Through the synthesis of knowledge extracted from the SLR, this study presents a conceptual framework tailored for privacy- and security-aware AI systems. This framework is designed to assist diverse stakeholders, including organizations, academic institutions, and governmental bodies, in both the development and critical assessment of AI systems. Essentially, the proposed framework serves as a guide for ethical decision-making, fostering an environment wherein AI is developed and utilized with a strong commitment to ethical principles. In addition, the study unravels the key issues and challenges surrounding the privacy and security dimensions, delineating promising avenues for future research, thereby contributing to the ongoing dialogue on the globalization and democratization of AI ethics."
https://arxiv.org/abs/2403.08623,2024-03-13,The algebraic structure of hyperbolic graph braid groups,"['B. Appiah', 'P. Dani', 'W. Ge', 'C. Hudson', 'S. Jain', 'Y. Lee', 'M. Lemoine', 'J. Murphy', 'J. Murray', 'A. Pandikkadan', 'K. Schreve']","Genevois has recently classified which graph braid groups on $\ge 3$ strands are word hyperbolic. In the $3$-strand case, he asked whether all such word hyperbolic groups are actually free; this reduced to checking two infinite classes of graphs. We give positive and negative answers to this question: one class of graphs always has free $3$-strand braid group, while the braid groups in the other class usually contain surface subgroups."
https://arxiv.org/abs/2403.08622,2024-03-13,Environment-Induced Information Scrambling Transition with Charge Conservations,"['Pengfei Zhang', 'Zhenhua Yu']","In generic closed quantum systems, the complexity of operators increases under time evolution governed by the Heisenberg equation, reflecting the scrambling of local quantum information. However, when systems interact with an external environment, the system-environment coupling allows operators to escape from the system, inducing a dynamical transition between the scrambling phase and the dissipative phase. This transition is known as the environment-induced information scrambling transition, originally proposed in Majorana fermion systems. In this work, we advance this dicovery by investigating the transition in charge-conserved systems with space-time randomness. We construct solvable Brownian Sachdev-Ye-Kitaev models of complex fermions coupled to an environment, enabling the analytical computation of operator growth. We determine the critical dissipation strength, which is proportional to $n(1-n)$ with $n$ being the density of the complex fermions, arising from the suppression in the quantum Lyapunov exponent due to the Pauli blockade in the scattering process. We further analyze the density dependence of maximally scrambled operators at late time. Our results shed light on the intriguing interplay between information scrambling, dissipation, and conservation laws."
https://arxiv.org/abs/2403.08621,2024-03-13,Spin-resolved counting statistics as a sensitive probe of spin correlation in transport through a quantum dot spin valve,"['Guanjian Hu', 'Shikuan Wang', 'Jing Hu', 'RuiQiang Li', 'Yiying Yan', 'JunYan Luo']","We investigate the noise in spin transport through a single quantum dot (QD) tunnel coupled to ferromagnetic electrodes with noncollinear magnetizations. Based on a spin-resolved quantum master equation, auto- and cross-correlations of spin-resolved currents are analyzed to reveal the underlying spin transport dynamics and characteristics for various polarizations. We find the currents of majority and minority spins could be strongly autocorrelated despite uncorrelated charge transfer. The interplay between tunnel coupling and the Coulomb interaction gives rise to an exchange magnetic field, leading to the precession of the accumulated spin in the QD. It strongly suppresses the bunching of spin tunneling events and results in a unique double-peak structure in the noise of the net spin current. The spin autocorrelation is found to be susceptible to magnetization alignments, which may serve as a sensitive tool to measure the magnetization directions between the ferromagnetic electrodes."
https://arxiv.org/abs/2403.08620,2024-03-13,Demailly-Lelong numbers on complex spaces,['Chung-Ming Pan'],"We establish a pointwise comparison of two notions of Lelong numbers of plurisubharmonic functions defined on singular complex spaces. This shows a conjecture proposed by Berman-Boucksom-Eyssidieux-Guedj-Zeriahi, affirming that the Demailly-Lelong number can be determined through a combination of intersection numbers given by the divisorial part of the potential and the SNC divisors over a log resolution of the maximal ideal of a given point. We also provide an estimate for quotient singularities and sharp estimates for two-dimensional ADE singularities."
https://arxiv.org/abs/2403.08619,2024-03-13,Measurements of the charge ratio and polarization of cosmic-ray muons with the Super-Kamiokande detector,"['H. Kitagawa', 'T. Tada', 'K. Abe', 'C. Bronner', 'Y. Hayato', 'K. Hiraide', 'K. Hosokawa', 'K. Ieki', 'M. Ikeda', 'J. Kameda', 'Y. Kanemura', 'R. Kaneshima', 'Y. Kashiwagi', 'Y. Kataoka', 'S. Miki', 'S. Mine', 'M. Miura', 'S. Moriyama', 'Y. Nakano', 'M. Nakahata', 'S. Nakayama', 'Y. Noguchi', 'K. Okamoto', 'K. Sato', 'H. Sekiya']","We present the results of the charge ratio ($R$) and polarization ($P^μ_{0}$) measurements using the decay electron events collected from 2008 September to 2022 June by the Super-Kamiokande detector. Because of its underground location and long operation, we performed high precision measurements by accumulating cosmic-ray muons. We measured the muon charge ratio to be $R=1.32 \pm 0.02$ $(\mathrm{stat.}{+}\mathrm{syst.})$ at $E_μ\cos θ_{\mathrm{Zenith}}=0.7^{+0.3}_{-0.2}$ $\mathrm{TeV}$, where $E_μ$ is the muon energy and $θ_{\mathrm{Zenith}}$ is the zenith angle of incoming cosmic-ray muons. This result is consistent with the Honda flux model while this suggests a tension with the $πK$ model of $1.9σ$. We also measured the muon polarization at the production location to be $P^μ_{0}=0.52 \pm 0.02$ $(\mathrm{stat.}{+}\mathrm{syst.})$ at the muon momentum of $0.9^{+0.6}_{-0.1}$ $\mathrm{TeV}/c$ at the surface of the mountain; this also suggests a tension with the Honda flux model of $1.5σ$. This is the most precise measurement ever to experimentally determine the cosmic-ray muon polarization near $1~\mathrm{TeV}/c$. These measurement results are useful to improve the atmospheric neutrino simulations."
https://arxiv.org/abs/2403.08618,2024-03-13,Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples,"['Sangamesh Kodge', 'Deepak Ravikumar', 'Gobinda Saha', 'Kaushik Roy']","Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness on both synthetic and real-world label noise. Experiments on the CIFAR dataset with 25% synthetic corruption show 7.36% generalization improvements on average. Additionally, we observe generalization improvements of up to 2.63% on naturally corrupted datasets like WebVision1.0 and Clothing1M."
https://arxiv.org/abs/2403.08617,2024-03-13,A semidefinite programming characterization of the Crawford number,"['Shmuel Friedland', 'Cynthia Vinzant']",We give a semidefinite programming characterization of the Crawford number. We show that the computation of the Crawford number within $\varepsilon$ precision is computable in polynomial time in the data and $|\log \varepsilon |$.
https://arxiv.org/abs/2403.08616,2024-03-13,An upper bound for the second moment of the length of the period of the continued fraction expansion for $\sqrt{d}$,['M. A. Korolev'],"If $d$ is not a perfect square, we define $T(d)$ as the length of the minimal period of the simple continued fraction expansion for $\sqrt{d}$. Otherwise, we put $T(d) = 0$. In the recent paper (2024), F.Battistoni, L.Grenié and G.Molteni established (in particular) an upper bound for the second moment of $T(d)$ over the segment $x<d\leqslant 2x$. As a corollary, they derived a new upper estimate for the number of $d$ such that $T(d)>α\sqrt{x}$. In this paper, we improve slightly this result of three authors."
https://arxiv.org/abs/2403.08615,2024-03-13,TopoTB: A software package for calculating the electronic structure and topological properties of the tight-binding model,"['Xinliang Huang', 'Fawei Zheng', 'Ning Hao']","We present TopoTB, a software package written in the Mathematica language, designed to compute electronic structures, topological properties, and phase diagrams based on tight-binding models. TopoTB is user-friendly, with an interactive user interface that enables the tuning of model parameters for fitting the target energy bands in a WYSIWYG way. In addition, TopoTB also includes functionalities for processing results from Density Functional Theory calculations. The outputs of TopoTB are rich and readable, and they can be displayed in various styles. These features make TopoTB a useful tool for the theoretical study of materials."
https://arxiv.org/abs/2403.08614,2024-03-13,Real-Time Sensor-Based Feedback Control for Obstacle Avoidance in Unknown Environments,"['Lyes Smaili', 'Soulaimane Berkane']","We revisit the Safety Velocity Cones (SVCs) obstacle avoidance approach for real-time autonomous navigation in an unknown $n$-dimensional environment. We propose a locally Lipschitz continuous implementation of the SVC controller using the distance-to-the-obstacle function and its gradient. We then show that the proposed implementation guarantees safe navigation in generic environments and almost globally asymptotic stability (AGAS) of the desired destination when the workspace contains strongly convex obstacles. The proposed computationally efficient control algorithm can be implemented onboard vehicles equipped with limited range sensors (e.g., LiDAR, depth camera), allowing the controller to be locally evaluated without requiring prior knowledge of the environment."
https://arxiv.org/abs/2403.08613,2024-03-13,Link Prediction for Social Networks using Representation Learning and Heuristic-based Features,"['Samarth Khanna', 'Sree Bhattacharyya', 'Sudipto Ghosh', 'Kushagra Agarwal', 'Asit Kumar Das']","The exponential growth in scale and relevance of social networks enable them to provide expansive insights. Predicting missing links in social networks efficiently can help in various modern-day business applications ranging from generating recommendations to influence analysis. Several categories of solutions exist for the same. Here, we explore various feature extraction techniques to generate representations of nodes and edges in a social network that allow us to predict missing links. We compare the results of using ten feature extraction techniques categorized across Structural embeddings, Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics, followed by modeling with ensemble classifiers and custom Neural Networks. Further, we propose combining heuristic-based features and learned representations that demonstrate improved performance for the link prediction task on social network datasets. Using this method to generate accurate recommendations for many applications is a matter of further study that appears very promising. The code for all the experiments has been made public."
https://arxiv.org/abs/2403.08612,2024-03-13,Tangential Fixpoint Iterations for Gromov-Wasserstein Barycenters,"['Florian Beier', 'Robert Beinert']","The Gromov-Wasserstein (GW) transport problem is a relaxation of classic optimal transport, which seeks a transport between two measures while preserving their internal geometry. Due to meeting this theoretical underpinning, it is a valuable tool for the analysis of objects that do not possess a natural embedding or should be studied independently of it. Prime applications can thus be found in e.g. shape matching, classification and interpolation tasks. To tackle the latter, one theoretically justified approach is the employment of multi-marginal GW transport and GW barycenters, which are Fréchet means with respect to the GW distance. However, because the computation of GW itself already poses a quadratic and non-convex optimization problem, the determination of GW barycenters is a hard task and algorithms for their computation are scarce. In this paper, we revisit a known procedure for the determination of Fréchet means in Riemannian manifolds via tangential approximations in the context of GW. We provide a characterization of barycenters in the GW tangent space, which ultimately gives rise to a fixpoint iteration for approximating GW barycenters using multi-marginal plans. We propose a relaxation of this fixpoint iteration and show that it monotonously decreases the barycenter loss. In certain cases our proposed method naturally provides us with barycentric embeddings. The resulting algorithm is capable of producing qualitative shape interpolations between multiple 3d shapes with support sizes of over thousands of points in reasonable time. In addition, we verify our method on shape classification and multi-graph matching tasks."
https://arxiv.org/abs/2403.08611,2024-03-13,Galaxy dispersion measured by Fast Radio Bursts as a probe of baryonic feedback models,"['Alexander Theis', 'Steffen Hagstotz', 'Robert Reischke', 'Jochen Weller']","Fast Radio Bursts (FRBs) are a sensitive probe of the electron distribution in both the large-scale structure and their host galaxies through the dispersion measure (DM) of the radio pulse. Baryonic feedback models are crucial for modelling small scales for ongoing cosmological surveys that are expected to change the electron distribution in galaxies in a way that can be probed by FRB observations. In this paper, we explore the impact of baryonic feedback on FRB hosts using numerical simulations and make a detailed study of the host galaxy dispersion as a function of redshift, galaxy type, feedback model and how these properties vary in independent simulation codes. We find that the host galaxy dispersion varies dramatically between different implementations of baryonic feedback, allowing FRBs with host identification to be a valuable probe of feedback physics and thus provide necessary priors for upcoming analysis of the statistical properties of the large-scale structure."
https://arxiv.org/abs/2403.08610,2024-03-13,An Algorithmic Theory of Simplicity in Mechanism Design,"['Diodato Ferraioli', 'Carmine Ventre']","A growing body of work in economics and computation focuses on the trade-off between implementability and simplicity in mechanism design. The goal is to develop a theory that not only allows to design an incentive structure easy to grasp for imperfectly rational agents, but also understand the ensuing limitations on the class of mechanisms that enforce it. In this context, the concept of OSP mechanisms has assumed a prominent role since they provably account for the absence of contingent reasoning skills, a specific cognitive limitation. For single-dimensional agents, it is known that OSP mechanisms need to use certain greedy algorithms."
https://arxiv.org/abs/2403.08609,2024-03-13,On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors,"['Tim Rensmeyer', 'Oliver Niggemann']","Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction. Bayesian neural networks are a promising approach for modeling uncertainties in deep neural networks. Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge. One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand. Over the past years, several papers have introduced sampling algorithms with claims that they achieve this property. However, do they indeed converge to the correct distribution? In this paper, we demonstrate that these methods can have a substantial bias in the distribution they sample, even in the limit of vanishing step sizes and at full batch size."
https://arxiv.org/abs/2403.08608,2024-03-13,Nuclear coherent $π^0$ photoproduction with charge-exchange and spin-flip rescattering,"['Viacheslav Tsaran', 'Marc Vanderhaeghen']","In this work, we present an updated model for nuclear $π^0$ photoproduction, which incorporates pion second-order rescattering on intermediate excited nuclear states. Our approach is based on the distorted wave impulse approximation in momentum space. The many-body medium effects are incorporated in the complex effective $Δ$ self-energy, employing the results of the recently developed second-order pion-nucleus scattering potential. The experimental data for ${}^{12}$C and ${}^{40}$Ca are successfully described without the need to fit the model parameters of the photoproduction amplitude as a result of incorporating the second-order part of nuclear photoproduction potential, which involves intermediate nucleon spin-flip and charge exchange."
https://arxiv.org/abs/2403.08607,2024-03-13,MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models,"['Subash Neupane', 'Shaswata Mitra', 'Sudip Mittal', 'Noorbakhsh Amiri Golilarz', 'Shahram Rahimi', 'Amin Amirlatifi']","Large Language Models (LLMs) have shown impressive capabilities in generating human-like responses. However, their lack of domain-specific knowledge limits their applicability in healthcare settings, where contextual and comprehensive responses are vital. To address this challenge and enable the generation of patient-centric responses that are contextually relevant and comprehensive, we propose MedInsight:a novel retrieval augmented framework that augments LLM inputs (prompts) with relevant background information from multiple sources. MedInsight extracts pertinent details from the patient's medical record or consultation transcript. It then integrates information from authoritative medical textbooks and curated web resources based on the patient's health history and condition. By constructing an augmented context combining the patient's record with relevant medical knowledge, MedInsight generates enriched, patient-specific responses tailored for healthcare applications such as diagnosis, treatment recommendations, or patient education. Experiments on the MTSamples dataset validate MedInsight's effectiveness in generating contextually appropriate medical responses. Quantitative evaluation using the Ragas metric and TruLens for answer similarity and answer correctness demonstrates the model's efficacy. Furthermore, human evaluation studies involving Subject Matter Expert (SMEs) confirm MedInsight's utility, with moderate inter-rater agreement on the relevance and correctness of the generated responses."
https://arxiv.org/abs/2403.08606,2024-03-13,Effect of Earth's Oblateness on Black Hole Imaging Through Earth-Space and Space-Space VLBI,"['Aditya Tamar', 'Ben Hudson', 'Daniel Palumbo']","Earth-based Very Long Baseline Interferometry (VLBI) has made rapid advances in imaging black holes. However, due to the limitations imposed on terrestrial VLBI by the Earth's finite size and turbulent atmosphere, it is imperative to have a space-based component in future VLBI missions. Herein, this paper investigates the effect of Earth's oblateness, also known as the $J_{2}$ effect, on orbiters in Earth-Space and Space-Space VLBI. The paper provides an extensive discussion on how the $J_{2}$ effect can directly impact orbit selection for black hole observations and how through informed choices of orbital parameters, the effect can be used to the mission's advantage, a fact that has not been addressed in existing space-VLBI investigations. We provide a comprehensive study of how the orbital parameters of several current space VLBI proposals will vary specifically due to the $J_{2}$ effect. For black hole accretion flow targets of interest, we have demonstrated how the $J_{2}$ effect leads to modest increase in shorter baseline coverage, filling gaps in the $(u,v)$ plane. Subsequently, we construct a simple analytical formalism that allows isolation of the impact of the $J_{2}$ effect on the $(u,v)$ plane without requiring computationally intensive orbit propagation simulations. By directly constructing $(u,v)$ coverage using the $J_{2}$ affected and invariant equations of motion, we obtain distinct coverage patterns for M87* and SgrA* that show extremely dense coverage on short baselines as well as long term orbital stability on longer baselines."
https://arxiv.org/abs/2403.08605,2024-03-13,Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation,"['Daniel Honerkamp', 'Martin Buchner', 'Fabien Despinoy', 'Tim Welschehold', 'Abhinav Valada']","To fully leverage the capabilities of mobile manipulation robots, it is imperative that they are able to autonomously execute long-horizon tasks in large unexplored environments. While large language models (LLMs) have shown emergent reasoning skills on arbitrary tasks, existing work primarily concentrates on explored environments, typically focusing on either navigation or manipulation tasks in isolation. In this work, we propose MoMa-LLM, a novel approach that grounds language models within structured representations derived from open-vocabulary scene graphs, dynamically updated as the environment is explored. We tightly interleave these representations with an object-centric action space. The resulting approach is zero-shot, open-vocabulary, and readily extendable to a spectrum of mobile manipulation and household robotic tasks. We demonstrate the effectiveness of MoMa-LLM in a novel semantic interactive search task in large realistic indoor environments. In extensive experiments in both simulation and the real world, we show substantially improved search efficiency compared to conventional baselines and state-of-the-art approaches, as well as its applicability to more abstract tasks. We make the code publicly available at http://moma-llm.cs.uni-freiburg.de."
https://arxiv.org/abs/2403.08604,2024-03-13,DevBench: A Comprehensive Benchmark for Software Development,"['Bowen Li', 'Wenhan Wu', 'Ziwei Tang', 'Lin Shi', 'John Yang', 'Jinyang Li', 'Shunyu Yao', 'Chen Qian', 'Binyuan Hui', 'Qicheng Zhang', 'Zhiyin Yu', 'He Du', 'Ping Yang', 'Dahua Lin', 'Chao Peng', 'Kai Chen']","Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications. Our benchmark is available at https://github.com/open-compass/DevBench"
https://arxiv.org/abs/2403.08603,2024-03-13,Hyperbolic Anderson equations with general time-independent Gaussian noise: Stratonovich regime,"['Xia Chen', 'Yaozhong Hu']","In this paper, we investigate the"
https://arxiv.org/abs/2403.08602,2024-03-13,Exploring global symmetry-breaking superradiant phase via phase competition,"['Hai-Chao Li', 'Wen Huang', 'Wei Xiong']","Superradiant phase transitions play a fundamental role in understanding the mechanism of collective light-matter interaction at the quantum level. Here we investigate multiple superradiant phases and phase transitions with different symmetry-breaking patterns in a two-mode V-type Dicke model. Interestingly, we show that there exists a quadruple point where one normal phase, one global symmetry-breaking superradiant phase and two local symmetry-breaking superradiant phases meet. Such a global phase results from the phase competition between two local superradiant phases and can not occur in the standard $Λ$- and $Ξ$-type three-level configurations in quantum optics. Moreover, we exhibit a sequential first-order quantum phase transition from one local to the global again to the other local superradiant phase. Our study opens up a perspective of exploring multi-level quantum critical phenomena with global symmetry breaking."
https://arxiv.org/abs/2403.08601,2024-03-13,Toward mapping turbulence in the intracluster medium III. Constraints on the turbulent power spectrum with Athena/X-IFU,"['Sophie Beaumont', 'Alexeï Molin', 'Nicolas Clerc', 'Étienne Pointecouteau', 'Mélina Vanel', 'Edoardo Cucchetti', 'Philippe Peille', 'François Pajot']","Context. Future X-ray observatories with high spectral resolution and imaging capabilities will enable measurements and mappings of emission line shifts in the intracluster medium (ICM). Such direct measurements can serve as unique probes of turbulent motions in the ICM. Determining the level and scales of turbulence will improve our understanding of the galaxy cluster dynamical evolution and assembly, together with a more precise evaluation of the non thermal support pressure budget. This will allow for more accurate constraints to be placed on the masses of galaxy clusters, among other potential benfits. Aims. In this view, we implemented the methods presented in the previous instalments of our work to characterize the turbulence in the ICM in a feasibility study with the X-IFU on board the future European X-ray observatory, Athena. Methods. From idealized mock observations of a toy model cluster, we reconstructed the second-order structure function built with the observed velocity field to constrain the turbulence. We carefully accounted for the various sources of errors to derive the most realistic and comprehensive error budget within the limits of our approach. With prior assumptions on the dissipation scale and power spectrum slope, we constrained the parameters of the turbulent power spectrum model through the use of MCMC sampling. Results. With favourable assumptions, we were able to retrieve the injection scale, velocity dispersion, and power spectrum slope, with 1sigma uncertainties better than ~15% of the input values. We demonstrated the efficiency of our carefully set framework to constrain the turbulence in the ICM from high-resolution X-ray spectroscopic observations, paving the way for more in-depth investigation of the optimal required observing strategy within a more restrictive observational setup with the future X-IFU instrument."
https://arxiv.org/abs/2403.08600,2024-03-13,Evaluation of Control/User-Plane Denial-of-Service (DoS) Attack on O-RAN Fronthaul Interface,"['Ferlinda Feliana', 'Ting-Wei Hung', 'Binbin Chen', 'Ray-Guang Cheng']","The open fronthaul interface defined by O-RAN ALLIANCE aims to support the interoperability between multi-vendor open radio access network (O-RAN) radio units (O-RU) and O-RAN distributed units (O-DU). This paper introduces a new tool that could be used to evaluate Denial-of-Service (DoS) attacks against the open fronthaul interface. We launched an array of control/user planes (C/U-Planes) attacks with the tool under different traffic types and data rates, and we evaluated their impacts on the throughput and block error rate (BLER) of real-world O-RAN systems with commercial hardware."
https://arxiv.org/abs/2403.08599,2024-03-13,The role of susceptible individuals in spreading dynamics,"['Chang Su', 'Fang Zhou', 'Linyuan Lü']","Exploring the internal mechanism of information spreading is critical for understanding and controlling the process. Traditional spreading models often assume individuals play the same role in the spreading process. In reality, however, individuals' diverse characteristics contribute differently to the spreading performance, leading to a heterogeneous infection rate across the system. To investigate network spreading dynamics under heterogeneous infection rates, we integrate two individual-level features -- influence (i.e., the ability to influence neighbors) and susceptibility (i.e., the extent to be influenced by neighbors) -- into the independent cascade model. Our findings reveal significant differences in spreading performance under heterogeneous and constant infection rates, with traditional structural centrality metrics proving more effective in the latter scenario. Additionally, we take the constant and heterogeneous infection rates into a state-of-the-art maximization algorithm, the well-known TIM algorithm, and find the seeds selected by heterogeneous infection rates are more dispersed compared to those under constant rates. Lastly, we find that both individuals' influence and susceptibility are vital to the spreading performance. Strikingly, susceptible individuals are particularly important to spreading when information is disseminated by social celebrities. By integrating influence and susceptibility into the spreading model, we gain a more profound understanding of the underlying mechanisms driving information spreading."
https://arxiv.org/abs/2403.08598,2024-03-13,"Adaptive morphing of wing and tail for stable, resilient, and energy-efficient flight of avian-informed drones","['Simon L. Jeger', 'Valentin Wüest', 'Charbel Toumieh', 'Dario Floreano']","Avian-informed drones feature morphing wing and tail surfaces, enhancing agility and adaptability in flight. Despite their large potential, realising their full capabilities remains challenging due to the lack of generalized control strategies accommodating their large degrees of freedom and cross-coupling effects between their control surfaces. Here we propose a new body-rate controller for avian-informed drones that uses all available actuators to control the motion of the drone. The method exhibits robustness against physical perturbations, turbulent airflow, and even loss of certain actuators mid-flight. Furthermore, wing and tail morphing is leveraged to enhance energy efficiency at 8m/s, 10m/s and 12m/s using in-flight Bayesian optimization. The resulting morphing configurations yield significant gains across all three speeds of up to 11.5% compared to non-morphing configurations and display a strong resemblance to avian flight at different speeds. This research lays the groundwork for the development of autonomous avian-informed drones that operate under diverse wind conditions, emphasizing the role of morphing in improving energy efficiency."
https://arxiv.org/abs/2403.08597,2024-03-13,Hamiltonian Boundary Value Methods (HBVMs) for functional differential equations with piecewise continuous arguments,"['Gianmarco Gurioli', 'Weijie Wang', 'Xiaoqiang Yan']","In this paper, a class of high-order methods to numerically solve Functional Differential Equations with Piecewise Continuous Arguments (FDEPCAs) is discussed. The framework stems from the expansion of the vector field associated with the reference differential equation along the shifted and scaled Legendre polynomial orthonormal basis, working on a suitable extension of Hamiltonian Boundary Value Methods. Within the design of the methods, a proper generalization of the perturbation results coming from the field of ordinary differential equations is considered, with the aim of handling the case of FDEPCAs. The error analysis of the devised family of methods is performed, while a few numerical tests on Hamiltonian FDEPCAs are provided, to give evidence to the theoretical findings and show the effectiveness of the obtained resolution strategy."
https://arxiv.org/abs/2403.08596,2024-03-13,Patching-based Deep Learning model for the Inpainting of Bragg Coherent Diffraction patterns affected by detectors' gaps,"['Matteo Masto', 'Vincent Favre-Nicolin', 'Steven Leake', 'Tobias Schülli', 'Marie-Ingrid Richard', 'Ewen Bellec']","We propose a deep learning algorithm for the inpainting of Bragg Coherent Diffraction Imaging (BCDI) patterns affected by detector gaps. These regions of missing intensity can compromise the accuracy of reconstruction algorithms, inducing artifacts in the final result. It is thus desirable to restore the intensity in these regions in order to ensure more reliable reconstructions. The key aspect of our method lies in the choice of training the neural network with cropped sections of both experimental diffraction data and simulated data and subsequently patching the predictions generated by the model along the gap, thus completing the full diffraction peak. This provides us with more experimental training data and allows for a faster model training due to the limited size, while the neural network can be applied to arbitrarily larger BCDI datasets. Moreover, our method not only broadens the scope of application but also ensures the preservation of data integrity and reliability in the face of challenging experimental conditions."
https://arxiv.org/abs/2403.08595,2024-03-13,Subnormal subgroups of almost locally simple artinian algebras with involutions,"['Dau Thi Hue', 'Huynh Viet Khanh', 'Bui Xuan Hai']","In this paper, we investigate subnormal subgroups of the multiplicative group of an almost locally simple artinian algebra with involution. In particular, we show that if either the set of traces or the set of norms of such a subgroup with respect to this involution is central, then the algebra must be either a quaternion division algebra or the matrix ring of degree $2$ over a field."
https://arxiv.org/abs/2403.08594,2024-03-13,CMB spectral distortions from enhanced primordial perturbations: the role of spectator axions,"['Margherita Putti', 'Nicola Bartolo', 'Sukannya Bhattacharya', 'Marco Peloso']","Primordial tensor modes can induce Cosmic Microwave Background spectral distortions during horizon re-entry. We investigate a specific mechanism proposed for this purpose, characterized by the coupling of an SU(2) gauge field to an axion undergoing a momentary stage of rapid evolution during inflation. Examining also the scalar perturbations produced by this model, we find that spectral distortions from the scalar modes significantly dominate those arising from the tensors. This holds true also for an earlier version of the model based on a U(1) gauge field. The scalar-induced distortions might be observed in future experiments, and the current COBE/FIRAS constraints already limit the parameter space of these models. Additionally, we find that delaying the onset of fast roll in the SU(2) scenario (to enhance the modes at the scales relevant for spectral distortions, while respecting the CMB constraints at larger scales) poses a greater challenge compared to the U(1) case. We propose a way to control the axion speed by varying the size of its coupling to the gauge fields."
https://arxiv.org/abs/2403.08593,2024-03-13,Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments,"['Sitao Cheng', 'Ziyuan Zhuang', 'Yong Xu', 'Fangkai Yang', 'Chaoyun Zhang', 'Xiaoting Qin', 'Xiang Huang', 'Ling Chen', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang']","Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA datasets and two TableQA datasets show the effectiveness of Readi, significantly surpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9% on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and 74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ). Our code will be available upon publication."
https://arxiv.org/abs/2403.08592,2024-03-13,Data-Efficient Sleep Staging with Synthetic Time Series Pretraining,"['Niklas Grieger', 'Siamak Mehrkanoon', 'Stephan Bialonski']","Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets. To address these challenges, various strategies, such as self-supervised learning, have been suggested, but they typically rely on extensive empirical datasets. Inspired by recent advances in computer vision, we propose a pretraining task termed ""frequency pretraining"" to pretrain a neural network for sleep staging by predicting the frequency content of randomly generated synthetic time series. Our experiments demonstrate that our method surpasses fully supervised learning in scenarios with limited data and few subjects, and matches its performance in regimes with many subjects. Furthermore, our results underline the relevance of frequency information for sleep stage scoring, while also demonstrating that deep neural networks utilize information beyond frequencies to enhance sleep staging performance, which is consistent with previous research. We anticipate that our approach will be advantageous across a broad spectrum of applications where EEG data is limited or derived from a small number of subjects, including the domain of brain-computer interfaces."
https://arxiv.org/abs/2403.08591,2024-03-13,ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning in Instructional Videos,"['Lei Shi', 'Paul Bürkner', 'Andreas Bulling']","We present ActionDiffusion -- a novel diffusion model for procedure planning in instructional videos that is the first to take temporal inter-dependencies between actions into account in a diffusion model for procedure planning. This approach is in stark contrast to existing methods that fail to exploit the rich information content available in the particular order in which actions are performed. Our method unifies the learning of temporal dependencies between actions and denoising of the action plan in the diffusion process by projecting the action information into the noise space. This is achieved 1) by adding action embeddings in the noise masks in the noise-adding phase and 2) by introducing an attention mechanism in the noise prediction network to learn the correlations between different action steps. We report extensive experiments on three instructional video benchmark datasets (CrossTask, Coin, and NIV) and show that our method outperforms previous state-of-the-art methods on all metrics on CrossTask and NIV and all metrics except accuracy on Coin dataset. We show that by adding action embeddings into the noise mask the diffusion model can better learn action temporal dependencies and increase the performances on procedure planning."
https://arxiv.org/abs/2403.08590,2024-03-13,"Long-term monitoring of large-scale magnetic fields across optical and near-infrared domains with ESPaDOnS, Narval and SPIRou. The cases of EV Lac, DS Leo, and CN Leo","['S. Bellotti', 'J. Morin', 'L. T. Lehmann', 'P. Petit', 'G. A. J. Hussain', 'J. -F. Donati', 'C. P. Folsom', 'A. Carmona', 'E. Martioli', 'B. Klein', 'P. Fouque', 'C. Moutou', 'S. Alencar', 'E. Artigau', 'I. Boisse', 'F. Bouchy', 'J. Bouvier', 'N. J. Cook', 'X. Delfosse', 'R. Doyon', 'G. Hebrard']","Dynamo models of stellar magnetic fields for partly and fully convective stars are guided by observational constraints. Zeeman-Doppler imaging has revealed a variety of magnetic field geometries and, for fully convective stars in particular, a dichotomy: either strong, mostly axisymmetric, and dipole-dominated or weak, non-axisymmetric, and multipole-dominated. This dichotomy is explained by dynamo bistability or by long-term magnetic cycles, but there is no definite conclusion on the matter. We analysed optical spectropolarimetric data sets collected with ESPaDOnS and Narval between 2005 and 2016, and near-infrared SPIRou data obtained between 2019 and 2022 for three active M dwarfs with masses between 0.1 and 0.6 MSun: EV Lac, DS Leo, and CN Leo. We looked for changes in time series of longitudinal magnetic field, width of unpolarised mean-line profiles, and large-scale field topology as retrieved with principal component analysis and Zeeman-Doppler imaging. We retrieved pulsating (EV Lac), stable (DS Leo), and sine-like (CN Leo) long-term trends in longitudinal field. The width of near-infrared mean-line profiles exhibits rotational modulation only for DS Leo, whereas in the optical it is evident for both EV Lac and DS Leo. The line width variations are not necessarily correlated to those of the longitudinal field, suggesting complex relations between small- and large-scale field. We also recorded topological changes: a reduced axisymmetry for EV Lac and a transition from toroidal- to poloidal-dominated regime for DS Leo. For CN Leo, the topology remained dipolar and axisymmetric, with only an oscillation in field strength. Our results show a peculiar evolution of the magnetic field for each M dwarf, confirming that M dwarfs with distinct masses and rotation periods can undergo magnetic long-term variations, and suggesting a variety of cyclic behaviours of their magnetic fields."
https://arxiv.org/abs/2403.08589,2024-03-13,Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?,"['Gianmarco Guglielmo', 'Andrea Montessori', 'Jean-Michel Tucny', 'Michele La Rocca', 'Pietro Prestininzi']","Application of Neural Networks to river hydraulics is fledgling, despite the field suffering from data scarcity, a challenge for machine learning techniques. Consequently, many purely data-driven Neural Networks proved to lack predictive capabilities. In this work, we propose to mitigate such problem by introducing physical information into the training phase. The idea is borrowed from Physics-Informed Neural Networks which have been recently proposed in other contexts. Physics-Informed Neural Networks embed physical information in the form of the residual of the Partial Differential Equations (PDEs) governing the phenomenon and, as such, are conceived as neural solvers, i.e. an alternative to traditional numerical solvers. Such approach is seldom suitable for environmental hydraulics, where epistemic uncertainties are large, and computing residuals of PDEs exhibits difficulties similar to those faced by classical numerical methods. Instead, we envisaged the employment of Neural Networks as neural operators, featuring physical constraints formulated without resorting to PDEs. The proposed novel methodology shares similarities with data augmentation and regularization. We show that incorporating such soft physical information can improve predictive capabilities."
https://arxiv.org/abs/2403.08588,2024-03-13,Quantum plasmonics model of refractive index sensing using photon correlations,"['L. C. Ugwuoke', 'T. P. J. Krüger', 'M. S. Tame']","The interaction between the electric dipole moments of a quantum emitter and a metal nanoparticle gives rise to unique optical properties, such as interference-induced photon correlations, that could be useful for enhanced intensity-based sensing. Using the quantum theory of photodetection, we propose a nanosensor system comprising a quantum emitter and a metal nanoparticle that explores the possibility of utilizing higher-order photon correlations for refractive index sensing. Both the refractive index sensitivity and resolution of the nanosensor, whose scattering spectrum lies within the visible region, are predicted. The sensor is supported by a substrate and driven weakly by a coherent field. By calculating the mean photocount and its second factorial moment resulting from the scattered field of the system, the sensing performance of the intensity and intensity-intensity correlation, are compared at optimal driving wavelengths. The mean photocount was found to be inherently low, inhibiting the role of interference-induced photon antibunching in minimizing the sensor's intensity shot noise. However, a regime in which the noise could be reduced below the shot noise limit is identified, leading to a quantum enhancement in the sensing performance."
https://arxiv.org/abs/2403.08587,2024-03-13,An improved particle swarm optimization algorithm and its application to search for new magnetic ground states in the Hubbard model,"['Ze Ruan', 'Xiu-Cai Jiang', 'Ze-Yi Song', 'Yu-Zhong Zhang']","An improved particle swarm optimization algorithm is proposed and its superiority over standard particle swarm optimization algorithm is tested on two typical benchmark functions. By employing this algorithm to search for the magnetic ground states of the Hubbard model on the real-space square lattice with finite size based on the mean-field approximation, two new magnetic states, namely the double striped-type antiferromagnetic state and the triple antiferromagnetic state, are found. We further perform mean-field calculations in the thermodynamical limit to confirm that these two new magnetic states are not a result of a finite-size effect, where the properties of the double striped-type antiferromagnetic state are also presented."
https://arxiv.org/abs/2403.08586,2024-03-13,PRAGO: Differentiable Multi-View Pose Optimization From Objectness Detections,"['Matteo Taiana', 'Matteo Toso', 'Stuart James', 'Alessio Del Bue']","Robustly estimating camera poses from a set of images is a fundamental task which remains challenging for differentiable methods, especially in the case of small and sparse camera pose graphs. To overcome this challenge, we propose Pose-refined Rotation Averaging Graph Optimization (PRAGO). From a set of objectness detections on unordered images, our method reconstructs the rotational pose, and in turn, the absolute pose, in a differentiable manner benefiting from the optimization of a sequence of geometrical tasks. We show how our objectness pose-refinement module in PRAGO is able to refine the inherent ambiguities in pairwise relative pose estimation without removing edges and avoiding making early decisions on the viability of graph edges. PRAGO then refines the absolute rotations through iterative graph construction, reweighting the graph edges to compute the final rotational pose, which can be converted into absolute poses using translation averaging. We show that PRAGO is able to outperform non-differentiable solvers on small and sparse scenes extracted from 7-Scenes achieving a relative improvement of 21% for rotations while achieving similar translation estimates."
https://arxiv.org/abs/2403.08585,2024-03-13,Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems,"['Junwei Su', 'Difan Zou', 'Chuan Wu']","Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \& preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD with preconditioning. First, we establish excess risk bounds (generalization performance) for preconditioned SGD and ridge regression under an arbitrary preconditions matrix. Second, leveraging the excessive risk characterization of preconditioned SGD and ridge regression, we show that (through construction) there exists a simple preconditioned matrix that can outperform (standard \& preconditioned) ridge regression. Finally, we show that our proposed preconditioning matrix is straightforward enough to allow robust estimation from finite samples while maintaining a theoretical advantage over ridge regression. Our empirical results align with our theoretical findings, collectively showcasing the enhanced regularization effect of preconditioned SGD."
https://arxiv.org/abs/2403.08584,2024-03-13,Local Binary and Multiclass SVMs Trained on a Quantum Annealer,"['Enrico Zardini', 'Amer Delilbasic', 'Enrico Blanzieri', 'Gabriele Cavallaro', 'Davide Pastorello']","Support vector machines (SVMs) are widely used machine learning models (e.g., in remote sensing), with formulations for both classification and regression tasks. In the last years, with the advent of working quantum annealers, hybrid SVM models characterised by quantum training and classical execution have been introduced. These models have demonstrated comparable performance to their classical counterparts. However, they are limited in the training set size due to the restricted connectivity of the current quantum annealers. Hence, to take advantage of large datasets (like those related to Earth observation), a strategy is required. In the classical domain, local SVMs, namely, SVMs trained on the data samples selected by a k-nearest neighbors model, have already proven successful. Here, the local application of quantum-trained SVM models is proposed and empirically assessed. In particular, this approach allows overcoming the constraints on the training set size of the quantum-trained models while enhancing their performance. In practice, the FaLK-SVM method, designed for efficient local SVMs, has been combined with quantum-trained SVM models for binary and multiclass classification. In addition, for comparison, FaLK-SVM has been interfaced for the first time with a classical single-step multiclass SVM model (CS SVM). Concerning the empirical evaluation, D-Wave's quantum annealers and real-world datasets taken from the remote sensing domain have been employed. The results have shown the effectiveness and scalability of the proposed approach, but also its practical applicability in a real-world large-scale scenario."
https://arxiv.org/abs/2403.08583,2024-03-13,Electroweak Evolution Equations and Isospin Conservation,"['Paolo Ciafaloni', ""Giampaolo Co'"", 'Dimitri Colferai', 'Denis Comelli']","In processes taking place at energies much higher than the weak scale, electroweak corrections can be taken into account by using electroweak evolution equations, that are analogous to the DGLAP equations in QCD. We show that weak isospin conservation in these equations imposes to modify the expressions of the splitting functions commonly used in the literature. These modifications have a profound impact on the parton distribution functions."
https://arxiv.org/abs/2403.08582,2024-03-13,Simultaneous mapping of magnetic and atomic structure for direct visualization of nanoscale magnetoelastic coupling,"['Sangjun Kang', 'Maximilian Töllner', 'Di Wang', 'Christian Minnert', 'Karsten Durst', 'Arnaud Caron', 'Rafal E. Dunin-Borkowski', 'Jeffrey McCord', 'Christian Kübel', 'Xiaoke Mu']","Achieving a correlative measurement of both magnetic and atomic structures at the nanoscale is imperative to understand the fundamental magnetism of matters and for fostering the development of new magnetic nanomaterials. Conventional microscopy methods fall short in providing the two information simultaneously. Here, we develop a new approach to simultaneously map the magnetic field and atomic structure at the nanoscale using Lorentz 4-dimensional scanning transmission electron microscopy (Ltz-4D-STEM). This method enables precise measurement of the characteristic atomic and magnetic structures across an extensive field of view, a critical aspect for investigating real-world ferromagnetic materials. It offers a comprehensive visualization and statistical evaluation of the different structural information at a pixel-by-pixel correlation. The new method allows to directly visualize the magnetoelastic coupling and the resulting complex magnetization arrangement as well as the competition between magnetoelastic and magnetostatic energy. This approach opens new avenues for in-depth studying the structure-property correlation of nanoscale magnetic materials."
https://arxiv.org/abs/2403.08581,2024-03-13,Impact of spin-entropy on the thermoelectric properties of a 2D magnet,"['Alessandra Canetta', 'Serhii Volosheniuk', 'Sayooj Satheesh', 'José Pedro Alvarinhas Batista', 'Aloïs Castellano', 'Riccardo Conte', 'Daniel G. Chica', 'Kenji Watanabe', 'Takashi Taniguchi', 'Xavier Roy', 'Herre S. J. van der Zant', 'Marko Burghard', 'Matthieu J. Verstraete', 'Pascal Gehring']","Heat-to-charge conversion efficiency of thermoelectric materials is closely linked to the entropy per charge carrier. Thus, magnetic materials are promising building blocks for highly efficient energy harvesters, as their carrier entropy is boosted by a spin degree of freedom. In this work, we investigate how this spin entropy impacts heat-to-charge conversion in A-type antiferromagnet CrSBr. We perform simultaneous measurements of electrical conductance and thermocurrent while changing magnetic order using temperature and magnetic field as tuning parameters. We find a strong enhancement of the thermoelectric power factor around the Néel temperature. We further reveal that the power factor at low temperature can be increased by up to 600% upon applying a magnetic field. Our results demonstrate that the thermoelectric properties of 2D magnets can be optimized by exploiting the sizeable impact of spin entropy and confirm thermoelectric measurements as a sensitive tool to investigate subtle magnetic phase transitions in low-dimensional magnets."
https://arxiv.org/abs/2403.08580,2024-03-13,Leveraging Compressed Frame Sizes For Ultra-Fast Video Classification,"['Yuxing Han', 'Yunan Ding', 'Chen Ye Gan', 'Jiangtao Wen']","Classifying videos into distinct categories, such as Sport and Music Video, is crucial for multimedia understanding and retrieval, especially when an immense volume of video content is being constantly generated. Traditional methods require video decompression to extract pixel-level features like color, texture, and motion, thereby increasing computational and storage demands. Moreover, these methods often suffer from performance degradation in low-quality videos. We present a novel approach that examines only the post-compression bitstream of a video to perform classification, eliminating the need for bitstream decoding. To validate our approach, we built a comprehensive data set comprising over 29,000 YouTube video clips, totaling 6,000 hours and spanning 11 distinct categories. Our evaluations indicate precision, accuracy, and recall rates consistently above 80%, many exceeding 90%, and some reaching 99%. The algorithm operates approximately 15,000 times faster than real-time for 30fps videos, outperforming traditional Dynamic Time Warping (DTW) algorithm by seven orders of magnitude."
https://arxiv.org/abs/2403.08579,2024-03-13,Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation,"['Hannes Waclawek', 'Stefan Huber']","Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points. While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements. Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent. Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs. Our approach is to utilize the versatility of PP models and combine it with the potential of modern ML optimizers for the use in function approximation in 1D trajectory planning in the context of electronic cam design. We utilize available optimizers of the ML framework TensorFlow directly, outside of the scope of ANNs, to optimize model parameters of our PP model. In this paper, we show how an orthogonal polynomial basis contributes to improving approximation and continuity optimization performance. Utilizing Chebyshev polynomials of the first kind, we develop a novel regularization approach enabling clearly improved convergence behavior. We show that, using this regularization approach, Chebyshev basis performs better than power basis for all relevant optimizers in the combined approximation and continuity optimization setting and demonstrate usability of the presented approach within the electronic cam domain."
https://arxiv.org/abs/2403.08578,2024-03-13,Coherent competition and control between three-wave mixing and four-wave mixing in superconducting circuits,"['Miao-Xiang Liang', 'Yu-Xiang Qiu', 'Hai-Chao Li', 'Wei Xiong']","Exploring intermixing and interplay between different frequency-mixing processes has always been one of the interesting subjects at the interface of nonlinear optics with quantum optics. Here we investigate coherent competition and control between three-wave mixing (TWM) and four-wave mixing (FWM) in a cyclic three-level superconducting quantum system. In the weak control-field regime, strong competition leads to an alternating oscillation between TWM and FWM signals and this oscillation is a signature of strong energy exchange between these two nonlinear processes. In particular, such oscillation is absent from conventional multi-wave mixing in atomic systems. Surprisingly, synchronous TWM and FWM processes are demonstrated in the strong control-field regime and, at the same time, their efficiencies can be as high as 40% and 45%, respectively. Our study shows that these competitive behaviors between TWM and FWM can be manipulated by tuning the control-field intensity."
https://arxiv.org/abs/2403.08577,2024-03-13,Evaluation and comparison of covariate balance metrics in studies with time-dependent confounding,"['David Adenyo', 'Jason R. Guertin', 'Bernard Candas', 'Caroline Sirois', 'Denis Talbot']","Marginal structural models have been increasingly used by analysts in recent years to account for confounding bias in studies with time-varying treatments. The parameters of these models are often estimated using inverse probability of treatment weighting. To ensure that the estimated weights adequately control confounding, it is possible to check for residual imbalance between treatment groups in the weighted data. Several balance metrics have been developed and compared in the cross-sectional case but have not yet been evaluated and compared in longitudinal studies with time-varying treatment. We have first extended the definition of several balance metrics to the case of a time-varying treatment, with or without censoring. We then compared the performance of these balance metrics in a simulation study by assessing the strength of the association between their estimated level of imbalance and bias. We found that the Mahalanobis balance performed best.Finally, the method was illustrated for estimating the cumulative effect of statins exposure over one year on the risk of cardiovascular disease or death in people aged 65 and over in population-wide administrative data. This illustration confirms the feasibility of employing our proposed metrics in large databases with multiple time-points."
https://arxiv.org/abs/2403.08576,2024-03-13,Global solutions of the one-dimensional compressible Euler equations with nonlocal interactions via the inviscid limit,"['Jose A. Carrillo', 'Gui-Qiang G. Chen', 'Difan Yuan', 'Ewelina Zatorska']","We are concerned with the global existence of finite-energy entropy solutions of the one-dimensional compressible Euler equations with (possibly) damping, alignment forces, and nonlocal interactions: Newtonian repulsion and quadratic confinement. Both the polytropic gas law and the general gas law are analyzed. This is achieved by constructing a sequence of solutions of the one-dimensional compressible Navier-Stokes-type equations with density-dependent viscosity under the stress-free boundary condition and then taking the vanishing viscosity limit. The main difficulties in this paper arise from the appearance of the nonlocal terms. In particular, some uniform higher moment estimates for the compressible Navier-Stokes equations on expanding intervals with stress-free boundary conditions are obtained by careful design of the approximate initial data."
https://arxiv.org/abs/2403.08575,2024-03-13,Higher order Schauder estimates for parabolic equations with degenerate or singular weights,"['Alessandro Audrito', 'Gabriele Fioravanti', 'Stefano Vita']","In this paper, we complete the analysis initiated in [AFV24] establishing some higher order $C^{k+2,α}$ Schauder estimates ($k \in \mathbb{N}$) for a a class of parabolic equations with weights that are degenerate/singular on a characteristic hyperplane. The $C^{2,α}$-estimates are obtained through a blow-up argument and a Liouville theorem, while the higher order estimates are obtained by a fine iteration procedure. As a byproduct, we present two applications. First, we prove similar Schauder estimates when the degeneracy/singularity of the weight occurs on a regular hypersurface of cylindrical type. Second, we provide an alternative proof of the higher order boundary Harnack principles established in [BG16,Kuk22]."
https://arxiv.org/abs/2403.08574,2024-03-13,Optimizing Conical Intersections Without Explicit Use of Non-Adiabatic Couplings,"['Juan Sanz García', 'Rosa Maskri', 'Alexander Mitrushchenkov', 'Loïc Joubert-Doriol']","We present two alternative methods for optimizing minimum energy conical intersection (MECI) molecular geometries without knowledge of the derivative coupling (DC). These methods are based on the utilization of Lagrange multipliers: i) one method uses an approximate calculation of the DC, while the other ii) do not require the DC. Both methods use the fact that information of the DC is contained in the Hessian of the squared energy difference. Tests done on a set of small molecular systems, in comparison with other methods, show the ability of the proposed methods to optimize MECIs. Finally, we apply the methods to the furimamide molecule, to optimize and characterize its S$_1$ /S$_2$ MECI, and to optimizing the S$_0$ /S$_1$ MECI of the silver trimer."
https://arxiv.org/abs/2403.08573,2024-03-13,System-bath correlations and finite-time operation enhance the efficiency of a dissipative quantum battery,"['Daniel Feliú', 'Felipe Barra']","The reduced state of a small system strongly coupled to a thermal bath may be athermal and used as a small battery once disconnected. If the disconnecting process is too slow, the coupling between the battery and the bath weakens, and at some point, the battery will be in a thermal state that can not be used as a battery. Thus, the unitarily extractable energy (a.k.a ergotropy) decreases with the disconnection time. The work required to disconnect the battery also depends on the disconnection time. We study the efficiency of this battery, defined as the ratio between the ergotropy to the work cost of disconnecting and connecting the battery back to the bath to close the cycle, as a function of the disconnecting time in the Caldeira-Leggett model of a quantum battery. We consider two scenarios. In the first scenario, we assume that the discharged battery is uncorrelated to the bath at the connecting time and find that the efficiency peaks at an optimal disconnecting time. In the second scenario, the discharged battery is correlated to the bath, and find that the optimal efficiency corresponds to an instantaneous disconnection. On top of these results, we analyze various thermodynamic quantities for these Caldeira-Leggett quantum batteries that allow us to express the first and second laws of thermodynamics in the mentioned cycles in simple form despite the system-bath initial correlations and strong coupling regime of the working device."
https://arxiv.org/abs/2403.08572,2024-03-13,Caformer: Rethinking Time Series Analysis from Causal Perspective,"['Kexuan Zhang', 'Xiaobei Zou', 'Yang Tang']","Time series analysis is a vital task with broad applications in various domains. However, effectively capturing cross-dimension and cross-time dependencies in non-stationary time series poses significant challenges, particularly in the context of environmental factors. The spurious correlation induced by the environment confounds the causal relationships between cross-dimension and cross-time dependencies. In this paper, we introduce a novel framework called Caformer (\underline{\textbf{Ca}}usal Trans\underline{\textbf{former}}) for time series analysis from a causal perspective. Specifically, our framework comprises three components: Dynamic Learner, Environment Learner, and Dependency Learner. The Dynamic Learner unveils dynamic interactions among dimensions, the Environment Learner mitigates spurious correlations caused by environment with a back-door adjustment, and the Dependency Learner aims to infer robust interactions across both time and dimensions. Our Caformer demonstrates consistent state-of-the-art performance across five mainstream time series analysis tasks, including long- and short-term forecasting, imputation, classification, and anomaly detection, with proper interpretability."
https://arxiv.org/abs/2403.08571,2024-03-13,Bright ultra-broadband fiber-based biphoton source,"['Maksim A. Smirnov', 'Ilya V. Fedotov', 'Anastasia M. Smirnova', 'Albert F. Khairullin', 'Andrei B. Fedotov', 'Sergey A. Moiseev']","In this Letter, we report a first experimental realization of bright ultra-broadband (180 THz) fiber-based biphoton source with widely spectrally separated signal and idler photons. Such a two-photon source is realized due to the joint use of broadband phase-matching of interacting light waves and high optical nonlinearity of a silica-core photonic crystal fiber. The high performance of the developed fiber source identifies it as an important and useful tool for a wide range of optical quantum applications."
https://arxiv.org/abs/2403.08570,2024-03-13,Ab initio Density Response and Local Field Factor of Warm Dense Hydrogen,"['Tobias Dornheim', 'Sebastian Schwalbe', 'Panagiotis Tolias', 'Maximilan Böhme', 'Zhandos Moldabekov', 'Jan Vorberger']","We present quasi-exact ab initio path integral Monte Carlo (PIMC) results for the partial static density responses and local field factors of hydrogen in the warm dense matter regime, from solid density conditions to the strongly compressed case. The full dynamic treatment of electrons and protons on the same footing allows us to rigorously quantify both electronic and ionic exchange--correlation effects in the system, and to compare with earlier incomplete models such as the archetypal uniform electron gas [Phys. Rev. Lett. 125, 235001 (2020)] or electrons in a fixed ion snapshot potential [Phys. Rev. Lett. 129, 066402 (2022)] that do not take into account the interplay between the two constituents. The full electronic density response is highly sensitive to electronic localization around the ions, and our results constitute unambiguous predictions for upcoming X-ray Thomson scattering (XRTS) experiments with hydrogen jets and fusion plasmas. All PIMC results are made freely available and can directly be used for a gamut of applications, including inertial confinement fusion calculations and the modelling of dense astrophysical objects. Moreover, they constitute invaluable benchmark data for approximate but computationally less demanding approaches such as density functional theory or PIMC within the fixed-node approximation."
https://arxiv.org/abs/2403.08569,2024-03-13,A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations,"['Hang Hu', 'Sidi Wu', 'Guoxiong Cai', 'Na Liu']","Physics-informed neural networks (PINNs) have successfully addressed various computational physics problems based on partial differential equations (PDEs). However, while tackling issues related to irregularities like singularities and oscillations, trained solutions usually suffer low accuracy. In addition, most current works only offer the trained solution for predetermined input parameters. If any change occurs in input parameters, transfer learning or retraining is required, and traditional numerical techniques also need an independent simulation. In this work, a physics-driven GraphSAGE approach (PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal basis functions is presented to solve computational problems governed by irregular PDEs and to develop parametric PDE surrogate models. This approach employs graph representations of physical domains, thereby reducing the demands for evaluated points due to local refinement. A distance-related edge feature and a feature mapping strategy are devised to help training and convergence for singularity and oscillation situations, respectively. The merits of the proposed method are demonstrated through a couple of cases. Moreover, the robust PDE surrogate model for heat conduction problems parameterized by the Gaussian random field source is successfully established, which not only provides the solution accurately but is several times faster than the finite element method in our experiments."
https://arxiv.org/abs/2403.08568,2024-03-13,Consistent Prompting for Rehearsal-Free Continual Learning,"['Zhanxin Gao', 'Jun Cen', 'Xiaobin Chang']","Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifier consistency learning. In addition, prompt consistency learning is proposed to enhance prediction robustness and boost prompt selection accuracy. Our Consistent Prompting surpasses its prompt-based counterparts and achieves state-of-the-art performance on multiple continual learning benchmarks. Detailed analysis shows that improvements come from more consistent training and testing."
https://arxiv.org/abs/2403.08567,2024-03-13,Quenched CLT for ancestral lineages of logistic branching random walks,"['Matthias Birkner', 'Andrej Depperschmidt', 'Timo Schlüter']","We consider random walks in dynamic random environments which arise naturally as spatial embeddings of ancestral lineages in spatial locally regulated population models. In particular, as the main result, we prove the quenched central limit theorem for a random walk in dynamic random environment generated by time reversal of logistic branching random walks in a regime where the population density is sufficiently high. As an important tool we consider as auxiliary models random walks in dynamic random environments defined in terms of the time-reversal of oriented percolation. We show that the quenched central limit theorem holds if the influence of the random medium on the walks is suitably weak. The proofs of the quenched central limit theorems in these models rely on coarse-graining arguments and a construction of regeneration times for a pair of conditionally independent random walks in the same medium, combined with a coupling that relates them to a pair of independent random walks in two independent copies of the medium."
https://arxiv.org/abs/2403.08566,2024-03-13,A Novel Implicit Neural Representation for Volume Data,"['Armin Sheibanifard', 'Hongchuan Yu']","The storage of medical images is one of the challenges in the medical imaging field. There are variable works that use implicit neural representation (INR) to compress volumetric medical images. However, there is room to improve the compression rate for volumetric medical images. Most of the INR techniques need a huge amount of GPU memory and a long training time for high-quality medical volume rendering. In this paper, we present a novel implicit neural representation to compress volume data using our proposed architecture, that is, the Lanczos downsampling scheme, SIREN deep network, and SRDenseNet high-resolution scheme. Our architecture can effectively reduce training time, and gain a high compression rate while retaining the final rendering quality. Moreover, it can save GPU memory in comparison with the existing works. The experiments show that the quality of reconstructed images and training speed using our architecture is higher than current works which use the SIREN only. Besides, the GPU memory cost is evidently decreased"
https://arxiv.org/abs/2403.08565,2024-03-13,Deep Learning based Positioning with Multi-task Learning and Uncertainty-based Fusion,"['Anastasios Foliadis', 'Mario H. Castañeda', 'Richard A. Stirling-Gallacher', 'Reiner S. Thomä']","Deep learning (DL) methods have been shown to improve the performance of several use cases for the fifth-generation (5G) New radio (NR) air interface. In this paper we investigate user equipment (UE) positioning using the channel state information (CSI) fingerprints between a UE and multiple base stations (BSs). In such a setup, a single DL model can be trained for UE positioning using the CSI fingerprints of the multiple BSs as input. Alternatively, based on the CSI at each BS, a separate DL model can be trained at each BS and then the output of the different models are combined to determine the UE's position. In this work we compare these different fusion techniques and show that fusing the output of separate models achieves higher positioning accuracy, especially in a dynamic scenario. We also show that the fusion of multiple outputs further benefits from considering the uncertainty of the output of the DL model at each BS. For a more efficient training of the DL model across BSs, we additionally propose a multi-task learning (MTL) scheme by sharing some parameters across the models while jointly training all models. This method, not only improves the accuracy of the individual models, but also of the final combined estimate. Lastly, we evaluate the reliability of the uncertainty estimation to ascertain which of the fusion methods provides the highest quality of uncertainty estimates."
https://arxiv.org/abs/2403.08564,2024-03-13,Non-discrimination Criteria for Generative Language Models,"['Sara Sterlie', 'Nina Weng', 'Aasa Feragen']","Within recent years, generative AI, such as large language models, has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models. In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context. Our results address the presence of occupational gender bias within such conversational language models."
https://arxiv.org/abs/2403.08563,2024-03-13,Distributed Deep Learning for Modulation Classification in 6G Cell-Free Wireless Networks,"['Dieter Verbruggen', 'Hazem Salluoha', 'Sofie Pollin']","In the evolution of 6th Generation (6G) technology, the emergence of cell-free networking presents a paradigm shift, revolutionizing user experiences within densely deployed networks where distributed access points collaborate. However, the integration of intelligent mechanisms is crucial for optimizing the efficiency, scalability, and adaptability of these 6G cell-free networks. One application aiming to optimize spectrum usage is Automatic Modulation Classification (AMC), a vital component for classifying and dynamically adjusting modulation schemes. This paper explores different distributed solutions for AMC in cell-free networks, addressing the training, computational complexity, and accuracy of two practical approaches. The first approach addresses scenarios where signal sharing is not feasible due to privacy concerns or fronthaul limitations. Our findings reveal that maintaining comparable accuracy is remarkably achievable, yet it comes with an increase in computational demand. The second approach considers a central model and multiple distributed models collaboratively classifying the modulation. This hybrid model leverages diversity gain through signal combining and requires synchronization and signal sharing. The hybrid model demonstrates superior performance, achieving a 2.5% improvement in accuracy with equivalent total computational load. Notably, the hybrid model distributes the computational load across multiple devices, resulting in a lower individual computational load."
https://arxiv.org/abs/2403.08562,2024-03-13,Structural perspective on constraint-based learning of Markov networks,"['Tuukka Korhonen', 'Fedor V. Fomin', 'Pekka Parviainen']","Markov networks are probabilistic graphical models that employ undirected graphs to depict conditional independence relationships among variables. Our focus lies in constraint-based structure learning, which entails learning the undirected graph from data through the execution of conditional independence tests. We establish theoretical limits concerning two critical aspects of constraint-based learning of Markov networks: the number of tests and the sizes of the conditioning sets. These bounds uncover an exciting interplay between the structural properties of the graph and the amount of tests required to learn a Markov network. The starting point of our work is that the graph parameter maximum pairwise connectivity, $κ$, that is, the maximum number of vertex-disjoint paths connecting a pair of vertices in the graph, is responsible for the sizes of independence tests required to learn the graph. On one hand, we show that at least one test with the size of the conditioning set at least $κ$ is always necessary. On the other hand, we prove that any graph can be learned by performing tests of size at most $κ$. This completely resolves the question of the minimum size of conditioning sets required to learn the graph. When it comes to the number of tests, our upper bound on the sizes of conditioning sets implies that every $n$-vertex graph can be learned by at most $n^κ$ tests with conditioning sets of sizes at most $κ$. We show that for any upper bound $q$ on the sizes of the conditioning sets, there exist graphs with $O(n q)$ vertices that require at least $n^{Ω(κ)}$ tests to learn. This lower bound holds even when the treewidth and the maximum degree of the graph are at most $κ+2$. On the positive side, we prove that every graph of bounded treewidth can be learned by a polynomial number of tests with conditioning sets of sizes at most $2κ$."
https://arxiv.org/abs/2403.08561,2024-03-13,Microdosimetry of a clinical carbon-ion pencil beam at MedAustron -- Part 1: experimental characterization,"['Cynthia Meouchi', 'Sandra Barna', 'Anatoly Rosenfeld', 'Linh T. Tran', 'Hugo Palmans', 'Giulio Magrin']","This paper characterizes the microdosimetric spectra of a single-energy carbon-ion pencil beam at MedAustron using a miniature solid-state silicon microdosimeter to estimate the impact of the lateral distribution of the different fragments on the microdosimetric spectra. The microdosimeter was fixed at one depth and then laterally moved away from the central beam axis in steps of approximately 2 mm. The measurements were taken in both horizontal and vertical direction in a water phantom at different depths. In a position on the distal dose fall-off beyond the Bragg peak, the frequency-mean and the dose-mean lineal energies were derived using either the entire range of y-values, or a sub-range of y values, presumingly corresponding mainly to contributions from primary particles. The measured microdosimetric spectra do not exhibit a significant change up to 4 mm away from the beam central axis. For lateral positions more than 4 mm away from the central axis, the relative contribution of the lower lineal-energy part of the spectrum increases with lateral distance due to the increased partial dose from secondary fragments. The average values yF and yD are almost constant for each partial contribution. However, when all particles are considered together, the average value of yF and yD varies with distance from the axis due to the changing dose fractions of these two components varying by 30 % and 10 % respectively up to the most off axis vertical position. Characteristic features in the microdosimetric spectra providing strong indications of the presence of helium and boron fragments have been observed downstream of the distal part of the Bragg peak. We were able to investigate the radiation quality as function of off-axis position. These measurements emphasize variation of the radiation quality within the beam and this has implications in terms of relative biological effectiveness."
https://arxiv.org/abs/2403.08560,2024-03-13,Decoupling of the structure functions in momentum space based on the Laplace transformation,"['G. R. Boroun', 'Phuoc Ha']","Using Laplace transform techniques, we describe the determination of the longitudinal structure function $F_{L}(x,Q^2)$, at the leading-order approximation in momentum space, from the structure function $F_{2}(x,Q^2)$ and its derivative with respect to ${\ln}Q^2$ in a kinematical region of low values of the Bjorken variable $x$. Since the $x$ dependence of $F_2(x,Q^2)$ and its evolution with $Q^2$ are determined much better by the data than $F_L(x,Q^2)$, this method provides both a direct check on $F_L(x,Q^2)$ where measured, and a way of extending $F_L(x,Q^2)$ into regions of $x$ and $Q^2$ where there are currently no data. In our calculations, we ultilize the Block-Durand-Ha parametrization for the structure function $F_{2}(x,Q^2)$ [M. M. Block, L. Durand and P. Ha, Phys.Rev.D {\bf89}, 094027 (2014)]. We find that the Laplace transform method in momentum space provides correct behaviors of the extracted longitudinal structure function $F_{L}(x,Q^2)$ and that our obtained results are in line with data from the H1 Collaboration and other results for $F_{L}(x,Q^2)$ obtained using Mellin transform method."
https://arxiv.org/abs/2403.08559,2024-03-13,End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier Models,"['Lauri Juvela', 'Eero-Pekka Damskägg', 'Aleksi Peussa', 'Jaakko Mäkinen', 'Thomas Sherson', 'Stylianos I. Mimilakis', 'Athanasios Gotsopoulos']","This paper describes a data-driven approach to creating real-time neural network models of guitar amplifiers, recreating the amplifiers' sonic response to arbitrary inputs at the full range of controls present on the physical device. While the focus on the paper is on the data collection pipeline, we demonstrate the effectiveness of this conditioned black-box approach by training an LSTM model to the task, and comparing its performance to an offline white-box SPICE circuit simulation. Our listening test results demonstrate that the neural amplifier modeling approach can match the subjective performance of a high-quality SPICE model, all while using an automated, non-intrusive data collection process, and an end-to-end trainable, real-time feasible neural network model."
https://arxiv.org/abs/2403.08558,2024-03-13,Narrowly-Banded Spectra with Peak Frequency Around 1 GHz of FRB 20201124A: Implications for Energy Function and Radiation Physics,"['Fen Lyu', 'En-Wei Liang', 'D. Li']","The radiation physics of fast radio bursts (FRBs) remains an open question. Current observations have discovered that narrowly-banded bursts of FRB 20201124A are active in 0.4-2 GHz and their spectral peak frequency ($ν^{\rm obs}_{p}$) are mostly toward $\sim 1$ GHz. Utilizing a sample of 1268 bursts of FRB 20201124A detected with the FAST telescope, we show that the $1σ$ spectral regime of 71.4\% events (in-band bursts) is within the FAST bandpass. Their intrinsic burst energies ($E^{\rm obs}_{\rm BWe}$) and spectral widths ($σ_s^{\rm obs}$) are well measured by fitting the spectral profile with a Gaussian function. The derived $E^{\rm obs}_{\rm BWe}$ and $σ_s^{\rm obs}$ distributions are log-normal and centering at $\log E^{\rm obs}_{\rm BWe}/{\rm erg}=37.2~ (σ=0.76)$ and $\log σ_s^{\rm obs}/{\rm GHz}=-1.16~ (σ=0.17)$. Our Monte Carlo simulation analysis infers its intrinsic $ν_p$ distribution as a normal function centered at $ν_{p,c}=1.16$ GHz ($σ=0.22$) and its intrinsic energy function as $Φ(E)\propto E^{-0.60}e^{-E/E_c}$ with $E_c=9.49 \times 10^{37}$ erg. We compare these results with that of typical repeating FRBs 20121102A and 20190520B that are active over a broad frequency range at several specific frequencies and discuss possible observational biases on the estimation of the event rate and energy function. Based on these results, we argue that FRB 20201124A likely occurs in a fine-tuned plasma for maser radiations at a narrow frequency range, while FRB 20121102A and FRB 20190520B could involve clumpy plasma conditions that make maser emission around several specific frequencies in a broad range."
https://arxiv.org/abs/2403.08557,2024-03-13,Occluded Cloth-Changing Person Re-Identification,"['Zhihao Chen', 'Yiyuan Ge']","Cloth-changing person re-identification aims to retrieve and identify spe-cific pedestrians by using cloth-irrelevant features in person cloth-changing scenarios. However, pedestrian images captured by surveillance probes usually contain occlusions in real-world scenarios. The perfor-mance of existing cloth-changing re-identification methods is significantly degraded due to the reduction of discriminative cloth-irrelevant features caused by occlusion. We define cloth-changing person re-identification in occlusion scenarios as occluded cloth-changing person re-identification (Occ-CC-ReID), and to the best of our knowledge, we are the first to pro-pose occluded cloth-changing person re-identification as a new task. We constructed two occluded cloth-changing person re-identification datasets for different occlusion scenarios: Occluded-PRCC and Occluded-LTCC. The datasets can be obtained from the following link: https://github.com/1024AILab/Occluded-Cloth-Changing-Person- Re-Identification."
https://arxiv.org/abs/2403.08556,2024-03-13,SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple Cameras and Scenes by One Model,"['Yihao Liu', 'Feng Xue', 'Anlong Ming']","The generalization of monocular metric depth estimation (MMDE) has been a longstanding challenge. Recent methods made progress by combining relative and metric depth or aligning input image focal length. However, they are still beset by challenges in camera, scene, and data levels: (1) Sensitivity to different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on massive training data. This paper proposes SM4Depth, a seamless MMDE method, to address all the issues above within a single network. First, we reveal that a consistent field of view (FOV) is the key to resolve ``metric ambiguity'' across cameras, which guides us to propose a more straightforward preprocessing unit. Second, to achieve consistently high accuracy across scenes, we explicitly model the metric scale determination as discretizing the depth interval into bins and propose variation-based unnormalized depth bins. This method bridges the depth gap of diverse scenes by reducing the ambiguity of the conventional metric bin. Third, to reduce the reliance on massive training data, we propose a ``divide and conquer"" solution. Instead of estimating directly from the vast solution space, the correct metric bins are estimated from multiple solution sub-spaces for complexity reduction. Finally, with just 150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves state-of-the-art performance on most previously unseen datasets, especially surpassing ZoeDepth and Metric3D on mRI$_θ$. The code can be found at https://github.com/1hao-Liu/SM4Depth."
https://arxiv.org/abs/2403.08555,2024-03-13,Heterogeneous Nucleation and Growth of Sessile Chemically Active Droplets,"['Noah Ziethen', 'David Zwicker']","Droplets are essential for spatially controlling biomolecules in cells. To work properly, cells need to control the emergence and morphology of droplets. On the one hand, driven chemical reactions can affect droplets profoundly. For instance, reactions can control how droplets nucleate and how large they grow. On the other hand, droplets coexist with various organelles and other structures inside cells, which could affect their nucleation and morphology. To understand the interplay of these two aspects, we study a continuous field theory of active phase separation. Our numerical simulations reveal that reactions suppress nucleation while attractive walls enhance it. Intriguingly, these two effects are coupled, leading to shapes that deviate substantially from the spherical caps predicted for passive systems. These distortions result from anisotropic fluxes responding to the boundary conditions dictated by the Young-Dupré equation. Interestingly, an electrostatic analogy of chemical reactions confirms these effects. We thus demonstrate how driven chemical reactions affect the emergence and morphology of droplets, which could be crucial for understanding biological cells and improving technical applications, e.g., in chemical engineering."
https://arxiv.org/abs/2403.08554,2024-03-13,Federated Knowledge Graph Unlearning via Diffusion Model,"['Bingchen Liu', 'Yuanyuan Fang']","Federated learning (FL) promotes the development and application of artificial intelligence technologies by enabling model sharing and collaboration while safeguarding data privacy. Knowledge graph (KG) embedding representation provides a foundation for knowledge reasoning and applications by mapping entities and relations into vector space. Federated KG embedding enables the utilization of knowledge from diverse client sources while safeguarding the privacy of local data. However, due to demands such as privacy protection and the need to adapt to dynamic data changes, investigations into machine unlearning (MU) have been sparked. However, it is challenging to maintain the performance of KG embedding models while forgetting the influence of specific forgotten data on the model. In this paper, we propose FedDM, a novel framework tailored for machine unlearning in federated knowledge graphs. Leveraging diffusion models, we generate noisy data to sensibly mitigate the influence of specific knowledge on FL models while preserving the overall performance concerning the remaining data. We conduct experimental evaluations on benchmark datasets to assess the efficacy of the proposed model. Extensive experiments demonstrate that FedDM yields promising results in knowledge forgetting."
https://arxiv.org/abs/2403.08553,2024-03-13,Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG,"['Ting-Jui Chang', 'Shahin Shahrampour']","Recent advancement in online optimization and control has provided novel tools to study online linear quadratic regulator (LQR) problems, where cost matrices are varying adversarially over time. However, the controller parameterization of existing works may not satisfy practical conditions like sparsity due to physical connections. In this work, we study online linear quadratic Gaussian problems with a given linear constraint imposed on the controller. Inspired by the recent work of [1] which proposed, for a linearly constrained policy optimization of an offline LQR, a second order method equipped with a Riemannian metric that emerges naturally in the context of optimal control problems, we propose online optimistic Newton on manifold (OONM) which provides an online controller based on the prediction on the first and second order information of the function sequence. To quantify the proposed algorithm, we leverage the notion of regret defined as the sub-optimality of its cumulative cost to that of a (locally) minimizing controller sequence and provide the regret bound in terms of the path-length of the minimizer sequence. Simulation results are also provided to verify the property of OONM."
https://arxiv.org/abs/2403.08552,2024-03-13,Young asteroid families as the primary source of meteorites,"['M. Brož', 'P. Vernazza', 'M. Marsset', 'F. E. DeMeo', 'R. P. Binzel', 'D. Vokrouhlický', 'D. Nesvorný']","Understanding the origin of bright shooting stars and their meteorite samples is among the most ancient astronomy-related questions that at larger scales has human consequences [1-3]. As of today, only ${\sim}\,6\%$ of meteorite falls have been firmly linked to their sources (Moon, Mars, and asteroid (4) Vesta [4-6]). Here, we show that ${\sim}\,70\%$ of meteorites originate from three recent breakups of $D > 30\,{\rm km}$ asteroids that occurred 5.8, 7.5 and less than ${\sim}\,40$ million years ago. These breakups, including the well-known Karin family [7], took place in the prominent yet old Koronis and Massalia families and are at the origin of the dominance of H and L ordinary chondrites among meteorite falls. These young families distinguish themselves amidst all main belt asteroids by having a uniquely high abundance of small fragments. Their size-frequency distribution remains steep for a few tens of millions of years, exceeding temporarily the production of metre-sized fragments by the largest old asteroid families (e.g., Flora, Vesta). Supporting evidence includes the existence of associated dust bands [8-10], the cosmic-ray exposure ages of H-chondrite meteorites [11,12], or the distribution of pre-atmospheric orbits of meteorites [13-15]."
https://arxiv.org/abs/2403.08551,2024-03-13,GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting,"['Xinjie Zhang', 'Xingtong Ge', 'Tongda Xu', 'Dailan He', 'Yan Wang', 'Hongwei Qin', 'Guo Lu', 'Jing Geng', 'Jun Zhang']","Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. Furthermore, we integrate existing vector quantization technique to build an image codec. Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 1000 FPS. Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding."
https://arxiv.org/abs/2403.08550,2024-03-13,CINA: Conditional Implicit Neural Atlas for Spatio-Temporal Representation of Fetal Brains,"['Maik Dannecker', 'Vanessa Kyriakopoulou', 'Lucilio Cordero-Grande', 'Anthony N. Price', 'Joseph V. Hajnal', 'Daniel Rueckert']","We introduce a conditional implicit neural atlas (CINA) for spatio-temporal atlas generation from Magnetic Resonance Images (MRI) of the neurotypical and pathological fetal brain, that is fully independent of affine or non-rigid registration. During training, CINA learns a general representation of the fetal brain and encodes subject specific information into latent code. After training, CINA can construct a faithful atlas with tissue probability maps of the fetal brain for any gestational age (GA) and anatomical variation covered within the training domain. Thus, CINA is competent to represent both, neurotypical and pathological brains. Furthermore, a trained CINA model can be fit to brain MRI of unseen subjects via test-time optimization of the latent code. CINA can then produce probabilistic tissue maps tailored to a particular subject. We evaluate our method on a total of 198 T2 weighted MRI of normal and abnormal fetal brains from the dHCP and FeTA datasets. We demonstrate CINA's capability to represent a fetal brain atlas that can be flexibly conditioned on GA and on anatomical variations like ventricular volume or degree of cortical folding, making it a suitable tool for modeling both neurotypical and pathological brains. We quantify the fidelity of our atlas by means of tissue segmentation and age prediction and compare it to an established baseline. CINA demonstrates superior accuracy for neurotypical brains and pathological brains with ventriculomegaly. Moreover, CINA scores a mean absolute error of 0.23 weeks in fetal brain age prediction, further confirming an accurate representation of fetal brain development."
https://arxiv.org/abs/2403.08549,2024-03-13,Wet TinyML: Chemical Neural Network Using Gene Regulation and Cell Plasticity,"['Samitha Somathilaka', 'Adrian Ratwatte', 'Sasitharan Balasubramaniam', 'Mehmet Can Vuran', 'Witawas Srisa-an', 'Pietro Liò']","In our earlier work, we introduced the concept of Gene Regulatory Neural Network (GRNN), which utilizes natural neural network-like structures inherent in biological cells to perform computing tasks using chemical inputs. We define this form of chemical-based neural network as Wet TinyML. The GRNN structures are based on the gene regulatory network and have weights associated with each link based on the estimated interactions between the genes. The GRNNs can be used for conventional computing by employing an application-based search process similar to the Network Architecture Search. This study advances this concept by incorporating cell plasticity, to further exploit natural cell's adaptability, in order to diversify the GRNN search that can match larger spectrum as well as dynamic computing tasks. As an example application, we show that through the directed cell plasticity, we can extract the mathematical regression evolution enabling it to match to dynamic system applications. We also conduct energy analysis by comparing the chemical energy of the GRNN to its silicon counterpart, where this analysis includes both artificial neural network algorithms executed on von Neumann architecture as well as neuromorphic processors. The concept of Wet TinyML can pave the way for the new emergence of chemical-based, energy-efficient and miniature Biological AI."
https://arxiv.org/abs/2403.08548,2024-03-13,The Massalia asteroid family as the origin of ordinary L chondrites,"['Michaël Marsset', 'Pierre Vernazza', 'Miroslav Brož', 'Cristina A. Thomas', 'Francesca E. DeMeo', 'Brian Burt', 'Richard P. Binzel', 'Vishnu Reddy', 'Allison McGraw', 'Chrysa Avdellidou', 'Benoit Carry', 'Stephen M. Slivan', 'David Polishook']","Studies of micrometeorites in mid-Ordovician limestones and Earth's impact craters indicate that our planet witnessed a massive infall of ordinary L chondrite material 466 million years (My) ago (Heck et al. 2017, Schmieder & Kring 2020, Kenkmann 2021) that may have been at the origin of the first major mass extinction event (Schmitz et al. 2019). The breakup of a large asteroid in the main belt is the likely cause of this massive infall. In modern times, material originating from this breakup still dominates meteorite falls (>20% of all falls) (Swindle et al. 2014). Here, we provide spectroscopic observations and dynamical evidence that the Massalia collisional family is the only plausible source of this catastrophic event and of the most abundant class of meteorites falling on Earth today. It is suitably located in the inner belt, at low-inclination orbits, which corresponds to the observed distribution of L-chondrite-like near-Earth objects (NEOs) and of interplanetary dust concentrated at 1.4 degrees (Sykes 1990, Reach et al. 1997)."
https://arxiv.org/abs/2403.08547,2024-03-13,Search for low-mass resonances decaying into two jets and produced in association with a photon or a jet at $\sqrt{s}=13$ TeV with the ATLAS detector,[' ATLAS Collaboration'],"A search is performed for localized excesses in the low-mass dijet invariant mass distribution, targeting a hypothetical new particle decaying into two jets and produced in association with either a high transverse momentum photon or a jet. The search uses the full Run 2 data sample from LHC proton-proton collisions collected by the ATLAS experiment at a center-of-mass energy of 13 TeV during 2015-2018. Two variants of the search are presented for each type of initial-state radiation: one that makes no jet flavor requirements and one that requires both of the jets to have been identified as containing $b$-hadrons. No excess is observed relative to the Standard Model prediction, and the data are used to set upper limits on the production cross-section for a benchmark $Z'$ model and, separately, for generic, beyond the Standard Model scenarios which might produce a Gaussian-shaped contribution to dijet invariant mass distributions. The results extend the current constraints on dijet resonances to the mass range between 200 and 650 GeV."
https://arxiv.org/abs/2403.08546,2024-03-13,Semantic Segmentation of Solar Radio Spikes at Low Frequencies,"['Pearse C. Murphy', 'Stéphane Aicardi', 'Baptiste Cecconi', 'Carine Briand', 'Thibault Peccoux']","Solar radio spikes are short lived, narrow bandwidth features in low frequency solar radio observations. The timing of their occurrence and the number of spikes in a given observation is often unpredictable. The high temporal and frequency of resolution of modern radio telescopes such as NenuFAR mean that manually identifying radio spikes is an arduous task. Machine learning approaches to data exploration in solar radio data is on the rise. Here we describe a convolutional neural network to identify the per pixel location of radio spikes as well as determine some simple characteristics of duration, spectral width and drift rate. The model, which we call SpikeNet, was trained using an Nvidia Tesla T4 16GB GPU with ~100000 sample spikes in a total time of 2.2 hours. The segmentation performs well with an intersection over union in the test set of ~0.85. The root mean squared error for predicted spike properties is of the order of 23%. Applying the algorithm to unlabelled data successfully generates segmentation masks although the accuracy of the predicted properties is less reliable, particularly when more than one spike is present in the same 64 X 64 pixel time-frequency range. We have successfully demonstrated that our convolutional neural network can locate and characterise solar radio spikes in a number of seconds compared to the weeks it would take for manual identification."
https://arxiv.org/abs/2403.08545,2024-03-13,Crash Chronicles: relative contribution from comets and carbonaceous asteroids to Earth's volatile budget in the context of an Early Instability,"['Sarah Joiret', 'Sean N. Raymond', 'Guillaume Avice', 'Matthew S. Clement']","Recent models of solar system formation suggest that a dynamical instability among the giant planets happened within the first 100 Myr after disk dispersal, perhaps before the Moon-forming impact. As a direct consequence, a bombardment of volatile-rich impactors may have taken place on Earth before internal and atmospheric reservoirs were decoupled. However, such a timing has been interpreted to potentially be at odds with the disparate inventories of Xe isotopes in Earth's mantle compared to its atmosphere. This study aims to assess the dynamical effects of an Early Instability on the delivery of carbonaceous asteroids and comets to Earth, and address the implications for the Earth's volatile budget. We perform 20 high-resolution dynamical simulations of solar system formation from the time of gas disk dispersal, each starting with 1600 carbonaceous asteroids and 10000 comets, taking into account the dynamical perturbations from an early giant planet instability. Before the Moon-forming impact, the cumulative collision rate of comets with Earth is about 4 orders of magnitude lower than that of carbonaceous asteroids. After the Moon-forming impact, this ratio either decreases or increases, often by orders of magnitude, depending on the dynamics of individual simulations. An increase in the relative contribution of comets happens in 30\% of our simulations. In these cases, the delivery of noble gases from each source is comparable, given that the abundance of 132Xe is 3 orders of magnitude greater in comets than in carbonaceous chondrites. The increase in cometary flux relative to carbonaceous asteroids at late times may thus offer an explanation for the Xe signature dichotomy between the Earth's mantle and atmosphere."
https://arxiv.org/abs/2403.08544,2024-03-13,Comparing jet-shaped point symmetry in cluster cooling flows and supernovae,['Noam Soker'],"I point out similarities between point-symmetric X-ray morphologies in cooling flow groups and clusters of galaxies, which are observed to be shaped by jets, and point-symmetric morphologies of eight core-collapse supernova (CCSN) remnants. I use these similarities to strengthen the jittering jet explosion mechanism (JJEM) of CCSNe, which predicts that the last pairs of jets to be launched by the newly born neutron star might shape some CCSN remnants to point-symmetric morphology. The point-symmetric morphologies in both types of objects are composed of two or more pairs of opposite bubbles (cavities), nozzles, some clumps, small protrusions (termed ears), and rims. The typically large volume of a CCSN remnant shaped by jets implies that the shaping jets carry an energy comparable to that of the ejecta, which in turn implies that jets exploded the remnant's massive star progenitor. The morphological similarities studied here add to the similarity of CCSN remnants, not only point-symmetric ones, to planetary nebulae shaped by jets. Together, these similarities solidify the JJEM as the main explosion mechanism of CCSNe. I consider the identification of point-symmetry in CCSNe, as expected by jet-shaping in the JJEM, to be the most severe challenge to the competing neutrino-driven explosion mechanism. I reiterate my earlier claim, but in a more vocal voice, that the main explosion mechanism of CCSNe is the JJEM."
https://arxiv.org/abs/2403.08543,2024-03-13,Direct numerical simulation of transition under free-stream turbulence and the influence of large integral length scales,"['Kristina Đurović', 'Ardeshir Hanifi', 'Philipp Schlatter', 'Kenzo Sasaki', 'Dan S. Henningson']","Under action of free-stream turbulence (FST), elongated streamwise streaky structures are generated inside the boundary layer, and their amplitude and wavelength are crucial for the transition onset. While turbulence intensity is strongly correlated with the transitional Reynolds number, characteristic length scales of the FST are often considered to have a slight impact on the transition location. However, a recent experiment by Fransson & Shahinfar (2020} shows significant effects of FST scales. They found that, for higher free-stream turbulence levels and larger integral length scales, an increase in the length scale postpones transition, contrary to established literature. Here, we aim at understanding these results by performing a series of high-fidelity simulations. These results provide understanding why the FST integral length scale affects the transition location differently. These integral length scales are so large that the wide streaks introduced in the boundary layer have substantially lower growth in the laminar region upstream of the transition to turbulence, than streaks induced by smaller integral length scales. The energy in the boundary layer subsequently propagate to smaller spanwise scales as a result of the non-linear interaction. When the energy has reached smaller spanwise scales larger amplitude streaks results in regions where the streak growth are larger. It takes longer for the energy from the wider streaks to propagate to the spanwise scales associated with the breakdown to turbulence, than for the those with smaller spanwise scales. Thus there is a faster transition for FST with lower integral length scales in this case."
https://arxiv.org/abs/2403.08542,2024-03-13,AIGCs Confuse AI Too: Investigating and Explaining Synthetic Image-induced Hallucinations in Large Vision-Language Models,"['Yifei Gao', 'Jiaqi Wang', 'Zhiyu Lin', 'Jitao Sang']","The evolution of Artificial Intelligence Generated Contents (AIGCs) is advancing towards higher quality. The growing interactions with AIGCs present a new challenge to the data-driven AI community: While AI-generated contents have played a crucial role in a wide range of AI models, the potential hidden risks they introduce have not been thoroughly examined. Beyond human-oriented forgery detection, AI-generated content poses potential issues for AI models originally designed to process natural data. In this study, we underscore the exacerbated hallucination phenomena in Large Vision-Language Models (LVLMs) caused by AI-synthetic images. Remarkably, our findings shed light on a consistent AIGC \textbf{hallucination bias}: the object hallucinations induced by synthetic images are characterized by a greater quantity and a more uniform position distribution, even these synthetic images do not manifest unrealistic or additional relevant visual features compared to natural images. Moreover, our investigations on Q-former and Linear projector reveal that synthetic images may present token deviations after visual projection, thereby amplifying the hallucination bias."
https://arxiv.org/abs/2403.08541,2024-03-13,On harvesting physical predictions from asymptotically safe quantum field theories,"['Frank Saueressig', 'Agustín Silva']","Asymptotic safety is a powerful mechanism for obtaining a consistent and predictive quantum field theory beyond the realm of perturbation theory. It hinges on an interacting fixed point of the Wilsonian renormalization group flow which controls the microscopic dynamics. Connecting the fixed point to observations requires constructing the set of effective actions compatible with this microscopic dynamics. Technically, this information is stored in the UV-critical surface of the fixed point. In this work, we describe a novel approach for extracting this information based on analytical and pseudo-spectral methods. Our construction is illustrated at the level of the two-dimensional Ising model and easily generalizes to any asymptotically safe quantum field theory. It also constitutes an important step towards setting up a well-founded swampland program within the gravitational asymptotic safety program."
https://arxiv.org/abs/2403.08540,2024-03-13,Language models scale reliably with over-training and on downstream tasks,"['Samir Yitzhak Gadre', 'Georgios Smyrnis', 'Vaishaal Shankar', 'Suchin Gururangan', 'Mitchell Wortsman', 'Rulin Shao', 'Jean Mercat', 'Alex Fang', 'Jeffrey Li', 'Sedrick Keh', 'Rui Xin', 'Marianna Nezhurina', 'Igor Vasiljevic', 'Jenia Jitsev', 'Alexandros G. Dimakis', 'Gabriel Ilharco', 'Shuran Song', 'Thomas Kollar', 'Yair Carmon', 'Achal Dave', 'Reinhard Heckel', 'Niklas Muennighoff', 'Ludwig Schmidt']","Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., ""Chinchilla optimal"" regime); however, in practice, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance. In this paper, we address both shortcomings. To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we investigate scaling in the over-trained regime. We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\times$ over-trained) and a 6.9B parameter, 138B token run$\unicode{x2014}$each from experiments that take 300$\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance via a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models using experiments that take 20$\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling."
https://arxiv.org/abs/2403.08539,2024-03-13,Generation and high-resolution imaging of higher-order polarization via metasurface,"['Xiang Yuan', 'Hanming Guo', 'Songlin Zhuang', 'Jinbing Hu']","The generation and focusing properties of higher-order polarized beams have attracted lots of interests due to its significant applications. In this paper,we derived the formula of transforming linear polarization into higher-order polarization, which is applicable to generating arbitrary order polarization. Based on the derived formula, the focusing properties of higher-order polarization by dielectric metasurface lens are studied , which exhibit an Abbe-limit-breaking feature for small numerical aperture, i.e., NA<0.6. When a binary phase (0 & π) is further imposed on the aperture of metasurface lens, the focusing spot of fourth-order polarization breaks Abbe limit even by 14.3% at NA= 0.6. In addition, the effect of fabrication tolerance, say, substrate thickness and central deviation, on the focusing feature of higher-order polarization is also investigated. Our study may find significant applications in achieving higher-resolution lithography and imaging, say, by just replacing conventional linear or circular polarization with higher-order polarization."
https://arxiv.org/abs/2403.08538,2024-03-13,Calibrating coordinate system alignment in a scanning transmission electron microscope using a digital twin,"['Dieter Weber', 'David Landers', 'Chen Huang', 'Emanuela Liberti', 'Emiliya Poghosyan', 'Matthew Bryan', 'Alexander Clausen', 'Daniel G. Stroppa', 'Angus I. Kirkland', 'Elisabeth Müller', 'Andrew Stewart', 'Rafal E. Dunin-Borkowski']","In four-dimensional scanning transmission electron microscopy (4D STEM) a focused beam is scanned over a specimen and a diffraction pattern is recorded at each position using a pixelated detector. During the experiment, it must be ensured that the scan coordinate system of the beam is correctly calibrated relative to the detector coordinate system. Various simplified and approximate models are used implicitly and explicitly for understanding and analyzing the recorded data, requiring translation between the physical reality of the instrument and the abstractions used in data interpretation. Here, we introduce a calibration method where interactive live data processing in combination with a digital twin is used to match a set of models and their parameters with the action of a real-world instrument."
https://arxiv.org/abs/2403.08537,2024-03-13,On Terwilliger $\mathbb{F}$-algebras of factorial association schemes,['Yu Jiang'],"The Terwilliger algebras of association schemes over an arbitrary field $\mathbb{F}$ were called the Terwilliger $\mathbb{F}$-algebras of association schemes in [8]. In this paper, we study the Terwilliger $\mathbb{F}$-algebras of factorial association schemes. We determine the $\mathbb{F}$-dimensions, the centers, the semisimplicity, the Jacobson radicals, and the algebraic structures of the Terwilliger $\mathbb{F}$-algebras of factorial association schemes."
https://arxiv.org/abs/2403.08536,2024-03-13,HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers,"['Francesco Dibitonto', 'Fabio Garcea', 'André Panisson', 'Alan Perotti', 'Lia Morra']","Convolutional Neural Networks (CNNs) are nowadays the model of choice in Computer Vision, thanks to their ability to automatize the feature extraction process in visual tasks. However, the knowledge acquired during training is fully subsymbolic, and hence difficult to understand and explain to end users. In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based Semantic inspection) that decomposes a label into a set of related concepts, and provides component-level explanations for an image classification model. Specifically, HOLMES leverages ontologies, web scraping and transfer learning to automatically construct meronym (parts)-based detectors for a given holonym (class). Then, it produces heatmaps at the meronym level and finally, by probing the holonym CNN with occluded images, it highlights the importance of each part on the classification output. Compared to state-of-the-art saliency methods, HOLMES takes a step further and provides information about both where and what the holonym CNN is looking at, without relying on densely annotated datasets and without forcing concepts to be associated to single computational units. Extensive experimental evaluation on different categories of objects (animals, tools and vehicles) shows the feasibility of our approach. On average, HOLMES explanations include at least two meronyms, and the ablation of a single meronym roughly halves the holonym model confidence. The resulting heatmaps were quantitatively evaluated using the deletion/insertion/preservation curves. All metrics were comparable to those achieved by GradCAM, while offering the advantage of further decomposing the heatmap in human-understandable concepts, thus highlighting both the relevance of meronyms to object classification, as well as HOLMES ability to capture it. The code is available at https://github.com/FrancesC0de/HOLMES."
https://arxiv.org/abs/2403.08535,2024-03-13,Robustness of Random Networks with Selective Reinforcement against Attacks,"['Tomoyo Kawasumi', 'Takehisa Hasegawa']","We investigate the robustness of random networks reinforced by adding hidden edges against targeted attacks. This study focuses on two types of reinforcement: uniform reinforcement, where edges are randomly added to all nodes, and selective reinforcement, where edges are randomly added only to the minimum degree nodes of the given network. We use generating functions to derive the giant component size and the critical threshold for the targeted attacks on reinforced networks. Applying our analysis and Monte Carlo simulations to the targeted attacks on scale-free networks, it becomes clear that selective reinforcement significantly improves the robustness of networks against the targeted attacks."
https://arxiv.org/abs/2403.08534,2024-03-13,Ensuring connectedness for the Maximum Quasi-clique and Densest $k$-subgraph problems,"['Daniela Scherer dos Santos', 'Kathrin Klamroth', 'Pedro Martins', 'Luís Paquete']","Given an undirected graph $G$, a quasi-clique is a subgraph of $G$ whose density is at least $γ$ $(0 < γ\leq 1)$. Two optimization problems can be defined for quasi-cliques: the Maximum Quasi-Clique (MQC) Problem, which finds a quasi-clique with maximum vertex cardinality, and the Densest $k$-Subgraph (DKS) Problem, which finds the densest subgraph given a fixed cardinality constraint. Most existing approaches to solve both problems often disregard the requirement of connectedness, which may lead to solutions containing isolated components that are meaningless for many real-life applications. To address this issue, we propose two flow-based connectedness constraints to be integrated into known Mixed-Integer Linear Programming (MILP) formulations for either MQC or DKS problems. We compare the performance of MILP formulations enhanced with our connectedness constraints in terms of both running time and number of solved instances against existing approaches that ensure quasi-clique connectedness. Experimental results demonstrate that our constraints are quite competitive, making them valuable for practical applications requiring connectedness."
https://arxiv.org/abs/2403.08533,2024-03-13,Empirical Band-Gap Correction for LDA-Derived Atomic Effective Pseudopotentials,"['Surender Kumar', 'Hanh Bui', 'Gabriel Bester']","Atomic effective pseudopotentials enable atomistic calculations at the level of accuracy of density functional theory for semiconductor nanostructures with up to fifty thousand atoms. Since they are directly derived from ab-initio calculations performed in the local density approximation (LDA), they inherit the typical underestimated band gaps and effective masses. We propose an empirical correction based on the modification of the non-local part of the pseudopotential and demonstrate good performance for bulk binary materials (InP, ZnS, HgTe, GaAs) and quantum dots (InP, CdSe, GaAs) with diameters ranging from 1.0 nm to 4.45 nm. Additionally, we provide a simple analytic expression to obtain accurate quasiparticle and optical band gaps for InP, CdSe, and GaAs QDs, from standard LDA calculation."
https://arxiv.org/abs/2403.08532,2024-03-13,The social value of overreaction to information,"['Matteo Bizzarri', ""Daniele d'Arienzo""]","We study the welfare effects of overreaction to information in the form of diagnostic expectations in markets with asymmetric information, and the effect of a simple intervention in the form of a tax or a subsidy. A large enough level of overreaction is always welfare-decreasing and can rationalize a tax on financial transactions. A small degree of overreaction to private information can both increase or decrease welfare. This is because there are two competing externalities: an information externality, due to the informational role of prices, and a pecuniary externality, due to the allocative role of prices. When the information externality prevails on the pecuniary externality, the loading on private information in agents' trades is too small compared to the welfare optimum: in this case, a small degree of overreaction is welfare-improving."
https://arxiv.org/abs/2403.08531,2024-03-13,"Free fermions, neutrality and modular transformations","['Mbavhalelo Mulokwe', 'Konstantinos Zoubos']","With a view towards higher-spin applications, we study the partition function of a free complex fermion in 2d CFT, restricted to the neutral (zero fermion number) sector. This restriction leads to a partial theta function with a combinatoric interpretation in terms of Dyson's crank of a partition. More crucially, this partition function can be expressed in terms of a q-hypergeometric function with quantum modular properties. This allows us to find its high-temperature asymptotics, including subleading terms which agree with, but also go beyond, what one obtains by imposing neutrality thermodynamically through a chemical potential. We evaluate the asymptotic density of states for this neutral partition function, including the first few subleading terms. Our results should be extendable to more fermions, as well as to higher-spin chemical potentials, which would be of relevance to the higher-spin/minimal model correspondence."
https://arxiv.org/abs/2403.08530,2024-03-13,Inflation and reheating in quadratic metric-affine gravity with derivative couplings,"['Ioannis D. Gialamas', 'Theodoros Katsoulas', 'Kyriakos Tamvakis']","Within the framework of metric-affine theories of gravity, where both the metric and connection are treated as independent variables, we consider actions quadratic in the Ricci scalar curvature coupled non-minimally to a scalar field through derivative couplings. Our analysis delves into the inflationary predictions, revealing their consistency with the latest observational constraints across a wide range of parameters. This compatibility permits adjustments such as an increase in the spectral index and a reduction in the tensor-to-scalar ratio. While we do not propose a specific reheating mechanism, our analysis demonstrates that within the quadratic model of inflation, the maximum reheating temperature can reach $\sim 3\times10^{15}\, {\rm GeV}$."
https://arxiv.org/abs/2403.08529,2024-03-13,Near horizon symmetry of extremal spacelike-stretched black holes,"['B. Cvetković', 'D. Rakonjac']","We analyze the near horizon structure of the extremal spacelike stretched black holes, exact solutions of topologically massive gravity. We show that the algebra of improved canonical generator is realized as a single centrally extended Virasoro algebra. We obtain the entropy of the solution by using the Cardy formula and compare the results with the corresponding non-extremal case."
https://arxiv.org/abs/2403.08528,2024-03-13,"Pig aggression classification using CNN, Transformers and Recurrent Networks","['Junior Silva Souza', 'Eduardo Bedin', 'Gabriel Toshio Hirokawa Higa', 'Newton Loebens', 'Hemerson Pistori']","The development of techniques that can be used to analyze and detect animal behavior is a crucial activity for the livestock sector, as it is possible to monitor the stress and animal welfare and contributes to decision making in the farm. Thus, the development of applications can assist breeders in making decisions to improve production performance and reduce costs, once the animal behavior is analyzed by humans and this can lead to susceptible errors and time consumption. Aggressiveness in pigs is an example of behavior that is studied to reduce its impact through animal classification and identification. However, this process is laborious and susceptible to errors, which can be reduced through automation by visually classifying videos captured in controlled environment. The captured videos can be used for training and, as a result, for classification through computer vision and artificial intelligence, employing neural network techniques. The main techniques utilized in this study are variants of transformers: STAM, TimeSformer, and ViViT, as well as techniques using convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm. These techniques were employed for pig video classification with the objective of identifying aggressive and non-aggressive behaviors. In this work, various techniques were compared to analyze the contribution of using transformers, in addition to the effectiveness of the convolution technique in video classification. The performance was evaluated using accuracy, precision, and recall. The TimerSformer technique showed the best results in video classification, with median accuracy of 0.729."
https://arxiv.org/abs/2403.08527,2024-03-13,Time-bin entanglement in the deterministic generation of linear photonic cluster states,"['David Bauch', 'Nikolas Köcher', 'Nils Heinisch', 'Stefan Schumacher']","We investigate strategies for the efficient deterministic creation of trains of time-bin entangled photons using an individual quantum emitter described by a $Λ$-type electronic system. We explicitly demonstrate generation of high-quality linear cluster states of substantial length in our full microscopic numerical simulations. The underlying scheme is based on the manipulation of ground state coherences through precise optical driving. One important finding is that the most easily accessible quality metrics, the achievable rotation fidelities, fall short in assessing the actual quantum correlations of the emitted photons in the face of losses. To address this, we explicitly calculate stabilizer generator expectation values as a superior gauge for the quantum properties of the many-photon state. Our results illustrate that with controlled minimization of losses and realistic system parameters for quantum-dot type systems, useful linear cluster states of significant lengths can be generated, showcasing promise of scalability for quantum information processing endeavors."
https://arxiv.org/abs/2403.08526,2024-03-13,"Field demonstration of a fully managed, L1 encrypted 3-node network with hybrid relayed-QKD and centralized symmetric classical key management","['N. Makris', 'A. Papageorgopoulos', 'K. Tsimvrakidis', 'P. Konteli', 'Y. Gautier', 'M. Terenziani', 'E. Daudin', 'D. Ntoulias', 'T. Fragkioudakis', 'I. Meletios', 'M. Mosca', 'D. Hobbs', 'T. Rosati', 'I. Papastamatiou', 'O. Prnjat', 'K. Koumantaros', 'D. Mitropoulos', 'Jean-Robert Morax', 'Bruno Huttner', 'O. K. Christodoulopoulos', 'G. T. Kanellos', 'D. Syvridis']","We successfully demonstrated a fully-managed, field-deployed, three-node QKD ring network with L1-OTNsec encryption, that employs a hybrid scheme of QKD and classical yet quantum-safe centrally-generated symmetric keys to support point-to-point and relay consumers."
https://arxiv.org/abs/2403.08525,2024-03-13,From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning,"['John Martinsson', 'Olof Mogren', 'Maria Sandsten', 'Tuomas Virtanen']","In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments. The goal is to maximize the amount of information gained about the temporal activation's of the target sounds. For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation. The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset. The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop. The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities. We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favorable results for A-CPD when compared to two baseline query strategies."
https://arxiv.org/abs/2403.08524,2024-03-13,Analytical Forward Dynamics Modeling of Linearly Actuated Heavy-Duty Parallel-Serial Manipulators,"['Paz Alvaro', 'Jouni Mattila']","This paper presents a new geometric and recursive algorithm for analytically computing the forward dynamics of heavy-duty parallel-serial mechanisms. Our solution relies on expressing the dynamics of a class of linearly-actuated parallel mechanism to a lower dimensional dual Lie algebra to find an analytical solution for the inverse dynamics problem. Thus, by applying the articulated-body inertias method, we successfully provide analytic expressions for the total wrench in the linear-actuator reference frame, the linear acceleration of the actuator, and the total wrench exerted in the base reference frame of the closed loop. This new formulation allows to backwardly project and assemble inertia matrices and wrench bias of multiple closed-loops mechanisms. The final algorithm holds an O(n) algorithmic complexity, where $n$ is the number of degrees of freedom (DoF). We provide accuracy results to demonstrate its efficiency with 1-DoF closed-loop mechanism and 4-DoF manipulator composed by serial and parallel mechanisms. Additionally, we release a URDF multi-DoF code for this recursive algorithm."
https://arxiv.org/abs/2403.08523,2024-03-13,Elliptic billiard with harmonic potential: Classical description,"['Bernardo Barrera', 'Juan P. Ruz-Cuen', 'Julio C. Gutiérrez-Vega']","The classical dynamics of the isotropic two-dimensional harmonic oscillator confined by an elliptic hard wall is discussed. The interplay between the harmonic potential with circular symmetry and the boundary with elliptical symmetry does not spoil the separability in elliptic coordinates; however, it generates non-trivial energy and momentum dependencies in the billiard. We analyze the equi-momentum surfaces in the parameters space and classify the kinds of motion the particle can have in the billiard. The winding numbers and periods of the rotational and librational trajectories are analytically calculated and numerically verified. A remarkable finding is the possibility of having degenerate rotational trajectories with the same energy but different second constant of motion and different caustics and periods. The conditions to get these degenerate trajectories are analyzed. Similarly, we show that obtaining two different rotational trajectories with the same period and second constant of motion but different energy is possible."
https://arxiv.org/abs/2403.08522,2024-03-13,Random Groups are not $n$-Cubulated,['Zachary Munro'],"A group $G$ has $F\mathcal C_n$ if every action on a $n$-dimensional $\mathrm{CAT}(0)$ cube complex has a global fixed point. This provides a natural stratification between Serre's $FA$ and Kazhdan's $(T)$. For every $n$, we show that random groups in the plain words density model have $F\mathcal C_n$ with overwhelming probability. The same result holds for random groups in the reduced words density model assuming there are sufficiently many generators. These are the first examples of cubulated hyperbolic groups with $F\mathcal C_n$ for $n$ arbitrarily large."
https://arxiv.org/abs/2403.08521,2024-03-13,Quantised $\mathfrak{sl}_2$-differential algebras,"['Andrey Krutov', 'Pavle Pandžić']",We propose a definition of a quantised $\mathfrak{sl}_2$-differential algebra and show that the quantised exterior algebra (defined by Berenstein and Zwicknagl) and the quantised Clifford algebra (defined by the authors) of $\mathfrak{sl}_2$ are natural examples of such algebras.
https://arxiv.org/abs/2403.08520,2024-03-13,Colloidal Homogenisation for the Hydrodynamics of Nematic Liquid Crystals,"['Francesco De Anna', 'Anja Schloemerkemper', 'Arghir Zarnescu']","This paper analytically explores a simplified model for the hydrodynamics of nematic liquid crystal colloids. We integrate a Stokes equation for the velocity field with a Ginzburg-Landau transported heat flow for the director field. The study focuses on a bounded spatial domain containing periodically distributed colloidal particles, which impose no-anchoring conditions on the nematic liquid crystal. By progressively reducing the particle size to zero and simultaneously increasing the number of particles, we delve into the associated homogenisation problem. Our analysis uncovers a form of decoupling where the velocity field asymptotically satisfies a Darcy equation, independent of the director, while the director follows a gradient flow, unaffected by the velocity field. One of the most intricate aspects of the homogenisation process is the absence of an extension operator for the director field that preserves the uniform estimates related to the system's energy. We address this challenge with a novel variation of the Aubin-Lions lemma, specifically adapted for homogenisation problems."
https://arxiv.org/abs/2403.08519,2024-03-13,Projective Quantum Eigensolver via Adiabatically Decoupled Subsystem Evolution: a Resource Efficient Approach to Molecular Energetics in Noisy Quantum Computers,"['Chayan Patra', 'Sonaldeep Halder', 'Rahul Maitra']","Quantum computers hold immense potential in the field of chemistry, ushering new frontiers to solve complex many body problems that are beyond the reach of classical computers. However, noise in the current quantum hardware limits their applicability to large chemical systems. This work encompasses the development of a projective formalism that aims to compute ground-state energies of molecular systems accurately using Noisy Intermediate Scale Quantum (NISQ) hardware in a resource efficient manner. Our approach is reliant upon the formulation of a bipartitely decoupled parameterized ansatz within the disentangled unitary coupled cluster (dUCC) framework based on the principles of synergetics. Such decoupling emulates the total parameter optimization in a lower dimensional manifold, while a mutual synergistic relationship among the parameters is exploited to ensure characteristic accuracy. Without any pre-circuit measurements, our method leads to a highly compact fixed-depth ansatz with shallower circuits and fewer expectation value evaluations. Through analytical and numerical demonstrations, we demonstrate the method's superior performance under noise while concurrently ensuring requisite accuracy in future fault-tolerant systems. This approach enables rapid exploration of emerging chemical spaces by efficient utilization of near-term quantum hardware resources."
https://arxiv.org/abs/2403.08518,2024-03-13,Stability of Weyl node merging processes under symmetry constraints,"['Gabriele Naselli', 'György Frank', 'Dániel Varjas', 'Ion Cosma Fulga', 'Gergő Pintér', 'András Pályi', 'Viktor Könye']","Changes in the number of Weyl nodes in Weyl semimetals occur through merging processes, usually involving a pair of oppositely charged nodes. More complicated processes involving multiple Weyl nodes are also possible, but they typically require fine tuning and are thus less stable. In this work, we study how symmetries affect the allowed merging processes and their stability, focusing on the combination of a two-fold rotation and time-reversal ($C_2\mathcal{T}$) symmetry. We find that, counter-intuitively, processes involving a merging of three nodes are more generic than processes involving only two nodes. Our work suggests that multi-Weyl-merging may be observed in a large variety of quantum materials, and we discuss SrSi$_2$ and bilayer graphene as potential candidates"
https://arxiv.org/abs/2403.08517,2024-03-13,An Extended View on Measuring Tor AS-level Adversaries,"['Gabriel Karl Gegenhuber', 'Markus Maier', 'Florian Holzbauer', 'Wilfried Mayer', 'Georg Merzdovnik', 'Edgar Weippl', 'Johanna Ullrich']","Tor provides anonymity to millions of users around the globe which has made it a valuable target for malicious actors. As a low-latency anonymity system, it is vulnerable to traffic correlation attacks from strong passive adversaries such as large autonomous systems (ASes). In preliminary work, we have developed a measurement approach utilizing the RIPE Atlas framework -- a network of more than 11,000 probes worldwide -- to infer the risk of deanonymization for IPv4 clients in Germany and the US."
https://arxiv.org/abs/2403.08516,2024-03-13,What Does the Large Magellanic Cloud Look Like? It Depends on [M/H] and Age,"['Neige Frankel', 'Rene Andrae', 'Hans-Walter Rix', 'Joshua Povick', 'Vedant Chandra']","We offer a new way to look at the Large Magellanic Cloud through stellar mono-abundance and mono-age-mono-abundance maps. These maps are based on $\gtrsim 500\,000$ member stars with photo-spectroscopic [M/H] and age estimates from Gaia DR3 data, and they are the first area-complete, metallicity- and age-differentiated stellar maps of any disk galaxy. Azimuthally averaged, these maps reveal a surprisingly simple picture of the Milky Way's largest satellite galaxy. For any [M/H] below -0.1 dex, the LMC's radial profile is well described by a simple exponential, but with a scale length that steadily shrinks towards higher metallicities, from nearly 2.3~kpc at [M/H]$=-1.8$ to only 0.75~kpc at [M/H]$=-0.25$. The prominence of the bar decreases dramatically with [M/H], making it barely discernible at [M/H]$\lesssim -1.5$. Yet, even for metal-rich populations, the bar has little impact on the azimuthally averaged profile of the mono-abundance components. Including ages, we find that the scale length is a greater function of age than of metallicity, with younger populations far more centrally concentrated. At old ages, the scale length decreases with increasing metallicity; at young ages, the scale-length is independent of metallicity. These findings provide quantitative support for a scenario where the LMC built its stellar structure effectively outside in."
https://arxiv.org/abs/2403.08515,2024-03-13,Plotinus: A Satellite Internet Digital Twin System,"['Yue Gao', 'Kun Qiu', 'Zhe Chen', 'Wenjun Zhu', 'Qi Zhang', 'Handong Luo', 'Quanwei Lin', 'Ziheng Yang', 'Wenhao Liu']","The development of integrated space-air-ground network (SAGIN) requires sophisticated satellite Internet emulation tools that can handle complex, dynamic topologies and offer in-depth analysis. Existing emulation platforms struggle with challenges like the need for detailed implementation across all network layers, real-time response times, and the ability to scale. Plotinus, a new digital twin system based on microservices for satellite Internet emulation, aims to solve these problems. It features a modular design, allowing for easy replacement of the physical layer to emulate different aerial vehicles and analyze channel interference. It also enables the replacement of path computation methods to simplify testing and deploying algorithms. In particular, Plotinus allows for real-time emulation with live network traffic, enhancing the realism of network models. Evaluation result shows that Plotinus's effective emulation of dynamic satellite networks with real-world devices. Its adaptability for various communication models and algorithm testing highlights Plotinus's role as a vital tool for developing and analyzing SAGIN systems, offering a scalable, real-time response, and flexible digital twin system."
https://arxiv.org/abs/2403.08514,2024-03-13,Spatial Latent Gaussian Modelling with Change of Support,"['Erick A. Chacón-Montalván', 'Peter M. Atkinson', 'Christopher Nemeth', 'Benjamin M. Taylor', 'Paula Moraga']","Spatial data are often derived from multiple sources (e.g. satellites, in-situ sensors, survey samples) with different supports, but associated with the same properties of a spatial phenomenon of interest. It is common for predictors to also be measured on different spatial supports than the response variables. Although there is no standard way to work with spatial data with different supports, a prevalent approach used by practitioners has been to use downscaling or interpolation to project all the variables of analysis towards a common support, and then using standard spatial models. The main disadvantage with this approach is that simple interpolation can introduce biases and, more importantly, the uncertainty associated with the change of support is not taken into account in parameter estimation. In this article, we propose a Bayesian spatial latent Gaussian model that can handle data with different rectilinear supports in both the response variable and predictors. Our approach allows to handle changes of support more naturally according to the properties of the spatial stochastic process being used, and to take into account the uncertainty from the change of support in parameter estimation and prediction. We use spatial stochastic processes as linear combinations of basis functions where Gaussian Markov random fields define the weights. Our hierarchical modelling approach can be described by the following steps: (i) define a latent model where response variables and predictors are considered as latent stochastic processes with continuous support, (ii) link the continuous-index set stochastic processes with its projection to the support of the observed data, (iii) link the projected process with the observed data. We show the applicability of our approach by simulation studies and modelling land suitability for improved grassland in Rhondda Cynon Taf, a county borough in Wales."
https://arxiv.org/abs/2403.08513,2024-03-13,3D Spectrum Mapping and Reconstruction under Multi-Radiation Source Scenarios,"['Wang Jie', 'Lin Zhipeng', 'Zhu Qiuming', 'Wu Qihui', 'Lan Tianxu', 'Zhao Yi', 'Bai Yunpeng', 'Zhong Weizhi']","Spectrum map construction, which is crucial in cognitive radio (CR) system, visualizes the invisible space of the electromagnetic spectrum for spectrum-resource management and allocation. Traditional reconstruction methods are generally for two-dimensional (2D) spectrum map and driven by abundant sampling data. In this paper, we propose a data-model-knowledge-driven reconstruction scheme to construct the three-dimensional (3D) spectrum map under multi-radiation source scenarios. We firstly design a maximum and minimum path loss difference (MMPLD) clustering algorithm to detect the number of radiation sources in a 3D space. Then, we develop a joint location-power estimation method based on the heuristic population evolutionary optimization algorithm. Considering the variation of electromagnetic environment, we self-learn the path loss (PL) model based on the sampling data. Finally, the 3D spectrum is reconstructed according to the self-learned PL model and the extracted knowledge of radiation sources. Simulations show that the proposed 3D spectrum map reconstruction scheme not only has splendid adaptability to the environment, but also achieves high spectrum construction accuracy even when the sampling rate is very low."
https://arxiv.org/abs/2403.08512,2024-03-13,UniLiDAR: Bridge the domain gap among different LiDARs for continual learning,"['Zikun Xu', 'Jianqiang Wang', 'Shaobing Xu']","LiDAR-based 3D perception algorithms have evolved rapidly alongside the emergence of large datasets. Nonetheless, considerable performance degradation often ensues when models trained on a specific dataset are applied to other datasets or real-world scenarios with different LiDAR. This paper aims to develop a unified model capable of handling different LiDARs, enabling continual learning across diverse LiDAR datasets and seamless deployment across heterogeneous platforms. We observe that the gaps among datasets primarily manifest in geometric disparities (such as variations in beams and point counts) and semantic inconsistencies (taxonomy conflicts). To this end, this paper proposes UniLiDAR, an occupancy prediction pipeline that leverages geometric realignment and semantic label mapping to facilitate multiple datasets training and mitigate performance degradation during deployment on heterogeneous platforms. Moreover, our method can be easily combined with existing 3D perception models. The efficacy of the proposed approach in bridging LiDAR domain gaps is verified by comprehensive experiments on two prominent datasets: OpenOccupancy-nuScenes and SemanticKITTI. UniLiDAR elevates the mIoU of occupancy prediction by 15.7% and 12.5%, respectively, compared to the model trained on the directly merged dataset. Moreover, it outperforms several SOTA methods trained on individual datasets. We expect our research to facilitate further study of 3D generalization, the code will be available soon."
https://arxiv.org/abs/2403.08511,2024-03-13,A Multimodal Fusion Network For Student Emotion Recognition Based on Transformer and Tensor Product,"['Ao Xiang', 'Zongqing Qi', 'Han Wang', 'Qin Yang', 'Danqing Ma']","In recent years, there have been frequent incidents of foreign objects intruding into railway and Airport runways. These objects can include pedestrians, vehicles, animals, and debris. This paper introduces an improved YOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance the detection of foreign objects on railways and Airport runways. This study proposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which combines two public datasets for detecting foreign objects in aviation and railway systems. The dataset aims to improve the recognition capabilities of foreign object targets. Experimental results on this large dataset have demonstrated significant performance improvements of the proposed model over the baseline YOLOv5 model, reducing computational requirements. improved YOLO model shows a significant improvement in precision by 1.2%, recall rate by 1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters were reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%. In the ablation experiment, it is found that the FasterNet module can significantly reduce the number of parameters of the model, and the reference of the attention mechanism can slow down the performance loss caused by lightweight."
https://arxiv.org/abs/2403.08510,2024-03-13,An optically defined phononic crystal defect,"['Thomas J. Clark', 'Simon Bernard', 'Jiaxing Ma', 'Vincent Dumont', 'Jack C. Sankey']","We present a phononic crystal with a defect mode defined and controlled entirely by optical forces. By applying a strong optical trap to a single unit cell of a Si$_3$N$_4$ phononic crystal membrane, we coherently couple many modes to each other, and smoothly transfer a single defect mode into the phononic bandgap. This is accompanied by localization of the mode's spatial profile from one spanning the entire crystal to one confined within just a few unit cells, as evidenced by a 37-fold reduction in the participating mass. This proof-of-concept coupling between light and mechanical mode shape significantly broadens the optomechanical design space. It also demonstrates the feasibility of optically programmable lattice potentials, enabling (in addition to defects) reconfigurable waveguides, superlattices, and disorder, with corresponding avenues of research toward creating unique states of macroscopic quantum motion."
https://arxiv.org/abs/2403.08509,2024-03-13,Torsion-free connections of second-order maximally superintegrable systems,['Andreas Vollmer'],"Second-order (maximally) conformally superintegrable systems play an important role as models of mechanical systems, including systems such as the Kepler-Coulomb system and the isotropic harmonic oscillator. The present paper is dedicated to understanding non- and semi-degenerate systems. We obtain ""projective flatness"" results for two torsion-free connections naturally associated to such systems. This viewpoint sheds some light onto the interrelationship of properly and conformally (second-order maximally) superintegrable systems from a geometrical perspective. It is shown that the semi-degenerate secondary structure tensor can be viewed as the Ricci curvature of a natural torsion-free connection defined by the primary structure tensor (and similarly in the non-degenerate case). It is also shown that properly semi-degenerate systems are characterised, similar to the non-degenerate case, by the vanishing of the secondary structure tensor."
https://arxiv.org/abs/2403.08508,2024-03-13,Quantum simulation in hybrid transmission lines,"['Alessandro Ferreri', 'Frank K. Wilhelm']","Platforms based on transmission lines are nowadays employed for the simulation of standard phenomena in quantum electrodynamics and quantum field theory. In this work, we propose a hybrid platform, in which a right-handed transmission line is connected to a left-handed transmission line by means of a superconducting quantum interference device (SQUID). We examine the interaction between the two transmission lines, as well as the excitation flow along the composed platform. We show that, by activating specific resonance conditions, this platform can be used as a quantum simulator of different phenomena in quantum optics, multimode quantum systems and quantum thermodynamics."
https://arxiv.org/abs/2403.08507,2024-03-13,MobileAtlas: Geographically Decoupled Measurements in Cellular Networks for Security and Privacy Research,"['Gabriel Karl Gegenhuber', 'Wilfried Mayer', 'Edgar Weippl', 'Adrian Dabrowski']",Cellular networks are not merely data access networks to the Internet. Their distinct services and ability to form large complex compounds for roaming purposes make them an attractive research target in their own right. Their promise of providing a consistent service with comparable privacy and security across roaming partners falls apart at close inspection.
https://arxiv.org/abs/2403.08506,2024-03-11,DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning,"['Sikai Bai', 'Jie Zhang', 'Shuaicheng Li', 'Song Guo', 'Jingcai Guo', 'Jun Hou', 'Tao Han', 'Xiaocheng Lu']","Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., global prompt to capture general knowledge across all clients and domain prompts to capture domain-specific knowledge. They eliminate the restriction on the one-to-one mapping between source domains and local clients. Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep text-image alignments based on prompt tuning without labor-intensive annotation. Extensive experiments on multiple datasets demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods when domain labels are not provided, and even outperforms many centralized learning methods using domain labels."
https://arxiv.org/abs/2403.08505,2024-03-13,Content-aware Masked Image Modeling Transformer for Stereo Image Compression,"['Xinjie Zhang', 'Shenyuan Gao', 'Zhening Liu', 'Xingtong Ge', 'Dailan He', 'Tongda Xu', 'Yan Wang', 'Jun Zhang']","Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-distortion performance on two stereo image datasets Cityscapes and InStereo2K with fast encoding and decoding speed."
https://arxiv.org/abs/2403.08504,2024-03-13,OccFiner: Offboard Occupancy Refinement with Hybrid Propagation,"['Hao Shi', 'Song Wang', 'Jiaming Zhang', 'Xiaoting Yin', 'Zhongdao Wang', 'Zhijian Zhao', 'Guangming Wang', 'Jianke Zhu', 'Kailun Yang', 'Kaiwei Wang']","Vision-based occupancy prediction, also known as 3D Semantic Scene Completion (SSC), presents a significant challenge in computer vision. Previous methods, confined to onboard processing, struggle with simultaneous geometric and semantic estimation, continuity across varying viewpoints, and single-view occlusion. Our paper introduces OccFiner, a novel offboard framework designed to enhance the accuracy of vision-based occupancy predictions. OccFiner operates in two hybrid phases: 1) a multi-to-multi local propagation network that implicitly aligns and processes multiple local frames for correcting onboard model errors and consistently enhancing occupancy accuracy across all distances. 2) the region-centric global propagation, focuses on refining labels using explicit multi-view geometry and integrating sensor bias, especially to increase the accuracy of distant occupied voxels. Extensive experiments demonstrate that OccFiner improves both geometric and semantic accuracy across various types of coarse occupancy, setting a new state-of-the-art performance on the SemanticKITTI dataset. Notably, OccFiner elevates vision-based SSC models to a level even surpassing that of LiDAR-based onboard SSC models."
https://arxiv.org/abs/2403.08503,2024-03-13,Small field chaos in spin glasses: universal predictions from the ultrametric tree and comparison with numerical simulations,"['Miguel Aguilar-Janita', 'Silvio Franz', 'Victor Martin-Mayor', 'Javier Moreno-Gordo', 'Giorgio Parisi', 'Federico Ricci-Tersenghi', 'Juan J. Ruiz-Lorenzo']","We study the chaotic behavior of the Gibbs state of spin-glasses under the application of an external magnetic field, in the crossover region where the field intensity scales proportional to $1/\sqrt{N}$, being $N$ the system size. We show that Replica Symmetry Breaking (RSB) theory provides universal predictions for chaotic behavior: they depend only on the zero-field overlap probability function $P(q)$ and are independent of other features of the system. Using solely $P(q)$ as input we can analytically predict quantitatively the statistics of the states in a small field. In the infinite volume limit, each spin-glass sample is characterized by an infinite number of states that have a tree-like structure. We generate the corresponding probability distribution through efficient sampling using a representation based on the Bolthausen-Snitmann coalescent. In this way, we can compute quantitatively properties in the presence of a magnetic field in the crossover region, the overlap probability distribution in the presence of a small field and the degree of decorrelation as the field is increased. To test our computations, we have simulated the Bethe lattice spin glass and the 4D Edwards-Anderson model, finding in both cases excellent agreement with the universal predictions."
https://arxiv.org/abs/2403.08502,2024-03-13,Masked Generative Story Transformer with Character Guidance and Caption Augmentation,"['Christos Papadimitriou', 'Giorgos Filandrianos', 'Maria Lymperaiou', 'Giorgos Stamou']","Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences. Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters. On the contrary, we embrace a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency. Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space. We also employ a caption-augmentation technique, carried out by a Large Language Model (LLM), to enhance the robustness of our approach. The combination of these methods culminates into state-of-the-art (SOTA) results over various metrics in the most prominent SV benchmark (Pororo-SV), attained with constraint resources while achieving superior computational complexity compared to previous arts. The validity of our quantitative results is supported by a human survey."
https://arxiv.org/abs/2403.08501,2024-03-13,Governing Through the Cloud: The Intermediary Role of Compute Providers in AI Regulation,"['Lennart Heim', 'Tim Fist', 'Janet Egan', 'Sihao Huang', 'Stephen Zekany', 'Robert Trager', 'Michael A Osborne', 'Noa Zilberman']","As jurisdictions around the world take their first steps toward regulating the most powerful AI systems, such as the EU AI Act and the US Executive Order 14110, there is a growing need for effective enforcement mechanisms that can verify compliance and respond to violations. We argue that compute providers should have legal obligations and ethical responsibilities associated with AI development and deployment, both to provide secure infrastructure and to serve as intermediaries for AI regulation. Compute providers can play an essential role in a regulatory ecosystem via four key capacities: as securers, safeguarding AI systems and critical infrastructure; as record keepers, enhancing visibility for policymakers; as verifiers of customer activities, ensuring oversight; and as enforcers, taking actions against rule violations. We analyze the technical feasibility of performing these functions in a targeted and privacy-conscious manner and present a range of technical instruments. In particular, we describe how non-confidential information, to which compute providers largely already have access, can provide two key governance-relevant properties of a computational workload: its type-e.g., large-scale training or inference-and the amount of compute it has consumed. Using AI Executive Order 14110 as a case study, we outline how the US is beginning to implement record keeping requirements for compute providers. We also explore how verification and enforcement roles could be added to establish a comprehensive AI compute oversight scheme. We argue that internationalization will be key to effective implementation, and highlight the critical challenge of balancing confidentiality and privacy with risk mitigation as the role of compute providers in AI regulation expands."
https://arxiv.org/abs/2403.08500,2024-03-13,Highly confined epsilon-near-zero- and surface-phonon polaritons in SrTiO3 membranes,"['Ruijuan Xu', 'Iris Crassee', 'Hans A. Bechtel', 'Yixi Zhou', 'Adrien Bercher', 'Lukas Korosec', 'Carl Willem Rischau', 'Jérémie Teyssier', 'Kevin J. Crust', 'Yonghun Lee', 'Stephanie N. Gilbert Corder', 'Jiarui Li', 'Jennifer A. Dionne', 'Harold Y. Hwang', 'Alexey B. Kuzmenko', 'Yin Liu']","Recent theoretical studies have suggested that transition metal perovskite oxide membranes can enable surface phonon polaritons in the infrared range with low loss and much stronger subwavelength confinement than bulk crystals. Such modes, however, have not been experimentally observed so far. Here, using a combination of far-field Fourier-transform infrared (FTIR) spectroscopy and near-field synchrotron infrared nanospectroscopy (SINS) imaging, we study the phonon-polaritons in a 100 nm thick freestanding crystalline membrane of SrTiO3 transferred on metallic and dielectric substrates. We observe a symmetric-antisymmetric mode splitting giving rise to epsilon-near-zero and Berreman modes as well as highly confined (by a factor of 10) propagating phonon polaritons, both of which result from the deep-subwavelength thickness of the membranes. Theoretical modeling based on the analytical finite-dipole model and numerical finite-difference methods fully corroborate the experimental results. Our work reveals the potential of oxide membranes as a promising platform for infrared photonics and polaritonics."
https://arxiv.org/abs/2403.08499,2024-03-13,Improved YOLOv5 Based on Attention Mechanism and FasterNet for Foreign Object Detection on Railway and Airway tracks,"['Zongqing Qi', 'Danqing Ma', 'Jingyu Xu', 'Ao Xiang', 'Hedi Qu']","In recent years, there have been frequent incidents of foreign objects intruding into railway and Airport runways. These objects can include pedestrians, vehicles, animals, and debris. This paper introduces an improved YOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance the detection of foreign objects on railways and Airport runways. This study proposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which combines two public datasets for detecting foreign objects in aviation and railway systems.The dataset aims to improve the recognition capabilities of foreign object targets. Experimental results on this large dataset have demonstrated significant performance improvements of the proposed model over the baseline YOLOv5 model, reducing computational requirements.Improved YOLO model shows a significant improvement in precision by 1.2%, recall rate by 1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters were reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%. In the ablation experiment, it is found that the FasterNet module can significantly reduce the number of parameters of the model, and the reference of the attention mechanism can slow down the performance loss caused by lightweight."
https://arxiv.org/abs/2403.08498,2024-03-13,Gaussian Splatting in Style,"['Abhishek Saroha', 'Mariia Gladkova', 'Cecilia Curreli', 'Tarun Yenamandra', 'Daniel Cremers']","Scene stylization extends the work of neural style transfer to three spatial dimensions. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting. A vast majority of the previous works achieve this by optimizing the scene with a specific style image. In contrast, we propose a novel architecture trained on a collection of style images, that at test time produces high quality stylized novel views. Our work builds up on the framework of 3D Gaussian splatting. For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views. The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime. This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications. Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data."
https://arxiv.org/abs/2403.08497,2024-03-13,Viro's patchworking and the signed reduced A-discriminant,"['Weixun Deng', 'J. Maurice Rojas', 'Máté L. Telek']","Computing the isotopy type of a hypersurface, defined as the positive real zero set of a multivariate polynomial, is a challenging problem in real algebraic geometry. We focus on the case where the defining polynomial has combinatorially restricted exponent vectors and fixed coefficient signs, enabling faster computation of the isotopy type. In particular, Viro's patchworking provides a polyhedral complex that has the same isotopy type as the hypersurface, for certain choices of the coefficients. So we present properties of the signed support, focussing mainly on the case of n-variate (n+3)-nomials, that ensure all possible isotopy types can be obtained via patchworking. To prove this, we study the signed reduced A-discriminant and show that it has a simple structure if the signed support satisfies some combinatorial conditions."
https://arxiv.org/abs/2403.08496,2024-03-13,Universal and robust quantum coherent control based on a chirped-pulse driving protocol,"['Yue-Hao Yin', 'Jin-Xin Yang', 'Li-Xiang Cen']","We propose a chirped-pulse driving protocol and reveal its exceptional property for quantum coherent control. The nonadiabatic passage generated by the driving protocol, which includes the population inversion and the nonadiabaticity-induced transition as its ingredients, is shown to be robust against pulse truncation. We further demonstrate that the protocol allows for universal manipulation on the qubit system through designing pulse sequences with either properly adjusted sweeping frequency or pulsing intensity."
https://arxiv.org/abs/2403.08495,2024-03-13,Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator,"['Yusheng Liao', 'Yutong Meng', 'Yuhao Wang', 'Hongcheng Liu', 'Yanfeng Wang', 'Yu Wang']","Large Language Models (LLMs) have demonstrated remarkable proficiency in human interactions, yet their application within the medical field remains insufficiently explored. Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of LLMs on clinical tasks. In the quest to enhance the application of Large Language Models (LLMs) in healthcare, this paper introduces the Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS), targeting the gap between traditional LLM evaluations and the nuanced demands of clinical practice. Unlike prior methods that rely on static medical knowledge assessments, AIE and SAPS provide a dynamic, realistic platform for assessing LLMs through multi-turn doctor-patient simulations. This approach offers a closer approximation to real clinical scenarios and allows for a detailed analysis of LLM behaviors in response to complex patient interactions. Our extensive experimental validation demonstrates the effectiveness of the AIE framework, with outcomes that align well with human evaluations, underscoring its potential to revolutionize medical LLM testing for improved healthcare delivery."
https://arxiv.org/abs/2403.08494,2024-03-13,On the structure of graded Lie superalgebras,"['Antonio J. Calderón', 'José M. Sanchez']","We study the structure of graded Lie superalgebras with arbitrary dimension and over an arbitrary field ${\mathbb K}$. We show that any of such algebras ${\mathfrak L}$ with a symmetric $G$-support is of the form ${\mathfrak L} = U + \sum\limits_{j}I_{j}$ with $U$ a subspace of ${\mathfrak L}_1$ and any $I_{j}$ a well described graded ideal of ${\mathfrak L}$, satisfying $[I_j,I_k] = 0$ if $j\neq k$. Under certain conditions, it is shown that ${\mathfrak L} = (\bigoplus\limits_{k \in K} I_k) \oplus (\bigoplus\limits_{q \in Q} I_q),$ where any $I_k$ is a gr-simple graded ideal of ${\mathfrak L}$ and any $I_q$ a completely determined low dimensional non gr-simple graded ideal of ${\mathfrak L}$, satisfying $[I_q,I_{q'}] = 0$ for any $q'\in Q$ with $q \neq q'$."
https://arxiv.org/abs/2403.08493,2024-03-13,A Prediction Model for Rumor Forwarding Behavior Based on Uncertain Time Series,"['Ruihong Wang', 'Fingming Liu']","The rapid spread of rumors in social media is mainly caused by individual retweets. This paper applies uncertainty time series analysis (UTSA) to analyze a rumor retweeting behavior on Weibo. First, the rumor forwarding is modeled using uncertain time series, including order selection, parameter estimation, residual analysis, uncertainty hypothesis testing and forecast, and the validity of using uncertain time series analysis is further supported by analyzing the characteristics of the residual plot. The experimental results show that the uncertain time series can better predict the next stage of rumor forwarding. The results of the study have important practical significance for rumor management and the management of social media information dissemination."
https://arxiv.org/abs/2403.08492,2024-03-13,Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking,"['Ming Dong', 'Yujing Chen', 'Miao Zhang', 'Hao Sun', 'Tingting He']","Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verified the superiority of our proposed framework."
https://arxiv.org/abs/2403.08491,2024-03-13,Compliant Hierarchical Control for Arbitrary Equality and Inequality Tasks with Strict and Soft Priorities,['Gianluca Garofalo'],"When a robotic system is redundant with respect to a given task, the remaining degrees of freedom can be used to satisfy additional objectives. With current robotic systems having more and more degrees of freedom, this can lead to an entire hierarchy of tasks that need to be solved according to given priorities. In this paper, the first compliant control strategy is presented that allows to consider an arbitrary number of equality and inequality tasks, while still preserving the natural inertia of the robot. The approach is therefore a generalization of a passivity-based controller to the case of an arbitrary number of equality and inequality tasks. The key idea of the method is to use a Weighted Hierarchical Quadratic Problem to extract the set of active tasks and use the latter to perform a coordinate transformation that inertially decouples the tasks. Thereby unifying the line of research focusing on optimization-based and passivity-based multi-task controllers. The method is validated in simulation."
https://arxiv.org/abs/2403.08490,2024-03-13,Sharp detection of the onset of Floquet heating using eigenstate sensitivity,"['Sourav Bhattacharjee', 'Souvik Bandyopadhyay', 'Anatoli Polkovnikov']","Chaotic Floquet systems at sufficiently low driving frequencies are known to heat up to an infinite temperature ensemble in the thermodynamic limit. However at high driving frequencies, Floquet systems remain energetically stable in a robust prethermal phase with exponentially long heating times. We propose sensitivity (susceptibility) of Floquet eigenstates against infinitesimal deformations of the drive, as a sharp and sensitive measure to detect this heating transition. It also captures various regimes (timescales) of Floquet thermalization accurately. Particularly, we find that at low frequencies near the onset of unbounded heating, Floquet eigenstates are maximally sensitive to perturbations and consequently the scaled susceptibility develops a sharp maximum. We further connect our results to the relaxation dynamics of local observables to show that near the onset of Floquet heating, the system is nonergodic with slow glassy dynamics despite being nonintegrable at all driving frequencies."
https://arxiv.org/abs/2403.08489,2024-03-13,Effect of the charge asymmetry and orbital angular momentum in the entrance channel on the hindrance to complete fusion,"['Elzod Khusanov', 'Avazbek Nasirov', 'Mukhtorali Nishonov']",The hindrance to complete fusion is studied as a function of the charge asymmetry of colliding nuclei and orbital angular momentum of the collision. The formation and evolution of a dinuclear system (DNS) in the heavy ion collisions at energies near the Coulomb barrier is calculated in the framework of the DNS model. The DNS evolution is considered as nucleon transfer between its fragments. The results prove that a hindrance at formation of a compound nucleus (CN) is related with the quasifission process which is breakup of the DNS into products instead to reach the equilibrated state of the CN. The role of the angular momentum in the charge (mass) distribution of the reaction products for the given mass asymmetry of the colliding nuclei has been demonstrated. The results of this work have been compared with the measured data for the quasifission yields in the $^{12}$C+$^{204}$Pb and $^{48}$Ca+$^{168}$Er reactions to show the role of the mass asymmetry of the entrance channel.
https://arxiv.org/abs/2403.08488,2024-03-13,CAM: A Collection of Snapshots of GitHub Java Repositories Together with Metrics,['Yegor Bugayenko'],"Even though numerous researchers require stable datasets along with source code and basic metrics calculated on them, neither GitHub nor any other code hosting platform provides such a resource. Consequently, each researcher must download their own data, compute the necessary metrics, and then publish the dataset somewhere to ensure it remains accessible indefinitely. Our CAM (stands for ``Classes and Metrics'') project addresses this need. It is an open-source software capable of cloning Java repositories from GitHub, filtering out unnecessary files, parsing Java classes, and computing metrics such as Cyclomatic Complexity, Halstead Effort and Volume, C\&K metrics, Maintainability Metrics, LCOM5 and HND, as well as some Git-based Metrics. At least once a year, we execute the entire script, a process which requires a minimum of ten days on a very powerful server, to generate a new dataset. Subsequently, we publish it on Amazon S3, thereby ensuring its availability as a reference for researchers. The latest archive of 2.2Gb that we published on the 2nd of March, 2024 includes 532K Java classes with 48 metrics for each class."
https://arxiv.org/abs/2403.08487,2024-03-13,Model Will Tell: Training Membership Inference for Diffusion Models,"['Xiaomeng Fu', 'Xi Wang', 'Qiao Li', 'Jin Liu', 'Jiao Dai', 'Jizhong Han']","Diffusion models pose risks of privacy breaches and copyright disputes, primarily stemming from the potential utilization of unauthorized data during the training phase. The Training Membership Inference (TMI) task aims to determine whether a specific sample has been used in the training process of a target model, representing a critical tool for privacy violation verification. However, the increased stochasticity inherent in diffusion renders traditional shadow-model-based or metric-based methods ineffective when applied to diffusion models. Moreover, existing methods only yield binary classification labels which lack necessary comprehensibility in practical applications. In this paper, we explore a novel perspective for the TMI task by leveraging the intrinsic generative priors within the diffusion model. Compared with unseen samples, training samples exhibit stronger generative priors within the diffusion model, enabling the successful reconstruction of substantially degraded training images. Consequently, we propose the Degrade Restore Compare (DRC) framework. In this framework, an image undergoes sequential degradation and restoration, and its membership is determined by comparing it with the restored counterpart. Experimental results verify that our approach not only significantly outperforms existing methods in terms of accuracy but also provides comprehensible decision criteria, offering evidence for potential privacy violations."
https://arxiv.org/abs/2403.08486,2024-03-13,Protocol Optimization for Functional Cardiac CT Imaging Using Noise Emulation in the Raw Data Domain,"['Zhye Yin', 'Pengwei Wu', 'Ashish Manohar', 'Elliot R. McVeigh', 'Jed D. Pack']","Four-dimensional (4D) wide coverage computed tomography (CT) is an effective imaging modality for measuring the mechanical function of the myocardium. However, repeated CT measurement across several heartbeats is still a concern. A projection-domain noise emulation method is presented to generate accurate low-dose (mA modulated) 4D cardiac CT scans from high-dose scans, enabling protocol optimization to deliver sufficient image quality for functional cardiac analysis while using a dose level that is as low as reasonably achievable. Given a targeted low-dose mA modulation curve, the proposed noise emulation method injects both quantum and electronic noise of proper magnitude and correlation to the high-dose data in projection domain. A spatially varying detector gain term as well as its calibration method were proposed to further improve the noise emulation accuracy. To determine the low dose threshold, a projection domain image quality (IQ) metric was proposed that is based on the number of projection rays that do not fall under the non-linear region of the detector. Experiments were performed to validate the noise emulation method with both phantom and clinical data. For both phantom and clinical data, the low-dose emulated images exhibited similar noise magnitude, artifacts, and texture to that of the real low-dose images. The proposed channel-dependent detector gain term resulted in additional increase in emulation accuracy. Using the proposed IQ metric, recommended kVp and mA settings were calculated for low dose 4D Cardiac CT acquisitions for patients of different sizes. In conclusion, a detailed method to estimate system-dependent parameters for a raw-data based low dose emulation framework was described. The proposed low-dose emulation method can be used to prospectively select patient-specific minimal-dose protocols for functional cardiac CT."
https://arxiv.org/abs/2403.08485,2024-03-13,Opportunities and open questions in modern $β$ decay,['Leendert Hayen'],"For well over half a century, precision studies of neutron and nuclear $β$ decays have been at the forefront of searches for exotic electroweak physics. Recent advances in nuclear ab initio theory and the widespread use of effective field theories means that its modern understanding is going through a transitional phase. This has been propelled by current tensions in the global data set leading to renewed scrutiny of its theoretical ingredients. In parallel, a host of novel techniques and methods are being investigated that are able to sidestep many traditional systematic uncertainties and require a diverse palette of skills and collaboration with material science and condensed matter physics. We highlight the current opportunities and open questions with the aim of facilitating the transition to a more modern understanding of $β$ decay."
https://arxiv.org/abs/2403.08484,2024-03-13,Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning,"['Ming Dong', 'Kang Xue', 'Bolong Zheng', 'Tingting He']","In view of the huge number of parameters of Large language models (LLMs) , tuning all parameters is very costly, and accordingly fine-tuning specific parameters is more sensible. Most of parameter efficient fine-tuning (PEFT) concentrate on parameter selection strategies, such as additive method, selective method and reparametrization-based method. However, there are few methods that consider the impact of data samples on parameter selecting, such as Fish Mask based method. Fish Mask randomly choose a part of data samples and treat them equally during parameter selection, which is unable to dynamically select optimal parameters for inconstant data distributions. In this work, we adopt a data-oriented perspective, then proposing an IRD ($\mathrm{\underline I}$terative sample-parameter $\mathrm{\underline R}$ange $\mathrm{\underline D}$ecreasing) algorithm to search the best setting of sample-parameter pair for FISH Mask. In each iteration, by searching the set of samples and parameters with larger Fish information, IRD can find better sample-parameter pair in most scale. We demonstrate the effectiveness and rationality of proposed strategy by conducting experiments on GLUE benchmark. Experimental results show our strategy optimizes the parameter selection and achieves preferable performance."
https://arxiv.org/abs/2403.08483,2024-03-13,Study of Physical Characteristics of the New Half-Heusler Alloy BaHgSn by DFT Analysis,"['A. Jabar', 'S. Benyoussef', 'L. Bahmad']","To investigate the physical characteristics of the half-Heusler BaHgSn molecule, we used theoretical calculations within the Density Functional Theory (DFT) framework utilizing the LSDA+mBJ technique in this study. Using the optimal lattice parameters, we discover that half-Heusler BaHgSn exhibits a Dirac semimetal behavior with a band gap of 0.1 eV. Thomas Charpin's numerical first-principles calculation approach was applied to determine the elastic constants of hexagonal BaHgSn alloys. The material's optical characteristics verified its prospective use in infrared-visible devices. According to a thermo-electric properties analysis, at 20x10^18 Ω-1.m-1.s-1, the electrical conductivity reaches its maximum after increasing gradually up to 500 K. Compared to other compounds, these results indicate that BaHgSn has potential for use in opto-electronic and thermo-electric devices."
https://arxiv.org/abs/2403.08482,2024-03-13,Bose-Einstein condensation in canonical ensemble with fixed total momentum,"['Andrey S. Plyashechnik', 'Alexey A. Sokolik', 'Yurii E. Lozovik']","We consider Bose-Einstein condensation of noninteracting homogeneous three-dimensional gas in canonical ensemble when both particle number $N$ and total momentum $\mathbf{P}$ of all particles are fixed. Using the saddle point method, we derive the large-$N$ analytical approximations for partition function, free energy, and statistical distributions of occupation numbers of different single-particle energy levels. At temperatures below the critical point of phase transition, we predict, in some ranges of $\mathbf{P}$, fragmentation of the condensate, when more than one single-particle level is macroscopically occupied. The occupation number distributions have approximately Gaussian shapes for the levels hosting the condensate, and exponential shapes for other, noncondensate levels. Our analysis demonstrates breaking of Galilean invariance of moving finite-temperature many-particle system in the presence of Bose-Einstein condensation and extends the theory of moving and rotating quantum systems to the finite-temperature large-$N$ limit."
https://arxiv.org/abs/2403.08481,2024-03-13,SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks,"['Guy Amit', 'Abigail Goldsteen', 'Ariel Farkash']","Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require fine-tuning generic base models on customized, proprietary datasets. This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We find that some training methods provide significantly reduced privacy risk, with the combination of differential privacy and low-rank adaptors achieving the best privacy protection against these attacks."
https://arxiv.org/abs/2403.08480,2024-03-13,Understanding and Evaluating Developer Behaviour in Programming Tasks,"['Martin Schröer', 'Rainer Koschke']","To evaluate how developers perform differently in solving programming tasks, i.e., which actions and behaviours are more beneficial to them than others and if there are any specific strategies and behaviours that may indicate good versus poor understanding of the task and program given to them, we used the MIMESIS plug-in to record developers' interactions with the IDE. In a series of three studies we investigated the specific behaviour of developers solving a specific programming task. We focused on which source code files they visited, how they related pieces of code and knowledge to others and when and how successful they performed code edits. To cope with the variety of behaviours due to interpersonal differences such as different level of knowledge, development style or problem solving stratiegies, we used an abstraction of the observed behaviour, which enables for a better comparison between different individual attributes such as skill, speed and used stratiegies and also facilitates later automatic evaluation of behaviours, i.e. by using a software to react to."
https://arxiv.org/abs/2403.08479,2024-03-13,MD-Dose: A Diffusion Model based on the Mamba for Radiotherapy Dose Prediction,"['Linjie Fu', 'Xia Li', 'Xiuding Cai', 'Yingkai Wang', 'Xueyao Wang', 'Yali Shen', 'Yu Yao']","Radiation therapy is crucial in cancer treatment. Experienced experts typically iteratively generate high-quality dose distribution maps, forming the basis for excellent radiation therapy plans. Therefore, automated prediction of dose distribution maps is significant in expediting the treatment process and providing a better starting point for developing radiation therapy plans. With the remarkable results of diffusion models in predicting high-frequency regions of dose distribution maps, dose prediction methods based on diffusion models have been extensively studied. However, existing methods mainly utilize CNNs or Transformers as denoising networks. CNNs lack the capture of global receptive fields, resulting in suboptimal prediction performance. Transformers excel in global modeling but face quadratic complexity with image size, resulting in significant computational overhead. To tackle these challenges, we introduce a novel diffusion model, MD-Dose, based on the Mamba architecture for predicting radiation therapy dose distribution in thoracic cancer patients. In the forward process, MD-Dose adds Gaussian noise to dose distribution maps to obtain pure noise images. In the backward process, MD-Dose utilizes a noise predictor based on the Mamba to predict the noise, ultimately outputting the dose distribution maps. Furthermore, We develop a Mamba encoder to extract structural information and integrate it into the noise predictor for localizing dose regions in the planning target volume (PTV) and organs at risk (OARs). Through extensive experiments on a dataset of 300 thoracic tumor patients, we showcase the superiority of MD-Dose in various metrics and time consumption."
https://arxiv.org/abs/2403.08478,2024-03-13,Thermal Hall effect incorporating magnon damping in localized spin systems,"['Shinnosuke Koyama', 'Joji Nasu']","We propose a theory for thermal Hall transport mediated by magnons to address the impact of their damping resulting from magnon-magnon interactions in insulating magnets. This phenomenon is anticipated to be particularly significant in systems characterized by strong quantum fluctuations, exemplified by spin-1/2 systems. Employing a nonlinear flavor-wave theory, we analyze a general model for localized electron systems and develop a formulation for thermal conductivity based on a perturbation theory, utilizing bosonic Green's functions with a nonzero self-energy. We derive the expression of the thermal Hall conductivity incorporating magnon damping. To demonstrate the applicability of the obtained representation, we adopt it to two $S=1/2$ quantum spin models on a honeycomb lattice. In calculations for these systems, we make use of the self-consistent imaginary Dyson equation approach at finite temperatures for evaluating the magnon damping rate. In both systems, the thermal Hall conductivity is diminished due to the introduction of magnon damping over a wide temperature range. This effect arises due to the smearing of magnon spectra with nonzero Berry curvatures. We also discuss the relation to the damping of chiral edge modes of magnons. Our formulation can be applied to various localized electron systems as we begin with a general Hamiltonian for these systems. Our findings shed light on a new aspect of topological magnonics emergent from many-body effects and will stimulate further investigations on the impact of magnon damping on topological phenomena."
https://arxiv.org/abs/2403.08477,2024-03-13,Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts,"['Shengzhuang Chen', 'Jihoon Tack', 'Yunqiao Yang', 'Yee Whye Teh', 'Jonathan Richard Schwarz', 'Ying Wei']","Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-domain and out-of-domain generalization. Our code is publicly available."
https://arxiv.org/abs/2403.08476,2024-03-13,Emergent Continuous Time Crystal in Dissipative Quantum Spin System without Driving,"['Shu Yang', 'Zeqing Wang', 'Libin Fu', 'Jianwen Jie']","Time crystal, a nonequilibrium phenomenon extending spontaneous symmetry breaking into the temporal dimension, holds fundamental significance in understanding quantum many-body physics. In this work, we explore the nonequilibrium phase diagram of a two-dimensional dissipative Heisenberg spin system in the absence of explicit driving. We numerically identify the emergence of novel nonstationary oscillatory states by analyzing the spin dynamics. These states are categorized as limit cycle and chaos based on the Lyapunov exponent. Remarkably, the observed limit cycle behavior represents a continuous time crystal (CTC), spontaneously breaking the continuous time translation symmetry of the system. We further confirm those oscillatory behaviors by studying the stability against local perturbations applied to the system. Finally, we investigate the robustness of the emergent CTC by introducing isotropic Gaussian-type white noise into the interactions. This study provides many insights into the intricate interplay between dissipation-induced decay processes and interaction-induced spin precession, deepening our understanding of dissipative quantum many-body systems."
https://arxiv.org/abs/2403.08475,2024-03-13,NLQxform-UI: A Natural Language Interface for Querying DBLP Interactively,"['Ruijie Wang', 'Zhiruo Zhang', 'Luca Rossetto', 'Florian Ruosch', 'Abraham Bernstein']","In recent years, the DBLP computer science bibliography has been prominently used for searching scholarly information, such as publications, scholars, and venues. However, its current search service lacks the capability to handle complex queries, which limits the usability of DBLP. In this paper, we present NLQxform-UI, a web-based natural language interface that enables users to query DBLP directly with complex natural language questions. NLQxform-UI automatically translates given questions into SPARQL queries and executes the queries over the DBLP knowledge graph to retrieve answers. The querying process is presented to users in an interactive manner, which improves the transparency of the system and helps examine the returned answers. Also, intermediate results in the querying process can be previewed and manually altered to improve the accuracy of the system. NLQxform-UI has been completely open-sourced: https://github.com/ruijie-wang-uzh/NLQxform-UI."
https://arxiv.org/abs/2403.08474,2024-03-13,Negative Wigner function by decaying interaction from equilibrium,"['Michal Kolář', 'Radim Filip']","Bosonic systems with negative Wigner function superposition states are fundamentally witnessing nonlinear quantum dynamics beyond linearized systems and, recently, have become essential resources of quantum technology with many applications. Typically, they appear due to sophisticated combination of external drives, nonlinear control, measurements or strong nonlinear dissipation of subsystems to an environment. Here, we propose a conceptually different and more autonomous way to obtain such states, avoiding these ingredients, using purely sudden interaction decay in the paradigmatic interacting qubit-oscillator system weakly coupled to bath at thermal equilibrium in a low-temperature limit. We demonstrate simultaneously detectable unconditional negative Wigner function and quantum coherence and their qualitative enhancement employing more qubits."
https://arxiv.org/abs/2403.08473,2024-03-13,Mechanism Design Optimization through CAD-Based Bayesian Optimization and Quantified Constraints,"['Abdelmajid Ben Yahya', 'Santiago Ramos Garces', 'Nick Van Oosterwyck', 'Annie Cuyt', 'Stijn Derammelaere']","This research delves into optimizing mechanism design, with an emphasis on the energy efficiency and the expansive design possibilities of reciprocating mechanisms. It investigates how to efficiently integrate Computer-Aided Design (CAD) simulations with Bayesian Optimization (BO) and a constrained design space, aiming to enhance the design optimization process beyond the confines of traditional kinematic and dynamic analysis. The study sets out to create a novel optimization framework that merges CAD simulations with a BO strategy. Initially, the feasibility of a mechanism design is assessed through CAD-motion simulations, which gauge its practicality. Upon deeming a design feasible, an evaluation via CAD-motion simulations is conducted to ascertain the objective value. This research proposes utilizing non-parametric Gaussian processes for crafting a surrogate model of the objective function, considering the design space's static and dynamic constraints. The findings reveal that the introduced CAD-based Bayesian Optimization framework adeptly identifies optimal design parameters that minimize root mean square (RMS) torque while complying with predetermined constraints. This method markedly diminishes the complexity seen in analytical approaches, rendering it adaptable to intricate mechanisms and practicable for machine builders. The framework evidences the utility of integrating constraints in the optimization process, showing promise for attaining globally optimal designs efficiently. A case study on an emergency ventilator, with three design parameters, demonstrates a 71% RMS torque reduction after 255 CAD-based evaluations, underscoring the approach's effectiveness and its potential for refining mechanism design optimization."
https://arxiv.org/abs/2403.08472,2024-03-13,Beilinson-Parshin adeles via solid algebraic geometry,"['Christopher Brav', 'Grigorii Konovalov']","In this paper, we apply Clausen-Scholze's theory of solid modules to the existence of adelic decompositions for schemes of finite type over $\mathbb{Z}$. Specifically, we use the six-functor formalism for solid modules to define the skeletal filtration of a scheme, and then we show that decomposing a quasi-coherent sheaf with respect to this filtration gives rise to a new construction of the Beilinson-Parshin adelic resolution."
https://arxiv.org/abs/2403.08471,2024-03-13,Selective probing of longitudinal and transverse plasmon modes with electron phase-matching,"['Franck Aguilar', 'Hugo Lourenço-Martins', 'Damián Montero', 'Xiaoyan Li', 'Mathieu Kociak', 'Alfredo Campos']","The optical properties of metallic nanoparticles are dominated by localized surface plasmons (LSPs). Their properties only depend on the constituting material, the size and shape of the nano-object as well as its surrounding medium. In anisotropic structures, such as metallic nanorods, two families of modes generally exist, transverse and longitudinal. Their spectral and spatial overlaps usually impede their separate measurements in electron energy loss spectroscopy (EELS). In this work, we propose three different strategies enabling to overcome this difficulty and selectively probe longitudinal and transverse modes. The first strategy is numeric and relies on morphing of nano-structures, rooted in the geometrical nature of LSPs. The two other strategies exploit the relativistic and wave nature of the electrons in an EELS experiment. The first one is the phase-matching between the electron and the plasmon excitation to enhance their coupling by either tilting the sample and modifying the electron kinetic energy. The second one - polarized EELS (pEELS) - exploits the wave nature of electrons to mimic selection rules analogous to the one existing in light spectroscopies. The above-mentioned strategies are exemplified - either experimentally or numerically - on a canonical plasmonic toy model: the nano-rod. The goal of the paper is to bring together the state-of-the-art concepts of EELS for plasmonics to tackle a pedestrian problem in this field."
https://arxiv.org/abs/2403.08470,2024-03-13,Convergence of ADAM for Lipschitz Objective Functions,"['Juan Ferrera', 'Javier Gómez Gil']","The aim of this paper is to prove the exponential convergence, local"
https://arxiv.org/abs/2403.08469,2024-03-13,An Analysis of Human Alignment of Latent Diffusion Models,"['Lorenz Linhardt', 'Marco Morik', 'Sidney Bender', 'Naima Elosegui Borras']","Diffusion models, trained on large amounts of data, showed remarkable performance for image synthesis. They have high error consistency with humans and low texture bias when used for classification. Furthermore, prior work demonstrated the decomposability of their bottleneck layer representations into semantic directions. In this work, we analyze how well such representations are aligned to human responses on a triplet odd-one-out task. We find that despite the aforementioned observations: I) The representational alignment with humans is comparable to that of models trained only on ImageNet-1k. II) The most aligned layers of the denoiser U-Net are intermediate layers and not the bottleneck. III) Text conditioning greatly improves alignment at high noise levels, hinting at the importance of abstract textual information, especially in the early stage of generation."
https://arxiv.org/abs/2403.08468,2024-03-13,Kondo Effect in Micron Size Device Fabricated From Flakes of Mn Doped Bi2Se3 Topological Insulator,"['Vishal K. Maurya', 'Jeetendra K. Tiwari', 'S. Ghosh', 'S. Patnaik']",Single crystals of Mn0.03Bi1.97Se3 were synthesized by modified Bridgman technique and phase purity was confirmed via XRD analysis. EDAX analysis has verified the stoichiometric ratio of elements in the sample. Sample flakes were transferred to the SiO2/Si n-type substrate by mechanical exfoliation technique. Four probe gold contacts were etched with the help of e-beam lithography by masking and lift off process. Resistivity measurement was performed in four probe configurations in 2-300 K temperature range. We report evidence for Kon-do effect in Mn0.03Bi1.97Se3 micro-flakes with Tmin of 14.4 K.
https://arxiv.org/abs/2403.08467,2024-03-13,Uniqueness of extremal charged black holes in de Sitter,['David Katona'],"We prove a uniqueness theorem for the charged Nariai black holes and ultracold black holes in four dimensions. In particular, we show that an analytic solution to four-dimensional Einstein-Maxwell theory with a positive cosmological constant containing a static extremal Killing horizon with spherical cross-sections of large radius (compared to the cosmological scale), must be locally isometric to the extremal Reissner-Nordström-de Sitter black hole or its near-horizon geometry. The theorem generalises to extremal static horizons with small radius, establishing uniqueness of cold black holes for generic values of the radius."
https://arxiv.org/abs/2403.08466,2024-03-13,"Evidence of robust, universal conformal invariance in living biological matter","['Benjamin H. Andersen', 'Francisco M. R. Safara', 'Valeriia Grudtsyna', 'Oliver J. Meacock', 'Simon G. Andersen', 'William M. Durham', 'Nuno A. M. Araujo', 'Amin Doostmohammadi']","Collective cellular movement plays a crucial role in many processes fundamental to health, including development, reproduction, infection, wound healing, and cancer. The emergent dynamics that arise in these systems are typically thought to depend on how cells interact with one another and the mechanisms used to drive motility, both of which exhibit remarkable diversity across different biological systems. Here, we report experimental evidence of a universal feature in the patterns of flow that spontaneously emerges in groups of collectively moving cells. Specifically, we demonstrate that the flows generated by collectively moving dog kidney cells, human breast cancer cells, and by two different strains of pathogenic bacteria, all exhibit conformal invariance. Remarkably, not only do our results show that all of these very different systems display robust conformal invariance, but we also discovered that the precise form of the invariance in all four systems is described by the Schramm-Loewner Evolution (SLE), and belongs to the percolation universality class. A continuum model of active matter can recapitulate both the observed conformal invariance and SLE form found in experiments. The presence of universal conformal invariance reveals that the macroscopic features of living biological matter exhibit universal translational, rotational, and scale symmetries that are independent of the microscopic properties of its constituents. Our results show that the patterns of flows generated by diverse cellular systems are highly conserved and that biological systems can unexpectedly be used to experimentally test predictions from the theories for conformally invariant structures"
https://arxiv.org/abs/2403.08465,2024-03-13,New Invariants for Partitioning a Graph into 2-connected Subgraphs,"['Michitaka Furuya', 'Masaki Kashima', 'Katsuhiro Ota']","A vertex partition in which every part induces a 2-connected subgraph is called a 2-proper partition. This concept was introduced by Ferrara et al. in 2013, and Borozan et al. gave the best possible minimum degree condition for the existence of a 2-proper partition in 2016. Later, in 2022, Chen et al. extended the result by showing a minimum degree sum condition for the existence of 2-proper partition. In this paper, we introduce two new invariants of graph, denoted by $σ^*(G)$ and $α^*(G)$. These two invariants are defined from degree sum on all independent sets with some property. We prove that if a graph $G$ satisfies $σ^*(G)\geq |V(G)|$, then with some exceptions, $G$ has a 2-proper partition with at most $α^*(G)$ parts. This result is best possible, and implies both of the results by Borozan et al. and by Chen et al.. Moreover, as a corollary of our result, we give a minimum degree product condition for the existence of a 2-proper partition."
https://arxiv.org/abs/2403.08464,2024-03-13,Diffusion Models with Implicit Guidance for Medical Anomaly Detection,"['Cosmin I. Bercea', 'Benedikt Wiestler', 'Daniel Rueckert', 'Julia A. Schnabel']","Diffusion models have advanced unsupervised anomaly detection by improving the transformation of pathological images into pseudo-healthy equivalents. Nonetheless, standard approaches may compromise critical information during pathology removal, leading to restorations that do not align with unaffected regions in the original scans. Such discrepancies can inadvertently increase false positive rates and reduce specificity, complicating radiological evaluations. This paper introduces Temporal Harmonization for Optimal Restoration (THOR), which refines the de-noising process by integrating implicit guidance through temporal anomaly maps. THOR aims to preserve the integrity of healthy tissue in areas unaffected by pathology. Comparative evaluations show that THOR surpasses existing diffusion-based methods in detecting and segmenting anomalies in brain MRIs and wrist X-rays. Code: https://github.com/ci-ber/THOR_DDPM."
https://arxiv.org/abs/2403.08463,2024-03-13,A Comparison of SynDiffix Multi-table versus Single-table Synthetic Data,['Paul Francis'],"SynDiffix is a new open-source tool for structured data synthesis. It has anonymization features that allow it to generate multiple synthetic tables while maintaining strong anonymity. Compared to the more common single-table approach, multi-table leads to more accurate data, since only the features of interest for a given analysis need be synthesized. This paper compares SynDiffix with 15 other commercial and academic synthetic data techniques using the SDNIST analysis framework, modified by us to accommodate multi-table synthetic data. The results show that SynDiffix is many times more accurate than other approaches for low-dimension tables, but somewhat worse than the best single-table techniques for high-dimension tables."
https://arxiv.org/abs/2403.08462,2024-03-13,Authorship Verification based on the Likelihood Ratio of Grammar Models,"['Andrea Nini', 'Oren Halvani', 'Lukas Graner', 'Valerio Gherardi', 'Shunichi Ishihara']","Authorship Verification (AV) is the process of analyzing a set of documents to determine whether they were written by a specific author. This problem often arises in forensic scenarios, e.g., in cases where the documents in question constitute evidence for a crime. Existing state-of-the-art AV methods use computational solutions that are not supported by a plausible scientific explanation for their functioning and that are often difficult for analysts to interpret. To address this, we propose a method relying on calculating a quantity we call $λ_G$ (LambdaG): the ratio between the likelihood of a document given a model of the Grammar for the candidate author and the likelihood of the same document given a model of the Grammar for a reference population. These Grammar Models are estimated using $n$-gram language models that are trained solely on grammatical features. Despite not needing large amounts of data for training, LambdaG still outperforms other established AV methods with higher computational complexity, including a fine-tuned Siamese Transformer network. Our empirical evaluation based on four baseline methods applied to twelve datasets shows that LambdaG leads to better results in terms of both accuracy and AUC in eleven cases and in all twelve cases if considering only topic-agnostic methods. The algorithm is also highly robust to important variations in the genre of the reference population in many cross-genre comparisons. In addition to these properties, we demonstrate how LambdaG is easier to interpret than the current state-of-the-art. We argue that the advantage of LambdaG over other methods is due to fact that it is compatible with Cognitive Linguistic theories of language processing."
https://arxiv.org/abs/2403.08461,2024-03-13,The Heisenberg-RIXS instrument at the European XFEL,"['Justine Schlappa', 'Giacomo Ghiringhelli', 'Benjamin E. Van Kuiken', 'Martin Teichmann', 'Piter S. Miedema', 'Jan Torben Delitz', 'Natalia Gerasimova', 'Serguei Molodtsov', 'Luigi Adriano', 'Bernard Baranasic', 'Carsten Broers', 'Robert Carley', 'Patrick Gessler', 'Nahid Ghodrati', 'David Hickin', 'Le Phuong Hoang', 'Manuel Izquierdo', 'Laurent Mercadier', 'Giuseppe Mercurio', 'Sergii Parchenko', 'Marijan Stupar', 'Zhong Yin', 'Leonardo Martinelli', 'Giacomo Merzoni', 'Ying Ying Peng']","Resonant Inelastic X-ray Scattering (RIXS) is an ideal X-ray spectroscopy method to push the combination of energy and time resolutions to the Fourier transform ultimate limit, because it is unaffected by the core-hole lifetime energy broadening. And in pump-probe experiments the interaction time is made very short by the same core-hole lifetime. RIXS is very photon hungry so it takes great advantage from high repetition rate pulsed X-ray sources like the European XFEL. The hRIXS instrument is designed for RIXS experiments in the soft X-ray range with energy resolution approaching the Fourier and the Heisenberg limits. It is based on a spherical grating with variable line spacing (VLS) and a position-sensitive 2D detector. Initially, two gratings are installed to adequately cover the whole photon energy range. With optimized spot size on the sample and small pixel detector the energy resolution can be better than 40 meV at any photon energy below 1000 eV. At the SCS instrument of the European XFEL the spectrometer can be easily positioned thanks to air-pads on a high-quality floor, allowing the scattering angle to be continuously adjusted over the 65-145 deg range. It can be coupled to two different sample interaction chamber, one for liquid jets and one for solids, each equipped at the state-of-the-art and compatible for optical laser pumping in collinear geometry. The measured performances, in terms of energy resolution and count rate on the detector, closely match design expectations. hRIXS is open to public users since the summer of 2022."
https://arxiv.org/abs/2403.08460,2024-03-13,Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal Diffusion Model,"['Ruibin Zhang', 'Donglai Xue', 'Yuhan Wang', 'Ruixu Geng', 'Fei Gao']","Millimeter wave (mmWave) radars have attracted significant attention from both academia and industry due to their capability to operate in extreme weather conditions. However, they face challenges in terms of sparsity and noise interference, which hinder their application in the field of micro aerial vehicle (MAV) autonomous navigation. To this end, this paper proposes a novel approach to dense and accurate mmWave radar point cloud construction via cross-modal learning. Specifically, we introduce diffusion models, which possess state-of-the-art performance in generative modeling, to predict LiDAR-like point clouds from paired raw radar data. We also incorporate the most recent diffusion model inference accelerating techniques to ensure that the proposed method can be implemented on MAVs with limited computing resources.We validate the proposed method through extensive benchmark comparisons and real-world experiments, demonstrating its superior performance and generalization ability. Code and pretrained models will be available at https://github.com/ZJU-FAST-Lab/Radar-Diffusion."
https://arxiv.org/abs/2403.08459,2024-03-13,Symmetry restoration and quantum Mpemba effect in symmetric random circuits,"['Shuo Liu', 'Hao-Kai Zhang', 'Shuai Yin', 'Shi-Xin Zhang']","Entanglement asymmetry, which serves as a diagnostic tool for symmetry breaking and a proxy for thermalization, has recently been proposed and studied in the context of symmetry restoration for quantum many-body systems undergoing a quench. In this Letter, we investigate symmetry restoration in various symmetric random quantum circuits, particularly focusing on the U(1) symmetry case. In contrast to non-symmetric random circuits where the U(1) symmetry of a small subsystem can always be restored at late times, we reveal that symmetry restoration can fail in U(1) symmetric circuits for certain small symmetry-broken initial states in finite-size systems. In the early-time dynamics, we observe an intriguing quantum Mpemba effect implying that symmetry is restored faster when the initial state is more asymmetric. Furthermore, we also investigate the entanglement asymmetry dynamics for SU(2) and $Z_{2}$ symmetric circuits and identify the presence and absence of the quantum Mpemba effect for the corresponding symmetries, respectively. A unified understanding of these results is provided through the lens of quantum thermalization with conserved charges."
https://arxiv.org/abs/2403.08458,2024-03-13,Dielectric microwave resonator with large optical apertures for spin-based quantum devices,"['Tatsuki Hamamoto', 'Amit Bhunia', 'Rupak Kumar Bhattacharya', 'Hiroki Takahashi', 'Yuimaru Kubo']","Towards a spin-based quantum microwave-optical photon transducer, we demonstrate a low-loss dielectric microwave resonator with an internal quality factor of $2.30\times10^4$ while accommodating optical apertures with a diameter of $8\, \mathrm{mm}$. The two seemingly conflicting requirements, high quality factor and large optical apertures, are satisfied thanks to the large dielectric constant of rutile ($\mathrm{TiO_2}$). The quality factor is limited by radiation loss, and we confirmed by numerical simulation that this dielectric resonator can achieve a quality factor exceeding $10^6$ by extending the height of the resonator enclosure. Using this resonator, we performed both continuous-wave (cw) and pulse electron spin resonance (ESR) spectroscopy on 2,2-diphenyl-1-picrylhydrazyl (DPPH) crystalline powder and P1 centers in a diamond crystal in a dilution refrigerator. The cw ESR spectroscopy demonstrated high-cooperativity and strong spin-resonator coupling with the DPPH and P1 centers respectively, while the pulse ESR spectroscopy successfully measured longitudinal and transverse relaxation times."
https://arxiv.org/abs/2403.08457,2024-03-13,Non-linear collision-induced breakage equation: finite volume and semi-analytical methods,"['Sanjiv Kumar Bariwal', 'Saddam Hussain', 'Rajesh Kumar']","The non-linear collision-induced breakage equation has significant applications in particulate processes. Two semi-analytical techniques, namely homotopy analysis method (HAM) and accelerated homotopy perturbation method (AHPM) are investigated along with the well-known finite volume method (FVM) to comprehend the dynamical behavior of the non-linear system, i.e., the concentration function, the total number and the total mass of the particles in the system. The theoretical convergence analyses of the series solutions of HAM and AHPM are discussed. In addition, the error estimations of the truncated solutions of both methods equip the maximum absolute error bound. To justify the applicability and accuracy of these methods, numerical simulations are compared with the findings of FVM and analytical solutions considering three physical problems."
https://arxiv.org/abs/2403.08456,2024-03-13,Layered Kagome Compound Na$_2$Ni$_3$S$_4$ with Topological Flat Band,"['Junyao Ye', 'Yihao Lin', 'Haozhe Wang', 'Zida Song', 'Ji Feng', 'Weiwei Xie', 'Shuang Jia']","We report structural and electronic properties of Na$_2$Ni$_3$S$_4$, a quasi-two-dimensional compound composed of alternating layers of [Ni$_3$S$_4$]$^{2-}$ and Na$^{+}$. The compound features a remarkable Ni-based kagome lattice with a square planar configuration of four surrounding S atoms for each Ni atom. Magnetization and electrical measurements reveal a weak paramagnetic insulator with a gap of about 0.5 eV. Our band structure calculation highlights a set of topological flat bands of the kagome lattice derived from the rotated d$_{xz}$-orbital with $C_\mathrm{3}$ + $T$ symmetry in the presence of crystal-field splitting."
https://arxiv.org/abs/2403.08455,2024-03-13,IAMCV Multi-Scenario Vehicle Interaction Dataset,"['Novel Certad', 'Enrico del Re', 'Helena Korndörfer', 'Gregory Schröder', 'Walter Morales-Alvarez', 'Sebastian Tschernuth', 'Delgermaa Gankhuyag', 'Luigi del Re', 'Cristina Olaverri-Monreal']","The acquisition and analysis of high-quality sensor data constitute an essential requirement in shaping the development of fully autonomous driving systems. This process is indispensable for enhancing road safety and ensuring the effectiveness of the technological advancements in the automotive industry. This study introduces the Interaction of Autonomous and Manually-Controlled Vehicles (IAMCV) dataset, a novel and extensive dataset focused on inter-vehicle interactions. The dataset, enriched with a sophisticated array of sensors such as Light Detection and Ranging, cameras, Inertial Measurement Unit/Global Positioning System, and vehicle bus data acquisition, provides a comprehensive representation of real-world driving scenarios that include roundabouts, intersections, country roads, and highways, recorded across diverse locations in Germany. Furthermore, the study shows the versatility of the IAMCV dataset through several proof-of-concept use cases. Firstly, an unsupervised trajectory clustering algorithm illustrates the dataset's capability in categorizing vehicle movements without the need for labeled training data. Secondly, we compare an online camera calibration method with the Robot Operating System-based standard, using images captured in the dataset. Finally, a preliminary test employing the YOLOv8 object-detection model is conducted, augmented by reflections on the transferability of object detection across various LIDAR resolutions. These use cases underscore the practical utility of the collected dataset, emphasizing its potential to advance research and innovation in the area of intelligent vehicles."
https://arxiv.org/abs/2403.08454,2024-03-13,Metallicities for more than 10 million stars derived from Gaia BP/RP spectra,"['T. Xylakis-Dornbusch', 'N. Christlieb', 'T. T. Hansen', 'T. Nordlander', 'K. B. Webber', 'J. Marshall']","Context. The third Gaia Data Release, which includes BP/RP spectra for 219 million sources, has opened a new window in the exploration of the chemical history and evolution of the Milky Way. The wealth of information encapsulated in these data is far greater than their low resolving power (R=50) at first glance would suggest, as shown in many studies. We zero in on the use of this data for the purpose of the detection of ''new'' metal-poor stars, which are hard to find yet essential for understanding - among other - several aspects of the origin of the Galaxy, star formation and the creation of the elements. Aims. We strive to refine a metal-poor candidate selection method which was developed with simulated Gaia BP/RP spectra, with an ultimate objective of providing the community with both a recipe to select stars for medium/high resolution observations and a catalogue of stellar metallicities. Methods. We used a datased comprised of GALAH DR3 and SAGA database stars in order to verify and adjust to real world data our selection method. For that purpose, we used dereddening as a mean to tackle the issue of extinction, and then we applied our fine-tuned method to select metal-poor candidates, which we thereafter observed and analysed. Results. We were able to infer metallicities for GALAH DR3 and SAGA stars - with color excesses up to E(B-V)<1.5 - with an uncertainty of 0.36 dex, which is good enough for the purpose of identifying new metal-poor stars. Further, we selected 26 metal-poor candidates - via our method - for observations. As spectral analysis showed, 100% of them had [Fe/H]<-2.0, 57% had [Fe/H]<-2.5 and 8% had [Fe/H]<-3.0. We inferred metallicities for these stars with an uncertainty of 0.31 dex, as was proven when comparing to the spectroscopic [Fe/H]. Finally, we assembled a catalogue of metallicities for 10 861 062 stars."
https://arxiv.org/abs/2403.08453,2024-03-13,Better Fit: Accommodate Variations in Clothing Types for Virtual Try-on,"['Xuanpu Zhang', 'Dan Song', 'Pengxin Zhan', 'Qingguo Chen', 'Kuilong Liu', 'Anan Liu']","Image-based virtual try-on aims to transfer target in-shop clothing to a dressed model image, the objectives of which are totally taking off original clothing while preserving the contents outside of the try-on area, naturally wearing target clothing and correctly inpainting the gap between target clothing and original clothing. Tremendous efforts have been made to facilitate this popular research area, but cannot keep the type of target clothing with the try-on area affected by original clothing. In this paper, we focus on the unpaired virtual try-on situation where target clothing and original clothing on the model are different, i.e., the practical scenario. To break the correlation between the try-on area and the original clothing and make the model learn the correct information to inpaint, we propose an adaptive mask training paradigm that dynamically adjusts training masks. It not only improves the alignment and fit of clothing but also significantly enhances the fidelity of virtual try-on experience. Furthermore, we for the first time propose two metrics for unpaired try-on evaluation, the Semantic-Densepose-Ratio (SDR) and Skeleton-LPIPS (S-LPIPS), to evaluate the correctness of clothing type and the accuracy of clothing texture. For unpaired try-on validation, we construct a comprehensive cross-try-on benchmark (Cross-27) with distinctive clothing items and model physiques, covering a broad try-on scenarios. Experiments demonstrate the effectiveness of the proposed methods, contributing to the advancement of virtual try-on technology and offering new insights and tools for future research in the field. The code, model and benchmark will be publicly released."
https://arxiv.org/abs/2403.08452,2024-03-13,Photocleavage of aliphatic C--C bonds in the interstellar medium,"['Guillermo Tajuelo-Castilla', 'Jesús I. Mendieta-Moreno', 'Mario Accolla', 'Jesús M. Sobrado', 'Sofia Canola', 'Pavel Jelínek', 'Gary J. Ellis', 'José Ángel Martín-Gago', 'Gonzalo Santoro']","Ultraviolet (UV) processing in the insterstellar medium (ISM) induces the dehydrogenation of hydrocarbons. Aliphatics, including alkanes, are present in different interstellar environments, being prevalently formed in evolved stars; thus, the dehydrogenation by UV photoprocessing of alkanes plays an important role in the chemistry of the ISM, leading to the formation of unsaturated hydrocarbons and eventually to aromatics, the latter ubiquitously detected in the ISM. Here, through combined experimental results and \textit{ab-initio} calculations, we show that UV absorption (mainly at the Ly-$α$ emission line of hydrogen at 121.6 nm) promotes an alkane to an excited Rydberg state from where it evolves towards fragmentation inducing the formation of olefinic C=C bonds, which are necessary precursors of aromatic hydrocarbons. We show that photochemistry of aliphatics in the ISM does not primarily produce direct hydrogen elimination but preferential C-C photocleavage. Our results provide an efficient synthetic route for the formation of unsaturated aliphatics, including propene and dienes, and suggest that aromatics could be formed in dark clouds by a bottom-up mechanism involving molecular fragments produced by UV photoprocessing of aliphatics."
https://arxiv.org/abs/2403.08451,2024-03-13,An Integrated Usability Framework for Evaluating Open Government Data Portals: Comparative Analysis of EU and GCC Countries,"['Fillip Molodtsov', 'Anastasija Nikiforova']","This study explores the critical role of open government data (OGD) portals in fostering transparency and collaboration between diverse stakeholders. Recognizing the challenges of usability, communication with diverse populations, and strategic value creation, this paper develops an integrated framework for evaluating OGD portal effectiveness that accommodates user diversity (regardless of their data literacy and language), evaluates collaboration and participation, and the ability of users to explore and understand the data provided through them. The framework is validated by applying it to 33 national portals across European Union and Gulf Cooperation Council (GCC) countries, as a result of which we rank OGD portals, identify some good practices that lower-performing portals can learn from, and common shortcomings. Notably, the study unveils the competitive and innovative nature of GCC OGD portals, pinpointing specific improvement areas such as multilingual support and data understandability. The findings underscore the growing trend of exposing data quality metrics and advocate for enhanced two-way communication channels between users and portal representatives. Overall, the study contributes to accelerating the development of user-friendly, collaborative, and sustainable OGD portals while addressing gaps identified in previous research."
https://arxiv.org/abs/2403.08450,2024-03-13,Increasing stability for inverse source problem with limited-aperture far field data at multi-frequencies,"['Ibtissem Ben Aïcha', 'Guanghui Hu', 'Suliang Si']","We study the increasing stability of an inverse source problem for the Helmholtz equation from limited-aperture far field data at multiple wave numbers. The measurement data are givenby the far field patterns $u^\infity(\hat{x},k)$ for all observation directions in some neighborhood of a fixed direction $\hat{x}$ and for all wave numbers k belonging to a finite interval $(0,K)$. In this paper, we discuss the increasing stability with respect to the width of the wavenumber interval $K>1$. In three dimensions we establish stability estimates of the $L^2$-norm and $H^{-1}$-norm of the source function from the far field data. The ill-posedness of the inverse source problem turns out to be of Hölder type while increasing the wavenumber band K. We also discuss an analytic continuation argument of the far-field data with respect to the wavenumbers at a fixed direction."
https://arxiv.org/abs/2403.08449,2024-03-13,Criticality in an imidazolium ionic liquid fully wetting a sapphire support,"['Kevin Höllring', 'Nataša Vučemilović-Alagić', 'David M. Smith', 'Ana-Sunčana Smith']","Hypothesis: Ionic liquids have various applications in catalytic reaction environments. In those systems, their interaction with interfaces is key to their performance as a liquid phase. We hypothesize that the way a monolayer ionic liquid phase interacts with interfaces like a sapphire substrate is significantly dependent on temperature and that critical behavior can be observed in the structural properties of the liquid film."
https://arxiv.org/abs/2403.08448,2024-03-13,Actor-Critic Physics-informed Neural Lyapunov Control,"['Jiarui Wang', 'Mahyar Fazlyab']","Designing control policies for stabilization tasks with provable guarantees is a long-standing problem in nonlinear control. A crucial performance metric is the size of the resulting region of attraction, which essentially serves as a robustness ""margin"" of the closed-loop system against uncertainties. In this paper, we propose a new method to train a stabilizing neural network controller along with its corresponding Lyapunov certificate, aiming to maximize the resulting region of attraction while respecting the actuation constraints. Crucial to our approach is the use of Zubov's Partial Differential Equation (PDE), which precisely characterizes the true region of attraction of a given control policy. Our framework follows an actor-critic pattern where we alternate between improving the control policy (actor) and learning a Zubov function (critic). Finally, we compute the largest certifiable region of attraction by invoking an SMT solver after the training procedure. Our numerical experiments on several design problems show consistent and significant improvements in the size of the resulting region of attraction."
https://arxiv.org/abs/2403.08447,2024-03-13,Generating Synthetic Computed Tomography for Radiotherapy: SynthRAD2023 Challenge Report,"['Evi M. C. Huijben', 'Maarten L. Terpstra', 'Arthur Jr. Galapon', 'Suraj Pai', 'Adrian Thummerer', 'Peter Koopmans', 'Manya Afonso', 'Maureen van Eijnatten', 'Oliver Gurney-Champion', 'Zeli Chen', 'Yiwen Zhang', 'Kaiyi Zheng', 'Chuanpu Li', 'Haowen Pang', 'Chuyang Ye', 'Runqi Wang', 'Tao Song', 'Fuxin Fan', 'Jingna Qiu', 'Yixing Huang', 'Juhyung Ha', 'Jong Sung Park', 'Alexandra Alain-Beaudoin', 'Silvain Bériault', 'Pengxin Yu']","Radiation therapy plays a crucial role in cancer treatment, necessitating precise delivery of radiation to tumors while sparing healthy tissues over multiple days. Computed tomography (CT) is integral for treatment planning, offering electron density data crucial for accurate dose calculations. However, accurately representing patient anatomy is challenging, especially in adaptive radiotherapy, where CT is not acquired daily. Magnetic resonance imaging (MRI) provides superior soft-tissue contrast. Still, it lacks electron density information while cone beam CT (CBCT) lacks direct electron density calibration and is mainly used for patient positioning. Adopting MRI-only or CBCT-based adaptive radiotherapy eliminates the need for CT planning but presents challenges. Synthetic CT (sCT) generation techniques aim to address these challenges by using image synthesis to bridge the gap between MRI, CBCT, and CT. The SynthRAD2023 challenge was organized to compare synthetic CT generation methods using multi-center ground truth data from 1080 patients, divided into two tasks: 1) MRI-to-CT and 2) CBCT-to-CT. The evaluation included image similarity and dose-based metrics from proton and photon plans. The challenge attracted significant participation, with 617 registrations and 22/17 valid submissions for tasks 1/2. Top-performing teams achieved high structural similarity indices (>0.87/0.90) and gamma pass rates for photon (>98.1%/99.0%) and proton (>99.0%/97.3%) plans. However, no significant correlation was found between image similarity metrics and dose accuracy, emphasizing the need for dose evaluation when assessing the clinical applicability of sCT. SynthRAD2023 facilitated the investigation and benchmarking of sCT generation techniques, providing insights for developing MRI-only and CBCT-based adaptive radiotherapy."
https://arxiv.org/abs/2403.08446,2024-03-13,Arithmetic on $q$-deformed rational numbers,"['Takeyoshi Kogiso', 'Kengo Miyamoto', 'Xin Ren', 'Michihisa Wakui', 'Kohji Yanagawa']","Recently, Morier-Genoud and Ovsienko introduced a $q$-analog of rational numbers. More precisely, for an irreducible fraction $\frac{r}s>0$, they constructed coprime polynomials ${\mathcal R}_{\frac{r}s}(q), {\mathcal S}_{\frac{r}s}(q) \in {\mathbb Z}[q]$ with ${\mathcal R}_{\frac{r}s}(1)=r, {\mathcal S}_{\frac{r}s}(1)=s$. Their theory has a rich background and many applications. By definition, if $r \equiv r' \pmod{s}$, then ${\mathcal S}_{\frac{r}s}(q)={\mathcal S}_{\frac{r'}s}(q)$. We show that $rr'=-1 \pmod{s}$ implies ${\mathcal S}_{\frac{r}s}(q)={\mathcal S}_{\frac{r'}s}(q)$, and it is conjectured that the converse holds if $s$ is prime (and $r \not \equiv r' \pmod{s}$). We also show that $s$ is a multiple of 3 (resp. 4) if and only if ${\mathcal S}_{\frac{r}s}(ζ)=0$ for $ζ=(-1+\sqrt{-3})/2$ (resp. $ζ=i$). We give applications to the representation theory of quivers of type $A$ and the Jones polynomials of rational links."
https://arxiv.org/abs/2403.08445,2024-03-13,$L^2$ decay for large perturbations of viscous shocks for multi-D Burgers equation,"['Moon-Jin Kang', 'HyeonSeop Oh']","We consider a planar viscous shock of moderate strength for a scalar viscous conservation law in multi-D. We consider a strictly convex flux, as a small perturbation of the Burgers flux, along the normal direction to the shock front. However, for the transversal directions, we do not have any restrictions on flux function. We first show the contraction property for any large perturbations in $L^2$ of the planar viscous shock. If the initial $L^2$-perturbation is also in $L^1$, the large perturbation converges to zero in $L^2$ as time goes to infinity with $t^{-1/4}$ decay rate. The contraction and decay estimates hold up to dynamical shift. For the results, we do not impose any smallness conditions on the initial value. This result extends the 1D case \cite{Kang-V-1} by the first author and Vasseur to the multi-dimensional case."
https://arxiv.org/abs/2403.08444,2024-03-13,COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud Environments,"['Roman Heinrich', 'Carsten Binnig', 'Harald Kornmayer', 'Manisha Luthra']","In this work, we present COSTREAM, a novel learned cost model for Distributed Stream Processing Systems that provides accurate predictions of the execution costs of a streaming query in an edge-cloud environment. The cost model can be used to find an initial placement of operators across heterogeneous hardware, which is particularly important in these environments. In our evaluation, we demonstrate that COSTREAM can produce highly accurate cost estimates for the initial operator placement and even generalize to unseen placements, queries, and hardware. When using COSTREAM to optimize the placements of streaming operators, a median speed-up of around 21x can be achieved compared to baselines."
https://arxiv.org/abs/2403.08443,2024-03-13,The top eigenvalue of uniformly random trees,"['Louigi Addario-Berry', 'Gábor Lugosi', 'Roberto Imbuzeiro Oliveira']","Let ${\mathbf T}_n$ be a uniformly random tree with vertex set $[n]=\{1,\ldots,n\}$, let $Δ_{{\mathbf T}_n}$ be the largest vertex degree in ${\mathbf T}_n$, and let $λ_1({\mathbf T}_n)$ be the largest eigenvalue of its adjacency matrix. We prove that $|λ_1({\mathbf T}_n)-\sqrt{Δ_{{\mathbf T}_n}}| \to 0$ in expectation as $n \to \infty$, and additionally prove probability tail bounds for $|λ_1({\mathbf T}_n)-\sqrt{Δ_{{\mathbf T}_n}}|$. The proof is based on the trace method and thus on counting closed walks in a random tree. To this end, we develop novel combinatorial tools for encoding walks in trees that we expect will find other applications. In order to apply these tools, we show that uniformly random trees -- after appropriate ""surgery"" -- satisfy, with high probability, the properties required for the combinatorial bounds to be effective."
https://arxiv.org/abs/2403.08442,2024-03-13,Sensor Network Localization via Riemannian Conjugate Gradient and Rank Reduction: An Extended Version,"['Yicheng Li', 'Xinghua Sun']","This paper addresses the Sensor Network Localization (SNL) problem using received signal strength. The SNL is formulated as an Euclidean Distance Matrix Completion (EDMC) problem under the unit ball sample model. Using the Burer-Monteiro factorization type cost function, the EDMC is solved by Riemannian conjugate gradient with Hager-Zhang line search method on a quotient manifold. A ""rank reduction"" preprocess is proposed for proper initialization and to achieve global convergence with high probability. Simulations on a synthetic scene show that our approach attains better localization accuracy and is computationally efficient compared to several baseline methods. Characterization of a small local basin of attraction around the global optima of the s-stress function under Bernoulli sampling rule and incoherence matrix completion framework is conducted for the first time. Theoretical result conjectures that the Euclidean distance problem with a structure-less sample mask can be effectively handled using spectral initialization followed by vanilla first-order methods. This preliminary analysis, along with the aforementioned numerical accomplishments, provides insights into revealing the landscape of the s-stress function and may stimulate the design of simpler algorithms to tackle the non-convex formulation of general EDMC problems."
https://arxiv.org/abs/2403.08441,2024-03-13,"Stabilizer ground states: theory, algorithms and applications","['Jiace Sun', 'Lixue Cheng', 'Shi-Xin Zhang']","Stabilizer states have been commonly utilized in quantum information, quantum error correction, and quantum circuit simulation due to their simple mathematical structure. In this work, we apply stabilizer states to tackle quantum many-body problems and introduce the concept of stabilizer ground states. We present a simplified equivalent formalism for identifying stabilizer ground states of general Pauli Hamiltonians. Moreover, we also develop an exact and linear-scaled algorithm to obtain stabilizer ground states of 1D local Hamiltonians and thus free from discrete optimization. This proposed theoretical formalism and linear-scaled algorithm are not only applicable to finite-size systems, but also adaptable to infinite periodic systems. The scalability and efficiency of the algorithms are numerically benchmarked on different Hamiltonians. Finally, we demonstrate that stabilizer ground states can find various promising applications including better design on variational quantum algorithms, qualitative understanding of phase transitions, and cornerstones for more advanced ground state ansatzes."
https://arxiv.org/abs/2403.08440,2024-03-13,Increasing stability for inverse acoustic source problems in the time domain,"['Chun Liu', 'Suliang Si', 'Guanghui Hu', 'Bo Zhang']","This paper is concerned with inverse source problems for the acoustic wave equation in the full space R^3, where the source term is compactly supported in both time and spatial variables. The main goal is to investigate increasing stability for the wave equation in terms of the interval length of given parameters (e.g., bandwith of the temporal component of the source function). We establish increasing stability estimates of the L^2 -norm of the source function by using only the Dirichlet boundary data. Our method relies on the Huygens principle, the Fourier transform and explicit bounds for the continuation of analytic functions."
https://arxiv.org/abs/2403.08439,2024-03-13,Characterisation of Anti-Arrhythmic Drug Effects on Cardiac Electrophysiology using Physics-Informed Neural Networks,"['Ching-En Chiu', 'Arieh Levy Pinto', 'Rasheda A Chowdhury', 'Kim Christensen', 'Marta Varela']","The ability to accurately infer cardiac electrophysiological (EP) properties is key to improving arrhythmia diagnosis and treatment. In this work, we developed a physics-informed neural networks (PINNs) framework to predict how different myocardial EP parameters are modulated by anti-arrhythmic drugs. Using $\textit{in vitro}$ optical mapping images and the 3-channel Fenton-Karma model, we estimated the changes in ionic channel conductance caused by these drugs."
https://arxiv.org/abs/2403.08438,2024-03-13,Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research,"['Tobias Hille', 'Maximilian Stubbemann', 'Tom Hanika']","Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years. Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data. This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings. Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work. For this we introduce an ontology of reproducibility in machine learning and apply it to methods for graph neural networks. Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data and impeding the training and inference processes. Using the closely linked concept of geometric intrinsic dimension we investigate to which extend the used machine learning models are influenced by the intrinsic dimension of the data sets they are trained on."
https://arxiv.org/abs/2403.08437,2024-03-13,How motility drives the glassy dynamics in confluent epithelial monolayers?,"['Souvik Sadhukhan', 'Manoj Kumar Nandi', 'Satyam Pandey', 'Matteo Paoluzzi', 'Chandan Dasgupta', 'Nir Gov', 'Saroj Kumar Nandi']","As wounds heal, embryos develop, cancer spreads, or asthma progresses, the cellular monolayer undergoes a glass transition from a solid-like jammed to a fluid-like flowing state. Two primary characteristics of these systems, confluency, and self-propulsion, make them distinct from particulate systems. Are the glassy dynamics in these biological systems and equilibrium particulate systems different? Despite the biological significance of glassiness in these systems, no analytical framework, which is indispensable for deeper insights, exists. Here, we extend one of the most popular theories of equilibrium glasses, the random first-order transition (RFOT) theory, for confluent systems with self-propulsion. One crucial result of this work is that, unlike in particulate systems, the confluency affects the effective persistence time-scale of the active force, described by its rotational diffusion $D_r^{\text{eff}}$. Unlike in particulate systems, this value differs from the bare rotational diffusion of the active propulsion force due to cell shape dynamics which acts to rectify the force dynamics: $D_r^{\text{eff}}$ is equal to $D_r$ when $D_r$ is small, and saturates when $D_r$ is large. We present simulation results for the glassy dynamics in active confluent models and find that the results are consistent with existing experimental data, and conform remarkably well with our theory. In addition, we show that the theoretical predictions agree nicely with and explain previously published simulation results. Our analytical theory provides a foundation for rationalizing and a quantitative understanding of various glassy characteristics of these biological systems."
https://arxiv.org/abs/2403.08436,2024-03-13,PFStorer: Personalized Face Restoration and Super-Resolution,"['Tuomas Varanka', 'Tapani Toivonen', 'Soumya Tripathy', 'Guoying Zhao', 'Erman Acar']","Recent developments in face restoration have achieved remarkable results in producing high-quality and lifelike outputs. The stunning results however often fail to be faithful with respect to the identity of the person as the models lack necessary context. In this paper, we explore the potential of personalized face restoration with diffusion models. In our approach a restoration model is personalized using a few images of the identity, leading to tailored restoration with respect to the identity while retaining fine-grained details. By using independent trainable blocks for personalization, the rich prior of a base restoration model can be exploited to its fullest. To avoid the model relying on parts of identity left in the conditioning low-quality images, a generative regularizer is employed. With a learnable parameter, the model learns to balance between the details generated based on the input image and the degree of personalization. Moreover, we improve the training pipeline of face restoration models to enable an alignment-free approach. We showcase the robust capabilities of our approach in several real-world scenarios with multiple identities, demonstrating our method's ability to generate fine-grained details with faithful restoration. In the user study we evaluate the perceptual quality and faithfulness of the genereated details, with our method being voted best 61% of the time compared to the second best with 25% of the votes."
https://arxiv.org/abs/2403.08435,2024-03-13,Asymptotic behaviour of integer programming and the $\text{v}$-function of a graded filtration,"['Antonino Ficarra', 'Emanuele Sgroi']","The $\text{v}$-function of a graded filtration $\mathcal{I}=\{I_{[k]}\}_{k\ge0}$ is introduced. Under the assumption that $\mathcal{I}$ is Noetherian, we prove that the $\text{v}$-function $\text{v}(I_{[k]})$ is an eventually quasi-linear function. This result applies to several situations, including ordinary powers, and integral closures of ordinary powers, among others. As another application, we investigate the asymptotic behaviour of certain integer programming problems. Finally, we present the Macaulay2 package \texttt{VNumber}."
https://arxiv.org/abs/2403.08434,2024-03-13,GRF-based Predictive Flocking Control with Dynamic Pattern Formation,"['Chenghao Yu', 'Dengyu Zhang', 'Qingrui Zhang']","It is promising but challenging to design flocking control for a robot swarm to autonomously follow changing patterns or shapes in a optimal distributed manner. The optimal flocking control with dynamic pattern formation is, therefore, investigated in this paper. A predictive flocking control algorithm is proposed based on a Gibbs random field (GRF), where bio-inspired potential energies are used to charaterize ``robot-robot'' and ``robot-environment'' interactions. Specialized performance-related energies, e.g., motion smoothness, are introduced in the proposed design to improve the flocking behaviors. The optimal control is obtained by maximizing a posterior distribution of a GRF. A region-based shape control is accomplished for pattern formation in light of a mean shift technique. The proposed algorithm is evaluated via the comparison with two state-of-the-art flocking control methods in an environment with obstacles. Both numerical simulations and real-world experiments are conducted to demonstrate the efficiency of the proposed design."
https://arxiv.org/abs/2403.08433,2024-03-13,An Empirical Study of Parameter Efficient Fine-tuning on Vision-Language Pre-train Model,"['Yuxin Tian', 'Mouxing Yang', 'Yunfan Li', 'Dayiheng Liu', 'Xingzhang Ren', 'Xi Peng', 'Jiancheng Lv']","Recent studies applied Parameter Efficient Fine-Tuning techniques (PEFTs) to efficiently narrow the performance gap between pre-training and downstream. There are two important factors for various PEFTs, namely, the accessible data size and fine-tunable parameter size. A natural expectation for PEFTs is that the performance of various PEFTs is positively related to the data size and fine-tunable parameter size. However, according to the evaluation of five PEFTs on two downstream vision-language (VL) tasks, we find that such an intuition holds only if the downstream data and task are not consistent with pre-training. For downstream fine-tuning consistent with pre-training, data size no longer affects the performance, while the influence of fine-tunable parameter size is not monotonous. We believe such an observation could guide the choice of training strategy for various PEFTs."
https://arxiv.org/abs/2403.08432,2024-03-13,Modelling of initially stressed solids: structure of the energy density in the incompressible limit,"['M. Magri', 'D. Riccobelli']","This study addresses the modelling of elastic bodies, particularly when the relaxed configuration is unknown or non-existent. We adopt the theory of initially stressed materials, incorporating the deformation gradient and stress state of the reference configuration (initial stress tensor) into the response function. We show that for the theory to be applicable, the response function of the relaxed material is invertible up to an element of the material symmetry group. Additionally, we establish that commonly imposed constitutive restrictions, namely the initial stress compatibility condition and initial stress reference independence, naturally arise when assuming an initial stress generated solely from elastic distortion. The paper delves into modelling aspects concerning incompressible materials, showcasing the expressibility of strain energy density as a function of the deviatoric part of the initial stress tensor and the isochoric part of the deformation gradient. This not only reduces the number of independent invariants in the energy functional, but also enhances numerical robustness in finite element simulations. The findings of this research hold significant implications for modelling materials with initial stress, extending potential applications to areas such as mechanobiology, soft robotics, and 4D printing."
https://arxiv.org/abs/2403.08431,2024-03-13,Spatially resolved emission lines in galaxies at $4\leq z < 10$ from the JADES survey: evidence for enhanced central star formation,"['Roberta Tripodi', ""Francesco D'Eugenio"", 'Roberto Maiolino', 'Mirko Curti', 'Jan Scholtz', 'Sandro Tacchella', 'Andrew J. Bunker', 'James A. A. Trussler', 'Alex J. Cameron', 'Santiago Arribas', 'William M. Baker', 'Maruša Bradač', 'Stefano Carniani', 'Stéfane Charlot', 'Xihan Ji', 'Zhiyuan Ji', 'Brant Robertson', 'Hannah Übler', 'Giacomo Venturi', 'Christopher N. A. Willmer', 'Joris Witstok']","We present the first statistical investigation of spatially resolved emission-line properties in a sample of 63 low-mass galaxies at $4\leq z<10$, using JWST/NIRSpec MSA data from the JWST Advanced Deep Extragalactic (JADES) survey focusing on deep, spatially resolved spectroscopy in the GOODS-S extragalactic field. By performing a stacking of the 2D spectra of the galaxies in our sample, we find an increasing or flat radial trend with increasing radius for [OIII]$\lambda5007$/H$β$ and a decreasing one for [NeIII]$\lambda3869$/[OII]$\lambda3727$ (3--4 $σ$ significance). These results are still valid when stacking the sample in two redshift bins (i.e., $4\leq z<5.5$ and $5.5\leq z<10$). The comparison with star-formation photoionization models suggests that the ionization parameter increases by $\sim 0.5$ dex with redshift. We find a tentative metallicity gradient that increases with radius (i.e., 'inverted') in both redshift bins. Moreover, our analysis reveals strong negative gradients for the equivalent width of \Hbeta (7$σ$ significance). This trend persists even after removing known AGN candidates, therefore, it is consistent with a radial gradient primarily in stellar age and secondarily in metallicity. Taken all together, our results suggest that the sample is dominated by active central star formation, with possibly inverted metallicity gradients sustained by recent episodes of accretion of pristine gas or strong radial flows. Deeper observations and larger samples are needed to confirm these preliminary results and to validate our interpretation."
https://arxiv.org/abs/2403.08430,2024-03-13,Search-based Optimisation of LLM Learning Shots for Story Point Estimation,"['Vali Tawosi', 'Salwa Alamir', 'Xiaomo Liu']","One of the ways Large Language Models (LLMs) are used to perform machine learning tasks is to provide them with a few examples before asking them to produce a prediction. This is a meta-learning process known as few-shot learning. In this paper, we use available Search-Based methods to optimise the number and combination of examples that can improve an LLM's estimation performance, when it is used to estimate story points for new agile tasks. Our preliminary results show that our SBSE technique improves the estimation performance of the LLM by 59.34% on average (in terms of mean absolute error of the estimation) over three datasets against a zero-shot setting."
https://arxiv.org/abs/2403.08429,2024-03-13,Software Vulnerability and Functionality Assessment using LLMs,"['Rasmus Ingemann Tuffveson Jensen', 'Vali Tawosi', 'Salwa Alamir']","While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities."
https://arxiv.org/abs/2403.08428,2024-03-13,DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued Neural Networks,"['Florian Eilers', 'Xiaoyi Jiang']","Deep Neural Networks are widely used in academy as well as corporate and public applications, including safety critical applications such as health care and autonomous driving. The ability to explain their output is critical for safety reasons as well as acceptance among applicants. A multitude of methods have been proposed to explain real-valued neural networks. Recently, complex-valued neural networks have emerged as a new class of neural networks dealing with complex-valued input data without the necessity of projecting them onto $\mathbb{R}^2$. This brings up the need to develop explanation algorithms for this kind of neural networks. In this paper we provide these developments. While we focus on adapting the widely used DeepSHAP algorithm to the complex domain, we also present versions of four gradient based explanation methods suitable for use in complex-valued neural networks. We evaluate the explanation quality of all presented algorithms and provide all of them as an open source library adaptable to most recent complex-valued neural network architectures."
https://arxiv.org/abs/2403.08427,2024-03-13,Annihilation of positrons from AGN jets as a possible source of cosmic gamma-ray background at energies below 511 keV,"['B. A. Nizamov', 'M. S. Pshirkov']","The origin of the diffuse gamma-ray background in the range from hundreds keV to several MeV is not known conclusively. From current models and observations it is believed that, at least partially, this background is formed by blazars and remnants of supernovae (SN) of type Ia in distant galaxies. However, these contributions are not sufficient to reproduce the observed level of the signal. In this work we propose another source which could contribute to this background, namely the jets of active galactic nuclei (AGN). The composition of jets is not known, but there are observational hints that the fraction of positrons there is substantial. Positrons are partially evacuated to the intergalactic medium and partially mix with the circumgalactic medium and annihilate there comparatively quickly. Using the AGN luminosity function, we estimated the positron production rate and the contribution of the positron annihilation to the cosmic background below 511 keV. We also estimated the analogous contribution from positron annihilation within SN Ia remnants in distant galaxies. The contribution of AGNs is estimated to be a factor of 5 - 10 smaller than the observed background intensity, and the contribution from SNe is yet smaller by one order of magnitude. Nevertheless, the contribution of AGNs appeared to be larger than the contribution of blazars estimated from Swift-BAT and Fermi-LAT observations. The main uncertainty in our model is the fraction of positrons remaining in the circumgalactic medium which makes our estimation an upper limit."
https://arxiv.org/abs/2403.08426,2024-03-13,Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation,"['Zicheng Zhang', 'Tong Zhang', 'Yi Zhu', 'Jianzhuang Liu', 'Xiaodan Liang', 'QiXiang Ye', 'Wei Ke']","The pre-trained vision-language model, exemplified by CLIP, advances zero-shot semantic segmentation by aligning visual features with class embeddings through a transformer decoder to generate semantic masks. Despite its effectiveness, prevailing methods within this paradigm encounter challenges, including overfitting on seen classes and small fragmentation in masks. To mitigate these issues, we propose a Language-Driven Visual Consensus (LDVC) approach, fostering improved alignment of semantic and visual information.Specifically, we leverage class embeddings as anchors due to their discrete and abstract nature, steering vision features toward class embeddings. Moreover, to circumvent noisy alignments from the vision part due to its redundant nature, we introduce route attention into self-attention for finding visual consensus, thereby enhancing semantic consistency within the same object. Equipped with a vision-language prompting strategy, our approach significantly boosts the generalization capacity of segmentation models for unseen classes. Experimental results underscore the effectiveness of our approach, showcasing mIoU gains of 4.5 on the PASCAL VOC 2012 and 3.6 on the COCO-Stuff 164k for unseen classes compared with the state-of-the-art methods."
https://arxiv.org/abs/2403.08425,2024-03-13,Specification Overfitting in Artificial Intelligence,"['Benjamin Roth', 'Pedro Henrique Luz de Araujo', 'Yuxi Xia', 'Saskia Kaltenbrunner', 'Christoph Korab']","Machine learning (ML) and artificial intelligence (AI) approaches are often criticized for their inherent bias and for their lack of control, accountability, and transparency. Consequently, regulatory bodies struggle with containing this technology's potential negative side effects. High-level requirements such as fairness and robustness need to be formalized into concrete specification metrics, imperfect proxies that capture isolated aspects of the underlying requirements. Given possible trade-offs between different metrics and their vulnerability to over-optimization, integrating specification metrics in system development processes is not trivial. This paper defines specification overfitting, a scenario where systems focus excessively on specified metrics to the detriment of high-level requirements and task performance. We present an extensive literature survey to categorize how researchers propose, measure, and optimize specification metrics in several AI fields (e.g., natural language processing, computer vision, reinforcement learning). Using a keyword-based search on papers from major AI conferences and journals between 2018 and mid-2023, we identify and analyze 74 papers that propose or optimize specification metrics. We find that although most papers implicitly address specification overfitting (e.g., by reporting more than one specification metric), they rarely discuss which role specification metrics should play in system development or explicitly define the scope and assumptions behind metric formulations."
https://arxiv.org/abs/2403.08424,2024-03-13,Tastle: Distract Large Language Models for Automatic Jailbreak Attack,"['Zeguan Xiao', 'Yan Yang', 'Guanhua Chen', 'Yun Chen']","Large language models (LLMs) have achieved significant advances in recent days. Extensive efforts have been made before the public release of LLMs to align their behaviors with human values. The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents. Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability. In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, motivated by the research about the distractibility and over-confidence phenomenon of LLMs. Extensive experiments of jailbreaking both open-source and proprietary LLMs demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability. We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies."
https://arxiv.org/abs/2403.08423,2024-03-13,Gluonic gravitational form factors of the proton,['Zein-Eddine Meziani'],"The gravitational form factors (GFFs) are a fundamental and elegant way to describe the structure of nucleons and nuclei. Their Fourier transform allows a description of the spatial distribution of the mass, angular momentum, pressure, and shear force densities for both quarks and gluons in the nucleon. While previous investigations predominantly focused on the proton electromagnetic form factors (EMFFs) leading to the charge and magnetization distributions determination, the current emphasis has shifted towards expanding our understanding of the gravitational form factors of quarks and gluons where little is known. In particular, more recently, the proton {\it gluonic} GFFs have been the target of an intensive investigation at Jefferson Lab. This endeavor, is not without its challenges, particularly in navigating the complexities associated with the near-threshold region. Nevertheless, it provides a bedrock for future nucleon and nuclei gluonic structure studies at the future EIC. In this talk, I will focus on the recent results of $J/ψ$ photoproduction near-threshold on the proton at Jefferson Lab to determine, in particular, the elusive {\it gluonic} gravitational form factors. We discuss the caveats of their extraction in the threshold region and mention the complementary measurements of $Υ$ at the EIC critical to access the trace anomaly and gain insight into the origin of the nucleon mass."
https://arxiv.org/abs/2403.08422,2024-03-13,Stochastic action for the entanglement of a noisy monitored two-qubit system,"['Dominic Shea', 'Alessandro Romito']",We study the effect of local unitary noise on the entanglement evolution of a two-qubit system subject to local monitoring and inter-qubit coupling. We construct a stochastic Hamiltonian by incorporating the noise into the Chantasri-Dressel-Jordan path integral and use it to identify the optimal entanglement dynamics and to develop a diagrammatic method for a closed-form approximation of the average entanglement dynamics with an analytical dependence on the noise and measurement intensity. We find that both the optimal trajectory and diagrammatic expansion capture the oscillations of entanglement at short times. Numerical investigation of long-time steady-state entanglement reveals a non-monotonic relationship between concurrence and noise strength.
https://arxiv.org/abs/2403.08421,2024-03-13,Measures of relevance to the success of streaming platforms,"['Juan Carlos Gonçalves-Dosantos', 'Ricardo Martínez', 'Joaquín Sánchez-Soriano']","Digital streaming platforms, including Twitch, Spotify, Netflix, Disney, and Kindle, have emerged as one of the main sources of entertainment with significant growth potential. Many of these platforms distribute royalties among streamers, artists, producers, or writers based on their impact. In this paper, we measure the relevance of each of these contributors to the overall success of the platform, which is information that can play a key role in revenue allocation. We perform an axiomatic analysis to provide normative foundations for three relevance metrics: the uniform, the proportional, and the subscriber-proportional indicators. The last two indicators implement the so-called pro-rata and user-centric models, which are extensively applied to distribute revenues in the music streaming market. The axioms we propose formalize different principles of fairness, stability, and non-manipulability, and are tailor-made for the streaming context. We complete our analysis with a case study that measures the influence of the 19 most-followed streamers worldwide on the Twitch platform."
https://arxiv.org/abs/2403.08420,2024-03-13,Low-Cost and Real-Time Industrial Human Action Recognitions Based on Large-Scale Foundation Models,"['Wensheng Liang', 'Ruiyan Zhuang', 'Xianwei Shi', 'Shuai Li', 'Zhicheng Wang', 'Xiaoguang Ma']","Industrial managements, including quality control, cost and safety optimization, etc., heavily rely on high quality industrial human action recognitions (IHARs) which were hard to be implemented in large-scale industrial scenes due to their high costs and poor real-time performance. In this paper, we proposed a large-scale foundation model(LSFM)-based IHAR method, wherein various LSFMs and lightweight methods were jointly used, for the first time, to fulfill low-cost dataset establishment and real-time IHARs. Comprehensive tests on in-situ large-scale industrial manufacturing lines elucidated that the proposed method realized great reduction on employment costs, superior real-time performance, and satisfactory accuracy and generalization capabilities, indicating its great potential as a backbone IHAR method, especially for large-scale industrial applications."
https://arxiv.org/abs/2403.08419,2024-03-13,Boundary and distributed optimal control for a population dynamics PDE model with discontinuous in time Galerkin FEM schemes,['EFthymios N. Karatzas'],"We consider fully discrete finite element approximations for a semilinear optimal control system of partial differential equations in two cases: for distributed and Robin boundary control. The ecological predator-prey optimal control model is approximated by conforming finite element methods mimicking the spatial part, while a discontinuous Galerkin method is used for the time discretization. We investigate the sensitivity of the solution distance from the target function, in cases with smooth and rough initial data. We employ low, and higher-order polynomials in time and space whenever proper regularity is present. The approximation schemes considered are with and without control constraints, driving efficiently the system to desired states realized using non-linear gradient methods."
https://arxiv.org/abs/2403.08418,2024-03-13,Powers and roots of partial isometric covariant representations,"['Dimple Saini', 'Harsh Trivedi', 'Shankar Veerabathiran']","Isometric covariant representations play an important role in the study of Cuntz-Pimsner algebras. In this article, we study partial isometric covariant representations and explore under what conditions powers and roots of partial isometric covariant representations are also partial isometric covariant representations."
https://arxiv.org/abs/2403.08417,2024-03-13,The Development and Performance of a Machine Learning Based Mobile Platform for Visually Determining the Etiology of Penile Pathology,"['Lao-Tzu Allan-Blitz', 'Sithira Ambepitiya', 'Raghavendra Tirupathi', 'Jeffrey D. Klausner', 'Yudara Kularathne']","Machine-learning algorithms can facilitate low-cost, user-guided visual diagnostic platforms for addressing disparities in access to sexual health services. We developed a clinical image dataset using original and augmented images for five penile diseases: herpes eruption, syphilitic chancres, penile candidiasis, penile cancer, and genital warts. We used a U-net architecture model for semantic pixel segmentation into background or subject image, the Inception-ResNet version 2 neural architecture to classify each pixel as diseased or non-diseased, and a salience map using GradCAM++. We trained the model on a random 91% sample of the image database using 150 epochs per image, and evaluated the model on the remaining 9% of images, assessing recall (or sensitivity), precision, specificity, and F1-score (accuracy). Of the 239 images in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%) were of HSV infection, 29 (12.1%) were of penile cancer, 40 (16.7%) were of penile candidiasis, 37 (15.5%) were of syphilitic chancres, and 45 (18.8%) were of non-diseased penises. The overall accuracy of the model for correctly classifying the diseased image was 0.944. Between July 1st and October 1st 2023, there were 2,640 unique users of the mobile platform. Among a random sample of submissions (n=437), 271 (62.0%) were from the United States, 64 (14.6%) from Singapore, 41 (9.4%) from Candia, 40 (9.2%) from the United Kingdom, and 21 (4.8%) from Vietnam. The majority (n=277 [63.4%]) were between 18 and 30 years old. We report on the development of a machine-learning model for classifying five penile diseases, which demonstrated excellent performance on a validation dataset. That model is currently in use globally and has the potential to improve access to diagnostic services for penile diseases."
https://arxiv.org/abs/2403.08416,2024-03-13,Superfluid fraction of interacting bosonic gases,"['Daniel Pérez-Cruz', 'Grigori E. Astrakharchik', 'Pietro Massignan']","The superfluid fraction $f$ of a quantum fluid is defined in terms of the response of the system to a weak and constant drag. Notably, Leggett long ago derived two simple expressions providing a rigorous upper bound and a heuristic lower bound for $f$. Here we study the superfluid fraction of bosonic gases in various two-dimensional potentials, such as regular optical lattices and disordered speckles, by solving the Gross-Pitaevskii equation and performing Diffusion Monte Carlo simulations. We show that under conditions relevant for most ultracold experiments the bounds proposed by Leggett provide a surprisingly narrow bracketing of the exact value of the superfluid fraction."
https://arxiv.org/abs/2403.08415,2024-03-13,Intersection of a Moran type Sierpinski carpet and a line with rational slope,['Simin Bao'],"In 2005, Liu et al. calculated the dimensionality of the intersection of Sierpinski carpet and a straight line with rational slope in the sense of Lebesgue measure.Sierpinski carpet is a self-similar set in two-dimensional planes obtained by an iterative function system, so each layer has the same structure. While the Sierpinski carpet set with Moran structure is the limit set obtained by the action of two iterative function systems in the two-dimensional plane, which we denote as ~$F_σ$~. And the structure of each layer may be different, controlled by 0,1 sequence ~$σ$~ and controlled by the set. In this paper, the upper and lower box dimensions of the set ~$F_σ$~ and the straight line ~$L_{a}$~ with rational slope are calculated, where ~$a$~ is the intercept of the straight line. In addition, we consider some related problem. The main difficulty in the research is that the structure of each layer of the set ~$F_σ$~ may be different, so the structure of each layer needs to be considered with the help of the sequence ~$σ$~ in the calculation process."
https://arxiv.org/abs/2403.08414,2024-03-13,Causal Graph Neural Networks for Wildfire Danger Prediction,"['Shan Zhao', 'Ioannis Prapas', 'Ilektra Karasante', 'Zhitong Xiong', 'Ioannis Papoutsis', 'Gustau Camps-Valls', 'Xiao Xiang Zhu']","Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships. Furthermore, SHAP values from our trained model further enhance our understanding of the model's inner workings."
https://arxiv.org/abs/2403.08413,2024-03-13,Electron scattering on $^4$He from coupled-cluster theory,['Joanna E. Sobczyk'],"We present a coupled-cluster calculation for the electron-$^4$He scattering in the region of the quasi-elastic peak. We show the longitudinal and transverse responses separately, and discuss results within two distinct theoretical methods: the Lorentz integral transform and spectral functions. The comparison between them allows to investigate the role of final state interactions, two-body currents and relativistic effects."
https://arxiv.org/abs/2403.08412,2024-03-13,Theoretical limits of magnetic detection of structural surface defects at the nanometer scale,"['Wolfgang Körner', 'Daniel F. Urban', 'Christian Elsässer']","We present a theoretical study on the magnetic signals of structural surface defects like cracks or indents combined with rough surfaces or subsurface inclusions of soft ferromagnetic metals like body-centered cubic Fe or amorphous CoFeB. We discuss limits of early detection of small surface defects on the basis of calculated magnetic stray fields few tens of nm above the surface. The considered surface imperfections have extensions of a few nm which correspond to low multiples of the magnetic exchange lengths of Fe or CoFeB. The detection of such small inhomogeneities requires that the sensor is about as close to the surface as the size of the inhomogeneity is. Furthermore, the step width of a scanning sensor must be of the same size as well. Both these requirements may be fulfilled for instance by scanning microscopy with diamond nitrogen-vacancy-center quantum sensors."
https://arxiv.org/abs/2403.08411,2024-03-13,Robust Distributed Compression with Learned Heegard-Berger Scheme,"['Eyyup Tasci', 'Ezgi Ozyilkan', 'Oguzhan Kubilay Ulger', 'Elza Erkip']","We consider lossy compression of an information source when decoder-only side information may be absent. This setup, also referred to as the Heegard-Berger or Kaspi problem, is a special case of robust distributed source coding. Building upon previous works on neural network-based distributed compressors developed for the decoder-only side information (Wyner-Ziv) case, we propose learning-based schemes that are amenable to the availability of side information. We find that our learned compressors mimic the achievability part of the Heegard-Berger theorem and yield interpretable results operating close to information-theoretic bounds. Depending on the availability of the side information, our neural compressors recover characteristics of the point-to-point (i.e., with no side information) and the Wyner-Ziv coding strategies that include binning in the source space, although no structure exploiting knowledge of the source and side information was imposed into the design."
https://arxiv.org/abs/2403.08410,2024-03-13,"Painlevé Analysis, Prelle-Singer Approach, Symmetries and Integrability of Damped Hénon-Heiles System","['C. Uma Maheswari', 'N. Muthuchamy', 'V. K. Chandrasekar', 'R. Sahadevan', 'M. Lakshmanan']","We consider a modified damped version of Hénon-Heiles system and investigate its integrability. By extending the Painlevé analysis of ordinary differential equations we find that the modified Hénon-Heiles system possesses the Painlevé property for three distinct parametric restrictions. For each of the identified cases, we construct two independent integrals of motion using the well known Prelle-Singer method. We then derive a set of nontrivial non-point symmetries for each of the identified integrable cases of the modified Hénon-Heiles system. We infer that the modified Hénon-Heiles system is integrable for three distinct parametric restrictions. Exact solutions are given explicitly for two integrable cases."
https://arxiv.org/abs/2403.08409,2024-03-13,On the universal properties of stochastic processes under optimally tuned Poisson restart,['Sergey Belan'],"Poisson restart assumes that a stochastic process is interrupted and starts again at random time moments. A number of studies have demonstrated that this strategy may minimize the expected completion time in some classes of random search tasks. What is more, it turned out that under optimally tuned restart rate, any stochastic process, regardless of its nature and statistical details, satisfies a number of universal relations for the statistical moments of completion time. In this paper, we describe several new universal properties of optimally restarted processes. Also we obtain a universal inequality for the quadratic statistical moments of completion time in the optimization problem where stochastic process has several possible completion scenarios."
https://arxiv.org/abs/2403.08408,2024-03-13,Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models,"['Mohammad Lashkari', 'Amin Gheibi']","The generalization performance of deep neural networks in classification tasks is a major concern in machine learning research. Despite widespread techniques used to diminish the over-fitting issue such as data augmentation, pseudo-labeling, regularization, and ensemble learning, this performance still needs to be enhanced with other approaches. In recent years, it has been theoretically demonstrated that the loss function characteristics i.e. its Lipschitzness and maximum value affect the generalization performance of deep neural networks which can be utilized as a guidance to propose novel distance measures. In this paper, by analyzing the aforementioned characteristics, we introduce a distance called Reduced Jeffries-Matusita as a loss function for training deep classification models to reduce the over-fitting issue. In our experiments, we evaluate the new loss function in two different problems: image classification in computer vision and node classification in the context of graph learning. The results show that the new distance measure stabilizes the training process significantly, enhances the generalization ability, and improves the performance of the models in the Accuracy and F1-score metrics, even if the training set size is small."
https://arxiv.org/abs/2403.08407,2024-03-13,Iterative Online Image Synthesis via Diffusion Model for Imbalanced Classification,"['Shuhan Li', 'Yi Lin', 'Hao Chen', 'Kwang-Ting Cheng']","Accurate and robust classification of diseases is important for proper diagnosis and treatment. However, medical datasets often face challenges related to limited sample sizes and inherent imbalanced distributions, due to difficulties in data collection and variations in disease prevalence across different types. In this paper, we introduce an Iterative Online Image Synthesis (IOIS) framework to address the class imbalance problem in medical image classification. Our framework incorporates two key modules, namely Online Image Synthesis (OIS) and Accuracy Adaptive Sampling (AAS), which collectively target the imbalance classification issue at both the instance level and the class level. The OIS module alleviates the data insufficiency problem by generating representative samples tailored for online training of the classifier. On the other hand, the AAS module dynamically balances the synthesized samples among various classes, targeting those with low training accuracy. To evaluate the effectiveness of our proposed method in addressing imbalanced classification, we conduct experiments on the HAM10000 and APTOS datasets. The results obtained demonstrate the superiority of our approach over state-of-the-art methods as well as the effectiveness of each component. The source code will be released upon acceptance."
https://arxiv.org/abs/2403.08406,2024-03-13,Topology of Discrete Quantum Feedback Control,"['Masaya Nakagawa', 'Masahito Ueda']",A general framework for analyzing topology of quantum channels of single-particle systems is developed to find a class of genuinely dynamical topological phases that can be realized by means of discrete quantum feedback control. We provide a symmetry classification of quantum channels by identifying ten symmetry classes of discrete quantum feedback control with projective measurements. We construct various types of topological feedback control by using topological Maxwell's demons that achieve robust feedback-controlled chiral or helical transport against noise and decoherence. Topological feedback control thus offers a versatile tool for creating and controlling nonequilibrium topological phases in open quantum systems that are distinct from non-Hermitian and Lindbladian systems and should provide a guiding principle for topology-based design of quantum feedback control.
https://arxiv.org/abs/2403.08405,2024-03-13,Coriolis darkening in late-type stars II. Effect of self-sustained magnetic fields in stratified convective envelope,"['C. Pinçon', 'L. Petitdemange', 'R. Raynaud', 'L. J. Garcia', 'A. Guseva', 'M. Rieutord', 'E. Alecian']","Modeling the surface brightness distribution of stars is of prime importance to interpret observations. Nevertheless, this remains quite challenging for cool stars as it requires one to model the MHD turbulence that develops in their convective envelope. In Paper I, the effect of the Coriolis acceleration on the surface heat flux has been studied by means of hydrodynamic simulations. In this paper, we aim to investigate the additional effect of dynamo magnetic fields. We focus on an envelope thickness that is representative of either a $\sim0.35~M_\odot$ M dwarf, a young red giant star or a pre-main sequence star. We performed a parametric study using numerical MHD simulations of anelastic convection in thick rotating spherical shells. For each model, we computed the mean surface distribution of the heat flux, and examined the leading-order effect of the magnetic field on the obtained latitudinal luminosity profile. We identify three different regimes. Close to the onset of convection, while the first unstable modes tend to convey heat more efficiently near the equator, magnetic fields are shown to generally enhance the mean heat flux close to the polar regions (and the tangent cylinder). By progressively increasing the Rayleigh number, the development of a prograde equatorial jet was previously shown to make the equator darker when no magnetic field is taken into account. For moderate Rayleigh numbers, magnetic fields can instead inverse the mean pole-equator brightness contrast (which means going from a darker to a brighter equator when a dynamo sets in) and finally induce a similar regime to that found close to the onset of convection. For more turbulent models with larger Rayleigh numbers, magnetic fields alternatively tend to smooth out the brightness contrast. This general behavior is shown to be related to the quenching of the surface differential rotation by magnetic fields."
https://arxiv.org/abs/2403.08404,2024-03-13,Stealthy and hyperuniform isotropic photonic bandgap structure in 3D,"['Lukas Siedentop', 'Gianluc Lui', 'Georg Maret', 'Paul M. Chaikin', 'Paul J. Steinhardt', 'Salvatore Torquato', 'Peter Keim', 'Marian Florescu']","In photonic crystals the propagation of light is governed by their photonic band structure, an ensemble of propagating states grouped into bands, separated by photonic band gaps. Due to discrete symmetries in spatially strictly periodic dielectric structures their photonic band structure is intrinsically anisotropic. However, for many applications, such as manufacturing artificial structural color materials or developing photonic computing devices, but also for the fundamental understanding of light-matter interactions, it is of major interest to seek materials with long range non-periodic dielectric structures which allow the formation of {\it isotropic} photonic band gaps. Here, we report the first ever 3D isotropic photonic band gap for an optimized disordered stealthy hyperuniform structure for microwaves. The transmission spectra are directly compared to a diamond pattern and an amorphous structure with similar node density. The band structure is measured experimentally for all three microwave structures, manufactured by 3D-Laser-printing for meta-materials with refractive index up to $n=2.1$. Results agree well with finite-difference-time-domain numerical investigations and a priori calculations of the band-gap for the hyperuniform structure: the diamond structure shows gaps but being anisotropic as expected, the stealthy hyperuniform pattern shows an isotropic gap of very similar magnitude, while the amorphous structure does not show a gap at all. The centimeter scaled microwave structures may serve as prototypes for micrometer scaled structures with bandgaps in the technologically very interesting region of infrared (IR)."
https://arxiv.org/abs/2403.08403,2024-03-13,FSDR: A Novel Deep Learning-based Feature Selection Algorithm for Pseudo Time-Series Data using Discrete Relaxation,"['Mohammad Rahman', 'Manzur Murshed', 'Shyh Wei Teng', 'Manoranjan Paul']","Conventional feature selection algorithms applied to Pseudo Time-Series (PTS) data, which consists of observations arranged in sequential order without adhering to a conventional temporal dimension, often exhibit impractical computational complexities with high dimensional data. To address this challenge, we introduce a Deep Learning (DL)-based feature selection algorithm: Feature Selection through Discrete Relaxation (FSDR), tailored for PTS data. Unlike the existing feature selection algorithms, FSDR learns the important features as model parameters using discrete relaxation, which refers to the process of approximating a discrete optimisation problem with a continuous one. FSDR is capable of accommodating a high number of feature dimensions, a capability beyond the reach of existing DL-based or traditional methods. Through testing on a hyperspectral dataset (i.e., a type of PTS data), our experimental results demonstrate that FSDR outperforms three commonly used feature selection algorithms, taking into account a balance among execution time, $R^2$, and $RMSE$."
https://arxiv.org/abs/2403.08402,2024-03-13,The prescribed Ricci curvature problem on 5-dimensional nilpotent Lie groups,"['Marius Landry Foka', 'Romain Pefoukeu Nimpa', 'Salomon Joseph Mbatakou', 'Michel Bertrand Ngaha Djiadeu', 'Thomas Bouetou Bouetou']","In this paper, using the Milnor-type theorem technique, we provide on each nilpotent five dimensional Lie group, some global existence result of a pair (g, c) consisting of a left-invariant Riemannian metric g and a positive constant c such that Ric(g) =cT, where Ric(g) is the Ricci curvature of g and T a given left-invariant symmetric (0, 2)-tensor field."
https://arxiv.org/abs/2403.08401,2024-03-13,"CLASSY IX: The Chemical Evolution of the Ne, S, Cl, and Ar Elements","['Karla Z. Arellano-Córdova', 'Danielle A. Berg', 'Matilde Mingozzi', 'Bethan L. James', 'Noah S. J. Rogers', 'Evan D. Skillman', 'Fergus Cullen', 'Ryan Alexander', 'Ricardo O. Amorín', 'John Chisholm', 'Matthew Hayes', 'Timothy Heckman', 'Svean Hernandez', 'Nimisha Kumari', 'Claus Leitherer', 'Crystal L. Martin', 'Michael Maseda', 'Themiya Nanayakkara', 'Kaelee Parker', 'Swara Ravindranath', 'Alisson L. Strom', 'Fiorenzo Vincenzo', 'Aida Wofford']","To study the chemical evolution across cosmic epochs, we investigate Ne, S, Cl, and Ar abundance patterns in the COS Legacy Archive Spectroscopic SurveY (CLASSY). CLASSY comprises local star-forming galaxies (0.02 < z < 0.18) with enhanced star-formation rates, making them strong analogues to high-z star-forming galaxies. With direct measurements of electron temperature, we derive accurate ionic abundances for all elements and assess ionization correction factors (ICFs) to account for unseen ions and derive total abundances. We find Ne/O, S/O, Cl/O, and Ar/O exhibit constant trends with gas-phase metallicity for 12+log(O/H) < 8.5 but significant correlation for Ne/O and Ar/O with metallicity for 12+log(O/H) > 8.5, likely due to ICFs. Thus, applicability of the ICFs to integrated spectra of galaxies could bias results, underestimating true abundance ratios. Using CLASSY as a local reference, we assess the evolution of Ne/O, S/O, and Ar/O in galaxies at z>3, finding no cosmic evolution of Ne/O, while the lack of direct abundance determinations for S/O and Ar/O can bias the interpretation of the evolution of these elements. We determine the fundamental metallicity relationship (FMR) for CLASSY and compare to the high-redshift FMR, finding no evolution. Finally, we perform the first mass-neon relationship analysis across cosmic epochs, finding a slight evolution to high Ne at later epochs. The robust abundance patterns of CLASSY galaxies and their broad range of physical properties provide essential benchmarks for interpreting the chemical enrichment of the early galaxies observed with the JWST."
https://arxiv.org/abs/2403.08400,2024-03-13,Regular Friedmann Universes and Matter Transformations,"['Alexander Kamenshchik', 'Polina Petriakova']","We apply a very simple procedure to construct non-singular cosmological models for flat Friedmann universes filled with minimally coupled scalar fields or by tachyon Born-Infeld-type fields. Remarkably, for the minimally coupled scalar field and the tachyon field, the regularity of the cosmological evolution, or in other words, the existence of bounce, implies the necessity of the transition between scalar fields with standard kinetic terms to those with phantom ones. In both cases, the potentials in the vicinity of the point of the transition have a non-analyticity of the cusp form that is characterized by the same exponent and is equal to 2/3. If, in the tachyon models evolution, the pressure changes its sign, then another transformation of the Born-Infeld-type field occurs: the tachyon transforms into a pseudotachyon, and vice versa. We also undertake an analysis of the stability of the cosmological evolution in our models; we rely on the study of the speed of sound squared."
https://arxiv.org/abs/2403.08399,2024-03-13,System for systematic literature review using multiple AI agents: Concept and an empirical evaluation,"['Abdul Malik Sami', 'Zeeshan Rasheed', 'Kai-Kristian Kemell', 'Muhammad Waseem', 'Terhi Kilamo', 'Mika Saari', 'Anh Nguyen Duc', 'Kari Systä', 'Pekka Abrahamsson']","Systematic Literature Reviews (SLRs) have become the foundation of evidence-based studies, enabling researchers to identify, classify, and combine existing studies based on specific research questions. Conducting an SLR is largely a manual process. Over the previous years, researchers have made significant progress in automating certain phases of the SLR process, aiming to reduce the effort and time needed to carry out high-quality SLRs. However, there is still a lack of AI agent-based models that automate the entire SLR process. To this end, we introduce a novel multi-AI agent model designed to fully automate the process of conducting an SLR. By utilizing the capabilities of Large Language Models (LLMs), our proposed model streamlines the review process, enhancing efficiency and accuracy. The model operates through a user-friendly interface where researchers input their topic, and in response, the model generates a search string used to retrieve relevant academic papers. Subsequently, an inclusive and exclusive filtering process is applied, focusing on titles relevant to the specific research area. The model then autonomously summarizes the abstracts of these papers, retaining only those directly related to the field of study. In the final phase, the model conducts a thorough analysis of the selected papers concerning predefined research questions. We also evaluated the proposed model by sharing it with ten competent software engineering researchers for testing and analysis. The researchers expressed strong satisfaction with the proposed model and provided feedback for further improvement. The code for this project can be found on the GitHub repository at https://github.com/GPT-Laboratory/SLR-automation."
https://arxiv.org/abs/2403.08398,2024-03-13,Remote UGV Control via Practical Wireless Channels: A Model Predictive Control Approach,"['inghao Cao', 'Subhan Khan', 'Wanchun Liu', 'Yonghui Li', 'Branka Vucetic']","In addressing wireless networked control systems (WNCS) subject to unexpected packet loss and uncertainties, this paper presents a practical Model Predictive Control (MPC) based control scheme with considerations of of packet dropouts, latency, process noise and measurement noise. A discussion of the quasi-static Rayleigh fading channel is presented herein to enhance the realism of the underlying assumption in a real-world context. To achieve a desirable performance, the proposed control scheme leverages the predictive capabilities of a direct multiple shooting MPC, employs a compensation strategy to mitigate the impact of wireless channel imperfections. Instead of feeding noisy measurements into the MPC, we employ an Extended Kalman Filter (EKF) to mitigate the influence of measurement noise and process disturbances. Finally, we implement the proposed MPC algorithm on a simulated Unmanned Ground Vehicle (UGV) and conduct a series of experiments to evaluate the performance of our control scheme across various scenarios. Through our simulation results and comparative analyses, we have substantiated the effectiveness and improvements brought about by our approach through the utilization of multiple metrics."
https://arxiv.org/abs/2403.08397,2024-03-13,Interactive environments for training children's curiosity through the practice of metacognitive skills: a pilot study,"['Rania Abdelghani', 'Edith Law', 'Chloé Desvaux', 'Pierre-Yves Oudeyer', 'Hélène Sauzéon']","Curiosity-driven learning has shown significant positive effects on students' learning experiences and outcomes. But despite this importance, reports show that children lack this skill, especially in formal educational settings. To address this challenge, we propose an 8-session workshop that aims to enhance children's curiosity through training a set of specific metacognitive skills we hypothesize are involved in its process. Our workshop contains animated videos presenting declarative knowledge about curiosity and the said metacognitive skills as well as practice sessions to apply these skills during a reading-comprehension task, using a web platform designed for this study (e.g. expressing uncertainty, formulating questions, etc). We conduct a pilot study with 15 primary school students, aged between 8 and 10. Our first results show a positive impact on children's metacognitive efficiency and their ability to express their curiosity through question-asking behaviors."
https://arxiv.org/abs/2403.08396,2024-03-13,A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance,"['Bruno Pereira Cipriano', 'Pedro Alves', 'Paul Denny']","Much research has highlighted the impressive capabilities of large language models (LLMs), like GPT and Bard, for solving introductory programming exercises. Recent work has shown that LLMs can effectively solve a range of more complex object-oriented programming (OOP) exercises with text-based specifications. This raises concerns about academic integrity, as students might use these models to complete assignments unethically, neglecting the development of important skills such as program design, problem-solving, and computational thinking. To address this, we propose an innovative approach to formulating OOP tasks using diagrams and videos, as a way to foster problem-solving and deter students from a copy-and-prompt approach in OOP courses. We introduce a novel notation system for specifying OOP assignments, encompassing structural and behavioral requirements, and assess its use in a classroom setting over a semester. Student perceptions of this approach are explored through a survey (n=56). Generally, students responded positively to diagrams and videos, with video-based projects being better received than diagram-based exercises. This notation appears to have several benefits, with students investing more effort in understanding the diagrams and feeling more motivated to engage with the video-based projects. Furthermore, students reported being less inclined to rely on LLM-based code generation tools for these diagram and video-based exercises. Experiments with GPT-4 and Bard's vision abilities revealed that they currently fall short in interpreting these diagrams to generate accurate code solutions."
https://arxiv.org/abs/2403.08395,2024-03-13,Complementarity of which-path information in induced and stimulated coherences via four-wave mixing process from warm Rb atomic ensemble,"['Danbi Kim', 'Jiho Park', 'Changhoon Baek', 'Sun Kyung Lee and', 'Han Seb Moon']","We report a systematic approach for establishing a complementary relationship between the interference visibility, concurrence, and predictability in the crossing of induced and stimulated coherences of two-mode squeezed coherent states. This is achieved using a double-path interferometer involving two independent four-wave mixing (FWM) atomic samples generated via spontaneous and stimulated FWM processes from a warm atomic ensemble of 87Rb. We demonstrate that the transition from quantum to classical behavior can be characterized by the induced coherence effect, distinguishing between the two-mode squeezed vacuum and coherent states. Moreover, our experimental scheme, employing two FWM atomic ensembles with long-coherent photons, provides valuable insights into the complementarity of which-path information in induced and stimulated coherences."
https://arxiv.org/abs/2403.08394,2024-03-13,Worst-Case to Expander-Case Reductions: Derandomized and Generalized,"['Amir Abboud', 'Nathan Wallheimer']","A recent paper by Abboud and Wallheimer [ITCS 2023] presents self-reductions for various fundamental graph problems, that transform worst-case instances to expanders, thus proving that the complexity remains unchanged if the input is assumed to be an expander. An interesting corollary of their self-reductions is that, if some problem admit such reduction, then the popular algorithmic paradigm based on expander-decompositions is useless against it. In this paper, we improve their core gadget, which augments a graph to make it an expander while retaining its important structure. Our new core construction has the benefit of being simple to analyze and generalize, while obtaining the following results:"
https://arxiv.org/abs/2403.08393,2024-03-13,A classification of $\mathbb{F}_{p^k}$-braces using bilinear forms,"['Riccardo Aragona', 'Giuseppe Nozzi']","Let $\mathbb{F}_{p^k}$ be a finite field of odd characteristic $p$. In this paper we give a classification, up to isomorphism, of the associative commutative $\mathbb{F}_{p^k}$-algebras such that the resulting ring is radical, starting from the connection with their bi-brace structure. Such classification is the generalization in odd characteristic of the result proved by Civino at al. in characteristic $2$."
https://arxiv.org/abs/2403.08392,2024-03-13,Nonwoven Reinforced Photocurable Poly(glycerol seba-cate)-Based Hydrogels,"['Michael Phillips', 'Giuseppe Tronci', 'Christopher M. Pask', 'Stephen J. Russell']","Implantable hydrogels should ideally possess mechanical properties matched to the surrounding tissues to enable adequate mechanical function while regeneration occurs. This can be challenging, especially when degradable systems with high water content and hydrolysable chemical bonds are required in anatomical sites under constant mechanical stimulation, e.g. a foot ulcer cavity. In these circumstances, the design of hydrogel composites is a promising strategy to provide controlled structural features and macroscopic properties over time. To explore this strategy, the synthesis of a new photocurable elastomeric polymer, poly(glycerol-co-sebacic acid-co-lactic acid-co-polyethylene glycol) acrylate (PGSLPA), is investigated, along with its processing into UV-cured hydrogels, electrospun nonwovens and fibre-reinforced variants, without the need for a high temperature curing step or use of hazardous solvents. The mechanical properties of bioresorbable PGSLPA hydrogels were studied with and without electrospun nonwoven reinforcement and with varied layered configurations, aiming to determine the effects of microstructure on bulk compressive strength and elasticity. The nonwoven reinforced PGSLPA hydrogels exhibited a 60 % increase in compressive strength and an 80 % increase in elastic moduli compared to fibre-free PGSLPA samples. Mechanical properties of the fibre-reinforced hydrogels could also be modulated by altering the layering arrangement of the nonwoven and hydrogel phase. The nanofibre reinforced PGSLPA hydrogels also exhibited good elastic recovery, as evidenced by hysteresis in compression fatigue stress-strain evaluations showing a return to original dimensions."
https://arxiv.org/abs/2403.08391,2024-03-13,Misinformation is not about Bad Facts: An Analysis of the Production and Consumption of Fringe Content,"['JooYoung Lee', 'Emily Booth', 'Hany Farid', 'Marian-Andrei Rizoiu']","What if misinformation is not an information problem at all? Our findings suggest that online fringe ideologies spread through the use of content that is consensus-based and ""factually correct"". We found that Australian news publishers with both moderate and far-right political leanings contain comparable levels of information completeness and quality; and furthermore, that far-right Twitter users often share from moderate sources. However, a stark difference emerges when we consider two additional factors: 1) the narrow topic selection of articles by far-right users, suggesting that they cherrypick only news articles that engage with specific topics of their concern, and 2) the difference between moderate and far-right publishers when we examine the writing style of their articles. Furthermore, we can even identify users prone to sharing misinformation based on their communication style. These findings have important implications for countering online misinformation, as they highlight the powerful role that users' personal bias towards specific topics, and publishers' writing styles, have in amplifying fringe ideologies online."
https://arxiv.org/abs/2403.08390,2024-03-13,Sub-100-fs formation of dark excitons in monolayer WS$_2$,"['Pavel V. Kolesnichenko', 'Lukas Wittenbecher', 'Qianhui Zhang', 'Run Yan Teh', 'Chandni Babu', 'Michael S. Fuhrer', 'Anders Mikkelsen', 'Donatas Zigmantas']","Two-dimensional semiconductors based on transition metal dichalcogenides are promising for electronics and optoelectronics applications owing to their properties governed by strongly-bound bright and dark excitons. Momentum-forbidden dark excitons have recently received attention as better alternatives to bright excitons for long-range transport. However, accessing the dynamics of dark excitons is challenging experimentally. The most direct, but very complicated, experiment is transient angle-resolved photoemission electron spectroscopy: sub-100-fs formation of K-$Λ$-excitons in monolayer WS$_2$ has been identified previously taking advantage of momentum resolution of detected signals [1]. Here, we use a simpler setting of transient photoemission electron microscopy (with spatial resolution of 75 nm), which is inherently sensitive to dark K-$Λ$ excitons in monolayers of transition metal dichalcogenide and has exceptionally high temporal resolution of 13 fs. We are able to directly observe intervalley scattering (dark-exciton formation) in monolayer WS$_2$ with scattering rates in the range of 14-50 fs followed by picosecond-scale dynamics mediated by defects."
https://arxiv.org/abs/2403.08389,2024-03-13,Order parameters in quasi-1D spin systems,['Garry Goldstein'],"In this work we extend the idea of the meanfield. Meanfields approximately map - through some self consistency relation - a complex, usually manybody, problem to a simpler more readily solvable problem. Prototypical examples of simpler meanfield problem (meanfield systems) are the single site and free particle problems - which are solvable. Here we propose a new class of simple meanfield systems where the simple problem to be solved is a 1D spin chain. These meanfields are particularly useful for studying quasi-1D models. We illustrate this idea by considering meanfields for the Ising and ferromagnetic Heisenberg models with one direction coupled much more strongly then the other directions (quasi-1D systems) which map at meanfield level onto the 1D Ising and 1D ferromagnetic Heisenberg models. Magnetic phase transition temperatures and are obtained for both models."
https://arxiv.org/abs/2403.08388,2024-03-13,Feasibility of detecting shadows in disks induced by infall,"['A. Krieger', 'M. Kuffmeier', 'S. Reissl', 'C. P. Dullemond', 'C. Ginski', 'S. Wolf']","Observations performed with high-resolution imaging techniques revealed the existence of shadows in circumstellar disks that can be explained by the misalignment of an inner with respect to an outer disk. The cause of misalignment, however, is still debated. In this study, we investigate the feasibility of observing shadows induced by one prominent scenario that may lead to misalignment, which involves the late infall of material onto a protostellar system. In particular, we use previously performed hydrodynamical simulations of such events, and generate flux maps in the visible, near-infrared, submillimeter, and millimeter wavelength range using Monte Carlo radiative transfer. Based on that, we derive synthetic observations of these systems performed with the instruments SPHERE/VLT and ALMA, which we use as a basis for our subsequent analysis. We find that near-infrared observations with SPHERE are particularly well suited for detecting shadows via direct imaging alongside other features such as gaps, arcs, and streamers. On the contrary, performing a shadow detection based on reconstructed ALMA observations is very challenging due to the high sensitivity that is required for this task. Thus, in cases that allow for a detection, sophisticated analyses may be needed, for instance by the utilization of carefully constructed azimuthal profiles, aiding the search for potentially shallow shadows. Lastly, we conclude that late infall-induced disk misalignment offers a plausible explanation for the emergence of shadows that are observed in various systems."
https://arxiv.org/abs/2403.08387,2024-03-13,Radial perturbations of Ellis-Bronnikov wormholes in slow rotation up to second order,"['Bahareh Azad', 'Jose Luis Blázquez-Salcedo', 'Fech Scen Khoo', 'Jutta Kunz']","We consider slowly rotating Ellis-Bronnikov wormholes and investigate their radial perturbations ($\mathrm{l}=0$), expanding up to second order in rotation. We present the detailed derivations in the general case, including symmetric and non-symmetric wormholes. The calculations show that the unstable mode present in the static case becomes less unstable with increasing rotation, until it reaches zero and then disappears. This indicates that wormhole solutions may become linearly mode stable at sufficiently fast rotation."
https://arxiv.org/abs/2403.08386,2024-03-13,Optimizing Risk-averse Human-AI Hybrid Teams,"['Andrew Fuchs', 'Andrea Passarella', 'Marco Conti']","We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team. The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread. However, their behavior is not error-free, making hybrid teams a very suitable solution. As such, we consider methods for improving performance for these teams of humans and AI systems. For hybrid teams, we will refer to both the humans and AI systems as agents. To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard Reinforcement Learning scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents. We further guide the manager's learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior. We demonstrate the optimality of our manager's performance in several grid environments which include failure states which terminate an episode and should be avoided. We perform our experiments with teams of agents with varying degrees of acceptable risk, in the form of proximity to a failure state, and measure the manager's ability to make effective delegation decisions with respect to its own risk-based constraints, then compare these to the optimal decisions. Our results show our manager can successfully learn desirable delegations which result in team paths near/exactly optimal with respect to path length and number of delegations."
https://arxiv.org/abs/2403.08385,2024-03-13,Unraveling many-body effects in ZnO: Combined study using momentum-resolved electron energy-loss spectroscopy and first-principles calculations,"['Dario A. Leon', 'Cana Elgvin', 'Phuong Dan Nguyen', 'Øystein Prytz', 'Fredrik S. Hage', 'Kristian Berland']","We present a detailed study of the dielectric response of ZnO using a combination of low-loss momentum-resolved electron energy-loss spectroscopy (EELS) and first-principles calculations at several levels of theory, from the independent particle and the random phase approximation with different variants of density functional theory (DFT), including hybrid and DFT$+U$ schemes; to the Bethe-Salpeter equation (BSE). We use a method based on the $f$-sum rule to obtain the momentum-resolved experimental loss function and absorption spectra from EELS measurements. We characterize the main features in the direct and inverse dielectric functions of ZnO and their dispersion, associating them to single-particle features in the electronic band structure, while highlighting the important role of many-body effects such as plasmons and excitons. We discuss different signatures of the high anisotropy in the response function of ZnO, including the symmetry of the excitonic wave-functions."
https://arxiv.org/abs/2403.08384,2024-03-13,AADNet: Attention aware Demoiréing Network,"['M Rakesh Reddy', 'Shubham Mandloi', 'Aman Kumar']","Moire pattern frequently appears in photographs captured with mobile devices and digital cameras, potentially degrading image quality. Despite recent advancements in computer vision, image demoire'ing remains a challenging task due to the dynamic textures and variations in colour, shape, and frequency of moire patterns. Most existing methods struggle to generalize to unseen datasets, limiting their effectiveness in removing moire patterns from real-world scenarios. In this paper, we propose a novel lightweight architecture, AADNet (Attention Aware Demoireing Network), for high-resolution image demoire'ing that effectively works across different frequency bands and generalizes well to unseen datasets. Extensive experiments conducted on the UHDM dataset validate the effectiveness of our approach, resulting in high-fidelity images."
https://arxiv.org/abs/2403.08383,2024-03-13,"RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion Attack in Federated Learning","['Can Liu', 'Jin Wang', 'Dongyang Yu']","Federated learning (FL) empowers privacy-preservation in model training by only exposing users' model gradients. Yet, FL users are susceptible to the gradient inversion (GI) attack which can reconstruct ground-truth training data such as images based on model gradients. However, reconstructing high-resolution images by existing GI attack works faces two challenges: inferior accuracy and slow-convergence, especially when the context is complicated, e.g., the training batch size is much greater than 1 on each FL user. To address these challenges, we present a Robust, Accurate and Fast-convergent GI attack algorithm, called RAF-GI, with two components: 1) Additional Convolution Block (ACB) which can restore labels with up to 20% improvement compared with existing works; 2) Total variance, three-channel mEan and cAnny edge detection regularization term (TEA), which is a white-box attack strategy to reconstruct images based on labels inferred by ACB. Moreover, RAF-GI is robust that can still accurately reconstruct ground-truth data when the users' training batch size is no more than 48. Our experimental results manifest that RAF-GI can diminish 94% time costs while achieving superb inversion quality in ImageNet dataset. Notably, with a batch size of 1, RAF-GI exhibits a 7.89 higher Peak Signal-to-Noise Ratio (PSNR) compared to the state-of-the-art baselines."
https://arxiv.org/abs/2403.08382,2024-03-13,Chiral spin state and nematic ferromagnet in the spin-1 Kitaev-$Γ$ model,"['Qiang Luo', 'Jize Zhao', 'Jinbin Li', 'Xiaoqun Wang']","The higher-spin Kitaev magnets, in which the Kitaev interaction and off-diagonal exchange couplings are overwhelmingly large, have emerged as a fertile avenue to explore exotic phases and unusual excitations. In this work, we study the quantum phase diagram of the spin-1 Kitaev-$Γ$ model on the honeycomb lattice using density-matrix renormalization group. It harbours six distinct phases and the intriguing findings are three magnetically ordered phases in which both time-reversal symmetry and lattice symmetry albeit of different sort are broken spontaneously. The chiral spin state originates from the order-by-disorder effect and exhibits an almost saturated scalar spin chirality at the quantum level. Depending on the relative strength of the two interactions, it also features columnar or plaquette valence-bond-solid-like pattern as a consequence of the translational symmetry breaking. In parallel, the nematic ferromagnets are situated at ferromagnetic Kitaev side and possess small but finite ferromagnetic ordering. The lattice-rotational symmetry breaking enforces nonequivalent bond energy along one of the three bonds. Although the intrinsic difference between the two nematic ferromagnets remains elusive, the discontinuities in the von Neumann entropy, hexagonal plaquette operator, and Wilson loop operator convincingly suggest that they are separated via a first-order phase transition."
https://arxiv.org/abs/2403.08381,2024-03-13,Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models,"['Pengze Zhang', 'Hubery Yin', 'Chen Li', 'Xiaohua Xie']","Most diffusion models assume that the reverse process adheres to a Gaussian distribution. However, this approximation has not been rigorously validated, especially at singularities, where t=0 and t=1. Improperly dealing with such singularities leads to an average brightness issue in applications, and limits the generation of images with extreme brightness or darkness. We primarily focus on tackling singularities from both theoretical and practical perspectives. Initially, we establish the error bounds for the reverse process approximation, and showcase its Gaussian characteristics at singularity time steps. Based on this theoretical insight, we confirm the singularity at t=1 is conditionally removable while it at t=0 is an inherent property. Upon these significant conclusions, we propose a novel plug-and-play method SingDiffusion to address the initial singular time step sampling, which not only effectively resolves the average brightness issue for a wide range of diffusion models without extra training efforts, but also enhances their generation capability in achieving notable lower FID scores. Code and models are released at https://github.com/PangzeCheung/SingDiffusion."
https://arxiv.org/abs/2403.08380,2024-03-13,Mitigate Target-level Insensitivity of Infrared Small Target Detection via Posterior Distribution Modeling,"['Haoqing Li', 'Jinfu Yang', 'Yifei Xu', 'Runshi Wang']","Infrared Small Target Detection (IRSTD) aims to segment small targets from infrared clutter background. Existing methods mainly focus on discriminative approaches, i.e., a pixel-level front-background binary segmentation. Since infrared small targets are small and low signal-to-clutter ratio, empirical risk has few disturbances when a certain false alarm and missed detection exist, which seriously affect the further improvement of such methods. Motivated by the dense prediction generative methods, in this paper, we propose a diffusion model framework for Infrared Small Target Detection which compensates pixel-level discriminant with mask posterior distribution modeling. Furthermore, we design a Low-frequency Isolation in the wavelet domain to suppress the interference of intrinsic infrared noise on the diffusion noise estimation. This transition from the discriminative paradigm to generative one enables us to bypass the target-level insensitivity. Experiments show that the proposed method achieves competitive performance gains over state-of-the-art methods on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets. Code are available at https://github.com/Li-Haoqing/IRSTD-Diff."
https://arxiv.org/abs/2403.08379,2024-03-13,Observational tests in scale invariance II: gravitational lensing,"['Andre Maeder', 'Frederic Courbin']","We study the path of light rays passing near a massive object, in the context of the scale invariant equation of the geodesics first obtained by Dirac (1973). Using the exterior Schwarzschild solution for the metric, we derive the complete equations of the geodesics in the scale invariant context. We find that scale invariance introduces two additional terms to the Einstein term producing the deflection angle and that can potentially act over cosmological distances. Numerical integration of the scale-invariant geodesics, for the specific case of the z_L=1.94 lens galaxy in the extreme system JWST-ER1 (van Dokkum et al. 2023) shows that the two additional terms introduce only negligible effects, typically 1E-06 of the Einstein term. We conclude that the lensing deflection angle derived in Einstein's General Relativity is essentially independent of the scale invariant effects and that the photon's geodesics remain unchanged. We also explore the possible origin of the differences in the mass estimates from lensing and photometry in JWST-ER1 and in the SLACS galaxies, differences which appear larger at higher redshifts."
https://arxiv.org/abs/2403.08378,2024-03-13,A Generalized Framework with Adaptive Weighted Soft-Margin for Imbalanced SVM Classification,"['Lu Jiang', 'Qi Wang', 'Yuhang Chang', 'Jianing Song', 'Haoyue Fu']","Category imbalance is one of the most popular and important issues in the domain of classification. In this paper, we present a new generalized framework with Adaptive Weight function for soft-margin Weighted SVM (AW-WSVM), which aims to enhance the issue of imbalance and outlier sensitivity in standard support vector machine (SVM) for classifying two-class data. The weight coefficient is introduced into the unconstrained soft-margin support vector machines, and the sample weights are updated before each training. The Adaptive Weight function (AW function) is constructed from the distance between the samples and the decision hyperplane, assigning different weights to each sample. A weight update method is proposed, taking into account the proximity of the support vectors to the decision hyperplane. Before training, the weights of the corresponding samples are initialized according to different categories. Subsequently, the samples close to the decision hyperplane are identified and assigned more weights. At the same time, lower weights are assigned to samples that are far from the decision hyperplane. Furthermore, we also put forward an effective way to eliminate noise. To evaluate the strength of the proposed generalized framework, we conducted experiments on standard datasets and emotion classification datasets with different imbalanced ratios (IR). The experimental results prove that the proposed generalized framework outperforms in terms of accuracy, recall metrics and G-mean, validating the effectiveness of the weighted strategy provided in this paper in enhancing support vector machines."
https://arxiv.org/abs/2403.08377,2024-03-13,Learning to Describe for Predicting Zero-shot Drug-Drug Interactions,"['Fangqi Zhu', 'Yongqi Zhang', 'Lei Chen', 'Bing Qin', 'Ruifeng Xu']","Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of concurrent drug administration, posing a significant challenge in healthcare. As the development of new drugs continues, the potential for unknown adverse effects resulting from DDIs becomes a growing concern. Traditional computational methods for DDI prediction may fail to capture interactions for new drugs due to the lack of knowledge. In this paper, we introduce a new problem setup as zero-shot DDI prediction that deals with the case of new drugs. Leveraging textual information from online databases like DrugBank and PubChem, we propose an innovative approach TextDDI with a language model-based DDI predictor and a reinforcement learning~(RL)-based information selector, enabling the selection of concise and pertinent text for accurate DDI prediction on new drugs. Empirical results show the benefits of the proposed approach on several settings including zero-shot and few-shot DDI prediction, and the selected texts are semantically relevant. Our code and data are available at \url{https://github.com/zhufq00/DDIs-Prediction}."
https://arxiv.org/abs/2403.08376,2024-03-13,Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy,"['Eleni D. Koronaki', 'Luise F. Kaven', 'Johannes M. M. Faust', 'Ioannis G. Kevrekidis', 'Alexander Mitsos']","Polymer particle size constitutes a crucial characteristic of product quality in polymerization. Raman spectroscopy is an established and reliable process analytical technology for in-line concentration monitoring. Recent approaches and some theoretical considerations show a correlation between Raman signals and particle sizes but do not determine polymer size from Raman spectroscopic measurements accurately and reliably. With this in mind, we propose three alternative machine learning workflows to perform this task, all involving diffusion maps, a nonlinear manifold learning technique for dimensionality reduction: (i) directly from diffusion maps, (ii) alternating diffusion maps, and (iii) conformal autoencoder neural networks. We apply the workflows to a data set of Raman spectra with associated size measured via dynamic light scattering of 47 microgel (cross-linked polymer) samples in a diameter range of 208nm to 483 nm. The conformal autoencoders substantially outperform state-of-the-art methods and results for the first time in a promising prediction of polymer size from Raman spectra."
https://arxiv.org/abs/2403.08375,2024-03-13,Translating between SQL Dialects for Cloud Migration,"['Ran Zmigrod', 'Salwa Alamir', 'Xiaomo Liu']","Migrations of systems from on-site premises to the cloud has been a fundamental endeavor by many industrial institutions. A crucial component of such cloud migrations is the transition of databases to be hosted online. In this work, we consider the difficulties of this migration for SQL databases. While SQL is one of the prominent methods for storing database procedures, there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.) which can complicate migrations when the on-premise SQL dialect differs to the dialect hosted on the cloud. Tools exist by common cloud provides such as AWS and Azure to aid in translating between dialects in order to mitigate the majority of the difficulties. However, these tools do not successfully translate $100\%$ of the code. Consequently, software engineers must manually convert the remainder of the untranslated database. For large organizations, this task quickly becomes intractable and so more innovative solutions are required. We consider this challenge a novel yet vital industrial research problem for any large corporation that is considering cloud migrations. Furthermore, we introduce potential avenues of research to tackle this challenge that have yielded promising preliminary results."
https://arxiv.org/abs/2403.08374,2024-03-13,Error-Free Near-Optimal Validated Agreement,"['Pierre Civit', 'Muhammad Ayaz Dzulfikar', 'Seth Gilbert', 'Rachid Guerraoui', 'Jovan Komatovic', 'Manuel Vidigueira', 'Igor Zablotchi']","Byzantine agreement enables n processes to agree on a common L-bit value, despite t > 0 arbitrary failures. A long line of work has been dedicated to improving the worst-case bit complexity of Byzantine agreement in synchrony. This has culminated in COOL, an error-free (deterministically secure against a computationally unbounded adversary) algorithm that achieves a near-optimal bit complexity of O(nL + n^2 log n). COOL satisfies strong validity: if all correct processes propose the same value, only that value can be decided. Thus, whenever correct processes do not a priori agree, COOL might decide on ""bottom"", thus limiting its application in today's state machine replication (SMR) and blockchain protocols. In this work, we focus on the aforementioned limitation. Can we design an error-free near-optimal Byzantine agreement algorithm applicable in today's SMR and blockchain protocols? Can we design an error-free near-optimal agreement algorithm with external validity (a.k.a. validated agreement) stipulating that only values valid according to a predetermined predicate can be decided?"
https://arxiv.org/abs/2403.08373,2024-03-13,Towards the superlubricity of polymer-steel interfaces with ionic liquids and carbon nanotubes,"['L. Wojciechowski', 'K. J. Kubiak', 'S. Boncel', 'A. Marek', 'B. Gapinski', 'T. Runka', 'R. Jedrysiak', 'S. Ruczka', 'P. Blaszkiewicz', 'T. G. Mathia']","Frictional losses are responsible for significant energy waste in many practical applications, and superlubricity with a coefficient of friction lower than 0.01 is the goal of tribologists. In this paper, metal-on-polymer contact was analysed and close to superlubricity conditions for this material configuration were explored. A new lubricant has been proposed hinge on the phosphorus-based ionic liquid and carbon nanotubes as thickeners. Additionally, carbon nanotube mesh was doped with copper nanoparticles that allowed for the close to superlubricity state in a mild steel/polymer contact configuration under low normal load conditions. The adsorption of phosphorus onto metallic and polymer surfaces has been reported in EDS analysis. The formulation of the new lubricant allowed for stable dispersion with a carbon nanotube content as low as 0.1% wt. The carbon nanotubes and Cu nanoparticles have been analysed using TEM and SEM imaging. A tribological test in a block-on-ring system has been carried out. The wear of material, topography, and surface free energy have been analysed along with SEM/EDS images to explore the underlying mechanisms of friction and wear."
https://arxiv.org/abs/2403.08372,2024-03-13,Negative Impact of Online Political Incivility on Willingness to See Political Comments,['Kohei Nishi'],"Recently, there has been significant attention on online political incivility. While previous research suggests that uncivil political comments lead people to be less willing to see more comments on the same issue, two critical questions have received limited exploration: (1) Are people exposed to uncivil political comments less willing to see other comments from the person who posted the uncivil comment?; (2) Are people exposed to uncivil political comments less willing to see comments from people who have different thoughts than them? To address these questions, the present study conducted a preregistered online survey experiment targeting Japanese citizens, focusing on the pro- vs anti-Kishida cabinet conflict in Japan. The results show that the participants were less willing to see other comments by the person who posted the comment when the comment was uncivil than when it was civil. In addition, the anti-Kishida participants were less willing to see political opinions posted online by people who have different thoughts than them when the comment was uncivil than when it was civil, while the participants in the other subgroups did not show a similar tendency. These findings suggest that uncivil expressions in online political communication might prompt people to avoid reading opinions from those who have different thoughts than them, which might promote political echo chambers."
https://arxiv.org/abs/2403.08371,2024-03-13,User-Centric Beam Selection and Precoding Design for Coordinated Multiple-Satellite Systems,"['Vu Nguyen Ha', 'Duy H. N. Nguyen', 'Juan C. -M. Duncan', 'Jorge L. Gonzalez-Rios', 'Juan A. Vasquez', 'Geoffrey Eappen', 'Luis M. Garces-Socarras', 'Rakesh Palisetty', 'Symeon Chatzinotas', 'Bjorn Ottersten']","This paper introduces a joint optimization framework for user-centric beam selection and linear precoding (LP) design in a coordinated multiple-satellite (CoMSat) system, employing a Digital-Fourier-Transform-based (DFT) beamforming (BF) technique. Regarding serving users at their target SINRs and minimizing the total transmit power, the scheme aims to efficiently determine satellites for users to associate with and activate the best cluster of beams together with optimizing LP for every satellite-to-user transmission. These technical objectives are first framed as a complex mixed-integer programming (MIP) challenge. To tackle this, we reformulate it into a joint cluster association and LP design problem. Then, by theoretically analyzing the duality relationship between downlink and uplink transmissions, we develop an efficient iterative method to identify the optimal solution. Additionally, a simpler duality approach for rapid beam selection and LP design is presented for comparison purposes. Simulation results underscore the effectiveness of our proposed schemes across various settings."
https://arxiv.org/abs/2403.08370,2024-03-13,SMART: Submodular Data Mixture Strategy for Instruction Tuning,"['H S V N S Kowndinya Renduchintala', 'Sumit Bhatia', 'Ganesh Ramakrishnan']","Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinya-renduchintala/SMART."
https://arxiv.org/abs/2403.08369,2024-03-13,Inhomogeneous Floquet thermalization,"['Soumya Bera', 'Ishita Modak', 'Roderich Moessner']","How a closed system thermalizes, especially in the absence of global conservation laws but in the presence of disorder and interactions, is one of the central questions in non-equilibrium statistical mechanics. We explore this for a disordered, periodically driven Ising chain. Our numerical results reveal inhomogeneous thermalization leading to a distribution of thermalization timescales within a single disordered sample, which we encode via a distribution of effective local temperatures. Using this, we find an excellent collapse $\textit{without}$ $\textit{any}$ $\textit{fitting}$ $\textit{parameters}$ of the local relaxation dynamics for the entire range of disorder values in the ergodic regime when adapting the disorder-averaged diagonal entanglement entropy as internal `time' of the system. This approach evidences a remarkably uniform parametrization of the dynamical many-body evolution of local temperature within the otherwise highly heterogeneous ergodic regime, independent of the strength of the disorder."
https://arxiv.org/abs/2403.08368,2024-03-13,METER: a mobile vision transformer architecture for monocular depth estimation,"['L. Papa', 'P. Russo', 'I. Amerini']","Depth estimation is a fundamental knowledge for autonomous systems that need to assess their own state and perceive the surrounding environment. Deep learning algorithms for depth estimation have gained significant interest in recent years, owing to the potential benefits of this methodology in overcoming the limitations of active depth sensing systems. Moreover, due to the low cost and size of monocular cameras, researchers have focused their attention on monocular depth estimation (MDE), which consists in estimating a dense depth map from a single RGB video frame. State of the art MDE models typically rely on vision transformers (ViT) architectures that are highly deep and complex, making them unsuitable for fast inference on devices with hardware constraints. Purposely, in this paper, we address the problem of exploiting ViT in MDE on embedded devices. Those systems are usually characterized by limited memory capabilities and low-power CPU/GPU. We propose METER, a novel lightweight vision transformer architecture capable of achieving state of the art estimations and low latency inference performances on the considered embedded hardwares: NVIDIA Jetson TX1 and NVIDIA Jetson Nano. We provide a solution consisting of three alternative configurations of METER, a novel loss function to balance pixel estimation and reconstruction of image details, and a new data augmentation strategy to improve the overall final predictions. The proposed method outperforms previous lightweight works over the two benchmark datasets: the indoor NYU Depth v2 and the outdoor KITTI."
https://arxiv.org/abs/2403.08367,2024-03-13,Lubin-Tate generalizations of the p-adic Fourier transform,['Laurent Berger'],"Fresnel and de Mathan proved that the p-adic Fourier transform is surjective. We reinterpret their result in terms of analytic boundaries, and extend it beyond the cyclotomic case. We also give some applications of their result to Schneider and Teitelbaum's p-adic Fourier theory, in particular to generalized Mahler expansions and to the geometry of the character variety."
https://arxiv.org/abs/2403.08366,2024-03-13,Amplified linear and nonlinear chiral sensing assisted by anapole modes in hybrid metasurfaces,"['Guillermo Serrera', 'Javier González-Colsa', 'Pablo Albella']","The interaction between chiral molecules and circularly polarized light is largely influenced by the local optical chirality density. This interaction prompts a substantial demand of the design of nanophotonic platforms capable of enhancing such effects across large and accessible volumes. Such a magnification requires nanostructures to provide high electric and magnetic field enhancements while keeping the phase relation of circular light. Dielectric nanostructures, particularly those able to support resonances, are uniquely suited for this task due to their capacity for high electric and magnetic field enhancements. On the other hand, efficient third harmonic generation calls for strong electric field resonances within dielectric materials, a feature often boosted by incorporating plasmonic materials into hybrid systems. In this numerical study, we propose a coupled silicon disk-gold ring system that can exploit the anapole-induced field confinement to provide a broadband magnified circular dichroism under realistic conditions, reaching values up to a 5-fold enhancement. We also demonstrate that this structure can be employed as an efficient third harmonic generator which, when integrated with chiral media, enables a 10-fold enhancement in circular dichroism. Furthermore, we numerically show that pulsed illumination at intensities up to 10 GW/cm2 does not induce temperature increments that could potentially damage the samples. These findings suggest that this system can be a promising and versatile approach towards ultrasensitive background-free chiral sensing."
https://arxiv.org/abs/2403.08365,2024-03-13,APACE: Agile and Perception-Aware Trajectory Generation for Quadrotor Flights,"['Xinyi Chen', 'Yichen Zhang', 'Boyu Zhou', 'Shaojie Shen']","Various perception-aware planning approaches have attempted to enhance the state estimation accuracy during maneuvers, while the feature matchability among frames, a crucial factor influencing estimation accuracy, has often been overlooked. In this paper, we present APACE, an Agile and Perception-Aware trajeCtory gEneration framework for quadrotors aggressive flight, that takes into account feature matchability during trajectory planning. We seek to generate a perception-aware trajectory that reduces the error of visual-based estimator while satisfying the constraints on smoothness, safety, agility and the quadrotor dynamics. The perception objective is achieved by maximizing the number of covisible features while ensuring small enough parallax angles. Additionally, we propose a differentiable and accurate visibility model that allows decomposition of the trajectory planning problem for efficient optimization resolution. Through validations conducted in both a photorealistic simulator and real-world experiments, we demonstrate that the trajectories generated by our method significantly improve state estimation accuracy, with root mean square error (RMSE) reduced by up to an order of magnitude. The source code will be released to benefit the community."
https://arxiv.org/abs/2403.08364,2024-03-13,Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics,"['Zhuoxin Chen', 'Zhenyu Wu', 'Yang Ji']","Federated learning is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions. This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance. To address this issue, we propose a two-stage Decoupled Federated learning framework using Feature Statistics (DFL-FS). In the first stage, the server estimates the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage. In the second stage, DFL-FS employs federated feature regeneration based on global feature statistics and utilizes resampling and weighted covariance to calibrate the global classifier to enhance the model's adaptability to long-tailed data distributions. We conducted experiments on CIFAR10-LT and CIFAR100-LT datasets with various long-tailed rates. The results demonstrate that our method outperforms state-of-the-art methods in both accuracy and convergence rate."
https://arxiv.org/abs/2403.08363,2024-03-13,ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment,"['Karthikeya Puttur Venkatraj', 'Wo Meijer', 'Monica Perusquía-Hernández', 'Gijs Huisman', 'Abdallah El Ali']","Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR). During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms. Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants. In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks (Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence, body ownership, and motion synchrony. We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players hand motions synchronized more in the targeted task. We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences."
https://arxiv.org/abs/2403.08362,2024-03-13,Mean-Field Microcanonical Gradient Descent,"['Marcus Häggbom', 'Morten Karlsmark', 'Joakim andén']","Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data."
https://arxiv.org/abs/2403.08361,2024-03-13,Search for cosmic-ray boosted sub-MeV dark matter-electron scatterings in PandaX-4T,"['Xiaofeng Shang', 'Abdusalam Abdukerim', 'Zihao Bo', 'Wei Chen', 'Xun Chen', 'Chen Cheng', 'Zhaokan Cheng', 'Xiangyi Cui', 'Yingjie Fan', 'Deqing Fang', 'Lisheng Geng', 'Karl Giboni', 'Xuyuan Guo', 'Chencheng Han', 'Ke Han', 'Changda He', 'Jinrong He', 'Di Huang', 'Junting Huang', 'Zhou Huang', 'Ruquan Hou', 'Yu Hou', 'Xiangdong Ji', 'Yonglin Ju', 'Chenxiang Li']","We report the first search for the elastic scatterings between cosmic-ray boosted sub-MeV dark matter and electrons in the PandaX-4T liquid xenon experiment. Sub-MeV dark matter particles can be accelerated by scattering with electrons in the cosmic rays and produce detectable electron recoil signals in the detector. Using the commissioning data from PandaX-4T of 0.63~tonne$\cdot$year exposure, we set new constraints on DM-electron scattering cross sections for DM masses ranging from 10~eV/$c^2$ to 3~keV/$c^2$."
https://arxiv.org/abs/2403.08360,2024-03-13,Improved Image-based Pose Regressor Models for Underwater Environments,"['Luyuan Peng', 'Hari Vishnu', 'Mandar Chitre', 'Yuen Min Too', 'Bharath Kalyan', 'Rajat Mishra']","We investigate the performance of image-based pose regressor models in underwater environments for relocalization. Leveraging PoseNet and PoseLSTM, we regress a 6-degree-of-freedom pose from single RGB images with high accuracy. Additionally, we explore data augmentation with stereo camera images to improve model accuracy. Experimental results demonstrate that the models achieve high accuracy in both simulated and clear waters, promising effective real-world underwater navigation and inspection applications."
https://arxiv.org/abs/2403.08359,2024-03-13,Assessment of background noise properties in time and time-frequency domains in the context of vibration-based local damage detection in real environment,"['Katarzyna Skowronek', 'Tomasz Barszcz', 'Jerome Antoni', 'Radosław Zimroz', 'Agnieszka Wyłomańska']","Any measurement in condition monitoring applications is associated with disturbing noise. Till now, most of the diagnostic procedures have assumed the Gaussian distribution for the noise. This paper shares a novel perspective to the problem of local damage detection. The acquired vector of observations is considered as an additive mixture of signal of interest (SOI) and noise with strongly non-Gaussian, heavy-tailed properties, that masks the SOI. The distribution properties of the background noise influence the selection of tools used for the signal analysis, particularly for local damage detection. Thus, it is extremely important to recognize and identify possible non-Gaussian behavior of the noise. The problem considered here is more general than the classical goodness-of-fit testing. The paper highlights the important role of variance, as most of the methods for signal analysis are based on the assumption of the finite-variance distribution of the underlying signal. The finite variance assumption is crucial but implicit to most indicators used in condition monitoring, (such as the root-mean-square value, the power spectral density, the kurtosis, the spectral correlation, etc.), in view that infinite variance implies moments higher than 2 are also infinite. The problem is demonstrated based on three popular types of non-Gaussian distributions observed for real vibration signals. We demonstrate how the properties of noise distribution in the time domain may change by its transformations to the time-frequency domain (spectrogram). Additionally, we propose a procedure to check the presence of the infinite-variance of the background noise. Our investigations are illustrated using simulation studies and real vibration signals from various machines."
https://arxiv.org/abs/2403.08358,2024-03-13,Log Summarisation for Defect Evolution Analysis,"['Rares Dolga', 'Ran Zmigrod', 'Rui Silva', 'Salwa Alamir', 'Sameena Shah']","Log analysis and monitoring are essential aspects in software maintenance and identifying defects. In particular, the temporal nature and vast size of log data leads to an interesting and important research question: How can logs be summarised and monitored over time? While this has been a fundamental topic of research in the software engineering community, work has typically focused on heuristic-, syntax-, or static-based methods. In this work, we suggest an online semantic-based clustering approach to error logs that dynamically updates the log clusters to enable monitoring code error life-cycles. We also introduce a novel metric to evaluate the performance of temporal log clusters. We test our system and evaluation metric with an industrial dataset and find that our solution outperforms similar systems. We hope that our work encourages further temporal exploration in defect datasets."
https://arxiv.org/abs/2403.08357,2024-03-13,Geometric and electronic properties of two kinds of CrO2 magnetic monolayers: D3d and D2h phases,"['Yang Zhang', 'Xianggong Bo', 'Jimeng Jing', 'Lixia Wang', 'Shiqian Qiao', 'Hong Wu', 'Yong Pu', 'Feng Li']","Due to the high magnetic coupling strength between the Cr elements, the bulk phase CrO2 is one of several ferromagnetic oxides known to have the highest Curie temperature. When the dimensionality of the material is reduced from 3D to 2D, the 2D CrO2 system material is expected to maintain a high Curie temperature. In this work, we predict two new phases of CrO2 monolayer (D3d and D2h) by using first-principles calculations. We have found that the Curie temperature of 2D CrO2 is much lower than that of its bulk phase, but still remains as high as 191K, which is comparable to that of Fe2Cr2Ge6. In addition, 1L D3d-CrO2 is in the ferromagnetic state, while 1L D2h-CrO2 is in the antiferromagnetic state. Also, the different geometric structure affects its electrical properties: the 1L D3d-CrO2 is a half-metal while 1L D2h-CrO2 is a semiconductor. Our studies have shown that there is a wealth of electrical and magnetic properties in CrO2."
https://arxiv.org/abs/2403.08356,2024-03-13,Parameter Constraints on Traversable Wormholes within Beyond Horndeski Theories through Quasi-Periodic Oscillations,"['Farukh Abdulkhamidov', 'Petya Nedkova', 'Javlon Rayibaev', 'Jutta Kunz', 'Bobomurat Ahmedov']","{\it Hunting} compact astrophysical objects such as black holes and wormholes, as well as testing gravity theories, are important issues in relativistic astrophysics. In this sense, theoretical and observational studies of quasiperiodic oscillations (QPOs) observed in (micro)quasars become helpful in exploring their central object, which can be a black hole or a wormhole. In the present work, we study the throat properties of traversable wormholes beyond Horndeski theory. Also, we investigate the circular motion of test particles orbiting the wormhole. We analyze the test particle's effective potential and angular momentum for circular orbits. Frequencies of radial and vertical oscillations of the particles around stable circular orbits have also been studied and applied in explaining the quasiperiodic oscillations mechanism in the relativistic precession (RP) model. Finally, we obtain constraint values for the parameters of Horndeski gravity and the mass of the wormhole candidates using QPOs observed in the microquasars GRO J1655-40, GRS 1915+105 \& XTE J1550-564 and at the center of Milky Way galaxy through Monte-Carlo-Markovian-Chain (MCMC) analyses."
https://arxiv.org/abs/2403.08355,2024-03-13,NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation,"['Ran Xu', 'Yan Shen', 'Xiaoqi Li', 'Ruihai Wu', 'Hao Dong']","Enabling home-assistant robots to perceive and manipulate a diverse range of 3D objects based on human language instructions is a pivotal challenge. Prior research has predominantly focused on simplistic and task-oriented instructions, i.e., ""Slide the top drawer open"". However, many real-world tasks demand intricate multi-step reasoning, and without human instructions, these will become extremely difficult for robot manipulation. To address these challenges, we introduce a comprehensive benchmark, NrVLM, comprising 15 distinct manipulation tasks, containing over 4500 episodes meticulously annotated with fine-grained language instructions. We split the long-term task process into several steps, with each step having a natural language instruction. Moreover, we propose a novel learning framework that completes the manipulation task step-by-step according to the fine-grained instructions. Specifically, we first identify the instruction to execute, taking into account visual observations and the end-effector's current state. Subsequently, our approach facilitates explicit learning through action-prompts and perception-prompts to promote manipulation-aware cross-modality alignment. Leveraging both visual observations and linguistic guidance, our model outputs a sequence of actionable predictions for manipulation, including contact points and end-effector poses. We evaluate our method and baselines using the proposed benchmark NrVLM. The experimental results demonstrate the effectiveness of our approach. For additional details, please refer to https://sites.google.com/view/naturalvlm."
https://arxiv.org/abs/2403.08354,2024-03-13,Centrality of star and monotone factorisations,"['Jesse Campion Loth', 'Amarpreet Rattan']","A factorisation problem in the symmetric group is central if any two permutations in the same conjugacy class have the same number of factorisations. We give the first fully combinatorial proof of the centrality of transitive star factorisations that is valid in all genera, which answers a natural question of Goulden and Jackson from 2009. Our proof is through a bijection to a class of monotone double Hurwitz factorisations. These latter factorisations are also not obviously central, so we also give a combinatorial proof of their centrality. As a corollary we obtain new formulae for some monotone double Hurwitz numbers, and a new relation between Hurwitz and monotone Hurwitz numbers. We also generalise a theorem of Goulden and Jackson from 2009 that states that the transitive power of Jucys-Murphy elements are central. Our theorem states that the transitive image of any symmetric function evaluated at Jucys-Murphy elements is central, which gives a transitive version of Jucys' original result from 1974."
https://arxiv.org/abs/2403.08353,2024-03-13,A continuous beam monochromator for matter waves,"['Johannes Fiedler', 'Bodil Holst']","Atom and, of late, molecule interferometers find application in both the crucible of fundamental research and industrial pursuits. A prevalent methodology in the construction of atom interferometers involves the utilisation of gratings fashioned from laser beams. While this approach imparts commendable precision, it is hampered by its incapacity to attain exceedingly short wavelengths and its dependence on intricate laser systems for operational efficacy. All applications require the control of matter waves, particularly the particle's velocity. In this manuscript, we propose a continuous beam monochromator scheme reaching enormously high velocity purification with speed ratios in the order of $10^3$ based on atom-surface diffraction. Beyond these high purifications, the proposed scheme simplifies the application by reducing the degree of freedom to a single angle, selecting the wanted particle's velocity."
https://arxiv.org/abs/2403.08352,2024-03-13,Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods,"['Alhassan Mumuni', 'Fuseini Mumuni']","Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of techniques for realizing each of the major subtasks of the data augmentation process: search space design, hyperparameter optimization and model evaluation. Finally, we carried out an extensive comparison and analysis of the performance of automated data augmentation techniques and state-of-the-art methods based on classical augmentation approaches. The results show that AutoML methods for data augmentation currently outperform state-of-the-art techniques based on conventional approaches."
https://arxiv.org/abs/2403.08351,2024-03-13,Solution on strong partition of $2$-balanced regular multipartite tournaments,"['Jiangdong Ai', 'Fankang He', 'Yihang Liu']","We call a partition of a $c$-partite tournament into tournaments of order $c$ is strong if each tournament is strongly connected. The strong partition number denoted as $ST(r)$, represents the minimum integer $c'$ such that every regular $r$-balanced $c$-partite tournament has a strong partition with $c\geq c'$. Figueroa, Montellano-Ballesteros and Olsen showed the existence of $ST(r)$ for all $r\geq 2$ and proved that $5\leq ST(2)\leq 7$. In this note, we establish that $ST(2)=6$ and we also show the unique $2$-balanced $5$-partite tournament which has no strong partition."
https://arxiv.org/abs/2403.08350,2024-03-13,CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model,"['Cheng Chen', 'Junchen Zhu', 'Xu Luo', 'Hengtao Shen', 'Lianli Gao', 'Jingkuan Song']","Instruction tuning represents a prevalent strategy employed by Multimodal Large Language Models (MLLMs) to align with human instructions and adapt to new tasks. Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated. In this paper, we present a comprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess existing MLLMs in the sequential instruction tuning paradigm. CoIN comprises 10 commonly used datasets spanning 8 task categories, ensuring a diverse range of instructions and tasks. Besides, the trained model is evaluated from two aspects: Instruction Following and General Knowledge, which assess the alignment with human intention and knowledge preserved for reasoning, respectively. Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and the failure in intention alignment assumes the main responsibility, instead of the knowledge forgetting. To this end, we introduce MoELoRA to MLLMs which is effective to retain the previous instruction alignment. Experimental results consistently illustrate the forgetting decreased from this method on CoIN."
https://arxiv.org/abs/2403.08349,2024-03-13,On the induced subgraphs of the zero-divisor graph of a matrix ring over number rings,"['WonTae Hwang', 'Ei Thu Thu Kyaw']","We provide a construction of the induced subgraphs of the zero-divisor graph of $M_2(R)$ for the ring $R$ of algebraic integers of some number fields that are neither complete nor connected, and study the structure of the induced subgraphs explicitly. As an application, we prove that the automorphism group of the zero-divisor graph of $M_2(R)$ is not a Jordan group."
https://arxiv.org/abs/2403.08348,2024-03-13,A programmable topological photonic chip,"['Tianxiang Dai', 'Anqi Ma', 'Jun Mao', 'Yutian Ao', 'Xinyu Jia', 'Yun Zheng', 'Chonghao Zhai', 'Yan Yang', 'Zhihua Li', 'Bo Tang', 'Jun Luo', 'Baile Zhang', 'Xiaoyong Hu', 'Qihuang Gong', 'Jianwei Wang']","Controlling topological phases of light has allowed experimental observations of abundant topological phenomena and development of robust photonic devices. The prospect of more sophisticated controls with topological photonic devices for practical implementations requires high-level programmability. Here, we demonstrate a fully programmable topological photonic chip with large-scale integration of silicon photonic nanocircuits and microresonators. Photonic artificial atoms and their interactions in our compound system can be individually addressed and controlled, therefore allowing arbitrary altering of structural parameters and geometrical configurations for the observations of dynamic topological phase transitions and diverse photonic topological insulators. By individually programming artificial atoms on the generic chip, it has allowed comprehensive statistic characterisations of topological robustness against relatively weak disorders, as well as counterintuitive topological Anderson phase transitions induced by strong disorders. Our generic topological photonic chip that can be rapidly reprogrammed to implement multifunctionalities, prototypes a flexible and versatile platform for possible applications across fundamental science and topological technologies."
https://arxiv.org/abs/2403.08347,2024-03-13,1/f frequency fluctuations due to kinetic inductance in CoSi$_2$ microwave cavities,"['Weijun Zeng', 'Ilari Lilja', 'Ekaterina Mukhanova', 'Elica Heredia', 'Chun-Wei Wu', 'Juhn-Jong Lin', 'Sheng-Shiuan Yeh', 'Pertti Hakonen']","Cobalt disilicide provides a promising nearly-epitaxial superconducting material on silicon, which is compatible with high-density integrated circuit technology. We have characterized CoSi$_{2}$ superconducting microwave cavities around 5.5 GHz for resonance frequency fluctuations at temperatures 10 - 200 mK. We found relatively weak fluctuations $(δf/f)^2$ following the spectral density $A/f^γ $, with $A \simeq 6 \times 10^{-16}$ and $γ$ slightly below 1 at an average number of photons of $10^4$; the noise decreased with measurement power as $1/P^{1/2}$. We identify the noise as arising from kinetic inductance fluctuations and discuss possible origins of such fluctuations."
https://arxiv.org/abs/2403.08346,2024-03-13,Discretization of Total Variation in Optimization with Integrality Constraint,"['Annika Schiemann', 'Paul Manns']","We introduce discretizations of infinite-dimensional optimization problems with total variation regularization and integrality constraints on the optimization variables. We advance the discretization of the dual formulation of the total variation term with Raviart--Thomas functions which is known from literature for certain convex problems. Since we have an integrality constraint, the previous analysis from Caillaud and Chambolle [10] does not hold anymore. Even weaker $Γ$-convergence results do not hold anymore because the recovery sequences generally need to attain non-integer values to recover the total variation of the limit function. We solve this issue by introducing a discretization of the input functions on an embedded, finer mesh. A superlinear coupling of the mesh sizes implies an averaging on the coarser mesh of the Raviart--Thomas ansatz, which enables to recover the total variation of integer-valued limit functions with integer-valued discretized input functions. Moreover, we are able to estimate the discretized total variation of the recovery sequence by the total variation of its limit and an error depending on the mesh size ratio. For the discretized optimization problems, we additionally add a constraint that vanishes in the limit and enforces compactness of the sequence of minimizers, which yields their convergence to a minimizer of the original problem. This constraint contains a degree of freedom whose admissible range is determined. Its choice may have a strong impact on the solutions in practice as we demonstrate with an example from imaging."
https://arxiv.org/abs/2403.08345,2024-03-13,From human experts to machines: An LLM supported approach to ontology and knowledge graph construction,"['Vamsi Krishna Kommineni', 'Birgitta König-Ries', 'Sheeba Samuel']","The conventional process of building Ontologies and Knowledge Graphs (KGs) heavily relies on human domain experts to define entities and relationship types, establish hierarchies, maintain relevance to the domain, fill the ABox (or populate with instances), and ensure data quality (including amongst others accuracy and completeness). On the other hand, Large Language Models (LLMs) have recently gained popularity for their ability to understand and generate human-like natural language, offering promising ways to automate aspects of this process. This work explores the (semi-)automatic construction of KGs facilitated by open-source LLMs. Our pipeline involves formulating competency questions (CQs), developing an ontology (TBox) based on these CQs, constructing KGs using the developed ontology, and evaluating the resultant KG with minimal to no involvement of human experts. We showcase the feasibility of our semi-automated pipeline by creating a KG on deep learning methodologies by exploiting scholarly publications. To evaluate the answers generated via Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically extracted using LLMs, we design a judge LLM, which rates the generated content based on ground truth. Our findings suggest that employing LLMs could potentially reduce the human effort involved in the construction of KGs, although a human-in-the-loop approach is recommended to evaluate automatically generated KGs."
https://arxiv.org/abs/2403.08344,2024-03-13,STMPL: Human Soft-Tissue Simulation,"['Anton Agafonov', 'Lihi Zelnik-Manor']","In various applications, such as virtual reality and gaming, simulating the deformation of soft tissues in the human body during interactions with external objects is essential. Traditionally, Finite Element Methods (FEM) have been employed for this purpose, but they tend to be slow and resource-intensive. In this paper, we propose a unified representation of human body shape and soft tissue with a data-driven simulator of non-rigid deformations. This approach enables rapid simulation of realistic interactions."
https://arxiv.org/abs/2403.08343,2024-03-13,Coverage and Rate Analysis for Integrated Sensing and Communication Networks,"['Xu Gan', 'Chongwen Huang', 'Zhaohui Yang', 'Xiaoming Chen', 'Jiguang He', 'Zhaoyang Zhang', 'Chau Yuen', 'Yong Liang Guan', 'Mérouane Debbah']","Integrated sensing and communication (ISAC) is increasingly recognized as a pivotal technology for next-generation cellular networks, offering mutual benefits in both sensing and communication capabilities. This advancement necessitates a re-examination of the fundamental limits within networks where these two functions coexist via shared spectrum and infrastructures. However, traditional stochastic geometry-based performance analyses are confined to either communication or sensing networks separately. This paper bridges this gap by introducing a generalized stochastic geometry framework in ISAC networks. Based on this framework, we define and calculate the coverage and ergodic rate of sensing and communication performance under resource constraints. Then, we shed light on the fundamental limits of ISAC networks by presenting theoretical results for the coverage rate of the unified performance, taking into account the coupling effects of dual functions in coexistence networks. Further, we obtain the analytical formulations for evaluating the ergodic sensing rate constrained by the maximum communication rate, and the ergodic communication rate constrained by the maximum sensing rate. Extensive numerical results validate the accuracy of all theoretical derivations, and also indicate that denser networks significantly enhance ISAC coverage. Specifically, increasing the base station density from $1$ $\text{km}^{-2}$ to $10$ $\text{km}^{-2}$ can boost the ISAC coverage rate from $1.4\%$ to $39.8\%$. Further, results also reveal that with the increase of the constrained sensing rate, the ergodic communication rate improves significantly, but the reverse is not obvious."
https://arxiv.org/abs/2403.08342,2024-03-13,Suppression of diffraction in deep-inelastic scattering on nuclei and dynamical mechanism of leading twist nuclear shadowing,"['V. Guzey', 'M. Strikman']","Using the leading twist approach (LTA) to nuclear shadowing, we calculate the ratios of diffractive and usual parton distributions for a heavy nucleus (Pb) and the proton, $R_{A/p}=(f_{i/A}^{D(3)}/f_{i/A})/(f_{i/p}^{D(3)}/f_{i/p})$, for coherent and summed (coherent plus quasi-elastic) nuclear deep-inelastic scattering. We find that $R_{A/p} \approx 0.5-1$ for quarks and $R_{A/p} \approx 0.5-1.3$ for gluons in a broad range of $x$, which reaffirms the difference from the nuclear enhancement of $R_{A/p}$ predicted in the gluon saturation framework. We demonstrate that the magnitude of $R_{A/p}$ is controlled by the cross section of the interaction of hadronic fluctuations of the virtual photon with target nucleons, which explains an enhancement of $R_{A/p}$ in the color dipole model and its suppression in LTA. We argue that the black disk limit of LTA corresponds to $R_{A/p}=1$ and $R^{\rm coh}_{A/p}=0.86$ for the summed and coherent scattering, respectively. Relying on an intuitive definition of the saturation scale, we show that the ratio of the saturation scales of a heavy nucleus and proton $Q_{sA}^2(b)/Q_{sp}^2(b) \approx 1$ at small impact parameters $b$ due to the strong leading twist nuclear shadowing and diluteness of the nuclear density."
https://arxiv.org/abs/2403.08341,2024-03-13,Schr{ö}dinger eigenfunctions sharing the same modulus and applications to the control of quantum systems,"['Ugo Boscain', ""Kévin Le Balc'H"", 'Mario Sigalotti']","In this paper we investigate when linearly independent eigenfunctions of the Schr\''odinger operator may have the same modulus. General properties are established and the one-dimensional case is treated in full generality. The study is motivated by its application to the bilinear control of the Schr{ö}dinger equation. By assuming that the potentials of interaction satisfy a saturation property and by adapting a strategy recently proposed by Duca and Nersesyan, we discuss when the system can be steered arbitrarily fast between energy levels. Extensions of the previous results to quantum graphs are finally presented."
https://arxiv.org/abs/2403.08340,2024-03-13,MorphoGear: An UAV with Multi-Limb Morphogenetic Gear for Rough-Terrain Locomotion,"['Mikhail Martynov', 'Zhanibek Darush', 'Aleksey Fedoseev', 'Dzmitry Tsetserukou']","Robots able to run, fly, and grasp have a high potential to solve a wide scope of tasks and navigate in complex environments. Several mechatronic designs of such robots with adaptive morphologies are emerging. However, the task of landing on an uneven surface, traversing rough terrain, and manipulating objects still presents high challenges."
https://arxiv.org/abs/2403.08339,2024-03-13,Low-Complexity Beam Training for Multi-RIS-Assisted Multi-User Communications,"['Yuan Xu', 'Chongwen Huang', 'Wei Li', 'Zhaohui Yang', 'Xiaoming Chen', 'Zhaoyang Zhang', 'Chau Yuen', 'Mérouane Debbah']","In this paper, we investigate the beam training problem in the multi-user millimeter wave (mmWave) communication system, where multiple reconfigurable intelligent surfaces (RISs) are deployed to improve the coverage and the achievable rate. However, existing beam training techniques in mmWave systems suffer from the high complexity (i.e., exponential order) and low identification accuracy. To address these problems, we propose a novel hashing multi-arm beam (HMB) training scheme that reduces the training complexity to the logarithmic order with the high accuracy. Specifically, we first design a generation mechanism for HMB codebooks. Then, we propose a demultiplexing algorithm based on the soft decision to distinguish signals from different RIS reflective links. Finally, we utilize a multi-round voting mechanism to align the beams. Simulation results show that the proposed HMB training scheme enables simultaneous training for multiple RISs and multiple users, and reduces the beam training overhead to the logarithmic level. Moreover, it also shows that our proposed scheme can significantly improve the identification accuracy by at least 20% compared to existing beam training techniques."
https://arxiv.org/abs/2403.08338,2024-03-13,Curved commutators in the plane,"['Kangwei Li', 'Henri Martikainen', 'Tuomas Oikari']","We complete the $L^p$ boundedness theory of commutators of Hilbert transforms along monomial curves by providing the previously missing lower bounds. This optimal result now covers all monomial curves while previous results had significant geometric restrictions. We also, for the first time, develop the corresponding necessity theory for curves with non-vanishing torsion."
https://arxiv.org/abs/2403.08337,2024-03-13,LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments,"['Maonan Wang', 'Aoyu Pang', 'Yuheng Kan', 'Man-On Pun', 'Chung Shue Chen', 'Bo Huang']","Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at \href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}."
https://arxiv.org/abs/2403.08336,2024-03-13,Mean field error estimate of the random batch method for large interacting particle system,"['Zhenyu Huang', 'Shi Jin', 'Lei Li']","The random batch method (RBM) proposed in [Jin et al., J. Comput. Phys., 400(2020), 108877] for large interacting particle systems is an efficient with linear complexity in particle numbers and highly scalable algorithm for $N$-particle interacting systems and their mean-field limits when $N$ is large. We consider in this work the quantitative error estimate of RBM toward its mean-field limit, the Fokker-Planck equation. Under mild assumptions, we obtain a uniform-in-time $O(τ^2 + 1/N)$ bound on the scaled relative entropy between the joint law of the random batch particles and the tensorized law at the mean-field limit, where $τ$ is the time step size and $N$ is the number of particles. Therefore, we improve the existing rate in discretization step size from $O(\sqrtτ)$ to $O(τ)$ in terms of the Wasserstein distance."
https://arxiv.org/abs/2403.08335,2024-03-13,A Sparsity Principle for Partially Observable Causal Representation Learning,"['Danru Xu', 'Dingling Yao', 'Sébastien Lachapelle', 'Perouz Taslakian', 'Julius von Kügelgen', 'Francesco Locatello', 'Sara Magliacane']","Causal representation learning aims at identifying high-level causal variables from perceptual data. Most methods assume that all latent causal variables are captured in the high-dimensional observations. We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state. Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents. Here, we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern. Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables. Based on these insights, we propose two methods for estimating the underlying causal variables by enforcing sparsity in the inferred representation. Experiments on different simulated datasets and established benchmarks highlight the effectiveness of our approach in recovering the ground-truth latents."
https://arxiv.org/abs/2403.08334,2024-03-13,DONAPI: Malicious NPM Packages Detector using Behavior Sequence Knowledge Mapping,"['Cheng Huang', 'Nannan Wang', 'Ziyan Wang', 'Siqi Sun', 'Lingzi Li', 'Junren Chen', 'Qianchong Zhao', 'Jiaxuan Han', 'Zhen Yang', 'Lei Shi']","With the growing popularity of modularity in software development comes the rise of package managers and language ecosystems. Among them, npm stands out as the most extensive package manager, hosting more than 2 million third-party open-source packages that greatly simplify the process of building code. However, this openness also brings security risks, as evidenced by numerous package poisoning incidents."
https://arxiv.org/abs/2403.08333,2024-03-13,Fast Inference of Removal-Based Node Influence,"['Weikai Li', 'Zhiping Xiao', 'Xiao Luo', 'Yizhou Sun']","Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, ""In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?"". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot directly satisfy our needs, since they do not focus on the global influence score for every node. We propose an efficient and intuitive method, NOde-Removal-based fAst GNN inference (NORA), which uses the gradient to approximate the node-removal influence. It only costs one forward propagation and one backpropagation to approximate the influence score for all nodes. Extensive experiments on six datasets and six GNN models verify the effectiveness of NORA. Our code is available at https://github.com/weikai-li/NORA.git."
https://arxiv.org/abs/2403.08332,2024-03-13,Autoregressive Score Generation for Multi-trait Essay Scoring,"['Heejin Do', 'Yunsu Kim', 'Gary Geunbae Lee']","Recently, encoder-only pre-trained models such as BERT have been successfully applied in automated essay scoring (AES) to predict a single overall score. However, studies have yet to explore these models in multi-trait AES, possibly due to the inefficiency of replicating BERT-based models for each trait. Breaking away from the existing sole use of encoder, we propose an autoregressive prediction of multi-trait scores (ArTS), incorporating a decoding process by leveraging the pre-trained T5. Unlike prior regression or classification methods, we redefine AES as a score-generation task, allowing a single model to predict multiple scores. During decoding, the subsequent trait prediction can benefit by conditioning on the preceding trait scores. Experimental results proved the efficacy of ArTS, showing over 5% average improvements in both prompts and traits."
https://arxiv.org/abs/2403.08331,2024-03-13,Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR,"['Yasunori Taguchi', 'Hiro Gangi']","Optimization of product and system characteristics is required in many fields, including design and control. Bayesian optimization (BO) is often used when there are high observing costs, because BO theoretically guarantees an upper bound on regret. However, computational costs increase exponentially with the number of parameters to be optimized, decreasing search efficiency. We propose a BO that limits the search region to lower dimensions and utilizes local Gaussian process regression (LGPR) to scale the BO to higher dimensions. LGPR treats the low-dimensional search region as ""local,"" improving prediction accuracies there. The LGPR model is trained on a local subset of data specific to that region. This improves prediction accuracy and search efficiency and reduces the time complexity of matrix inversion in the Gaussian process regression. In evaluations with 20D Ackley and Rosenbrock functions, search efficiencies are equal to or higher than those of the compared methods, improved by about 69% and 40% from the case without LGPR. We apply our method to an automatic design task for a power semiconductor device. We successfully reduce the specific on-resistance to 25% better than a conventional method and 3.4% better than without LGPR."
https://arxiv.org/abs/2403.08330,2024-03-13,Activating Wider Areas in Image Super-Resolution,"['Cheng Cheng', 'Hang Wang', 'Hongbin Sun']","The prevalence of convolution neural networks (CNNs) and vision transformers (ViTs) has markedly revolutionized the area of single-image super-resolution (SISR). To further boost the SR performances, several techniques, such as residual learning and attention mechanism, are introduced, which can be largely attributed to a wider range of activated area, that is, the input pixels that strongly influence the SR results. However, the possibility of further improving SR performance through another versatile vision backbone remains an unresolved challenge. To address this issue, in this paper, we unleash the representation potential of the modern state space model, i.e., Vision Mamba (Vim), in the context of SISR. Specifically, we present three recipes for better utilization of Vim-based models: 1) Integration into a MetaFormer-style block; 2) Pre-training on a larger and broader dataset; 3) Employing complementary attention mechanism, upon which we introduce the MMA. The resulting network MMA is capable of finding the most relevant and representative input pixels to reconstruct the corresponding high-resolution images. Comprehensive experimental analysis reveals that MMA not only achieves competitive or even superior performance compared to state-of-the-art SISR methods but also maintains relatively low memory and computational overheads (e.g., +0.5 dB PSNR elevation on Manga109 dataset with 19.8 M parameters at the scale of 2). Furthermore, MMA proves its versatility in lightweight SR applications. Through this work, we aim to illuminate the potential applications of state space models in the broader realm of image processing rather than SISR, encouraging further exploration in this innovative direction."
https://arxiv.org/abs/2403.08329,2024-03-13,Slow convergence of the moment-SOS hierarchy for an elementary polynomial optimization problem,"['Didier Henrion', 'Adrien Le Franc', 'Victor Magron']",We describe a parametric univariate quadratic optimization problem for which the moment-SOS hierarchy has finite but increasingly slow convergence when the parameter tends to its limit value. We estimate the order of finite convergence as a function of the parameter.
https://arxiv.org/abs/2403.08328,2024-03-13,"Structural investigation of the liquid crystalline phases of three homologues from the series of 4-pentylphenyl-4'-n-alkyloxythiobenzoates (n = 9, 10, 11)","['Aleksandra Deptuch', 'Bartosz Sęk', 'Sebastian Lalik', 'Mirosława D. Ossowska-Chruściel', 'Janusz Chruściel', 'Monika Marzec']","Polarizing optical microscopy and differential scanning calorimetry are used to determine the phase sequence of three liquid crystalline 4-pentylphenyl-4'-n-alkyloxythiobenzoates with n = 9, 10, 11. The X-ray diffraction method is applied for structural characterization of the liquid crystalline phases. The smectic layer spacing, tilt angle, average distance between the long axes of molecules and correlation length of the short-range order are determined as a function of temperature. For the crystal-like smectic phases with hexagonal or herring-bone packing, the unit cell parameters are obtained. The presence of the tilted hexagonal phase for n = 10, 11 and tilted herring-bone phase for n = 9, 10 is indicated, although the direction of the tilt cannot be determined."
https://arxiv.org/abs/2403.08327,2024-03-13,Noninteger high-harmonic generation from extended correlated systems,"['Christian Saugbjerg Lange', 'Thomas Hansen', 'Lars Bojer Madsen']","The spectra produced by high-harmonic generation (HHG) typically exhibit well-defined peaks at odd integers times the laser frequency. However, in recent investigations of HHG from correlated materials, spectra exhibit signals at noninteger harmonics which do not conform to the well-known symmetry-based selection rules for HHG-spectra. Here, we use the Fermi-Hubbard model to study HHG from a linear chain of atoms. This model allows us to study both the correlated and uncorrelated phases through a specification of the amount of onsite electron-electron repulsion. The presence of signal at noninteger harmonics can be interpreted as originating from the population of multiple Floquet states. We show how this coupling to different Floquet states depends on the characteristics of the driving pulse and the strength of the electron-electron interaction in the system."
https://arxiv.org/abs/2403.08326,2024-03-13,Positive Lynden-Bell derivative as a ticket to the bar trap?,"['Viktor D. Zozulia', 'Anton A. Smirnov', 'Natalia Ya. Sotnikova']","We have translated the results of $N$-body simulations of one barred model into the language of action variables and frequencies. Using this language, we analysed the behaviour of all orbits in the model on a large time scale at the stage of a mature bar. We show that the orbits join the bar while preserving their adiabatic invariant, which takes into account the 3D structure of the orbits. This allows us to apply the concept of the Lynden-Bell derivative for each of these orbits and trace how the sign of the derivative changes, i.e. how asynchronous changes in angular momentum $L_z$ and orbital precession rate $Ω_\mathrm{pr}$ (normal orbital mode) change to synchronous (abnormal mode). The transition to the abnormal mode occurs when $Ω_\mathrm{pr}$ reaches the angular velocity of the pattern $Ω_\mathrm{p}$, after which the orbit becomes stuck in the bar trap. All this happens against the background of secular changes in actions ($L_z$ decreases, $J_\mathrm{R}$ and $J_z$ increase). At the same time, corotation particles near two stable Lagrange points are also subject to secular changes in their actions. They increase $L_z$ and drift to the periphery, shifting corotation outwards. We also show that a change in the orbital mode from normal to abnormal and the trapping of orbits in a bar is possible only when the bar speed decreases with time, regardless of what is causing the bar to slow down. Our findings clarify and expand the picture of bar formation and evolution in numerical models."
https://arxiv.org/abs/2403.08325,2024-03-13,Dynamic flexoelectric instabilities in nematic liquid crystals,"['E. S. Pikina', 'A. R. Muratov', 'E. I. Kats', 'V. V. Lebedev']","Electro-hydrodynamic phenomena in liquid crystals constitute an old but still very active research area. The reason is that these phenomena play the key role in various applications of liquid crystals and due to the general interest of physical community to out-of-equilibrium systems. Nematic liquid crystals (NLCs) are ideally representative for such investigations. Our article aims to study theoretically the linear NLCs dynamics. We include into consideration orientation elastic energy, hydrodynamic motion, external alternating electric field, electric conductivity and flexoelectric polarization. We analyze the linear stability of the NLC film, determining dynamics of perturbations with respect to the homogeneous initial state of the NLC. For the purpose we compute eigen-values of the evolution matrix for a period of the external alternating electric field. These eigen-values determine the amplification factors for the modes during the period. The instability occurs when the principal eigen-value of the evolution matrix becomes unity by its absolute value. The condition determines the threshold (critical field) for the instability of the uniform state. It turns out that one might expect various types of the instability, only partially known and investigated in the literature. Particularly, we find that the flexoelectric instability may lead to two-dimensionally space modulated patterns exhibiting time oscillations. This type of the structures was somehow overlooked in the previous works."
https://arxiv.org/abs/2403.08324,2024-03-13,Mixed moments of $\rm GL(2)$ and symmetric square $L$-functions,"['Bingrong Huang', 'Liangxun Li']","In this paper, we prove asymptotic formulas of mixed moments of $\rm GL(2)$ and its symmetric square $L$-functions for both Hecke--Maass cusp forms and holomorphic Hecke eigenforms in short intervals. As an application, we prove quantitative simultaneous non-vanishing of central values of these $L$-functions."
https://arxiv.org/abs/2403.08323,2024-03-13,Sparse Bayesian Learning-Based Hierarchical Construction for 3D Radio Environment Maps Incorporating Channel Shadowing,"['Wang Jie', 'Zhu Qiuming', 'Lin Zhipeng', 'Chen Junting', 'Ding Guoru', 'Wu Qihui', 'Gu Guochen', 'Gao Qianhao']","The radio environment map (REM) visually displays the spectrum information over the geographical map and plays a significant role in monitoring, management, and security of spectrum resources.In this paper, we present an efficient 3D REM construction scheme based on the sparse Bayesian learning (SBL), which aims to recovery the accurate REM with limited and optimized sampling data.In order to reduce the number of sampling sensors, an efficient sparse sampling method for unknown scenarios is proposed. For the given construction accuracy and the priority of each location, the quantity and sampling locations can be jointly optimized.With the sparse sampled data, by mining the spectrum situation sparsity and channel propagation characteristics, an SBL-based spectrum data hierarchical recovery algorithm is developed to estimate the missing data of unsampled locations.Finally, the simulated 3D REM data in the campus scenario are used to verify the proposed methods as well as to compare with the state-of-the-art. We also analyze the recovery performance and the impact of different parameters on the constructed REMs. Numerical results demonstrate that the proposed scheme can ensure the construction accuracy and improve the computational efficiency under the low sampling rate."
https://arxiv.org/abs/2403.08322,2024-03-13,Generalized free energy and thermodynamic phases of black holes in the gauged Kaluza-Klein theory,"['Tran N. Hung', 'Cao H. Nam']","In the context of the generalized (off-shell) free energy, we explore the phase emergence and corresponding phase transitions of charged dilaton $\text{AdS}$ black holes in the gauged Kaluza-Klein (KK) theory where the KK vector field is gauged such that the fermionic fields are charged under the U(1)$_{\text{KK}}$ gauge group. The black hole solutions are asymptotic to the AdS$_D$ geometry and can be realized as the dimensional reduction of the gauged supergravities on the compact internal manifolds, leading to the restriction as $4\leq D\leq 7$. By studying the behavior of the generalized free energy under the change of the ensemble temperature, we determine the thermodynamic phases and the corresponding phase transitions of black holes. This is confirmed by investigating the heat capacity at the constant pressure and the on-shell free energy. In the canonical ensemble, the thermodynamics of black holes can be classified into three different classes as follows: (i) $D=4$, (ii) $D=5$, and (iii) $D=6,7$. Whereas, in the grand canonical ensemble, the thermodynamics of black holes is independent of the number of spacetime dimensions and the pressure, but depends on the chemical potential $Φ$. The thermodynamic behavior of black holes can be classified into three different classes as follows: (i) $Φ<1$, (ii) $Φ>1$, and (iii) $Φ=1$."
https://arxiv.org/abs/2403.08321,2024-03-13,ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation,"['Guanxing Lu', 'Shiyi Zhang', 'Ziwei Wang', 'Changliu Liu', 'Jiwen Lu', 'Yansong Tang']","Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1\% in average success rate."
https://arxiv.org/abs/2403.08320,2024-03-13,Benchmarking quantum master equations beyond ultraweak coupling,"['Camilo Santiago Tello Breuer', 'Tobias Becker', 'André Eckardt']","Recently, Nathan and Rudner derived a Gorini-Kossakowski-Sudarshan-Lindblad master equation from the Redfield equation. The claim is that the level of approximation is equal to that of the Redfield equation. This is ground breaking work since a quantum master equation that guarantees physical states and that is as accurate as the Redfield equation is still missing. Here we benchmark the Nathan-Rudner equation (NRE) against the exact solution of a damped harmonic oscillator and compare its performance to that of the time-dependent Redfield equation (RE). We find that which of the equations performs better depends on the regime considered. It turns out that the short-time dynamics is generally much better captured by the RE, whereas the NRE delivers similar results to the rotating-wave approximation. For the steady state, in the high-temperature limit the RE also performs better and its solution approaches the exact result for ultrahigh temperatures. Nevertheless, also here the NR equation constitutes a good approximation. In the low-temperature limit, in turn, the NRE still provides a good approximation to the steady state, while the solution of the RE becomes unphysical for too strong coupling. Moreover, we show that, like the RE, also the NRE approaches the correct steady state in the limit of vanishing system-bath coupling. However, in second-order system-bath coupling, where the RE is known to provide the steady-state coherences correctly but not the populations, the NRE generally neither reproduces the correct populations nor the coherences."
https://arxiv.org/abs/2403.08319,2024-03-13,Knowledge Conflicts for LLMs: A Survey,"['Rongwu Xu', 'Zehan Qi', 'Cunxiang Wang', 'Hongru Wang', 'Yue Zhang', 'Wei Xu']","This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area."
https://arxiv.org/abs/2403.08318,2024-03-13,DrFER: Learning Disentangled Representations for 3D Facial Expression Recognition,"['Hebeizi Li', 'Hongyu Yang', 'Di Huang']","Facial Expression Recognition (FER) has consistently been a focal point in the field of facial analysis. In the context of existing methodologies for 3D FER or 2D+3D FER, the extraction of expression features often gets entangled with identity information, compromising the distinctiveness of these features. To tackle this challenge, we introduce the innovative DrFER method, which brings the concept of disentangled representation learning to the field of 3D FER. DrFER employs a dual-branch framework to effectively disentangle expression information from identity information. Diverging from prior disentanglement endeavors in the 3D facial domain, we have carefully reconfigured both the loss functions and network structure to make the overall framework adaptable to point cloud data. This adaptation enhances the capability of the framework in recognizing facial expressions, even in cases involving varying head poses. Extensive evaluations conducted on the BU-3DFE and Bosphorus datasets substantiate that DrFER surpasses the performance of other 3D FER methods."
https://arxiv.org/abs/2403.08317,2024-03-13,From Channel Measurement to Training Data for PHY Layer AI Applications,"['Michael Zentarra', 'Julian Ahrens', 'Lia Ahrens']","Learning-based techniques such as artificial intelligence (AI) and machine learning (ML) play an increasingly important role in the development of future communication networks. The success of a learning algorithm depends on the quality and quantity of the available training data. In the physical layer (PHY), channel information data can be obtained either through measurement campaigns or through simulations based on predefined channel models. Performing measurements can be time consuming while only gaining information about one specific position or scenario. Simulated data, on the other hand, are more generalized and reflect in most cases not a real environment but instead, a statistical approximation based on a mathematical model. This paper presents a procedure for acquiring channel data by means of fast and flexible software defined radio (SDR) based channel measurements along with a method for a parameter extraction that provides configuration input to the simulator. The procedure from the measurement to the simulated channel data is demonstrated in two exemplary propagation scenarios. It is shown, that in both cases the simulated data is in good accordance to the measurements"
https://arxiv.org/abs/2403.08316,2024-03-13,NNLO QCD corrections to $ΔΓ_s$ in the $B_s-\overline{B}_s$ system,"['Marvin Gerlach', 'Ulrich Nierste', 'Pascal Reeck', 'Vladyslav Shtabovenko', 'Matthias Steinhauser']","This report summarises recent advances made in the calculation of the NNLO QCD corrections to the width difference $ΔΓ_s$ in the $B_s-\overline{B}_s$ system. The inclusion of the effects due to current-current operators leads to an updated prediction of $ΔΓ_s = (0.076\pm 0.017)\,\text{ps}^{-1}$, which narrows the gap between theory and experiment."
https://arxiv.org/abs/2403.08315,2024-03-13,Intense and Stable Blue Light Emission from CsPbBr$_3$/Cs$_4$PbBr$_6$ Heterostructures Embedded in Transparent Nanoporous Films,"['Carlos Romero-Perez', 'Natalia Fernandez Delgado', 'Miriam Herrera Collado', 'Mauricio E. Calvo', 'Hernan Miguez']","Lead halide perovskite nanocrystals are attractive for light emitting devices both as electroluminescent and color converting materials, since they combine intense and narrow emissions with good charge injection and transport properties. However, most perovskite nanocrystals shine at green and red wavelengths, the observation of intense and stable blue emission still being a challenging target. In this work, we report a method to attain intense and enduring blue emission (470-480 nm), with a photoluminescence quantum yield (PLQY) of 40%, originated from very small CsPbBr$_3$ nanocrystals (diameter<3nm) formed by controllably exposing Cs$_4$PbBr$_6$ to humidity. This process is mediated by the void network of a mesoporous transparent scaffold in which the zero-dimensional (0D) Cs$_4$PbBr$_6$ lattice is embedded, which allows the fine control over water adsorption and condensation that determines the optimization of the synthetic procedure and, eventually, the nanocrystal size. By temperature dependent photoemission analysis of samples with different [CsPbBr$_3$]/[Cs$_4$PbBr$_6$] volume ratios, we show that the bright blue emission observed results from the efficient charge transfer to the CsPbBr$_3$ inclusions from the Cs$_4$PbBr$_6$ host. Our approach provides a means to attain highly efficient transparent blue light emitting films that complete the palette offered by perovskite nanocrystals for lighting and display applications."
https://arxiv.org/abs/2403.08314,2024-03-13,Is Context Helpful for Chat Translation Evaluation?,"['Sweta Agrawal', 'Amin Farajian', 'Patrick Fernandes', 'Ricardo Rei', 'André F. T. Martins']","Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited. Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information. This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality. Motivated by this, we conduct a meta-evaluation of existing sentence-level automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats. We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings. We then investigate how incorporating conversational contextual information in these metrics affects their performance. Our findings show that augmenting neural learned metrics with contextual information helps improve correlation with human judgments in the reference-free scenario and when evaluating translations in out-of-English settings. Finally, we propose a new evaluation metric, Context-MQM, that utilizes bilingual context with a large language model (LLM) and further validate that adding context helps even for LLM-based evaluation metrics."
https://arxiv.org/abs/2403.08313,2024-03-13,An improvement on the Louvain algorithm using random walks,"['Duy Hieu Do', 'Thi Ha Duong Phan']","We will present improvements to famous algorithms for community detection, namely Newman's spectral method algorithm and the Louvain algorithm. The Newman algorithm begins by treating the original graph as a single cluster, then repeats the process to split each cluster into two, based on the signs of the eigenvector corresponding to the secondlargest eigenvalue. Our improvement involves replacing the time-consuming computation of eigenvalues with a random walk during the splitting process. The Louvain algorithm iteratively performs the following steps until no increase in modularity can be achieved anymore: each step consists of two phases, phase 1 for partitioning the graph into clusters, and phase 2 for constructing a new graph where each vertex represents one cluster obtained from phase 1. We propose an improvement to this algorithm by adding our random walk algorithm as an additional phase for refining clusters obtained from phase 1. It maintains a complexity comparable to the Louvain algorithm while exhibiting superior efficiency. To validate the robustness and effectiveness of our proposed algorithms, we conducted experiments using randomly generated graphs and real-world data."
https://arxiv.org/abs/2403.08312,2024-03-13,StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses,"['Jia-Nan Li', 'Quan Tu', 'Cunli Mao', 'Zhengtao Yu', 'Ji-Rong Wen', 'Rui Yan']","Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize information losses from reconstruction after compression, we design two learning strategies of short-memory reconstruction (SMR) and long-memory reactivation (LMR). Our method outperforms strong baselines in dialogue tasks and achieves a 4 $\times$ speedup while reducing memory usage by 18 $\times$ compared to dense attention recomputation."
https://arxiv.org/abs/2403.08311,2024-03-13,When Code Smells Meet ML: On the Lifecycle of ML-specific Code Smells in ML-enabled Systems,"['Gilberto Recupito', 'Giammaria Giordano', 'Filomena Ferrucci', 'Dario Di Nucci', 'Fabio Palomba']","Context. The adoption of Machine Learning (ML)--enabled systems is steadily increasing. Nevertheless, there is a shortage of ML-specific quality assurance approaches, possibly because of the limited knowledge of how quality-related concerns emerge and evolve in ML-enabled systems. Objective. We aim to investigate the emergence and evolution of specific types of quality-related concerns known as ML-specific code smells, i.e., sub-optimal implementation solutions applied on ML pipelines that may significantly decrease both the quality and maintainability of ML-enabled systems. More specifically, we present a plan to study ML-specific code smells by empirically analyzing (i) their prevalence in real ML-enabled systems, (ii) how they are introduced and removed, and (iii) their survivability. Method. We will conduct an exploratory study, mining a large dataset of ML-enabled systems and analyzing over 400k commits about 337 projects. We will track and inspect the introduction and evolution of ML smells through CodeSmile, a novel ML smell detector that we will build to enable our investigation and to detect ML-specific code smells."
https://arxiv.org/abs/2403.08310,2024-03-13,StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance Fields,"['Hongbin Xu', 'Weitao Chen', 'Feng Xiao', 'Baigui Sun', 'Wenxiong Kang']","4D style transfer aims at transferring arbitrary visual style to the synthesized novel views of a dynamic 4D scene with varying viewpoints and times. Existing efforts on 3D style transfer can effectively combine the visual features of style images and neural radiance fields (NeRF) but fail to handle the 4D dynamic scenes limited by the static scene assumption. Consequently, we aim to handle the novel challenging problem of 4D style transfer for the first time, which further requires the consistency of stylized results on dynamic objects. In this paper, we introduce StyleDyRF, a method that represents the 4D feature space by deforming a canonical feature volume and learns a linear style transformation matrix on the feature volume in a data-driven fashion. To obtain the canonical feature volume, the rays at each time step are deformed with the geometric prior of a pre-trained dynamic NeRF to render the feature map under the supervision of pre-trained visual encoders. With the content and style cues in the canonical feature volume and the style image, we can learn the style transformation matrix from their covariance matrices with lightweight neural networks. The learned style transformation matrix can reflect a direct matching of feature covariance from the content volume to the given style pattern, in analogy with the optimization of the Gram matrix in traditional 2D neural style transfer. The experimental results show that our method not only renders 4D photorealistic style transfer results in a zero-shot manner but also outperforms existing methods in terms of visual quality and consistency."
https://arxiv.org/abs/2403.08309,2024-03-13,HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback,"['Ang Li', 'Qiugen Xiao', 'Peng Cao', 'Jian Tang', 'Yi Yuan', 'Zijie Zhao', 'Xiaoyuan Chen', 'Liang Zhang', 'Xiangyang Li', 'Kaitong Yang', 'Weidong Guo', 'Yukang Gan', 'Daniell Wang', 'Ying Shan']","Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, it employs AI for Red Teaming, further improving the model's harmlessness. Human evaluation results show that HRLAIF inherits the ability of RLAIF to enhance human preference for outcomes at a low cost while also improving the satisfaction rate of responses. Compared to the policy model before Reinforcement Learning (RL), it achieves an increase of 2.08\% in satisfaction rate, effectively addressing the issue of a decrease of 4.58\% in satisfaction rate after basic RLAIF."
https://arxiv.org/abs/2403.08308,2024-03-13,Interval Replacements of Persistence Modules,"['Hideto Asashiba', 'Etienne Gauthier', 'Enhao Liu']","We define (1) a notion of a compression system $ξ$ for a finite poset $\mathbf{P}$, which assigns each interval subposet $I$ to a poset morphism $ξ_I \colon Q_I \to \mathbf{P}$ and (2) an $I$-rank of a persistence module $M$ with respect to $ξ$, the family of which is called the interval rank invariant. A compression system $ξ$ makes it possible to define the interval replacement (also called the interval-decomposable approximation) not only for 2D persistence modules but also for any persistence modules over any finite poset. We will show that the forming of the interval replacement preserves the interval rank invariant that is a stronger property than the preservation of the usual rank invariant. Moreover, we will give an explicit formula of the $I$-rank of $M$ with respect to $ξ$ in terms of the structure linear maps of $M$ under a mild existence condition of joins and meets in $I$ in the case where $ξ_I$ is the inclusion of $I$ into $\mathbf{P}$, or more generally, $ξ_I$ ''essentially covers'' $I$."
https://arxiv.org/abs/2403.08307,2024-03-13,An existence result for accretive growth in elastic solids,"['Elisa Davoli', 'Katerina Nik', 'Ulisse Stefanelli', 'Giuseppe Tomassetti']","We investigate a model for the accretive growth of an elastic solid. The reference configuration of the body is accreted in its normal direction, with space- and deformation-dependent accretion rate. The time-dependent reference configuration is identified via the level sets of the unique viscosity solution of a suitable generalized eikonal equation. After proving the global-in-time well-posedness of the quasistatic equilibrium under prescribed growth, we prove the existence of a local-in-time solution for the coupled equilibrium-growth problem, where both mechanical displacement and time-evolving set are unknown. A distinctive challenge is the limited regularity of the growing body, which calls for proving a new uniform Korn inequality."
https://arxiv.org/abs/2403.08306,2024-03-13,Research on the number of prime numbers between $n^2$ and ${(n+1)}^2$,"['Jimin Li', 'Haonan Li']","Let $p_{r+1}-1>n \geq p_r-1$, based on a sequence $\{1,2,3\cdots\ M_r(M_r=p_1p_2\cdots p_r)\}$, we compare the density of coprime numbers and establish a correlation between the proportions of coprime numbers in the ranges from 1 to consecutive square numbers. Then, we derive the relationship between the number of coprimes in the interval of $n^2 \sim {(n+1)}^2$ and the proportion of coprimes in the interval of $1 \sim n^2$, proving that there is at least one prime number between any $n^2$ and ${(n+1)}^2$. By extending our research to the range of $1 \sim M_r^2$, we establish the relationship between the proportions of backwards coprime numbers in the ranges from ${M_r}^2$ to consecutive square numbers; furthermore, we establish a relationship between the proportions of coprimes in small interval and the whole interval. Then, in conclusion, the number of coprimes between $n^2$ and ${(n+1)}^2$ is greater than $n\prod_{i=1}^{r}{(1-\frac{1}{p_i}})$, thus proving that there are at least 2 prime numbers between $n^2$ and ${(n+1)}^2$."
https://arxiv.org/abs/2403.08305,2024-03-13,Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform,"['Mingyue Cheng', 'Hao Zhang', 'Jiqian Yang', 'Qi Liu', 'Li Li', 'Xin Huang', 'Liwei Song', 'Zhi Li', 'Zhenya Huang', 'Enhong Chen']","Large language model evaluation plays a pivotal role in the enhancement of its capacity. Previously, numerous methods for evaluating large language models have been proposed in this area. Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for large language models. Additionally, these methods predominantly utilize centralized datasets for evaluation, with question banks concentrated within the evaluation platforms themselves. Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated. To address these limitations, we propose a novel anonymous crowd-sourcing evaluation platform, BingJian, for large language models that employs a competitive scoring mechanism where users participate in ranking models based on their performance. This platform stands out not only for its support of centralized evaluations to assess the general capabilities of models but also for offering an open evaluation gateway. Through this gateway, users have the opportunity to submit their questions, testing the models on a personalized and potentially broader range of capabilities. Furthermore, our platform introduces personalized evaluation scenarios, leveraging various forms of human-computer interaction to assess large language models in a manner that accounts for individual user preferences and contexts. The demonstration of BingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian."
https://arxiv.org/abs/2403.08304,2024-03-13,Freeness of hyperplane arrangements associated with gain graphs,"['Daisuke Suyama', 'Michele Torielli', 'Shuhei Tsujie']","Athanasiadis studied arrangements obtained by adding shifted hyperplanes to the braid arrangement. Bailey studied arrangements obtained by adding tilted hyperplanes to the braid arrangement. These two kinds of arrangements are associated with directed graphs and their freeness was characterized in terms of the graphs. The results show coincidence of freeness. Namely, if Athanasiadis' arrangement is free, then the corresponding Bailey's arrangement is free, and vice versa."
https://arxiv.org/abs/2403.08303,2024-03-13,Equivalence between Erdős-Hajnal and polynomial Rödl and Nikiforov conjectures,"['Matija Bucić', 'Jacob Fox', 'Huy Tuan Pham']","It is well-known that polynomial versions of theorems of Rödl and Nikiforov, as conjectured by Fox and Sudakov and Nguyen, Scott and Seymour imply the classical Erdős-Hajnal conjecture. In this note, we prove that these three conjectures are in fact equivalent, extending several previous particular results in this direction by Fox, Nguyen, Scott and Seymour; Nguyen, Scott and Seymour and Gishboliner and Shapira. We deduce that the family of string graphs satisfies the polynomial Rödl conjecture."
https://arxiv.org/abs/2403.08302,2024-03-13,Online Multi-Contact Feedback Model Predictive Control for Interactive Robotic Tasks,"['Seo Wook Han', 'Maged Iskandar', 'Jinoh Lee', 'Min Jun Kim']","In this paper, we propose a model predictive control (MPC) that accomplishes interactive robotic tasks, in which multiple contacts may occur at unknown locations. To address such scenarios, we made an explicit contact feedback loop in the MPC framework. An algorithm called Multi-Contact Particle Filter with Exploration Particle (MCP-EP) is employed to establish real-time feedback of multi-contact information. Then the interaction locations and forces are accommodated in the MPC framework via a spring contact model. Moreover, we achieved real-time control for a 7 degrees of freedom robot without any simplifying assumptions by employing a Differential-Dynamic-Programming algorithm. We achieved 6.8kHz, 1.9kHz, and 1.8kHz update rates of the MPC for 0, 1, and 2 contacts, respectively. This allows the robot to handle unexpected contacts in real time. Real-world experiments show the effectiveness of the proposed method in various scenarios."
https://arxiv.org/abs/2403.08301,2024-03-13,Probing the stellar populations and star formation history of early-type galaxies at $0 < z < 1.1$ in the rest-frame ultraviolet,"['Sadman Ali', 'Roberto De Propris', 'Chul Chung', 'Steven Phillipps', 'Malcolm Bremer', 'Masato Onodera', 'Marcin Sawicki', 'Guillaume Desprez', 'Stephen Gwyn']","We measure the evolution of the rest-frame $NUV-V$ colors for early-type galaxies in clusters at $0<z<1.1$ using data from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP), CFHT Large Area U-band Deep Survey (CLAUDS) and local SDSS clusters observed with GALEX. Our results show that there is an excess in the ultraviolet spectrum in most quiescent galaxies (compared to the expectations from models fitting their optical/infrared colors and spectra) below $z\sim0.6$, beyond which the excess UV emission fades rapidly. This evolution of the UV color is only consistent with the presence of a highly evolved, hot horizontal branch sub-population in these galaxies (amongst the majority cool and optically bright stars), comprising on average 10\% of the total stellar mass and forming at $z>3$. The blue UV colors of early-type galaxies at low-intermediate redshifts are likely driven by this sub-population being enriched in helium up to $\sim44\%$. At $z>0.8$ (when the extra UV component has not yet appeared) the data allows us to constrain the star formation histories of galaxies by fitting models to the evolution of their UV colors: we find that the epoch at which the stellar populations formed ranges between $3<z_{form}<10$ (corresponding to $0.5-2.2$ Gyrs after the Big Bang) with a star-formation e-folding timescale of $τ=0.35-0.7$ Gyr, suggesting that these galaxies formed the majority of stars at very high redshift, with a brief yet intense burst of star-formation activity. The star formation history and chemical evolution of early-type galaxies resemble those of globular clusters, albeit on much larger scales."
https://arxiv.org/abs/2403.08300,2024-03-13,Spin relaxation in inhomogeneous magnetic fields with depolarizing boundaries,"['Yue Chang', 'Shuangai Wan', 'Shichao Dong', 'Jie Qin']","Field-inhomogeneity-induced relaxation of atomic spins confined in vapor cells with depolarizing walls is studied. In contrast to nuclear spins, such as noble-gas spins, which experience minimal polarization loss at cell walls, atomic spins in uncoated cells undergo randomization at the boundaries. This distinct boundary condition results in a varied dependence of the relaxation rate on the field gradient. By solving the Bloch-Torrey equation under fully depolarizing boundary conditions, we illustrate that the relaxation rate induced by field inhomogeneity is more pronounced for spins with a smaller original relaxation rate (in the absence of the inhomogeneous field). We establish an upper limit for the relaxation rate through calculations in the perturbation regime. Moreover, we connect it to the spin-exchange-relaxation-free magnetometers, demonstrating that its linewidth is most sensitive to inhomogeneous fields along the magnetometer's sensitive axis. Our theoretical result agrees with the experimental data for cells subjected to small pump power. However, deviations in larger input-power scenarios underscore the importance of considering pump field attenuation, which leads to uniformly distributed light shift that behaves as an inhomogeneous magnetic field."
https://arxiv.org/abs/2403.08299,2024-03-13,AutoDev: Automated AI-Driven Development,"['Michele Tufano', 'Anisha Agarwal', 'Jinu Jang', 'Roshanak Zilouchian Moghaddam', 'Neel Sundaresan']","The landscape of software development has witnessed a paradigm shift with the advent of AI-powered assistants, exemplified by GitHub Copilot. However, existing solutions are not leveraging all the potential capabilities available in an IDE such as building, testing, executing code, git operations, etc. Therefore, they are constrained by their limited capabilities, primarily focusing on suggesting code snippets and file manipulation within a chat-based interface. To fill this gap, we present AutoDev, a fully automated AI-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks. AutoDev enables users to define complex software engineering objectives, which are assigned to AutoDev's autonomous AI Agents to achieve. These AI agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations. They also have access to files, compiler output, build and testing logs, static analysis tools, and more. This enables the AI Agents to execute tasks in a fully automated manner with a comprehensive understanding of the contextual information required. Furthermore, AutoDev establishes a secure development environment by confining all operations within Docker containers. This framework incorporates guardrails to ensure user privacy and file security, allowing users to define specific permitted or restricted commands and operations within AutoDev. In our evaluation, we tested AutoDev on the HumanEval dataset, obtaining promising results with 91.5% and 87.8% of Pass@1 for code generation and test generation respectively, demonstrating its effectiveness in automating software engineering tasks while maintaining a secure and user-controlled development environment."
https://arxiv.org/abs/2403.08298,2024-03-13,Physics-Informed Deep Learning for Motion-Corrected Reconstruction of Quantitative Brain MRI,"['Hannah Eichhorn', 'Veronika Spieker', 'Kerstin Hammernik', 'Elisa Saks', 'Kilian Weiss', 'Christine Preibisch', 'Julia A. Schnabel']","We propose PHIMO, a physics-informed learning-based motion correction method tailored to quantitative MRI. PHIMO leverages information from the signal evolution to exclude motion-corrupted k-space lines from a data-consistent reconstruction. We demonstrate the potential of PHIMO for the application of T2* quantification from gradient echo MRI, which is particularly sensitive to motion due to its sensitivity to magnetic field inhomogeneities. A state-of-the-art technique for motion correction requires redundant acquisition of the k-space center, prolonging the acquisition. We show that PHIMO can detect and exclude intra-scan motion events and, thus, correct for severe motion artifacts. PHIMO approaches the performance of the state-of-the-art motion correction method, while substantially reducing the acquisition time by over 40%, facilitating clinical applicability. Our code is available at https://github.com/HannahEichhorn/PHIMO."
https://arxiv.org/abs/2403.08297,2024-03-13,Semi-Transparent Image Sensors for Eye-Tracking Applications,"['Gabriel Mercier', 'Emre O. Polat', 'Shengtai Shi', 'Shuchi Gupta', 'Gerasimos Konstantatos', 'Stijn Goossens', 'Frank H. L. Koppens']","Image sensors hold a pivotal role in society due to their ability to capture vast amounts of information. Traditionally, image sensors are opaque due to light absorption in both the pixels and the read-out electronics that are stacked on top of each other. Making image sensors visibly transparent would have a far-reaching impact in numerous areas such as human-computer interfaces, smart displays, and both augmented and virtual reality. In this paper, we present the development and analysis of the first semi-transparent image sensor and its applicability as an eye-tracking device. The device consists of an 8x8 array of semi-transparent photodetectors and electrodes disposed on a fully transparent substrate. Each pixel of the array has a size of 60 x 140 μm and an optical transparency of 85-95%. Pixels have a high sensitivity, with more than 90% of them showing a noise equivalent irradiance < 10-4 W/m2 for wavelengths of 637 nm. As the semi-transparent photodetectors have a large amount of built-in gain, the opaque read-out electronics can be placed far away from the detector array to ensure maximum transparency and fill factor. Indeed, the operation and appearance of transparent image sensors present a fundamental shift in how we think about cameras and imaging, as these devices can be concealed in plain sight."
https://arxiv.org/abs/2403.08296,2024-03-13,Quasinormability and property $(Ω)$ for spaces of smooth and ultradifferentiable vectors associated to Lie group representations,"['Andreas Debrouwere', 'Michiel Huttener', 'Jasson Vindas']","We show that the spaces of smooth and ultradifferentiable vectors associated to a Lie group representation on a Fréchet space $E$ is quasinormable if $E$ is so. A similar result is shown for the linear topological invariant $(Ω)$. In the ultradifferentiable case, our results particularly apply to spaces of Gevrey vectors of order $λ>1$ of Beurling type. As an application, we study the quasinormability and the property $(Ω)$ for a broad class of weighted spaces of smooth and ultradifferentiable functions on Lie groups."
https://arxiv.org/abs/2403.08295,2024-03-13,Gemma: Open Models Based on Gemini Research and Technology,"[' Gemma Team', 'Thomas Mesnard', 'Cassidy Hardin', 'Robert Dadashi', 'Surya Bhupatiraju', 'Shreya Pathak', 'Laurent Sifre', 'Morgane Rivière', 'Mihir Sanjay Kale', 'Juliette Love', 'Pouya Tafti', 'Léonard Hussenot', 'Aakanksha Chowdhery', 'Adam Roberts', 'Aditya Barua', 'Alex Botev', 'Alex Castro-Ros', 'Ambrose Slone', 'Amélie Héliou', 'Andrea Tacchetti', 'Anna Bulanova', 'Antonia Paterson', 'Beth Tsai', 'Bobak Shahriari', 'Charline Le Lan']","This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations."
https://arxiv.org/abs/2403.08294,2024-03-13,Attack Deterministic Conditional Image Generative Models for Diverse and Controllable Generation,"['Tianyi Chu', 'Wei Xing', 'Jiafu Chen', 'Zhizhong Wang', 'Jiakai Sun', 'Lei Zhao', 'Haibo Chen', 'Huaizhong Lin']","Existing generative adversarial network (GAN) based conditional image generative models typically produce fixed output for the same conditional input, which is unreasonable for highly subjective tasks, such as large-mask image inpainting or style transfer. On the other hand, GAN-based diverse image generative methods require retraining/fine-tuning the network or designing complex noise injection functions, which is computationally expensive, task-specific, or struggle to generate high-quality results. Given that many deterministic conditional image generative models have been able to produce high-quality yet fixed results, we raise an intriguing question: is it possible for pre-trained deterministic conditional image generative models to generate diverse results without changing network structures or parameters? To answer this question, we re-examine the conditional image generation tasks from the perspective of adversarial attack and propose a simple and efficient plug-in projected gradient descent (PGD) like method for diverse and controllable image generation. The key idea is attacking the pre-trained deterministic generative models by adding a micro perturbation to the input condition. In this way, diverse results can be generated without any adjustment of network structures or fine-tuning of the pre-trained models. In addition, we can also control the diverse results to be generated by specifying the attack direction according to a reference text or image. Our work opens the door to applying adversarial attack to low-level vision tasks, and experiments on various conditional image generation tasks demonstrate the effectiveness and superiority of the proposed method."
https://arxiv.org/abs/2403.08293,2024-03-13,Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale,"['Xiang Hu', 'Pengyu Ji', 'Qingyang Zhu', 'Wei Wu', 'Kewei Tu']","A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering both language understanding and language generation. Meanwhile, GPST also significantly outperforms existing unsupervised SLMs on left-to-right grammar induction, while holding a substantial acceleration on training."
https://arxiv.org/abs/2403.08292,2024-03-13,Weak Collocation Regression for Inferring Stochastic Dynamics with Lévy Noise,"['Liya Guo', 'Liwei Lu', 'Zhijun Zeng', 'Pipi Hu', 'Yi Zhu']","With the rapid increase of observational, experimental and simulated data for stochastic systems, tremendous efforts have been devoted to identifying governing laws underlying the evolution of these systems. Despite the broad applications of non-Gaussian fluctuations in numerous physical phenomena, the data-driven approaches to extracting stochastic dynamics with Lévy noise are relatively few. In this work, we propose a Weak Collocation Regression (WCR) to explicitly reveal unknown stochastic dynamical systems, i.e., the Stochastic Differential Equation (SDE) with both $α$-stable Lévy noise and Gaussian noise, from discrete aggregate data. This method utilizes the evolution equation of the probability distribution function, i.e., the Fokker-Planck (FP) equation. With the weak form of the FP equation, the WCR constructs a linear system of unknown parameters where all integrals are evaluated by Monte Carlo method with the observations. Then, the unknown parameters are obtained by a sparse linear regression. For a SDE with Lévy noise, the corresponding FP equation is a partial integro-differential equation (PIDE), which contains nonlocal terms, and is difficult to deal with. The weak form can avoid complicated multiple integrals. Our approach can simultaneously distinguish mixed noise types, even in multi-dimensional problems. Numerical experiments demonstrate that our method is accurate and computationally efficient."
https://arxiv.org/abs/2403.08291,2024-03-13,CleanAgent: Automating Data Standardization with LLM-based Agents,"['Danrui Qi', 'Jiannan Wang']","Data standardization is a crucial part in data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the code generation of LLM with concise API calls. We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. Then we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based agents to automate the data standardization process. With CleanAgent, data scientists need only provide their requirements once, allowing for a hands-free, automatic standardization process."
https://arxiv.org/abs/2403.08290,2024-03-13,A fast wavefield evaluation method using a modified proxy-surface accelerated interpolative decomposition for scattering problems in two dimensions,['Yasuhiro Matsumoto'],"This paper presents a fast wavefield evaluation method for wave scattering problems. The typical previous fast method is the fast multipole method (FMM). The FMM, however, requires the combined use of non-fast direct evaluations near the boundaries of scatterers. The proposed method is based on a modified proxy-surface method accelerated interpolative decomposition. It is, therefore, effective even if evaluation points are near the boundary and, moreover, is free from analytical expansion of kernel functions unlike FMM. The validness and effectiveness of the proposed method are shown by numerical examples."
https://arxiv.org/abs/2403.08289,2024-03-13,Influence of cholesterol on hydrogen-bond dynamics of water molecules in lipid-bilayer systems at varying temperatures,"['Kokoro Shikata', 'Kento Kasahara', 'Nozomi Morishita Watanabe', 'Hiroshi Umakoshi', 'Kang Kim', 'Nobuyuki Matubayasi']","Cholesterol (Chol) plays a crucial role in shaping the intricate physicochemical attributes of biomembranes, exerting considerable influence on water molecules proximal to the membrane interface. In this study, we conducted molecular dynamics simulations on the bilayers of two lipid species, dipalmitoyl phosphatidylcholine (DPPC) and palmitoyl sphingomyelin (PSM); they are distinct with respect to the structures of the hydrogen-bond (H-bond) acceptors. Our investigation focuses on the dynamic properties and H-bonds of water molecules in the lipid-membrane systems, with particular emphasis on the influence of Chol at varying temperatures. Notably, in the gel phase at 303 K, the presence of Chol extends the lifetimes of H-bonds of the oxygen atoms acting as H-bond acceptors within DPPC with water molecules by a factor of 1.5 to 2.5. In the liquid-crystalline phase at 323 K, on the other hand, H-bonding dynamics with lipid membranes remain largely unaffected by Chol. This observed shift in H-bonding states serves as a crucial key to unraveling the subtle control mechanisms governing water dynamics in lipid-membrane systems."
https://arxiv.org/abs/2403.08288,2024-03-13,Lithographically Defined Zerogap Strain Sensors,"['Mahsa Haddadi Moghaddam', 'Zhihao Wang', 'Daryll J. C Dalayoan', 'Daehwan Park', 'Hwanhee Kim', 'Sunghoon Im', 'Kyungbin Ji', 'Daeshik Kang', 'Bamadev Das', 'Dai Sik Kim']","Metal thin films on soft polymers provide a unique opportunity for resistance-based strain sensors. A mechanical mismatch between the conductive film and the flexible substrate causes cracks to open and close, changing the electrical resistance as a function of strain. However, the very randomness of the formation, shape, length, orientation, and distance between adjacent cracks limits the sensing range as well as repeatability. Herein, we present a breakthrough: the Zerogap Strain Sensor, whereby lithography eliminates randomness and violent tearing process inherent in conventional crack sensors and allows for short periodicity between gaps with gentle sidewall contacts, critical in high strain sensing enabling operation over an unprecedently wide range. Our sensor achieves a gauge factor of over 15,000 at εext=18%, the highest known value. With the uniform gaps of three-to-ten thousand nanometer widths characterized by periodicity and strain, this approach has far reaching implications for future strain sensors whose range is limited only by that of the flexible substrate, with non-violent operations that always remain below the tensile limit of the metal."
https://arxiv.org/abs/2403.08287,2024-03-13,Performance assessment of the effective core potentials under the Fermionic neural network: first and second row elements,"['Mengsa Wang', 'Yuzhi Zhou', 'Han Wang']","The rapid development of deep learning techniques has driven the emergence of a neural network-based variational Monte Carlo method (referred to as FermiNet), which has manifested high accuracy and strong predictive power in the electronic structure calculations of atoms, molecules as well as some periodic systems. Recently, the implementation of the effective core potential (ECP) scheme in it further facilitates more efficient calculations in practice. But there still lack a more comprehensive assessment on the ECP's performance under the FermiNet. In this work, we set sail to fill this gap by conducting extensive tests on the first two row elements regarding their atomic spectral and molecular properties. Our major finding is that in general the qualities of ECPs have been correctly reflected under the FermiNet, and a more recently built ECP, ccECP, seems to prevail on the overall performance. Meanwhile, a variation of the transferability between different ECPs has also been observed in the results of the hydrides. On the other hand, the high accuracy of the all-electron calculations is hindered by the absence of relativistic effects as one gets to the second row. Meanwhile the numerical instabilities are more often seen in the all-electron calculations, which could be another source of errors. Finally, with more in-depth discussions, we generate possible directions for developing and improving the FermiNet in the near future."
https://arxiv.org/abs/2403.08286,2024-03-13,Optical-Cavity Manipulation Strategies of Conical Intersections Mediated Singlet Fission Systems,"['Kewei Sun', 'Maxim Gelin', 'Kaijun Shen', 'Yang Zhao']","We offer a theoretical perspective on simulation and engineering of polaritonic conical-intersection-driven singlet-fission (SF) materials. Using rubrene as an example and applying the numerically accurate Davydov-Ansatz methodology, we derive dynamic and spectroscopic responses of the system and demonstrate key mechanisms capable of SF manipulation, viz. cavity-induced enhancement/weakening/suppression of SF, population localization on the singlet state via engineering of the cavity-mode excitation, polaron/polariton decoupling, collective enhancement of SF. We outline unsolved problems and challenges in the field, and share our views on the development of the future lines of research. We emphasize the significance of careful modeling of cascades of polaritonic conical intersections in high excitation manifolds and envisage that collective geometric phase effects may remarkably affect the SF dynamics and yield. We argue that microscopic interpretation of the main regulatory mechanisms of the polaritonic conical-intersection-driven SF can substantially deepen our understanding of this process, thereby providing novel ideas and solutions for improving conversion efficiency in photovoltaics."
https://arxiv.org/abs/2403.08285,2024-03-13,Experimental observation of gapped shear waves and liquid-like to gas-like dynamical crossover in active granular matter,"['Cunyuan Jiang', 'Zihan Zheng', 'Yangrui Chen', 'Matteo Baggioli', 'Jie Zhang']","Unlike crystalline solids, liquids do not exhibit long-range order. Their atoms undergo frequent structural rearrangements, resulting in the long-wavelength dynamics of shear fluctuations in liquids being diffusive, rather than propagating waves, as observed in crystals. When considering shorter time and length scales, molecular dynamics simulations and theoretical propositions suggest that collective shear excitations in liquids display a gap in wave-vector space, referred to as the $k$-gap. Above this gap, solid-like transverse waves re-emerge. However, direct experimental verification of this phenomenon in classical liquids remains elusive, with the only documented evidence from studies in two-dimensional dusty plasmas. Active granular systems provide a novel platform for exploring the emergence of collective dynamics and showcasing a rich interplay of complex phases and phenomena. Our study focuses on bi-disperse active Brownian vibrators. Through measurements of the pair correlation functions, mean square displacements, velocity auto-correlation functions, vibrational density of states, and a detailed analysis of microscopic atomic motion, we demonstrate that this active system exhibits both gas-like and liquid-like phases, depending on the packing fraction, despite pure hard-disk-like repulsive interactions. Notably, within the granular liquid-like phase, we experimentally validate the existence of a $k$-gap in the dispersion of transverse excitations. This gap becomes more significant with a decrease in packing fraction and disappears into the gas phase, aligning with theoretical expectations. Our results offer a direct experimental confirmation of the $k$-gap phenomenon, extending its relevance beyond classical thermal liquids to active granular systems, and reveal the existence of intriguing similarities between the physics of active granular matter and supercritical fluids."
https://arxiv.org/abs/2403.08284,2024-03-13,MGIC: A Multi-Label Gradient Inversion Attack based on Canny Edge Detection on Federated Learning,"['Can Liu', 'Jin Wang']","As a new distributed computing framework that can protect data privacy, federated learning (FL) has attracted more and more attention in recent years. It receives gradients from users to train the global model and releases the trained global model to working users. Nonetheless, the gradient inversion (GI) attack reflects the risk of privacy leakage in federated learning. Attackers only need to use gradients through hundreds of thousands of simple iterations to obtain relatively accurate private data stored on users' local devices. For this, some works propose simple but effective strategies to obtain user data under a single-label dataset. However, these strategies induce a satisfactory visual effect of the inversion image at the expense of higher time costs. Due to the semantic limitation of a single label, the image obtained by gradient inversion may have semantic errors. We present a novel gradient inversion strategy based on canny edge detection (MGIC) in both the multi-label and single-label datasets. To reduce semantic errors caused by a single label, we add new convolution layers' blocks in the trained model to obtain the image's multi-label. Through multi-label representation, serious semantic errors in inversion images are reduced. Then, we analyze the impact of parameters on the difficulty of input image reconstruction and discuss how image multi-subjects affect the inversion performance. Our proposed strategy has better visual inversion image results than the most widely used ones, saving more than 78% of time costs in the ImageNet dataset."
https://arxiv.org/abs/2403.08283,2024-03-13,Optimized Detection and Classification on GTRSB: Advancing Traffic Sign Recognition with Convolutional Neural Networks,"['Dhruv Toshniwal', 'Saurabh Loya', 'Anuj Khot', 'Yash Marda']","In the rapidly evolving landscape of transportation, the proliferation of automobiles has made road traffic more complex, necessitating advanced vision-assisted technologies for enhanced safety and navigation. These technologies are imperative for providing critical traffic sign information, influencing driver behavior, and supporting vehicle control, especially for drivers with disabilities and in the burgeoning field of autonomous vehicles. Traffic sign detection and recognition have emerged as key areas of research due to their essential roles in ensuring road safety and compliance with traffic regulations. Traditional computer vision methods have faced challenges in achieving optimal accuracy and speed due to real-world variabilities. However, the advent of deep learning and Convolutional Neural Networks (CNNs) has revolutionized this domain, offering solutions that significantly surpass previous capabilities in terms of speed and reliability. This paper presents an innovative approach leveraging CNNs that achieves an accuracy of nearly 96\%, highlighting the potential for even greater precision through advanced localization techniques. Our findings not only contribute to the ongoing advancement of traffic sign recognition technology but also underscore the critical impact of these developments on road safety and the future of autonomous driving."
https://arxiv.org/abs/2403.08282,2024-03-13,Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation,"['Zhonghan Zhao', 'Kewei Chen', 'Dongxu Guo', 'Wenhao Chai', 'Tian Ye', 'Yanting Zhang', 'Gaoang Wang']","Navigating complex environments in Minecraft poses significant challenges for multi-agent systems due to the game's dynamic and unpredictable open-world setting. Agents need to interact with the environment and coordinate their actions with other agents to achieve common objectives. However, traditional approaches often struggle to efficiently manage inter-agent communication and task distribution, which are crucial for effective multi-agent navigation. Furthermore, processing and integrating multi-modal information (such as visual, textual, and auditory data) is essential for agents to fully comprehend their goals and navigate the environment successfully. To address this issue, we design the HAS framework to auto-organize groups of LLM-based agents to complete Navigation tasks. In our approach, we devise a hierarchical auto-organizing navigation system, which is characterized by 1) a hierarchical system for multi-agent organization, ensuring centralized planning and decentralized execution; 2) an auto-organizing and intra-communication mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal information platform, facilitating multi-modal perception to perform the three navigation tasks with one system. To assess organizational behavior, we design a series of navigation tasks in the Minecraft environment, which includes searching and exploring. We aim to develop embodied organizations that push the boundaries of embodied AI, moving it towards a more human-like organizational structure."
https://arxiv.org/abs/2403.08281,2024-03-13,"Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models","['Ning Ding', 'Yulin Chen', 'Ganqu Cui', 'Xingtai Lv', 'Ruobing Xie', 'Bowen Zhou', 'Zhiyuan Liu', 'Maosong Sun']","Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, UltraChat 2, which includes text, code, and mathematical content. This dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain. Experiments show that our model could simultaneously achieve mastery of the three crucial domains."
https://arxiv.org/abs/2403.08280,2024-03-13,Pre-examinations Improve Automated Metastases Detection on Cranial MRI,"['Katerina Deike-Hofmann', 'Dorottya Dancs', 'Daniel Paech', 'Heinz-Peter Schlemmer', 'Klaus Maier-Hein', 'Philipp Bäumer', 'Alexander Radbruch', 'Michael Götz']","Materials and methods: First, a dual-time approach was assessed, for which the CNN was provided sequences of the MRI that initially depicted new MM (diagnosis MRI) as well as of a prediagnosis MRI: inclusion of only contrast-enhanced T1-weighted images (CNNdual_ce) was compared with inclusion of also the native T1-weighted images, T2-weighted images, and FLAIR sequences of both time points (CNNdual_all).Second, results were compared with the corresponding single time approaches, in which the CNN was provided exclusively the respective sequences of the diagnosis MRI.Casewise diagnostic performance parameters were calculated from 5-fold cross-validation."
https://arxiv.org/abs/2403.08279,2024-03-13,On the conservation laws and the structure of the nonlinearity for SQG and its generalizations,"['Philip Isett', 'Andrew Ma']","Using a new definition for the nonlinear term, we prove that all weak solutions to the SQG equation (and mSQG) conserve the angular momentum. This result is new for the weak solutions of [Resnick, '95] and rules out the possibility of anomalous dissipation of angular momentum. We also prove conservation of the Hamiltonian under conjecturally optimal assumptions, sharpening a well-known criterion of [Cheskidov-Constantin-Friedlander-Shvydkoy, '08]. Moreover, we show that our new estimate for the nonlinearity is optimal and that it characterizes the mSQG nonlinearity uniquely among active scalar nonlinearities with a scaling symmetry."
https://arxiv.org/abs/2403.08278,2024-03-13,Point-to-set Principle and Constructive Dimension Faithfulness,"['Satyadev Nandakumar', 'Subin Pulari', 'Akhil S']","We introduce a constructive analogue of $Φ$-dimension, a notion of Hausdorff dimension developed using a restricted class of coverings of a set. A class of coverings $Φ$ is said to be ""faithful"" to Hausdorff dimension if the $Φ$-dimension and Hausdorff dimension coincide for every set."
https://arxiv.org/abs/2403.08277,2024-03-13,VIGFace: Virtual Identity Generation Model for Face Image Synthesis,"['Minsoo Kim', 'Min-Cheol Sagong', 'Gi Pyo Nam', 'Junghyun Cho', 'Ig-Jae Kim']","Deep learning-based face recognition continues to face challenges due to its reliance on huge datasets obtained from web crawling, which can be costly to gather and raise significant real-world privacy concerns. To address this issue, we propose VIGFace, a novel framework capable of generating synthetic facial images. Initially, we train the face recognition model using a real face dataset and create a feature space for both real and virtual IDs where virtual prototypes are orthogonal to other prototypes. Subsequently, we generate synthetic images by using the diffusion model based on the feature space. Our proposed framework provides two significant benefits. Firstly, it allows for creating virtual facial images without concerns about portrait rights, guaranteeing that the generated virtual face images are clearly differentiated from existing individuals. Secondly, it serves as an effective augmentation method by incorporating real existing images. Further experiments demonstrate the efficacy of our framework, achieving state-of-the-art results from both perspectives without any external data."
https://arxiv.org/abs/2403.08276,2024-03-13,Synergy between Spin and Orbital Angular Momenta on a Möbius Strip,"['Lei Liu', 'Xiao-Chen Sun', 'Yuan Tian', 'Xiujuan Zhang', 'Ming-Hui Lu', 'Yan-Feng Chen']","Spin and orbital angular momenta are fundamental physical characteristics described by polarization and spatial degrees of freedom, respectively. Polarization is a feature of vector fields while spatial phase gradient determines the orbital angular momentum ubiquitous to any scalar field. Common wisdom treats these two degrees of freedom as distinct and independent principles to manipulate wave propagations. Here, we demonstrate their synergy. This is achieved by introducing two orthogonal $p$-orbitals as eigenbases, whose spatial modal features are exploited to generate orbital angular momenta and the associated orbital orientations provide means to simultaneously manipulate polarizations. Through periodic modulation and directional coupling, we realize a full cyclic evolution of the synchronized and synergized spin-orbital angular momenta. Remarkably, this evolution acquires a nontrivial geometric phase, leading to its representation on a Möbius strip. Experimentally, an acoustic cavity array is designed, whose dipole resonances precisely mimic the $p$-orbitals. The acoustic waves, uniquely, see the pressure (scalar) field as a spatial feature and carry an intrinsic polarization defined by the velocity (vector) field, serving as an ideal platform to observe the synergy of spin and orbital angular momenta. Based on such a property, we further showcase a spin-orbital-Hall effect, highlighting the intricate locking of handedness, directionality, spin density and spatial mode profile. Our study unveils a fundamental connection between spin and orbital angular momenta, promising avenues for novel applications in information coding and high-capacity communications."
https://arxiv.org/abs/2403.08275,2024-03-13,Fully discrete finite difference schemes for the Fractional Korteweg-de Vries equation,"['Mukul Dwivedi', 'Tanmay Sarkar']","In this paper, we present and analyze fully discrete finite difference schemes designed for solving the initial value problem associated with the fractional Korteweg-de Vries (KdV) equation involving the fractional Laplacian. We design the scheme by introducing the discrete fractional Laplacian operator which is consistent with the continuous operator, and posses certain properties which are instrumental for the convergence analysis. Assuming the initial data (u_0 \in H^{1+α}(\mathbb{R})), where (α\in [1,2)), our study establishes the convergence of the approximate solutions obtained by the fully discrete finite difference schemes to a classical solution of the fractional KdV equation. Theoretical results are validated through several numerical illustrations for various values of fractional exponent $α$. Furthermore, we demonstrate that the Crank-Nicolson finite difference scheme preserves the inherent conserved quantities along with the improved convergence rates."
https://arxiv.org/abs/2403.08274,2024-03-13,Unique electronic and optical properties of stacking-modulated bilayer graphene under external magnetic fields,"['Chiun-Yan Lin', 'Da-We Weng', 'Chih-Wei Chiu', 'Godfrey Gumbs']","This study delves into the magneto-electronic and magneto-optical properties of stacking-modulated bilayer graphene. By manipulating domain walls (DWs) across AB-BA domains periodically, we unveil oscillatory Landau subbands and the associated optical excitations. The DWs act as periodic potentials, yielding fascinating 1D spectral features. Our exploration reveals 1D phenomena localized to Bernal stacking, DW regions, and stacking boundaries, highlighting the intriguing formation of Landau state quantization influenced by the commensuration between the magnetic length and the system. The stable quantized localization within different regions leads to the emergence of unconventional quantized subbands. This study provides valuable insights into the essential properties of stacking-modulated bilayer graphene."
https://arxiv.org/abs/2403.08273,2024-03-13,LiqD: A Dynamic Liquid Level Detection Model under Tricky Small Containers,"['Yukun Ma', 'Zikun Mao']","In daily life and industrial production, it is crucial to accurately detect changes in liquid level in containers. Traditional contact measurement methods have some limitations, while emerging non-contact image processing technology shows good application prospects. This paper proposes a container dynamic liquid level detection model based on U^2-Net. This model uses the SAM model to generate an initial data set, and then evaluates and filters out high-quality pseudo-label images through the SemiReward framework to build an exclusive data set. The model uses U^2-Net to extract mask images of containers from the data set, and uses morphological processing to compensate for mask defects. Subsequently, the model calculates the grayscale difference between adjacent video frame images at the same position, segments the liquid level change area by setting a difference threshold, and finally uses a lightweight neural network to classify the liquid level state. This approach not only mitigates the impact of intricate surroundings, but also reduces the demand for training data, showing strong robustness and versatility. A large number of experimental results show that the proposed model can effectively detect the dynamic liquid level changes of the liquid in the container, providing a novel and efficient solution for related fields."
https://arxiv.org/abs/2403.08272,2024-03-13,RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education,"['Jieun Han', 'Haneul Yoo', 'Junho Myung', 'Minsun Kim', 'Tak Yeon Lee', 'So-Yeon Ahn', 'Alice Oh']","The integration of generative AI in education is expanding, yet empirical analyses of large-scale and real-world interactions between students and AI systems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE for University), a dataset sourced from a semester-long experiment with 212 college students in English as Foreign Language (EFL) writing courses. During the study, students engaged in dialogues with ChatGPT to revise their essays. RECIPE4U includes comprehensive records of these interactions, including conversation logs, students' intent, students' self-rated satisfaction, and students' essay edit histories. In particular, we annotate the students' utterances in RECIPE4U with 13 intention labels based on our coding schemes. We establish baseline results for two subtasks in task-oriented dialogue systems within educational contexts: intent detection and satisfaction estimation. As a foundational step, we explore student-ChatGPT interaction patterns through RECIPE4U and analyze them by focusing on students' dialogue, essay data statistics, and students' essay edits. We further illustrate potential applications of RECIPE4U dataset for enhancing the incorporation of LLMs in educational frameworks. RECIPE4U is publicly available at https://zeunie.github.io/RECIPE4U/."
https://arxiv.org/abs/2403.08271,2024-03-13,Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification,"['Long Lan', 'Fengxiang Wang', 'Shuyan Li', 'Xiangtao Zheng', 'Zengmao Wang', 'Xinwang Liu']","Fine-grained ship classification in remote sensing (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods. Recent advancements in large pre-trained Vision-Language Models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To address these issues, we introduce a novel prompt tuning technique that employs a hierarchical, multi-granularity prompt design. Our approach integrates remote sensing ship priors through bias terms, learned from a small trainable network. This strategy enhances the model's generalization capabilities while improving its ability to discern intricate backgrounds and learn discriminative ship features. Furthermore, we contribute to the field by introducing a comprehensive dataset, FGSCM-52, significantly expanding existing datasets with more extensive data and detailed annotations for less common ship classes. Extensive experimental evaluations demonstrate the superiority of our proposed method over current state-of-the-art techniques. The source code will be made publicly available."
https://arxiv.org/abs/2403.08270,2024-03-13,Identity-aware Dual-constraint Network for Cloth-Changing Person Re-identification,"['Peini Guo', 'Mengyuan Liu', 'Hong Liu', 'Ruijia Fan', 'Guoquan Wang', 'Bin He']","Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify the target person in more realistic surveillance scenarios, where pedestrians usually change their clothing. Despite great progress, limited cloth-changing training samples in existing CC-ReID datasets still prevent the model from adequately learning cloth-irrelevant features. In addition, due to the absence of explicit supervision to keep the model constantly focused on cloth-irrelevant areas, existing methods are still hampered by the disruption of clothing variations. To solve the above issues, we propose an Identity-aware Dual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the model extract cloth-irrelevant clues, we propose a Clothes Diversity Augmentation (CDA), which generates more realistic cloth-changing samples by enriching the clothing color while preserving the texture. In addition, a Multi-scale Constraint Block (MCB) is designed, which extracts fine-grained identity-related features and effectively transfers cloth-irrelevant knowledge. Moreover, a Counterfactual-guided Attention Module (CAM) is presented, which learns cloth-irrelevant features from channel and space dimensions and utilizes the counterfactual intervention for supervising the attention map to highlight identity-related regions. Finally, a Semantic Alignment Constraint (SAC) is designed to facilitate high-level semantic feature interaction. Comprehensive experiments on four CC-ReID datasets indicate that our method outperforms prior state-of-the-art approaches."
https://arxiv.org/abs/2403.08269,2024-03-13,A posteriori error estimates for the Generalized Burgers-Huxley equation with weakly singular kernels,"['Sumit Mahajan', 'Arbaz Khan']","This paper explores the residual based a posteriori error estimations for the generalized Burgers-Huxley equation (GBHE) featuring weakly singular kernels. Initially, we present a reliable and efficient error estimator for both the stationary GBHE and the semi-discrete GBHE with memory, utilizing the discontinuous Galerkin finite element method (DGFEM) in spatial dimensions. Additionally, employing backward Euler and Crank Nicolson discretization in the temporal domain and DGFEM in spatial dimensions, we introduce an estimator for the fully discrete GBHE, taking into account the influence of past history. The paper also establishes optimal $L^2$ error estimates for both the stationary GBHE and GBHE. Ultimately, we validate the effectiveness of the proposed error estimator through numerical results, demonstrating its efficacy in an adaptive refinement strategy."
https://arxiv.org/abs/2403.08268,2024-03-13,Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts,"['Yue Ma', 'Yingqing He', 'Hongfa Wang', 'Andong Wang', 'Chenyang Qi', 'Chengfei Cai', 'Xiu Li', 'Zhifeng Li', 'Heung-Yeung Shum', 'Wei Liu', 'Qifeng Chen']","Despite recent advances in image-to-video generation, better controllability and local animation are less explored. Most existing image-to-video methods are not locally aware and tend to move the entire scene. However, human artists may need to control the movement of different objects or regions. Additionally, current I2V methods require users not only to describe the target motion but also to provide redundant detailed descriptions of frame contents. These two issues hinder the practical utilization of current I2V tools. In this paper, we propose a practical framework, named Follow-Your-Click, to achieve image animation with a simple user click (for specifying what to move) and a short motion prompt (for specifying how to move). Technically, we propose the first-frame masking strategy, which significantly improves the video generation quality, and a motion-augmented module equipped with a short motion prompt dataset to improve the short prompt following abilities of our model. To further control the motion speed, we propose flow-based motion magnitude control to control the speed of target movement more precisely. Our framework has simpler yet precise user control and better generation performance than previous methods. Extensive experiments compared with 7 baselines, including both commercial tools and research methods on 8 metrics, suggest the superiority of our approach. Project Page: https://follow-your-click.github.io/"
https://arxiv.org/abs/2403.08267,2024-03-13,SNOW-SCA: ML-assisted Side-Channel Attack on SNOW-V,"['Harshit Saurabh', 'Anupam Golder', 'Samarth Shivakumar Titti', 'Suparna Kundu', 'Chaoyun Li', 'Angshuman Karmakar', 'Debayan Das']","This paper presents SNOW-SCA, the first power side-channel analysis (SCA) attack of a 5G mobile communication security standard candidate, SNOW-V, running on a 32-bit ARM Cortex-M4 microcontroller. First, we perform a generic known-key correlation (KKC) analysis to identify the leakage points. Next, a correlation power analysis (CPA) attack is performed, which reduces the attack complexity to two key guesses for each key byte. The correct secret key is then uniquely identified utilizing linear discriminant analysis (LDA). The profiled SCA attack with LDA achieves 100% accuracy after training with $<200$ traces, which means the attack succeeds with just a single trace. Overall, using the \textit{combined CPA and LDA attack} model, the correct secret key byte is recovered with <50 traces collected using the ChipWhisperer platform. The entire 256-bit secret key of SNOW-V can be recovered incrementally using the proposed SCA attack. Finally, we suggest low-overhead countermeasures that can be used to prevent these SCA attacks."
https://arxiv.org/abs/2403.08266,2024-03-13,Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models,"['Jian Lin', 'Xueting Liu', 'Chengze Li', 'Minshan Xie', 'Tien-Tsin Wong']","While manga is a popular entertainment form, creating manga is tedious, especially adding screentones to the created sketch, namely manga screening. Unfortunately, there is no existing method that tailors for automatic manga screening, probably due to the difficulty of generating high-quality shaded high-frequency screentones. The classic manga screening approaches generally require user input to provide screentone exemplars or a reference manga image. The recent deep learning models enables the automatic generation by learning from a large-scale dataset. However, the state-of-the-art models still fail to generate high-quality shaded screentones due to the lack of a tailored model and high-quality manga training data. In this paper, we propose a novel sketch-to-manga framework that first generates a color illustration from the sketch and then generates a screentoned manga based on the intensity guidance. Our method significantly outperforms existing methods in generating high-quality manga with shaded high-frequency screentones."
https://arxiv.org/abs/2403.08265,2024-03-13,Random Search as a Baseline for Sparse Neural Network Architecture Search,['Rezsa Farahani'],"Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency. This has motivated a number of works to learn, induce, or search for high performing sparse networks. While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods. In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations. We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape. We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the same sparsity levels. We observe that for this architecture search task, initialized sparse networks found by Random Search neither perform better nor converge more efficiently than their random counterparts. Thus we conclude that Random Search may be viewed as a suitable neutral baseline for sparsity search methods."
https://arxiv.org/abs/2403.08264,2024-03-13,"GPT, Ontology, and CAABAC: A Tripartite Personalized Access Control Model Anchored by Compliance, Context and Attribute","['Raza Nowrozy', 'Khandakar Ahmed', 'Hua Wang']","As digital healthcare evolves, the security of electronic health records (EHR) becomes increasingly crucial. This study presents the GPT-Onto-CAABAC framework, integrating Generative Pretrained Transformer (GPT), medical-legal ontologies and Context-Aware Attribute-Based Access Control (CAABAC) to enhance EHR access security. Unlike traditional models, GPT-Onto-CAABAC dynamically interprets policies and adapts to changing healthcare and legal environments, offering customized access control solutions. Through empirical evaluation, this framework is shown to be effective in improving EHR security by accurately aligning access decisions with complex regulatory and situational requirements. The findings suggest its broader applicability in sectors where access control must meet stringent compliance and adaptability standards."
https://arxiv.org/abs/2403.08263,2024-03-13,Generic autonomous system approach to interacting dark energy models,"['Parth Shah', 'Gauranga C. Samanta', 'Kazuharu Bamba', 'R. Myrzakulov']","We explore an autonomous system analysis of dark energy models with interactions between dark energy and cold dark matter in a general systematic approach to cosmological fluids. We investigate two types of models such as local and non-local ones. In particular, a local form of interaction is directly proportional to only the energy density, while a non-local interaction is directly proportional to the energy density as well as the Hubble parameter. As a consequence, it is explicitly demonstrated that in both cases there exist the stability points in terms of cosmological parameters. This work aims at obtaining acceleration and stability using interaction models without modifying the matter or geometric component of the Universe."
https://arxiv.org/abs/2403.08262,2024-03-13,BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image,"['Minje Kim', 'Tae-Kyun Kim']","Creating personalized hand avatars is important to offer a realistic experience to users on AR / VR platforms. While most prior studies focused on reconstructing 3D hand shapes, some recent work has tackled the reconstruction of hand textures on top of shapes. However, these methods are often limited to capturing pixels on the visible side of a hand, requiring diverse views of the hand in a video or multiple images as input. In this paper, we propose a novel method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the first end-to-end trainable method for relightable, pose-free texture reconstruction of two interacting hands taking only a single RGB image, by three novel components: 1)\ bi-directional (left $\leftrightarrow$ right) texture reconstruction using the texture symmetry of left / right hands, 2) utilizing a texture parametric model for hand texture recovery, and 3)\ the overall coarse-to-fine stage pipeline for reconstructing personalized texture of two interacting hands. BiTT first estimates the scene light condition and albedo image from an input image, then reconstructs the texture of both hands through the texture parametric model and bi-directional texture reconstructor. In experiments using InterHand2.6M and RGB2Hands datasets, our method significantly outperforms state-of-the-art hand texture reconstruction methods quantitatively and qualitatively. The code is available at https://github.com/yunminjin2/BiTT"
https://arxiv.org/abs/2403.08261,2024-03-13,CoroNetGAN: Controlled Pruning of GANs via Hypernetworks,"['Aman Kumar', 'Khushboo Anand', 'Shubham Mandloi', 'Ashutosh Mishra', 'Avinash Thakur', 'Neeraj Kasera', 'Prathosh A P']","Generative Adversarial Networks (GANs) have proven to exhibit remarkable performance and are widely used across many generative computer vision applications. However, the unprecedented demand for the deployment of GANs on resource-constrained edge devices still poses a challenge due to huge number of parameters involved in the generation process. This has led to focused attention on the area of compressing GANs. Most of the existing works use knowledge distillation with the overhead of teacher dependency. Moreover, there is no ability to control the degree of compression in these methods. Hence, we propose CoroNet-GAN for compressing GAN using the combined strength of differentiable pruning method via hypernetworks. The proposed method provides the advantage of performing controllable compression while training along with reducing training time by a substantial factor. Experiments have been done on various conditional GAN architectures (Pix2Pix and CycleGAN) to signify the effectiveness of our approach on multiple benchmark datasets such as Edges-to-Shoes, Horse-to-Zebra and Summer-to-Winter. The results obtained illustrate that our approach succeeds to outperform the baselines on Zebra-to-Horse and Summer-to-Winter achieving the best FID score of 32.3 and 72.3 respectively, yielding high-fidelity images across all the datasets. Additionally, our approach also outperforms the state-of-the-art methods in achieving better inference time on various smart-phone chipsets and data-types making it a feasible solution for deployment on edge devices."
https://arxiv.org/abs/2403.08260,2024-03-13,"Understanding Reader Takeaways in Thematic Maps Under Varying Text, Detail, and Spatial Autocorrelation","['Arlen Fan', 'Fan Lei', 'Michelle Mancenido', 'Alan MacEachren', 'Ross Maciejewski']","Maps are crucial in conveying geospatial data in diverse contexts such as news and scientific reports. This research, utilizing thematic maps, probes deeper into the underexplored intersection of text framing and map types in influencing map interpretation. In this work, we conducted experiments to evaluate how textual detail and semantic content variations affect the quality of insights derived from map examination. We also explored the influence of explanatory annotations across different map types (e.g., choropleth, hexbin, isarithmic), base map details, and changing levels of spatial autocorrelation in the data. From two online experiments with $N=103$ participants, we found that annotations, their specific attributes, and map type used to present the data significantly shape the quality of takeaways. Notably, we found that the effectiveness of annotations hinges on their contextual integration. These findings offer valuable guidance to the visualization community for crafting impactful thematic geospatial representations."
https://arxiv.org/abs/2403.08259,2024-03-13,The trace operator of quasi-plurisubharmonic functions on compact Kähler manifolds,"['Tamás Darvas', 'Mingchen Xia']","We introduce the trace operator for quasi-plurisubharmonic functions on compact Kähler manifolds, allowing to study the singularities of such functions along submanifolds where their generic Lelong numbers vanish. Using this construction we obtain novel Ohsawa--Takegoshi extension theorems and give applications to restricted volumes of big line bundles."
https://arxiv.org/abs/2403.08258,2024-03-13,Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition,"['Wenjing Zhu', 'Sining Sun', 'Changhao Shan', 'Peng Fan', 'Qing Yang']","Conformer-based attention models have become the de facto backbone model for Automatic Speech Recognition tasks. A blank symbol is usually introduced to align the input and output sequences for CTC or RNN-T models. Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism. In this work, we propose a ""Skip-and-Recover"" Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously. Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring. The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output. Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile, the model can achieve better recognition accuracy and faster inference speed than recent baseline models. Our code is open-sourced and available online."
https://arxiv.org/abs/2403.08257,2024-03-13,Reconciling Conflicting Data Curation Actions: Transparency Through Argumentation,"['Yilin Xia', 'Shawn Bowers', 'Lan Li', 'Bertram Ludäscher']","We propose a new approach for modeling and reconciling conflicting data cleaning actions. Such conflicts arise naturally in collaborative data curation settings where multiple experts work independently and then aim to put their efforts together to improve and accelerate data cleaning. The key idea of our approach is to model conflicting updates as a formal \emph{argumentation framework}(AF). Such argumentation frameworks can be automatically analyzed and solved by translating them to a logic program $P_{AF}$ whose declarative semantics yield a transparent solution with many desirable properties, e.g., uncontroversial updates are accepted, unjustified ones are rejected, and the remaining ambiguities are exposed and presented to users for further analysis. After motivating the problem, we introduce our approach and illustrate it with a detailed running example introducing both well-founded and stable semantics to help understand the AF solutions. We have begun to develop open source tools and Jupyter notebooks that demonstrate the practicality of our approach. In future work we plan to develop a toolkit for conflict resolution that can be used in conjunction with OpenRefine, a popular interactive data cleaning tool."
https://arxiv.org/abs/2403.08256,2024-03-13,IG-FIQA: Improving Face Image Quality Assessment through Intra-class Variance Guidance robust to Inaccurate Pseudo-Labels,"['Minsoo Kim', 'Gi Pyo Nam', 'Haksub Kim', 'Haesol Park', 'Ig-Jae Kim']","In the realm of face image quality assesment (FIQA), method based on sample relative classification have shown impressive performance. However, the quality scores used as pseudo-labels assigned from images of classes with low intra-class variance could be unrelated to the actual quality in this method. To address this issue, we present IG-FIQA, a novel approach to guide FIQA training, introducing a weight parameter to alleviate the adverse impact of these classes. This method involves estimating sample intra-class variance at each iteration during training, ensuring minimal computational overhead and straightforward implementation. Furthermore, this paper proposes an on-the-fly data augmentation methodology for improved generalization performance in FIQA. On various benchmark datasets, our proposed method, IG-FIQA, achieved novel state-of-the-art (SOTA) performance."
https://arxiv.org/abs/2403.08255,2024-03-13,Make Me Happier: Evoking Emotions Through Image Diffusion Models,"['Qing Lin', 'Jingfeng Zhang', 'Yew Soon Ong', 'Mengmi Zhang']","Despite the rapid progress in image generation, emotional image editing remains under-explored. The semantics, context, and structure of an image can evoke emotional responses, making emotional image editing techniques valuable for various real-world applications, including treatment of psychological disorders, commercialization of products, and artistic design. For the first time, we present a novel challenge of emotion-evoked image generation, aiming to synthesize images that evoke target emotions while retaining the semantics and structures of the original scenes. To address this challenge, we propose a diffusion model capable of effectively understanding and editing source images to convey desired emotions and sentiments. Moreover, due to the lack of emotion editing datasets, we provide a unique dataset consisting of 340,000 pairs of images and their emotion annotations. Furthermore, we conduct human psychophysics experiments and introduce four new evaluation metrics to systematically benchmark all the methods. Experimental results demonstrate that our method surpasses all competitive baselines. Our diffusion model is capable of identifying emotional cues from original images, editing images that elicit desired emotions, and meanwhile, preserving the semantic structure of the original images. All code, model, and data will be made public."
https://arxiv.org/abs/2403.08254,2024-03-13,"Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects","['Na Li', 'Chunyi Zhou', 'Yansong Gao', 'Hui Chen', 'Anmin Fu', 'Zhi Zhang', 'Yu Shui']","Personal digital data is a critical asset, and governments worldwide have enforced laws and regulations to protect data privacy. Data users have been endowed with the right to be forgotten of their data. In the course of machine learning (ML), the forgotten right requires a model provider to delete user data and its subsequent impact on ML models upon user requests. Machine unlearning emerges to address this, which has garnered ever-increasing attention from both industry and academia. While the area has developed rapidly, there is a lack of comprehensive surveys to capture the latest advancements. Recognizing this shortage, we conduct an extensive exploration to map the landscape of machine unlearning including the (fine-grained) taxonomy of unlearning algorithms under centralized and distributed settings, debate on approximate unlearning, verification and evaluation metrics, challenges and solutions for unlearning under different applications, as well as attacks targeting machine unlearning. The survey concludes by outlining potential directions for future research, hoping to serve as a guide for interested scholars."
https://arxiv.org/abs/2403.08253,2024-03-13,Explicit radial basis function Runge-Kutta methods,"['Jiaxi Gu', 'Xinjuan Chen', 'Jae-Hun Jung']","The aim of this paper is to design the explicit radial basis function (RBF) Runge-Kutta methods for the initial value problem. We construct the two-, three- and four-stage RBF Runge-Kutta methods based on the Gaussian RBF Euler method with the shape parameter, where the analysis of the local truncation error shows that the s-stage RBF Runge-Kutta method could formally achieve order s+1. The proof for the convergence of those RBF Runge-Kutta methods follows. We then plot the stability region of each RBF Runge-Kutta method proposed and compare with the one of the correspondent Runge-Kutta method. Numerical experiments are provided to exhibit the improved behavior of the RBF Runge-Kutta methods over the standard ones."
https://arxiv.org/abs/2403.08252,2024-03-13,PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping,"['Jiafu Chen', 'Wei Xing', 'Jiakai Sun', 'Tianyi Chu', 'Yiling Huang', 'Boyan Ji', 'Lei Zhao', 'Huaizhong Lin', 'Haibo Chen', 'Zhizhong Wang']","3D scene stylization refers to transform the appearance of a 3D scene to match a given style image, ensuring that images rendered from different viewpoints exhibit the same style as the given style image, while maintaining the 3D consistency of the stylized scene. Several existing methods have obtained impressive results in stylizing 3D scenes. However, the models proposed by these methods need to be re-trained when applied to a new scene. In other words, their models are coupled with a specific scene and cannot adapt to arbitrary other scenes. To address this issue, we propose a novel 3D scene stylization framework to transfer an arbitrary style to an arbitrary scene, without any style-related or scene-related re-training. Concretely, we first map the appearance of the 3D scene into a 2D style pattern space, which realizes complete disentanglement of the geometry and appearance of the 3D scene and makes our model be generalized to arbitrary 3D scenes. Then we stylize the appearance of the 3D scene in the 2D style pattern space via a prompt-based 2D stylization algorithm. Experimental results demonstrate that our proposed framework is superior to SOTA methods in both visual quality and generalization."
https://arxiv.org/abs/2403.08251,2024-03-13,Emergence of Social Norms in Large Language Model-based Agent Societies,"['Siyue Ren', 'Zhiyao Cui', 'Ruiqi Song', 'Zhen Wang', 'Shuyue Hu']","The emergence of social norms has attracted much interest in a wide array of disciplines, ranging from social science and cognitive science to artificial intelligence. In this paper, we propose the first generative agent architecture that empowers the emergence of social norms within a population of large language model-based agents. Our architecture, named CRSEC, consists of four modules: Creation & Representation, Spreading, Evaluation, and Compliance. Our architecture addresses several important aspects of the emergent processes all in one: (i) where social norms come from, (ii) how they are formally represented, (iii) how they spread through agents' communications and observations, (iv) how they are examined with a sanity check and synthesized in the long term, and (v) how they are incorporated into agents' planning and actions. Our experiments deployed in the Smallville sandbox game environment demonstrate the capability of our architecture to establish social norms and reduce social conflicts within large language model-based multi-agent systems. The positive outcomes of our human evaluation, conducted with 30 evaluators, further affirm the effectiveness of our approach."
https://arxiv.org/abs/2403.08250,2024-03-13,Slowly rotating anisotropic relativistic stars,"['Philip Beltracchi', 'Camilo Posada']","The present paper is devoted to a study of the equilibrium configurations of slowly rotating anisotropic stars in the framework of general relativity. For that purpose, we provide the equations of structure where the rotation is treated to second order in the angular velocity. These equations extend those first derived by Hartle for slowly rotating isotropic stars. As an application of the new formalism, we study the rotational properties of Bowers-Liang fluid spheres. A result of particular interest is that the ellipticity and mass quadrupole moment are negative for certain highly anisotropic configurations, thus such systems are prolate rather than oblate. Furthermore, for configurations with high anisotropy, and compactness close to their critical value, quantities like the moment of inertia, change of mass, and mass quadrupole moment approach to the corresponding Kerr black hole values, similar to other ultracompact systems like sub-Buchdahl Schwarzschild stars and analytic rotating gravastars."
https://arxiv.org/abs/2403.08249,2024-03-13,Schatten--Lorentz characterization of Riesz transform commutator associated with Bessel operators,"['Zhijie Fan', 'Michael Lacey', 'Ji Li', 'Xiao Xiong']","Let $Δ_λ$ be the Bessel operator on the upper half space $\mathbb{R}_+^{n+1}$ with $n\geq 0$ and $λ>0$, and $R_{λ,j}$ be the $j-$th Bessel Riesz transform, $j=1,\ldots,n+1$. We demonstrate that the Schatten--Lorentz norm ($S^{p,q}$, $1<p<\infty$, $1\leq q\leq \infty$) of the commutator $[b,R_{λ,j}]$ can be characterized in terms of the oscillation space norm of the symbol $b$. In particular, for the case $p=q$, the Schatten norm of $[b,R_{λ,j}]$ can be further characterized in terms of the Besov norm of the symbol. Moreover, the critical index is also studied, which is $p=n+1$, the lower dimension of the Bessel measure (but not the upper dimension). Our approach relies on martingale and dyadic analysis, which enables us to bypass the use of Fourier analysis effectively."
https://arxiv.org/abs/2403.08248,2024-03-13,CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models,"['Haoxu Huang', 'Fanqi Lin', 'Yingdong Hu', 'Shengjie Wang', 'Yang Gao']","Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object's grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: https://copa-2024.github.io/"
https://arxiv.org/abs/2403.08247,2024-03-13,A Dual-domain Regularization Method for Ring Artifact Removal of X-ray CT,"['Hongyang Zhu', 'Xin Lu', 'Yanwei Qin', 'Xinran Yu', 'Tianjiao Sun', 'Yunsong Zhao']","Ring artifacts in computed tomography images, arising from the undesirable responses of detector units, significantly degrade image quality and diagnostic reliability. To address this challenge, we propose a dual-domain regularization model to effectively remove ring artifacts, while maintaining the integrity of the original CT image. The proposed model corrects the vertical stripe artifacts on the sinogram by innovatively updating the response inconsistency compensation coefficients of detector units, which is achieved by employing the group sparse constraint and the projection-view direction sparse constraint on the stripe artifacts. Simultaneously, we apply the sparse constraint on the reconstructed image to further rectified ring artifacts in the image domain. The key advantage of the proposed method lies in considering the relationship between the response inconsistency compensation coefficients of the detector units and the projection views, which enables a more accurate correction of the response of the detector units. An alternating minimization method is designed to solve the model. Comparative experiments on real photon counting detector data demonstrate that the proposed method not only surpasses existing methods in removing ring artifacts but also excels in preserving structural details and image fidelity."
https://arxiv.org/abs/2403.08246,2024-03-13,Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation,"['Yuting Liu', 'Yizhou Dang', 'Yuliang Liang', 'Qiang Liu', 'Guibing Guo', 'Jianzhe Zhao', 'Xingwei Wang']","Recently, sign-aware graph recommendation has drawn much attention as it will learn users' negative preferences besides positive ones from both positive and negative interactions (i.e., links in a graph) with items. To accommodate the different semantics of negative and positive links, existing works utilize two independent encoders to model users' positive and negative preferences, respectively. However, these approaches cannot learn the negative preferences from high-order heterogeneous interactions between users and items formed by multiple links with different signs, resulting in inaccurate and incomplete negative user preferences. To cope with these intractable issues, we propose a novel \textbf{L}ight \textbf{S}igned \textbf{G}raph Convolution Network specifically for \textbf{Rec}ommendation (\textbf{LSGRec}), which adopts a unified modeling approach to simultaneously model high-order users' positive and negative preferences on a signed user-item interaction graph. Specifically, for the negative preferences within high-order heterogeneous interactions, first-order negative preferences are captured by the negative links, while high-order negative preferences are propagated along positive edges. Then, recommendation results are generated based on positive preferences and optimized with negative ones. Finally, we train representations of users and items through different auxiliary tasks. Extensive experiments on three real-world datasets demonstrate that our method outperforms existing baselines regarding performance and computational efficiency. Our code is available at \url{https://anonymous.4open.science/r/LSGRec-BB95}."
https://arxiv.org/abs/2403.08245,2024-03-13,Scattered Mixture-of-Experts Implementation,"['Shawn Tan', 'Yikang Shen', 'Rameswar Panda', 'Aaron Courville']","We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE) on GPUs. ScatterMoE builds upon existing implementations, and overcoming some of the limitations to improve inference and training speed, and memory footprint. This implementation achieves this by avoiding padding and making excessive copies of the input. We introduce ParallelLinear, the main component we use to build our implementation and the various kernels used to speed up the operation. We benchmark our implementation against Megablocks, and show that it enables a higher throughput and lower memory footprint. We also show how ParallelLinear enables extension of the Mixture-of-Experts concept by demonstrating with an implementation of Mixture of Attention."
https://arxiv.org/abs/2403.08244,2024-03-13,Evaluating the Efficiency and Cost-effectiveness of RPB-based CO2 Capture: A Comprehensive Approach to Simultaneous Design and Operating Condition Optimization,"['Howoun Jung', 'Nohjin Park', 'Jay H. Lee']","Despite ongoing global initiatives to reduce CO2 emissions, implementing large-scale CO2 capture using amine solvents is fraught with economic uncertainties and technical hurdles. The Rotating Packed Bed (RPB) presents a promising alternative to traditional packed towers, offering compact design and adaptability. Nonetheless, scaling RPB processes to an industrial level is challenging due to the nascent nature of its application. The complexity of designing RPB units, setting operating conditions, and evaluating process performance adds layers of difficulty to the adoption of RPB-based systems in industries. This study introduces an optimization-driven design and evaluation for CO2 capture processes utilizing RPB columns. By employing detailed process simulation, we aim to concurrently optimize unit design and operating parameters, underscoring its advantage over conventional sequential approaches. Our process design method integrates heuristic design recommendations as constraints, resulting in 9.4% to 12.7% cost savings compared to conventional sequential design methods. Furthermore, our comprehensive process-level analysis reveals that using concentrated MEA solvent can yield total cost savings of 13.4% to 25.0% compared to the standard 30wt% MEA solvent. Additionally, the RPB unit can deliver an 8.5 to 23.6 times reduction in packing volume. While the commercial-scale feasibility of RPB technology has been established, the advancement of this field hinges on acquiring a broader and more robust dataset from commercial-scale implementations. Employing strategic methods like modularization could significantly reduce the entry barriers for CO2 capture projects, facilitating their broader adoption and implementation."
https://arxiv.org/abs/2403.08243,2024-03-13,Spin characters of the symmetric group which are proportional to linear characters in characteristic 2,"['Matthew Fayers', 'Eoghan McDowell']","For a finite group, it is interesting to determine when two ordinary irreducible representations have the same $p$-modular reduction; that is, when two rows of the decomposition matrix in characteristic $p$ are equal, or equivalently when the corresponding $p$-modular Brauer characters are the same. We complete this task for the double covers of the symmetric group when $p=2$, by determining when the $2$-modular reduction of an irreducible spin representation coincides with a $2$-modular Specht module. In fact, we obtain a more general result: we determine when an irreducible spin representation has $2$-modular Brauer character proportional to that of a Specht module. In the course of the proof, we use induction and restriction functors to construct a function on generalised characters which has the effect of swapping runners in abacus displays for the labelling partitions."
https://arxiv.org/abs/2403.08242,2024-03-13,Excited-State OH Masers in the Water Fountain Source IRAS 18460-0151,"['Xu-Jia Ouyang', 'Yong Zhang', 'Juan Li', 'Jun-ichi Nakashima', 'Xi Chen', 'Hai-Hua Qiao']","Water fountain objects are generally defined as ""evolved stars with low to intermediate initial mass accompanied by high-velocity molecular jets detectable in the 22.235 GHz H$_2$O maser line"". They are the key objects of understanding the morphological transitions of circumstellar envelopes during the post asymptotic giant branch phase. Masers are useful tools to trace the kinematic environments of the circumstellar envelopes. In this letter we report the discovery of exceptionally uncommon excited-state hydroxyl (ex-OH) masers at 4660 and 6031 MHz toward the water fountain source IRAS 18460-0151. These are the brightest ex-OH masers discovered in late-type objects to date. To the best of our knowledge, prior to the current work, no evolved stellar object has been observed in the 4660 MHz ex-OH maser line. The ground-state hydroxyl (g-OH) masers at 1612 and 1665 MHz are also observed. The velocity components of the 4660 MHz ex-OH maser line and the much weaker 1665 MHz g-OH maser line all can be seen in the 1612 MHz g-OH maser line profile. The blue-shifted components of the three masers are more intense than the red-shifted ones, in contrast to the ex-OH maser line at 6031 MHz. The relevance of the behaviors of the ex-OH masers to the circumstellar environments is unclear."
https://arxiv.org/abs/2403.08241,2024-03-13,The 3D Lyman-$α$ Forest Power Spectrum from eBOSS DR16,"['Roger de Belsunce', 'Oliver H. E. Philcox', 'Vid Irsic', 'Pat McDonald', 'Julien Guy', 'Nathalie Palanque-Delabrouille']","We measure the three-dimensional power spectrum (P3D) of the transmitted flux in the Lyman-$α$ (Ly-$α$) forest using the complete extended Baryon Oscillation Spectroscopic Survey data release 16 (eBOSS DR16). This sample consists of 205,012 quasar spectra in the redshift range 2 <= z <= 4 at an effective redshift z=2.334. We propose a pair-count spectral estimator in configuration space, weighting each pair by exp(ikr), for wave vector k and pixel pair separation r, effectively measuring the anisotropic power spectrum without the need for fast Fourier transforms. This accounts for the window matrix in a tractable way, avoiding artifacts found in Fourier-transform based power spectrum estimators due to the sparse sampling transverse to the line-of-sight of Ly-$α$ skewers. We extensively test our pipeline on two sets of mocks: (i) idealized Gaussian random fields with a sparse sampling of Ly-$α$ skewers, and (ii) log-normal LyaCoLoRe mocks including realistic noise levels, the eBOSS survey geometry and contaminants. On eBOSS DR16 data, the Kaiser formula with a non-linear correction term obtained from hydrodynamic simulations yields a good fit to the power spectrum data in the range 0.02 <= k <= 0.35 h/Mpc at the 1-2 sigma level with a covariance matrix derived from LyaCoLoRe mocks. We demonstrate a promising new approach for full-shape cosmological analyses of Ly-$α$ forest data from cosmological surveys such as eBOSS, the currently observing Dark Energy Spectroscopic Instrument and future surveys such as the Prime Focus Spectrograph, WEAVE-QSO and 4MOST."
https://arxiv.org/abs/2403.08240,2024-03-13,Capturing electronic correlations in electron-phonon interactions in molecular systems with the GW approximation,"['Antonios M. Alvertis', 'David B. Williams-Young', 'Fabien Bruneval', 'Jeffrey B. Neaton']","Electron-phonon interactions are of great importance to a variety of physical phenomena, and their accurate description is an important goal for first-principles calculations. Isolated examples of materials and molecular systems have emerged where electron-phonon coupling is enhanced over density functional theory (DFT) when using the Green's-function-based ab initio GW method, which provides a more accurate description of electronic correlations. It is however unclear how general this enhancement is, and how employing high-end quantum chemistry methods, which further improve the description of electronic correlations, might further alter electron-phonon interactions over GW or DFT. Here, we address these questions by computing the renormalization of the highest occupied molecular orbital energies of Thiel's set of organic molecules by harmonic vibrations using DFT, GW and equation-of-motion coupled-cluster calculations. We find that GW can increase the magnitude of the electron-phonon coupling across this set of molecules by an average factor of 1.1-1.8 compared to DFT, while equation-of-motion coupled-cluster leads to an increase of 1.4-2. The electron-phonon coupling predicted with the ab initio GW method is generally in much closer agreement to coupled cluster values compared to DFT, establishing GW as an accurate way of computing electron-phonon phenomena in molecules and beyond at a much lower computational cost than higher-end quantum chemistry techniques."
https://arxiv.org/abs/2403.08239,2024-03-13,Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization,"['Kento Kawaharazuka', 'Naoaki Kanazawa', 'Yoshiki Obinata', 'Kei Okada', 'Masayuki Inaba']","The state recognition of the environment and objects by robots is generally based on the judgement of the current state as a classification problem. On the other hand, state changes of food in cooking happen continuously and need to be captured not only at a certain time point but also continuously over time. In addition, the state changes of food are complex and cannot be easily described by manual programming. Therefore, we propose a method to recognize the continuous state changes of food for cooking robots through the spoken language using pre-trained large-scale vision-language models. By using models that can compute the similarity between images and texts continuously over time, we can capture the state changes of food while cooking. We also show that by adjusting the weighting of each text prompt based on fitting the similarity changes to a sigmoid function and then performing black-box optimization, more accurate and robust continuous state recognition can be achieved. We demonstrate the effectiveness and limitations of this method by performing the recognition of water boiling, butter melting, egg cooking, and onion stir-frying."
https://arxiv.org/abs/2403.08238,2024-03-13,A Novel Feature Learning-based Bio-inspired Neural Network for Real-time Collision-free Rescue of Multi-Robot Systems,"['Junfei Li', 'Simon X. Yang']","Natural disasters and urban accidents drive the demand for rescue robots to provide safer, faster, and more efficient rescue trajectories. In this paper, a feature learning-based bio-inspired neural network (FLBBINN) is proposed to quickly generate a heuristic rescue path in complex and dynamic environments, as traditional approaches usually cannot provide a satisfactory solution to real-time responses to sudden environmental changes. The neurodynamic model is incorporated into the feature learning method that can use environmental information to improve path planning strategies. Task assignment and collision-free rescue trajectory are generated through robot poses and the dynamic landscape of neural activity. A dual-channel scale filter, a neural activity channel, and a secondary distance fusion are employed to extract and filter feature neurons. After completion of the feature learning process, a neurodynamics-based feature matrix is established to quickly generate the new heuristic rescue paths with parameter-driven topological adaptability. The proposed FLBBINN aims to reduce the computational complexity of the neural network-based approach and enable the feature learning method to achieve real-time responses to environmental changes. Several simulations and experiments have been conducted to evaluate the performance of the proposed FLBBINN. The results show that the proposed FLBBINN would significantly improve the speed, efficiency, and optimality for rescue operations."
https://arxiv.org/abs/2403.08237,2024-03-13,The effect of cation-disorder on lithium transport in halide superionic conductors,"['Peichen Zhong', 'Sunny Gupta', 'Bowen Deng', 'KyuJung Jun', 'Gerbrand Ceder']","Among the chloride-based Li-ion solid electrolytes, Li$_2$ZrCl$_6$ (LZC) have emerged as potential candidates due to their affordability, moisture stability, and high ionic conductivity. LZC synthesized by solid-state heating exhibits limited Li-ion conductivity while the mechanochemical ball-milled material is more conductive. In this computational study, we integrate thermodynamic modeling, using cluster-expansion Monte Carlo, and kinetic modeling, using molecular dynamics, to investigate whether cation disorder can be achieved in LZC, and how it affects Li-ion transport. Our results indicate that fast Li-ion conductivity is induced by the activation of Li/vacancy disorder, which itself depends on the degree of Zr disorder. We find that the very high-temperature scale at which equilibrium Zr-disorder can form precludes any equilibrium synthesis processes for achieving fast Li-ion conductivity, rationalizing why only non-equilibrium synthesis methods, such as ball milling leads to good conductivity. We identify as the critical mechanism the lack of Li/vacancy disorder near room temperature when Zr is well-ordered. Our simulations further show that the Li/vacancy order-disorder transition temperature is lowered by Zr disorder, which is necessary for creating high Li diffusivity at room temperature. The insights obtained from this study raise a challenge for the large-scale production of these materials and the potential for the long-term stability of their properties."
https://arxiv.org/abs/2403.08236,2024-03-13,Point Cloud Compression via Constrained Optimal Transport,"['Zezeng Li', 'Weimin Wang', 'Ziliang Wang', 'Na Lei']","This paper presents a novel point cloud compression method COT-PCC by formulating the task as a constrained optimal transport (COT) problem. COT-PCC takes the bitrate of compressed features as an extra constraint of optimal transport (OT) which learns the distribution transformation between original and reconstructed points. Specifically, the formulated COT is implemented with a generative adversarial network (GAN) and a bitrate loss for training. The discriminator measures the Wasserstein distance between input and reconstructed points, and a generator calculates the optimal mapping between distributions of input and reconstructed point cloud. Moreover, we introduce a learnable sampling module for downsampling in the compression procedure. Extensive results on both sparse and dense point cloud datasets demonstrate that COT-PCC outperforms state-of-the-art methods in terms of both CD and PSNR metrics. Source codes are available at \url{https://github.com/cognaclee/PCC-COT}."
https://arxiv.org/abs/2403.08235,2024-03-13,First identification of a doublet wobbling excitation mode in $^{105}$Pd,"['A. Karmakar', 'P. Datta', 'N. Rather', 'S. Pal', 'R. Palit', 'A. Goswami', 'G. H. Bhat', 'J. A. Sheikh', 'S. Jehangir', 'S. Chattopadhyay', 'S. Frauendorf']","An experimental investigation of $^{105}$Pd has revealed, for the first time, the existence of two wobbling bands both having one phonon configuration and originating from the coupling of the wobbling phonon to the ground state band and to its signature partner. The doublet one-phonon wobbling bands are, in turn, found to be the signature partner bands. These observations have been drawn from the measured ratios of the inter-band and intra-band gamma transition rates. The model calculations based on the triaxial projected shell model (TPSM) approach have been performed and are found to be in good agreement with the experimental observations. These calculations provide an insight into the nature of the observed structures at a microscopic level."
https://arxiv.org/abs/2403.08234,2024-03-13,Phase transitions in typical $Pca2_1$ ferroelectrics,"['Heng Yu', 'Kan-Hao Xue', 'Nan Feng', 'Yunzhe Zheng', 'Yan Cheng', 'Ben Xu', 'Xiangshui Miao']","While ferroelectric hafnia ($\mathrm{HfO_2}$) has become a technically important material for microelectronics, the physical origin of its ferroelectricity remains poorly understood. The tetragonal $P4_2/nmc$ phase is commonly assigned as its paraelectric mother phase, but it has no soft mode at the Brillouin zone center. In this work, we propose that the paraelectric-ferroelectric transition in hafnia-like $Pca2_1$ ferroelectric family can be described by a $Pcca$-$Pca2_1$ transition, where the $Pcca$ mother phase will evolve into either the $Pca2_1$ ferroelectric phase or the centrosymmetric $P2_1/c$ monoclinic phase, depending on the strain conditions. The $Pcca$ phase is directly linked to both phases in the context of continuous phase transition. Hafnia is regarded as a special case of this family, in that it has accidental atomic degeneracy because all anions are oxygen. The theory is also correlated to the seven-coordination theory that explains the ferroelectricity in hafnia from a chemical perspective. In addition, the strain conditions to promote the ferroelectric phase in hafnia is discussed."
https://arxiv.org/abs/2403.08233,2024-03-13,A Parallel Beam Splitting Based on Gradient Metasurface: Preparation and Fusion of Quantum Entanglement,"['Qi Liu', 'Xuan Liu', 'Yu Tian', 'Zhaohua Tian', 'Guixin Li', 'Xi-Feng Ren', 'Qihuang Gong', 'Ying Gu']","Gradient metasurface, formed by a set of subwavelength unit cells with different phase modulation, is widely used in polarized beam splitting (BS) in the classical and quantum optics. Specifically, its phase gradient allows the path and polarization of multiple output lights to be locked by corresponding inputs.Using this unique path-polarization locked property, we demonstrate that the single metasurface can function as sequentially linked beamsplitters, enabling the parallelization of a series of BS processes. Such a parallel BS metasurface provides a multi-beam interference capability for both classical and quantum light manipulation. Taking this advantage, we first prepare path and polarization hybrid entangled states of two, three, and multi photons from unentangled photon sources. Then, the ability of parallel BS-facilitated entanglement is applied to demonstrate entanglement fusion among entangled photon pairs, which can greatly enlarge the entanglement dimension. The principle of parallel BS through the metasurface opens up a versatile way to manipulate the quantum state at the micro/nano scale, which will have potential applications in on-chip quantum optics and quantum information processing."
https://arxiv.org/abs/2403.08232,2024-03-13,Zero modes of velocity field and topological invariant in quantum torus,"['Annan Fan', 'Shi-Dong Liang']","We propose the velocity field approach to characterize topological invariants of quantum states. We introduce the indexes of the velocity field flow based on the zero modes of the velocity field and find that these zero modes play the role of effective topological charges or defects linking to Euler characteristic by the Poincaré-Hopf theorem. The global property of the indexes is topological invariants against the parameter deformation. We demonstrate this approach by the quantum torus model and compare the topological invariant with that obtained from the Chern number. We find that the physical mechanism of the topological invariant based on the zero modes of the velocity field is different from that of the topological invariant by the Chern number. The topological invariant characterized by the velocity field describes a homeomorphic topological invariant associated with the zero modes on the submanifold of the base manifold of the SU(2)-fibre bundle for quantum torus, whereas the Chern number characterizes a homotopy invariant associated with the exceptional points in the Brillouin zone. We also propose the generalized winding number in terms of the velocity field for both Hermitian and non-Hermitian systems. This gives a connection between the zero mode and winding number in the velocity space. These results enrich the topological invariants of quantum states and promises us a novel insight to understanding topological invariants of quantum states as well as expected to be further applied in more generic models."
https://arxiv.org/abs/2403.08231,2024-03-13,Object Permanence Filter for Robust Tracking with Interactive Robots,"['Shaoting Peng', 'Margaret X. Wang', 'Julie A. Shah', 'Nadia Figueroa']","Object permanence, which refers to the concept that objects continue to exist even when they are no longer perceivable through the senses, is a crucial aspect of human cognitive development. In this work, we seek to incorporate this understanding into interactive robots by proposing a set of assumptions and rules to represent object permanence in multi-object, multi-agent interactive scenarios. We integrate these rules into the particle filter, resulting in the Object Permanence Filter (OPF). For multi-object scenarios, we propose an ensemble of K interconnected OPFs, where each filter predicts plausible object tracks that are resilient to missing, noisy, and kinematically or dynamically infeasible measurements, thus bringing perceptional robustness. Through several interactive scenarios, we demonstrate that the proposed OPF approach provides robust tracking in human-robot interactive tasks agnostic to measurement type, even in the presence of prolonged and complete occlusion. Webpage: https://opfilter.github.io/."
https://arxiv.org/abs/2403.08230,2024-03-13,A vicious cycle along busy bus corridors and how to abate it,"['Minyu Shen', 'Weihua Gu', 'Michael J. Cassidy', 'Yongjie Lin', 'Wei Ni']","We unveil that a previously-unreported vicious cycle can be created when bus queues form at curbside stops along a corridor. Buses caught in this cycle exhibit growing variation in headways as they travel from stop to stop. Bus (and patron) delays accumulate in like fashion and can grow large on long, busy corridors. We show that this damaging cycle can be abated in simple ways. Present solutions entail holding buses at a corridor entrance and releasing them as per various strategies proposed in the literature. We introduce a modest variant to the simplest of these strategies. It releases buses at headways that are slightly less than, or equal to, the scheduled values. It turns out that periodically releasing buses at slightly smaller headways can substantially reduce bus delays caused by holding so that benefits can more readily outweigh costs in corridors that contain a sufficient number of serial bus stops. The simple variant is shown to perform about as well as, or better than, other bus-holding strategies in terms of saving delays, and is more effective than other strategies in regularizing bus headways. We also show that grouping buses from across multiple lines and holding them by group can be effective when patrons have the flexibility to choose buses from across all lines in a group. Findings come by formulating select models of bus-corridor dynamics and using these to simulate part of the Bus Rapid Transit corridor in Guangzhou, China."
https://arxiv.org/abs/2403.08229,2024-03-13,Boosting Disfluency Detection with Large Language Model as Disfluency Generator,"['Zhenrong Cheng', 'Jiayan Guo', 'Hao Sun', 'Yan Zhang']","Current disfluency detection methods heavily rely on costly and scarce human-annotated data. To tackle this issue, some approaches employ heuristic or statistical features to generate disfluent sentences, partially improving detection performance. However, these sentences often deviate from real-life scenarios, constraining overall model enhancement. In this study, we propose a lightweight data augmentation approach for disfluency detection, utilizing the superior generative and semantic understanding capabilities of large language model (LLM) to generate disfluent sentences as augmentation data. We leverage LLM to generate diverse and more realistic sentences guided by specific prompts, without the need for fine-tuning the LLM. Subsequently, we apply an uncertainty-aware data filtering approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance. Experiments using enhanced data yielded state-of-the-art results. The results showed that using a small amount of LLM-generated enhanced data can significantly improve performance, thereby further enhancing cost-effectiveness."
https://arxiv.org/abs/2403.08228,2024-03-13,Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs,"['Fujing Xie', 'Sören Schwertfeger']","Recently, Large Language Models (LLMs) have demonstrated great potential in robotic applications by providing essential general knowledge for situations that can not be pre-programmed beforehand. Generally speaking, mobile robots need to understand maps to execute tasks such as localization or navigation. In this letter, we address the problem of enabling LLMs to comprehend Area Graph, a text-based map representation, in order to enhance their applicability in the field of mobile robotics. Area Graph is a hierarchical, topometric semantic map representation utilizing polygons to demark areas such as rooms, corridors or buildings. In contrast to commonly used map representations, such as occupancy grid maps or point clouds, osmAG (Area Graph in OpensStreetMap format) is stored in a XML textual format naturally readable by LLMs. Furthermore, conventional robotic algorithms such as localization and path planning are compatible with osmAG, facilitating this map representation comprehensible by LLMs, traditional robotic algorithms and humans. Our experiments show that with a proper map representation, LLMs possess the capability to understand maps and answer queries based on that understanding. Following simple fine-tuning of LLaMA2 models, it surpassed ChatGPT-3.5 in tasks involving topology and hierarchy understanding. Our dataset, dataset generation code, fine-tuned LoRA adapters can be accessed at https://github.com/xiefujing/LLM-osmAG-Comprehension."
https://arxiv.org/abs/2403.08227,2024-03-13,Matching Non-Identical Objects,"['Yusuke Marumo', 'Kazuhiko Kawamoto', 'Hiroshi Kera']","Not identical but similar objects are everywhere in the world. Examples include four-legged animals such as dogs and cats, cars of different models, akin flowers in various colors, and countless others. In this study, we address a novel task of matching such non-identical objects. We propose a simple weighting scheme of descriptors that enhance various sparse image matching methods, which are originally designed for matching identical objects captured from different perspectives, and achieve semantically robust matching. The experiments show successful matching between non-identical objects in various cases including domain shift. Further, we present a first evaluation of the robustness of the image matching methods under common corruptions, which is a sort of domain shift, and the proposed method improves the matching in this case as well."
https://arxiv.org/abs/2403.08226,2024-03-13,Astrometric detection of exoplanets,['Fabo Feng'],"As the most ancient branch of astronomy, astrometry has been developed for thousands of years. However, it has only recently become possible to utilize astrometry for the detection of exoplanets. Gaia, an astrometric surveyor of 1 billion stars, is capable of measuring the position of stars with a precision as high as 20 $μ$as. Gaia is expected to discover more than 10,000 exoplanets by the end of its mission, surpassing the productivity of most exoplanet surveys."
https://arxiv.org/abs/2403.08225,2024-03-13,Probing the heavy Higgs boson production and decay $H_0$ of the Bestest Little Higgs Model at the LHC and the FCC-hh,"['E. Cruz-Albaro', 'A. Gutiérrez-Rodríguez', 'D. Espinosa-Gómez', 'T. Cisneros-Pérez', 'F. Ramírez-Zavaleta']","In the Bestest Little Higgs Model (BLHM) scenario, we analyze the branching ratios and production cross-section of the heavy Higgs boson $H_0$. The analysis is performed at the tree level and the one-loop level. In addition, we present results of the possible production of the heavy Higgs boson $H_0$ via gluon fusion for the center-of-mass energies and the integrated luminosities of the LHC, HE-LHC, HL-LHC, and FCC-hh. Our results show a very optimistic scenario for studying the $H_0$ scalar predicted by the BLHM and for the energies and luminosities of current and future hadron colliders."
https://arxiv.org/abs/2403.08224,2024-03-13,REPAIR: Rank Correlation and Noisy Pair Half-replacing with Memory for Noisy Correspondence,"['Ruochen Zheng', 'Jiahao Hong', 'Changxin Gao', 'Nong Sang']","The presence of noise in acquired data invariably leads to performance degradation in cross-modal matching. Unfortunately, obtaining precise annotations in the multimodal field is expensive, which has prompted some methods to tackle the mismatched data pair issue in cross-modal matching contexts, termed as noisy correspondence. However, most of these existing noisy correspondence methods exhibit the following limitations: a) the problem of self-reinforcing error accumulation, and b) improper handling of noisy data pair. To tackle the two problems, we propose a generalized framework termed as Rank corrElation and noisy Pair hAlf-replacing wIth memoRy (REPAIR), which benefits from maintaining a memory bank for features of matched pairs. Specifically, we calculate the distances between the features in the memory bank and those of the target pair for each respective modality, and use the rank correlation of these two sets of distances to estimate the soft correspondence label of the target pair. Estimating soft correspondence based on memory bank features rather than using a similarity network can avoid the accumulation of errors due to incorrect network identifications. For pairs that are completely mismatched, REPAIR searches the memory bank for the most matching feature to replace one feature of one modality, instead of using the original pair directly or merely discarding the mismatched pair. We conduct experiments on three cross-modal datasets, i.e., Flickr30K, MSCOCO, and CC152K, proving the effectiveness and robustness of our REPAIR on synthetic and real-world noise."
https://arxiv.org/abs/2403.08223,2024-03-12,The long-term evolution of the GW170817 remnant,"['Menquan Liu', 'Jie Zhang', 'Cong Wang']","GW170817 represents the first observed binary neutron star merger event by humanity. The observation of GW170817 has identified the correlation between Kilonova, gravitational wave and short GRB. The shocks from GW170817 have the capacity to inject significant thermal and kinetic energies into the interstellar medium and evolve for over a million years. In this letter, we adopt the special relativity fluid dynamics equations to simulate the evolution of the GW170817 remnant over a span of one million years. Our simulations yield the evolution profiles of the velocity, density, mass, radius, luminosity, and energies of the remnant. We estimate that the GW170817 remnant will reach the average maximum luminosity $ 2.56\times 10^{39}$ erg s$^{-1}$at approximately $3.96\times 10^4$ yr. At the end of the cooling stage, the contaminated radius and mass are $48.35$ pc and $2.25\times 10^4 M_{\odot}$, respectively."
https://arxiv.org/abs/2403.08222,2024-03-12,Robust Decision Aggregation with Adversarial Experts,"['Yongkang Guo', 'Yuqing Kong']","We consider a binary decision aggregation problem in the presence of both truthful and adversarial experts. The truthful experts will report their private signals truthfully with proper incentive, while the adversarial experts can report arbitrarily. The decision maker needs to design a robust aggregator to forecast the true state of the world based on the reports of experts. The decision maker does not know the specific information structure, which is a joint distribution of signals, states, and strategies of adversarial experts. We want to find the optimal aggregator minimizing regret under the worst information structure. The regret is defined by the difference in expected loss between the aggregator and a benchmark who makes the optimal decision given the joint distribution and reports of truthful experts."
https://arxiv.org/abs/2403.08221,2024-03-12,Help Supporters: Exploring the Design Space of Assistive Technologies to Support Face-to-Face Help Between Blind and Sighted Strangers,"['Yuanyang Teng', 'Connor Courtien', 'David Angel Rios', 'Yves M. Tseng', 'Jacqueline Gibson', 'Maryam Aziz', 'Avery Reyna', 'Rajan Vaish', 'Brian A. Smith']","Blind and low-vision (BLV) people face many challenges when venturing into public environments, often wishing it were easier to get help from people nearby. Ironically, while many sighted individuals are willing to help, such interactions are infrequent. Asking for help is socially awkward for BLV people, and sighted people lack experience in helping BLV people. Through a mixed-ability research-through-design process, we explore four diverse approaches toward how assistive technology can serve as help supporters that collaborate with both BLV and sighted parties throughout the help process. These approaches span two phases: the connection phase (finding someone to help) and the collaboration phase (facilitating help after finding someone). Our findings from a 20-participant mixed-ability study reveal how help supporters can best facilitate connection, which types of information they should present during both phases, and more. We discuss design implications for future approaches to support face-to-face help."
https://arxiv.org/abs/2403.08220,2024-03-12,Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators,"['Lianghao Cao', ""Thomas O'Leary-Roseberry"", 'Omar Ghattas']","We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems. While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations. We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter). Training such a surrogate via conventional operator learning using input--output samples often demands a prohibitively large number of model simulations. In this work, we present an extension of derivative-informed operator learning [O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] using input--output--derivative training samples. Such a learning method leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observable and its parametric derivative at a significantly lower training cost than the conventional method. Cost and error analysis for reduced basis DINO surrogates are provided. Numerical studies on PDE-constrained Bayesian inversion demonstrate that DINO-driven MCMC generates effective posterior samples 3--9 times faster than geometric MCMC and 60--97 times faster than prior geometry-based MCMC. Furthermore, the training cost of DINO surrogates breaks even after collecting merely 10--25 effective posterior samples compared to geometric MCMC."
https://arxiv.org/abs/2403.08219,2024-03-12,SpaceOctopus: An Octopus-inspired Motion Planning Framework for Multi-arm Space Robot,"['Wenbo Zhao', 'Shengjie Wang', 'Yixuan Fan', 'Yang Gao', 'Tao Zhang']","Space robots have played a critical role in autonomous maintenance and space junk removal. Multi-arm space robots can efficiently complete the target capture and base reorientation tasks due to their flexibility and the collaborative capabilities between the arms. However, the complex coupling properties arising from both the multiple arms and the free-floating base present challenges to the motion planning problems of multi-arm space robots. We observe that the octopus elegantly achieves similar goals when grabbing prey and escaping from danger. Inspired by the distributed control of octopuses' limbs, we develop a multi-level decentralized motion planning framework to manage the movement of different arms of space robots. This motion planning framework integrates naturally with the multi-agent reinforcement learning (MARL) paradigm. The results indicate that our method outperforms the previous method (centralized training). Leveraging the flexibility of the decentralized framework, we reassemble policies trained for different tasks, enabling the space robot to complete trajectory planning tasks while adjusting the base attitude without further learning. Furthermore, our experiments confirm the superior robustness of our method in the face of external disturbances, changing base masses, and even the failure of one arm."
https://arxiv.org/abs/2403.08218,2024-03-12,Non-Hermitian sensing in the absence of exceptional points,"['Lei Xiao', 'Yaoming Chu', 'Quan Lin', 'Haiqing Lin', 'Wei Yi', 'Jianming Cai', 'Peng Xue']","Open systems possess unique potentials in high-precision sensing, yet the majority of previous studies rely on the spectral singularities known as exceptional points. Here we theoretically propose and experimentally demonstrate universal non-Hermitian sensing in the absence of exceptional points. The scheme makes use of the intrinsic sensitivity of a non-Hermitian probe to weak external fields, which can be understood as the direct consequence of non-Hermiticity. We confirm the basic mechanism by simulating the sensor-field dynamics using photon interferometry, and, as a concrete example, demonstrate the enhanced sensing of signals encoded in the setting angle of a wave plate. While the sensitivity of the probe is ultimately limited by the measurement noise, we find the non-Hermitian sensor showing superior performance under background noises that cannot be suppressed through repetitive measurements. Our experiment opens the avenue of enhanced sensing without exceptional points, complementing existing efforts aimed at harnessing the unique features of open systems."
https://arxiv.org/abs/2403.08217,2024-03-12,Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis,"['Yichao Wu', 'Zhengyu Jin', 'Chenxi Shi', 'Penghao Liang', 'Tong Zhan']","This paper explores the application of deep learning techniques, particularly focusing on BERT models, in sentiment analysis. It begins by introducing the fundamental concept of sentiment analysis and how deep learning methods are utilized in this domain. Subsequently, it delves into the architecture and characteristics of BERT models. Through detailed explanation, it elucidates the application effects and optimization strategies of BERT models in sentiment analysis, supported by experimental validation. The experimental findings indicate that BERT models exhibit robust performance in sentiment analysis tasks, with notable enhancements post fine-tuning. Lastly, the paper concludes by summarizing the potential applications of BERT models in sentiment analysis and suggests directions for future research and practical implementations."
https://arxiv.org/abs/2403.08216,2024-03-12,PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise,"['Qinglong Meng', 'Chongkun Xia', 'Xueqian Wang']","Normalizing flow is a generative modeling approach with efficient sampling. However, Flow-based models suffer two issues, which are manifold and discrete data. If the target distribution is a manifold, which means the dimension of the latent target distribution and the dimension of the data distribution are unmatched, flow-based models might perform badly. Discrete data makes flow-based models collapse into a degenerate mixture of point masses. In this paper, to sidestep such two issues we propose PaddingFlow, a novel dequantization method, which improves normalizing flows with padding-dimensional noise. PaddingFlow is easy to implement, computationally cheap, widely suitable for various tasks, and generates samples that are unbiased estimations of the data. Especially, our method can overcome the limitation of existing dequantization methods that have to change the data distribution, which might degrade performance. We validate our method on the main benchmarks of unconditional density estimation, including five tabular datasets and four image datasets for VAE models, and the IK experiments which are conditional density estimation. The results show that PaddingFlow can provide improvement on all tasks in this paper."
https://arxiv.org/abs/2403.08215,2024-03-12,LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving,"['Sicen Guo', 'Zhiyuan Wu', 'Qijun Chen', 'Ioannis Pitas', 'Rui Fan']","Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available. Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue. This paper delves into this topic and resorts to knowledge distillation approaches to address this problem. We introduce the Learning to Infuse ""X"" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects. We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue. Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two technical novelties: feature recalibration via kernel regression and in-depth feature consistency quantification via centered kernel alignment. Extensive experiments conducted with intermediate-fusion and late-fusion networks across various public datasets provide both quantitative and qualitative evaluations, demonstrating the superior performance of our LIX framework when compared to other state-of-the-art approaches."
https://arxiv.org/abs/2403.08214,2024-03-12,"P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer","['Shuangjian Li', 'Tao Zhu', 'Mingxing Nie', 'Huansheng Ning', 'Zhenyu Liu', 'Liming Chen']","Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data. This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial. This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model. P2LHAP divides sensor data streams into a sequence of ""patches"", served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities. A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately. Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent Transformer encoders and decoders. All channels share embedding and Transformer weights across all sequences. Evaluated on three public datasets, P2LHAP significantly outperforms the state-of-the-art in all three tasks, demonstrating its effectiveness and potential for real-world applications."
https://arxiv.org/abs/2403.08213,2024-03-12,Can Large Language Models Identify Authorship?,"['Baixiang Huang', 'Canyu Chen', 'Kai Shu']","The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation. Large Language Models (LLMs) have demonstrated exceptional capacity for reasoning and problem-solving. However, their potential in authorship analysis, encompassing authorship verification and attribution, remains underexplored. This paper conducts a comprehensive evaluation of LLMs in these critical tasks. Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage text embeddings from pre-trained language models. These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability. This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively? (2) Are LLMs capable of accurately attributing authorship among multiple candidates authors (e.g., 10 and 20)? (3) How can LLMs provide explainability in authorship analysis, particularly through the role of linguistic features? Moreover, we investigate the integration of explicit linguistic features to guide LLMs in their reasoning processes. Our extensive assessment demonstrates LLMs' proficiency in both tasks without the need for domain-specific fine-tuning, providing insights into their decision-making via a detailed analysis of linguistic features. This establishes a new benchmark for future research on LLM-based authorship analysis. The code and data are available at https://github.com/baixianghuang/authorship-llm."
https://arxiv.org/abs/2403.08212,2024-03-12,On the structure of W-algebras in type A,"['Thomas Creutzig', 'Justine Fasquel', 'Andrew R. Linshaw', 'Shigenori Nakatsuka']","We formulate and prove examples of a conjecture which describes the W-algebras in type A as successive quantum Hamiltonian reductions of affine vertex algebras associated with several hook-type nilpotent orbits. This implies that the affine coset subalgebras of hook-type W-algebras are building blocks of the W-algebras in type A. In the rational case, it turns out that the building blocks for the simple quotients are provided by the minimal series of the regular W-algebras. In contrast, they are provided by singlet-type extensions of W-algebras at collapsing levels which are irrational. In the latter case, several new sporadic isomorphisms between different W-algebras are established."
https://arxiv.org/abs/2403.08211,2024-03-12,Large Language Models are Contrastive Reasoners,['Liang Yao'],"Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding ""Let's give a correct and a wrong answer."" before LLMs provide answers. Experiments on two large language models show that zero-shot contrastive prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comparable results when compared to state-of-the-art methods. Our code is available at https://github.com/yao8839836/cp"
https://arxiv.org/abs/2403.08210,2024-03-12,An exact upper bound for the minimum size of a path system that weakly separates a clique,"['George Kontogeorgiou', 'Maya Stein']","We show that for each natural number $n$ the clique $K_n$ has a weakly separating path system of size $n+2$, improving the previous best known upper bound of $(1+o(1))n$. Since $n-1$ is a lower bound, this is almost tight."
https://arxiv.org/abs/2403.08209,2024-03-12,Height-bounded Lempel-Ziv encodings,"['Hideo Bannai', 'Mitsuru Funakoshi', 'Diptarama Hendrian', 'Myuji Matsuda', 'Simon J. Puglisi']","We introduce height-bounded LZ encodings (LZHB), a new family of compressed representations that is a variant of Lempel-Ziv parsings with a focus on allowing fast access to arbitrary positions of the text directly via the compressed representation. Any LZHB encoding whose referencing height is bounded by $h$ allows access to an arbitrary position of the underlying text using $O(h)$ predecessor queries. We show that there exists a constant $c$ such that the size $\hat{z}_{\mathit{HB}(c\log n)}$ of the optimal (smallest) LZHB encoding whose height is bounded by $c\log n$ for any string of length $n$ is $O(\hat{g}_{\mathrm{rl}})$, where $\hat{g}_{\mathrm{rl}}$ is the size of the smallest run-length grammar. Furthermore, we show that there exists a family of strings such that $\hat{z}_{\mathit{HB}(c\log n)} = o(\hat{g}_{\mathrm{rl}})$, thus making $\hat{z}_{\mathit{HB}(c\log n)}$ one of the smallest known repetitiveness measures for which $O(\mathit{polylog} n)$ time access is possible using $O(\hat{z}_{\mathit{HB}(c\log n)})$ space. While computing the optimal LZHB representation for any given height seems difficult, we propose linear and near linear time greedy algorithms which we show experimentally can efficiently find small LZHB representations in practice."
https://arxiv.org/abs/2403.08208,2024-03-12,Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks,"['Khondoker Murad Hossain', 'Tim Oates']","In the rapidly evolving landscape of communication and network security, the increasing reliance on deep neural networks (DNNs) and cloud services for data processing presents a significant vulnerability: the potential for backdoors that can be exploited by malicious actors. Our approach leverages advanced tensor decomposition algorithms Independent Vector Analysis (IVA), Multiset Canonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2) to meticulously analyze the weights of pre-trained DNNs and distinguish between backdoored and clean models effectively. The key strengths of our method lie in its domain independence, adaptability to various network architectures, and ability to operate without access to the training data of the scrutinized models. This not only ensures versatility across different application scenarios but also addresses the challenge of identifying backdoors without prior knowledge of the specific triggers employed to alter network behavior. We have applied our detection pipeline to three distinct computer vision datasets, encompassing both image classification and object detection tasks. The results demonstrate a marked improvement in both accuracy and efficiency over existing backdoor detection methods. This advancement enhances the security of deep learning and AI in networked systems, providing essential cybersecurity against evolving threats in emerging technologies."
https://arxiv.org/abs/2403.08207,2024-03-12,BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network,"['Junwei Su', 'Lingjun Mao', 'Chuan Wu']","Many computer vision and machine learning problems are modelled as learning tasks on heterogeneous graphs, featuring a wide array of relations from diverse types of nodes and edges. Heterogeneous graph neural networks (HGNNs) stand out as a promising neural model class designed for heterogeneous graphs. Built on traditional GNNs, existing HGNNs employ different parameter spaces to model the varied relationships. However, the practical effectiveness of existing HGNNs is often limited to simple heterogeneous graphs with few relation types. This paper first highlights and demonstrates that the standard approach employed by existing HGNNs inevitably leads to parameter explosion and relation collapse, making HGNNs less effective or impractical for complex heterogeneous graphs with numerous relation types. To overcome this issue, we introduce a novel framework, Blend&Grind-HGNN (BG-HGNN), which effectively tackles the challenges by carefully integrating different relations into a unified feature space manageable by a single set of parameters. This results in a refined HGNN method that is more efficient and effective in learning from heterogeneous graphs, especially when the number of relations grows. Our empirical studies illustrate that BG-HGNN significantly surpasses existing HGNNs in terms of parameter efficiency (up to 28.96 $\times$), training throughput (up to 8.12 $\times$), and accuracy (up to 1.07 $\times$)."
https://arxiv.org/abs/2403.08206,2024-03-12,Discrete Semantic Tokenization for Deep CTR Prediction,"['Qijiong Liu', 'Hengchang Hu', 'Jiahao Wu', 'Jieming Zhu', 'Min-Yen Kan', 'Xiao-Ming Wu']","Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings and then caches them, prioritizes space over time. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction."
https://arxiv.org/abs/2403.08205,2024-03-12,PMCV hypersurfaces in non-flat pseudo-Riemannian space forms,"['Chao Yang', 'Jiancheng Liu', 'Li Du']","In this paper, we prove that PMCV (i.e. Δ\vec{H} is proportional to \vec{H}) hypersurface M^n_r of a non-flat pseudo-Riemannian space form N^{n+1}_s(c) with at most two distinct principal curvatures is minimal or locally isoparametric, and compute the mean curvature for the isoparametric ones. As an application, we give full classification results of such non-minimal Lorentzian hypersurfaces of non-flat Lorentz space forms."
https://arxiv.org/abs/2403.08204,2024-03-12,AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction,"['Siqi Li', 'Jun Chen', 'Jingyang Xiang', 'Chengrui Zhu', 'Yong Liu']","Structured pruning methods are developed to bridge the gap between the massive scale of neural networks and the limited hardware resources. Most current structured pruning methods rely on training datasets to fine-tune the compressed model, resulting in high computational burdens and being inapplicable for scenarios with stringent requirements on privacy and security. As an alternative, some data-free methods have been proposed, however, these methods often require handcraft parameter tuning and can only achieve inflexible reconstruction. In this paper, we propose the Automatic Data-Free Pruning (AutoDFP) method that achieves automatic pruning and reconstruction without fine-tuning. Our approach is based on the assumption that the loss of information can be partially compensated by retaining focused information from similar channels. Specifically, We formulate data-free pruning as an optimization problem, which can be effectively addressed through reinforcement learning. AutoDFP assesses the similarity of channels for each layer and provides this information to the reinforcement learning agent, guiding the pruning and reconstruction process of the network. We evaluate AutoDFP with multiple networks on multiple datasets, achieving impressive compression results. For instance, on the CIFAR-10 dataset, AutoDFP demonstrates a 2.87\% reduction in accuracy loss compared to the recently proposed data-free pruning method DFPC with fewer FLOPs on VGG-16. Furthermore, on the ImageNet dataset, AutoDFP achieves 43.17\% higher accuracy than the SOTA method with the same 80\% preserved ratio on MobileNet-V1."
https://arxiv.org/abs/2403.08203,2024-03-12,Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering,"['Yanting Yang', 'Beidi Zhao', 'Zhuohao Ni', 'Yize Zhao', 'Xiaoxiao Li']","Neuroscientific research has revealed that the complex brain network can be organized into distinct functional communities, each characterized by a cohesive group of regions of interest (ROIs) with strong interconnections. These communities play a crucial role in comprehending the functional organization of the brain and its implications for neurological conditions, including Autism Spectrum Disorder (ASD) and biological differences, such as in gender. Traditional models have been constrained by the necessity of predefined community clusters, limiting their flexibility and adaptability in deciphering the brain's functional organization. Furthermore, these models were restricted by a fixed number of communities, hindering their ability to accurately represent the brain's dynamic nature. In this study, we present a token clustering brain transformer-based model ($\texttt{TC-BrainTF}$) for joint community clustering and classification. Our approach proposes a novel token clustering (TC) module based on the transformer architecture, which utilizes learnable prompt tokens with orthogonal loss where each ROI embedding is projected onto the prompt embedding space, effectively clustering ROIs into communities and reducing the dimensions of the node representation via merging with communities. Our results demonstrate that our learnable community-aware model $\texttt{TC-BrainTF}$ offers improved accuracy in identifying ASD and classifying genders through rigorous testing on ABIDE and HCP datasets. Additionally, the qualitative analysis on $\texttt{TC-BrainTF}$ has demonstrated the effectiveness of the designed TC module and its relevance to neuroscience interpretations."
https://arxiv.org/abs/2403.08202,2024-03-12,Trading Large Orders in the Presence of Multiple High-Frequency Anticipatory Traders,"['Ziyi Xu', 'Xue Cheng']","We investigate a market with a normal-speed informed trader (IT) who may employ mixed strategy and multiple anticipatory high-frequency traders (HFTs) who are under different inventory pressures, in a three-period Kyle's model. The pure- and mixed-strategy equilibria are considered and the results provide recommendations for IT's randomization strategy with different numbers of HFTs. Some surprising results about investors' profits arise: the improvement of anticipatory traders' speed or a more precise prediction may harm themselves but help IT."
https://arxiv.org/abs/2403.08201,2024-03-12,Generalised Taylor dispersion of chiral microswimmers,"['Keito Ogawa', 'Kenta Ishimoto']","Transport phenomena of microswimmers in fluid flows play a crucial role in various biological processes, including bioconvection and cell sorting. In this paper, we investigate the dispersion behavior of chiral microswimmers in a simple shear flow utilizing the generalized Taylor dispersion (GTD) theory, motivated by biased locomotion of bacterial swimmers known as bacterial rheotaxis. We thus focus on the influence of shear-induced torque effects due to particle chirality, employing an extended Jeffery equation for individual deterministic dynamics. We then numerically calculate macroscopic parameters including averaged swimming velocity and effective diffusion tensor using spherical harmonic expansion, and argue the obtained results based on the fixed points and their stability of the orientational dynamical systems. Our results reveal that chiral effects induce biased locomotion and we observe qualitative transitions in the orientational distribution with increasing Peclét number, aligning with previous experimental findings. The diffusion tensor analysis highlights significant reduction in the diffusion coefficient perpendicular to the flow plane due to chirality. This suggests potential applications in flow-mediated cell separation and we numerically demonstrate such chirality-induced fluid transportation. The presented methods will be useful in predicting and controlling dispersion behaviors of such chiral microswimmers."
https://arxiv.org/abs/2403.08200,2024-03-12,Prototyping and Experimental Results for Environment-Aware Millimeter Wave Beam Alignment via Channel Knowledge Map,"['Zhuoyin Dai', 'Di Wu', 'Zhenjun Dong', 'Kun Li', 'Dingyang Ding', 'Sihan Wang', 'Yong Zeng']","Channel knowledge map (CKM), which aims to directly reflect the intrinsic channel properties of the local wireless environment, is a novel technique for achieving environmentaware communication. In this paper, to alleviate the large training overhead in millimeter wave (mmWave) beam alignment, an environment-aware and training-free beam alignment prototype is established based on a typical CKM, termed beam index map (BIM). To this end, a general CKM construction method is first presented, and an indoor BIM is constructed offline to learn the candidate transmit and receive beam index pairs for each grid in the experimental area. Furthermore, based on the location information of the receiver (or the dynamic obstacles) from the ultra-wide band (UWB) positioning system, the established BIM is used to achieve training-free beam alignment by directly providing the beam indexes for the transmitter and receiver. Three typical scenarios are considered in the experiment, including quasi-static environment with line-of-sight (LoS) link, quasistatic environment without LoS link and dynamic environment. Besides, the receiver orientation measured from the gyroscope is also used to help CKM predict more accurate beam indexes. The experiment results show that compared with the benchmark location-based beam alignment strategy, the CKM-based beam alignment strategy can achieve much higher received power, which is close to that achieved by exhaustive beam search, but with significantly reduced training overhead."
https://arxiv.org/abs/2403.08199,2024-03-12,Deep Submodular Peripteral Network,"['Gantavya Bhatt', 'Arnav Das', 'Jeff Bilmes']","Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style ""peripteral"" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strategies for training, including active-learning inspired submodular feedback. We demonstrate DSPNs' efficacy in learning submodularity from a costly target submodular function showing superiority in downstream tasks such as experimental design and streaming applications."
https://arxiv.org/abs/2403.08198,2024-03-12,Validating and Exploring Large Geographic Corpora,['Jonathan Dunn'],"This paper investigates the impact of corpus creation decisions on large multi-lingual geographic web corpora. Beginning with a 427 billion word corpus derived from the Common Crawl, three methods are used to improve the quality of sub-corpora representing specific language-country pairs like New Zealand English: (i) the agreement of independent language identification systems, (ii) hash-based deduplication, and (iii) location-specific outlier detection. The impact of each of these steps is then evaluated at the language level and the country level by using corpus similarity measures to compare each resulting corpus with baseline data sets. The goal is to understand the impact of upstream data cleaning decisions on downstream corpora with a specific focus on under-represented languages and populations. The evaluation shows that the validity of sub-corpora is improved with each stage of cleaning but that this improvement is unevenly distributed across languages and populations. This result shows how standard corpus creation techniques can accidentally exclude under-represented populations."
https://arxiv.org/abs/2403.08197,2024-03-12,PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare,"['Chia-Hao Li', 'Niraj K. Jha']","We propose PAGE, a domain-incremental adaptation strategy with past-agnostic generative replay for smart healthcare. PAGE enables generative replay without the aid of any preserved data or information from prior domains. When adapting to a new domain, it exploits real data from the new distribution and the current model to generate synthetic data that retain the learned knowledge of previous domains. By replaying the synthetic data with the new real data during training, PAGE achieves a good balance between domain adaptation and knowledge retention. In addition, we incorporate an extended inductive conformal prediction (EICP) method into PAGE to produce a confidence score and a credibility value for each detection result. This makes the predictions interpretable and provides statistical guarantees for disease detection in smart healthcare applications. We demonstrate PAGE's effectiveness in domain-incremental disease detection with three distinct disease datasets collected from commercially available WMSs. PAGE achieves highly competitive performance against state-of-the-art with superior scalability, data privacy, and feasibility. Furthermore, PAGE can enable up to 75% reduction in clinical workload with the help of EICP."
https://arxiv.org/abs/2403.08196,2024-03-12,SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech Recognition Evaluation,"['Jiayu Du', 'Jinpeng Li', 'Guoguo Chen', 'Wei-Qiang Zhang']","In the wake of the surging tide of deep learning over the past decade, Automatic Speech Recognition (ASR) has garnered substantial attention, leading to the emergence of numerous publicly accessible ASR systems that are actively being integrated into our daily lives. Nonetheless, the impartial and replicable evaluation of these ASR systems encounters challenges due to various crucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a general-purpose, open-source platform designed for ASR evaluation. With this platform: (i) We report a comprehensive benchmark, unveiling the current state-of-the-art panorama for ASR systems, covering both open-source models and industrial commercial services. (ii) We quantize how distinct nuances in the scoring pipeline influence the final benchmark outcomes. These include nuances related to capitalization, punctuation, interjection, contraction, synonym usage, compound words, etc. These issues have gained prominence in the context of the transition towards an End-to-End future. (iii) We propose a practical modification to the conventional Token-Error-Rate (TER) evaluation metric, with inspirations from Kolmogorov complexity and Normalized Information Distance (NID). This adaptation, called modified-TER (mTER), achieves proper normalization and symmetrical treatment of reference and hypothesis. By leveraging this platform as a large-scale testing ground, this study demonstrates the robustness and backward compatibility of mTER when compared to TER. The SpeechColab Leaderboard is accessible at https://github.com/SpeechColab/Leaderboard"
https://arxiv.org/abs/2403.08195,2024-03-12,Efficiently verifiable quantum advantage on near-term analog quantum simulators,"['Zhenning Liu', 'Dhruv Devulapalli', 'Dominik Hangleiter', 'Yi-Kai Liu', 'Alicia J. Kollár', 'Alexey V. Gorshkov', 'Andrew M. Childs']","Existing schemes for demonstrating quantum computational advantage are subject to various practical restrictions, including the hardness of verification and challenges in experimental implementation. Meanwhile, analog quantum simulators have been realized in many experiments to study novel physics. In this work, we propose a quantum advantage protocol based on single-step Feynman-Kitaev verification of an analog quantum simulation, in which the verifier need only run an $O(λ^2)$-time classical computation, and the prover need only prepare $O(1)$ samples of a history state and perform $O(λ^2)$ single-qubit measurements, for a security parameter $λ$. We also propose a near-term feasible strategy for honest provers and discuss potential experimental realizations."
https://arxiv.org/abs/2403.08194,2024-03-12,Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework,"['Yubo Ye', 'Sumeet Vadhavkar', 'Xiajun Jiang', 'Ryan Missel', 'Huafeng Liu', 'Linwei Wang']","Modern applications increasingly require unsupervised learning of latent dynamics from high-dimensional time-series. This presents a significant challenge of identifiability: many abstract latent representations may reconstruct observations, yet do they guarantee an adequate identification of the governing dynamics? This paper investigates this challenge from two angles: the use of physics inductive bias specific to the data being modeled, and a learn-to-identify strategy that separates forecasting objectives from the data used for the identification. We combine these two strategies in a novel framework for unsupervised meta-learning of hybrid latent dynamics (Meta-HyLaD) with: 1) a latent dynamic function that hybridize known mathematical expressions of prior physics with neural functions describing its unknown errors, and 2) a meta-learning formulation to learn to separately identify both components of the hybrid dynamics. Through extensive experiments on five physics and one biomedical systems, we provide strong evidence for the benefits of Meta-HyLaD to integrate rich prior knowledge while identifying their gap to observed data."
https://arxiv.org/abs/2403.08193,2024-03-12,Learning-driven Physically-aware Large-scale Circuit Gate Sizing,"['Yuyang Ye', 'Peng Xu', 'Lizheng Ren', 'Tinghuan Chen', 'Hao Yan', 'Bei Yu', 'Longxing Shi']","Gate sizing plays an important role in timing optimization after physical design. Existing machine learning-based gate sizing works cannot optimize timing on multiple timing paths simultaneously and neglect the physical constraint on layouts. They cause sub-optimal sizing solutions and low-efficiency issues when compared with commercial gate sizing tools. In this work, we propose a learning-driven physically-aware gate sizing framework to optimize timing performance on large-scale circuits efficiently. In our gradient descent optimization-based work, for obtaining accurate gradients, a multi-modal gate sizing-aware timing model is achieved via learning timing information on multiple timing paths and physical information on multiple-scaled layouts jointly. Then, gradient generation based on the sizing-oriented estimator and adaptive back-propagation are developed to update gate sizes. Our results demonstrate that our work achieves higher timing performance improvements in a faster way compared with the commercial gate sizing tool."
https://arxiv.org/abs/2403.08192,2024-03-12,MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension,"['Xingyu Lu', 'He Cao', 'Zijing Liu', 'Shengyuan Bai', 'Leqing Chen', 'Yuan Yao', 'Hai-Tao Zheng', 'Yu Li']","Large language models are playing an increasingly significant role in molecular research, yet existing models often generate erroneous information, posing challenges to accurate molecular comprehension. Traditional evaluation metrics for generated content fail to assess a model's accuracy in molecular understanding. To rectify the absence of factual evaluation, we present MoleculeQA, a novel question answering (QA) dataset which possesses 62K QA pairs over 23K molecules. Each QA pair, composed of a manual question, a positive option and three negative options, has consistent semantics with a molecular description from authoritative molecular corpus. MoleculeQA is not only the first benchmark for molecular factual bias evaluation but also the largest QA dataset for molecular research. A comprehensive evaluation on MoleculeQA for existing molecular LLMs exposes their deficiencies in specific areas and pinpoints several particularly crucial factors for molecular understanding."
https://arxiv.org/abs/2403.08191,2024-03-12,Synchronized Dual-arm Rearrangement via Cooperative mTSP,"['Wenhao Li', 'Shishun Zhang', 'Sisi Dai', 'Hui Huang', 'Ruizhen Hu', 'Xiaohong Chen', 'Kai Xu']","Synchronized dual-arm rearrangement is widely studied as a common scenario in industrial applications. It often faces scalability challenges due to the computational complexity of robotic arm rearrangement and the high-dimensional nature of dual-arm planning. To address these challenges, we formulated the problem as cooperative mTSP, a variant of mTSP where agents share cooperative costs, and utilized reinforcement learning for its solution. Our approach involved representing rearrangement tasks using a task state graph that captured spatial relationships and a cooperative cost matrix that provided details about action costs. Taking these representations as observations, we designed an attention-based network to effectively combine them and provide rational task scheduling. Furthermore, a cost predictor is also introduced to directly evaluate actions during both training and planning, significantly expediting the planning process. Our experimental results demonstrate that our approach outperforms existing methods in terms of both performance and planning efficiency."
https://arxiv.org/abs/2403.08190,2024-03-12,Generalized Chevalley criteria in simplicial homotopy type theory,['Jonathan Weinberger'],"We provide a generalized treatment of (co)cartesian arrows, fibrations, and functors. Compared to the classical conditions, the endpoint inclusions get replaced by arbitrary shape inclusions. Our framework is Riehl--Shulman's simplicial homotopy type theory which supports the development of synthetic internal $(\infty,1)$-category theory."
https://arxiv.org/abs/2403.08189,2024-03-12,Embedded Translations for Low-resource Automated Glossing,"['Changbing Yang', 'Garrett Nicolai', 'Miikka Silfverberg']","We investigate automatic interlinear glossing in low-resource settings. We augment a hard-attentional neural model with embedded translation information extracted from interlinear glossed text. After encoding these translations using large language models, specifically BERT and T5, we introduce a character-level decoder for generating glossed output. Aided by these enhancements, our model demonstrates an average improvement of 3.97\%-points over the previous state of the art on datasets from the SIGMORPHON 2023 Shared Task on Interlinear Glossing. In a simulated ultra low-resource setting, trained on as few as 100 sentences, our system achieves an average 9.78\%-point improvement over the plain hard-attentional baseline. These results highlight the critical role of translation information in boosting the system's performance, especially in processing and interpreting modest data sources. Our findings suggest a promising avenue for the documentation and preservation of languages, with our experiments on shared task datasets indicating significant advancements over the existing state of the art."
https://arxiv.org/abs/2403.08188,2024-03-12,Gauge invariant quantization for circuits including Josephson junctions,['Hiroyasu Koizumi'],"Recently, a new theory of superconductivity has been put forward that attributes the origin of superconductivity to the appearance of a non-trivial Berry connection from many-electron wave functions."
https://arxiv.org/abs/2403.08187,2024-03-12,Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of Speech Sound Disorders in Korean children,"['Taekyung Ahn', 'Yeonjung Hong', 'Younggon Im', 'Do Hyung Kim', 'Dayoung Kang', 'Joo Won Jeong', 'Jae Won Kim', 'Min Jung Kim', 'Ah-ra Cho', 'Dae-Hyun Jang', 'Hosung Nam']","This study presents a model of automatic speech recognition (ASR) designed to diagnose pronunciation issues in children with speech sound disorders (SSDs) to replace manual transcriptions in clinical procedures. Since ASR models trained for general purposes primarily predict input speech into real words, employing a well-known high-performance ASR model for evaluating pronunciation in children with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to recognize speech as pronounced rather than as existing words. The model was fine-tuned with a speech dataset from 137 children with inadequate speech production pronouncing 73 Korean words selected for actual clinical diagnosis. The model's predictions of the pronunciations of the words matched the human annotations with about 90% accuracy. While the model still requires improvement in recognizing unclear pronunciation, this study demonstrates that ASR models can streamline complex pronunciation error diagnostic procedures in clinical fields."
https://arxiv.org/abs/2403.08186,2024-03-12,Hybrid magnon-phonon cavity for large-amplitude terahertz spin-wave excitation,"['Shihao Zhuang', 'Xufeng Zhang', 'Yujie Zhu', 'Nian X. Sun', 'Chang-Beom Eom', 'Paul G. Evans', 'Jia-Mian Hu']","Terahertz (THz) spin waves or their quanta, magnons, can be efficiently excited by acoustic phonons because these excitations have similar wavevectors in the THz regime. THz acoustic phonons can be produced using photoacoustic phenomena but typically have a low population and thus a relatively low displacement amplitude. The magnetization amplitude and population of the acoustically excited THz magnons are thus usually small. Using analytical calculations and dynamical phase-field simulations, we show that a freestanding metal/magnetic-insulator (MI)/dielectric multilayer can be designed to produce large-amplitude THz spin wave via cavity-enhanced magnon-phonon interaction. The amplitude of the acoustically excited THz spin wave in the freestanding multilayer is predicted to be more than ten times larger than in a substrate-supported multilayer. Acoustically excited nonlinear magnon-magnon interaction is demonstrated in the freestanding multilayer. The simulations also indicate that the magnon modes can be detected by probing the charge current in the metal layer generated via spin-charge conversion across the MI/metal interface and the resulting THz radiation. Applications of the freestanding multilayer in THz optoelectronic transduction are computationally demonstrated."
https://arxiv.org/abs/2403.08185,2024-03-12,Perceive With Confidence: Statistical Safety Assurances for Navigation with Learning-Based Perception,"['Anushri Dixit', 'Zhiting Mei', 'Meghan Booker', 'Mariko Storey-Matsutani', 'Allen Z. Ren', 'Anirudha Majumdar']","Rapid advances in perception have enabled large pre-trained models to be used out of the box for processing high-dimensional, noisy, and partial observations of the world into rich geometric representations (e.g., occupancy predictions). However, safe integration of these models onto robots remains challenging due to a lack of reliable performance in unfamiliar environments. In this work, we present a framework for rigorously quantifying the uncertainty of pre-trained perception models for occupancy prediction in order to provide end-to-end statistical safety assurances for navigation. We build on techniques from conformal prediction for producing a calibrated perception system that lightly processes the outputs of a pre-trained model while ensuring generalization to novel environments and robustness to distribution shifts in states when perceptual outputs are used in conjunction with a planner. The calibrated system can be used in combination with any safe planner to provide an end-to-end statistical assurance on safety in a new environment with a user-specified threshold $1-ε$. We evaluate the resulting approach - which we refer to as Perceive with Confidence (PwC) - with experiments in simulation and on hardware where a quadruped robot navigates through indoor environments containing objects unseen during training or calibration. These experiments validate the safety assurances provided by PwC and demonstrate significant improvements in empirical safety rates compared to baselines."
https://arxiv.org/abs/2403.08184,2024-03-12,Quantum skyrmion dynamics studied by neural network quantum states,"['Ashish Joshi', 'Robert Peters', 'Thore Posske']","We study the dynamics of quantum skyrmions under a magnetic field gradient using neural network quantum states. First, we obtain a quantum skyrmion lattice ground state using variational Monte Carlo with a restricted Boltzmann machine as the variational ansatz for a quantum Heisenberg model with Dzyaloshinskii-Moriya interaction. Then, using the time-dependent variational principle, we study the real-time evolution of quantum skyrmions after a Hamiltonian quench with an inhomogeneous external magnetic field. We show that field gradients are an effective way of manipulating and moving quantum skyrmions. Furthermore, we demonstrate that quantum skyrmions can decay when interacting with each other. This work shows that neural network quantum states offer a promising way of studying the real-time evolution of quantum magnetic systems that are outside the realm of exact diagonalization."
https://arxiv.org/abs/2403.08183,2024-03-12,Causal Interpretation of Estimands Defined by Exposure Mappings,['Michael P. Leung'],"In settings with interference, it is common to utilize estimands defined by exposure mappings to summarize the impact of variation in treatment assignments local to the ego. This paper studies their causal interpretation under weak restrictions on interference. We demonstrate that the estimands can exhibit unpalatable sign reversals under conventional identification conditions. This motivates the formulation of sign preservation criteria for causal interpretability. To satisfy preferred criteria, it is necessary to impose restrictions on interference, either in potential outcomes or selection into treatment. We provide sufficient conditions and show that they are satisfied by a nonparametric model allowing for a complex form of interference in both the outcome and selection stages."
https://arxiv.org/abs/2403.08182,2024-03-12,SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention,"['Feng Xiao', 'Hongbin Xu', 'Qiuxia Wu', 'Wenxiong Kang']","3D visual grounding aims to automatically locate the 3D region of the specified object given the corresponding textual description. Existing works fail to distinguish similar objects especially when multiple referred objects are involved in the description. Experiments show that direct matching of language and visual modal has limited capacity to comprehend complex referential relationships in utterances. It is mainly due to the interference caused by redundant visual information in cross-modal alignment. To strengthen relation-orientated mapping between different modalities, we propose SeCG, a semantic-enhanced relational learning model based on a graph network with our designed memory graph attention layer. Our method replaces original language-independent encoding with cross-modal encoding in visual analysis. More text-related feature expressions are obtained through the guidance of global semantics and implicit relationships. Experimental results on ReferIt3D and ScanRefer benchmarks show that the proposed method outperforms the existing state-of-the-art methods, particularly improving the localization performance for the multi-relation challenges."
https://arxiv.org/abs/2403.08181,2024-03-12,Differential Privacy in Nonlinear Dynamical Systems with Tracking Performance Guarantees,"['Dhrubajit Chowdhury', 'Raman Goyal', 'Shantanu Rane']","We introduce a novel approach to make the tracking error of a class of nonlinear systems differentially private in addition to guaranteeing the tracking error performance. We use funnel control to make the tracking error evolve within a performance funnel that is pre-specified by the user. We make the performance funnel differentially private by adding a bounded continuous noise generated from an Ornstein-Uhlenbeck-type process. Since the funnel controller is a function of the performance funnel, the noise adds randomized perturbation to the control input. We show that, as a consequence of the differential privacy of the performance funnel, the tracking error is also differentially private. As a result, the tracking error is bounded by the noisy funnel boundary while maintaining privacy. We show a simulation result to demonstrate the framework."
https://arxiv.org/abs/2403.08180,2024-03-12,Thermal Hall effect in a van der Waals ferromagnet CrI3,"['Chunqiang Xu', 'Heda Zhang', 'Caitlin Carnahan', 'Pengpeng Zhang', 'Di Xiao', 'Xianglin Ke']","CrI3 is a prototypical van der Waals ferromagnet with a magnetic honeycomb lattice. Previous inelastic neutron scattering studies have suggested topological nature of its magnetic excitations with a magnon gap at the Dirac points, which are anticipated to give rise to magnon thermal Hall effect. Here we report thermal transport properties of CrI3 and show that the long-sought thermal Hall signal anticipated for topological magnons is fairly small. In contrast, we find that CrI3 exhibits an appreciable anomalous thermal Hall signal at lower temperature which may arise from magnon-phonon hybridization or magnon-phonon scattering. These findings are anticipated to stimulate further neutron scattering studies on CrI3 single crystal, which can shed light not only on the intrinsic nature of magnetic excitations but also on the magnon-phonon interaction."
https://arxiv.org/abs/2403.08179,2024-03-12,Effects of wave damping and finite perpendicular scale on three-dimensional Alfvén wave parametric decay in low-beta plasmas,"['Feiyu Li', 'Xiangrong Fu', 'Seth Dorfman']","Shear Alfven wave parametric decay instability (PDI) provides a potential path toward significant wave dissipation and plasma heating. However, fundamental questions regarding how PDI is excited in a realistic three-dimensional (3D) open system and how critically the finite perpendicular wave scale -- as found in both the laboratory and space plasmas -- affects the excitation remain poorly understood. Here, we present the first 3D, open-boundary, hybrid kinetic-fluid simulations of kinetic Alfven wave PDI in low-beta plasmas. Key findings are that the PDI excitation is strongly limited by the wave damping present, including electron-ion collisional damping (represented by a constant resistivity) and geometrical attenuation associated with the finite-scale Alfven wave, and ion Landau damping of the child acoustic wave. The perpendicular wave scale alone, however, plays no discernible role, with different wave scales exhibiting similar instability growth. These findings are corroborated by theoretical analysis and estimates. The new understanding of 3D kinetic Alfven wave PDI physics is essential for laboratory study of the basic plasma process and may also help evaluate the relevance/role of PDI in low-beta space plasmas."
https://arxiv.org/abs/2403.08178,2024-03-12,Learning Barrier-Certified Polynomial Dynamical Systems for Obstacle Avoidance with Robots,"['Martin Schonger', 'Hugo T. M. Kussaba', 'Lingyun Chen', 'Luis Figueredo', 'Abdalla Swikir', 'Aude Billard', 'Sami Haddadin']","Established techniques that enable robots to learn from demonstrations are based on learning a stable dynamical system (DS). To increase the robots' resilience to perturbations during tasks that involve static obstacle avoidance, we propose incorporating barrier certificates into an optimization problem to learn a stable and barrier-certified DS. Such optimization problem can be very complex or extremely conservative when the traditional linear parameter-varying formulation is used. Thus, different from previous approaches in the literature, we propose to use polynomial representations for DSs, which yields an optimization problem that can be tackled by sum-of-squares techniques. Finally, our approach can handle obstacle shapes that fall outside the scope of assumptions typically found in the literature concerning obstacle avoidance within the DS learning framework. Supplementary material can be found at the project webpage: https://martinschonger.github.io/abc-ds"
https://arxiv.org/abs/2403.08177,2024-03-12,A Direct Algorithm for Multi-Gyroscope Infield Calibration,"['Tianheng Wang', 'Stergios I. Roumeliotis']","In this paper, we address the problem of estimating the rotational extrinsics, as well as the scale factors of two gyroscopes rigidly mounted on the same device. In particular, we formulate the problem as a least-squares minimization and introduce a direct algorithm that computes the estimated quantities without any iterations, hence avoiding local minima and improving efficiency. Furthermore, we show that the rotational extrinsics are observable while the scale factors can be determined up to global scale for general configurations of the gyroscopes. To this end, we also study special placements of the gyroscopes where a pair, or all, of their axes are parallel and analyze their impact on the scale factors' observability. Lastly, we evaluate our algorithm in simulations and real-world experiments to assess its performance as a function of key motion and sensor characteristics."
https://arxiv.org/abs/2403.08176,2024-03-12,"Sentiment-aware Enhancements of PageRank-based Citation Metric, Impact Factor, and H-index for Ranking the Authors of Scholarly Articles","['Shikha Gupta', 'Animesh Kumar']","Heretofore, the only way to evaluate an author has been frequency-based citation metrics that assume citations to be of a neutral sentiment. However, considering the sentiment behind citations aids in a better understanding of the viewpoints of fellow researchers for the scholarly output of an author."
https://arxiv.org/abs/2403.08175,2024-03-12,A forward-modelling approach to overcome PSF smearing and fit flexible models to the chemical structure of galaxies,"['Benjamin Metha', 'Simon Birrer', 'Tommaso Treu', 'Michele Trenti', 'Xuheng Ding', 'Xin Wang']","Historically, metallicity profiles of galaxies have been modelled using a radially symmetric, two-parameter linear model, which reveals that most galaxies are more metal-rich in their central regions than their outskirts. However, this model is known to yield inaccurate results when the point-spread function (PSF) of a telescope is large. Furthermore, a radially symmetric model cannot capture asymmetric structures within a galaxy. In this work, we present an extension of the popular forward-modelling python package LENSTRONOMY, which allows the user to overcome both of these obstacles. We demonstrate the new features of this code base through two illustrative examples on simulated data. First, we show that through forward modelling, LENSTRONOMY is able to recover accurately the metallicity gradients of galaxies, even when the PSF is comparable to the size of a galaxy, as long as the data is observed with a sufficient number of pixels. Additionally, we demonstrate how LENSTRONOMY is able to fit irregular metallicity profiles to galaxies that are not well-described by a simple surface brightness profile. This opens up pathways for detailed investigations into the connections between morphology and chemical structure for galaxies at cosmological distances using the transformative capabilities of JWST. Our code is publicly available and open source, and can also be used to model spatial distributions of other galaxy properties that are traced by its surface brightness profile."
https://arxiv.org/abs/2403.08174,2024-03-12,Rethinking Loss Functions for Fact Verification,"['Yuta Mukobara', 'Yutaro Shigeto', 'Masashi Shimbo']","We explore loss functions for fact verification in the FEVER shared task. While the cross-entropy loss is a standard objective for training verdict predictors, it fails to capture the heterogeneity among the FEVER verdict classes. In this paper, we develop two task-specific objectives tailored to FEVER. Experimental results confirm that the proposed objective functions outperform the standard cross-entropy. Performance is further improved when these objectives are combined with simple class weighting, which effectively overcomes the imbalance in the training data. The souce code is available at https://github.com/yuta-mukobara/RLF-KGAT"
https://arxiv.org/abs/2403.08173,2024-03-12,"A bargain for mergesorts (functional pearl) -- How to prove your mergesort correct and stable, almost for free","['Cyril Cohen', 'Kazuhiko Sakaguchi']","We present a novel characterization of stable mergesort functions using relational parametricity, and show that it implies the correctness of mergesort. As a result, one can prove the correctness of several variations of mergesort (e.g., top-down, bottom-up, tail-recursive, non-tail-recursive, smooth, and non-smooth mergesorts) by proving the characterization property for each variation. To further motivate this work, we show a performance trade-off between tail-recursive and non-tail-recursive mergesorts that (1) the former in call-by-value evaluation avoids using up stack space and is efficient and (2) the latter in call-by-need evaluation is an optimal incremental sort, meaning that it performs only $\mathcal{O}(n + k \log k)$ comparisons to compute the least (or greatest) $k$ items of a list of length $n$. Thanks to our characterization and the parametricity translation, we deduced the correctness results, including stability, of various implementations of mergesort for lists, including highly optimized ones, in the Coq proof assistant."
https://arxiv.org/abs/2403.08172,2024-03-12,RKKY Interactions and Multipole Order in Ab initio Wannier Model of CeCoSi,"['Takemi Yamada', 'Yuki Yanagi', 'Keisuke Mitsumoto']","We calculate the RKKY interactions derived from ab initio calculations for the intermetallic compound CeCoSi exhibiting the hidden nonmagnetic order at $T_0$ and examine the instability towards possible multipole orders within the random phase approximation. All 36 multipole interactions up to rank 5 are investigated, and the maximum susceptibility exhibits an antiferro order with $\boldsymbol{q}=\boldsymbol{0}$ for nonmagnetic multipoles of monopole $I$ and hexadecapole $H_{0}$, yielding a charge imbalance of $f$ electrons at two Ce atoms in the unit cell, which is consistent with some experiments."
https://arxiv.org/abs/2403.08171,2024-03-12,Tractable Local Equilibria in Non-Concave Games,"['Yang Cai', 'Constantinos Daskalakis', 'Haipeng Luo', 'Chen-Yu Wei', 'Weiqiang Zheng']","While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when the utilities are non-concave, a situation that is common in machine learning applications where the agents' strategies are parameterized by deep neural networks, or the agents' utilities are computed by a neural network, or both. Indeed, non-concave games present a host of game-theoretic and optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash equilibria exist but are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria have infinite support in general, and are intractable. To sidestep these challenges we propose a new solution concept, termed $(\varepsilon, Φ(δ))$-local equilibrium, which generalizes local Nash equilibrium in non-concave games, as well as (coarse) correlated equilibrium in concave games. Importantly, we show that two instantiations of this solution concept capture the convergence guarantees of Online Gradient Descent and no-regret learning, which we show efficiently converge to this type of equilibrium in non-concave games with smooth utilities."
https://arxiv.org/abs/2403.08170,2024-03-12,Versatile Defense Against Adversarial Attacks on Image Recognition,"['Haibo Zhang', 'Zhihua Yao', 'Kouichi Sakurai']","Adversarial attacks present a significant security risk to image recognition tasks. Defending against these attacks in a real-life setting can be compared to the way antivirus software works, with a key consideration being how well the defense can adapt to new and evolving attacks. Another important factor is the resources involved in terms of time and cost for training defense models and updating the model database. Training many models that are specific to each type of attack can be time-consuming and expensive. Ideally, we should be able to train one single model that can handle a wide range of attacks. It appears that a defense method based on image-to-image translation may be capable of this. The proposed versatile defense approach in this paper only requires training one model to effectively resist various unknown adversarial attacks. The trained model has successfully improved the classification accuracy from nearly zero to an average of 86%, performing better than other defense methods proposed in prior studies. When facing the PGD attack and the MI-FGSM attack, versatile defense model even outperforms the attack-specific models trained based on these two attacks. The robustness check also shows that our versatile defense model performs stably regardless with the attack strength."
https://arxiv.org/abs/2403.08169,2024-03-12,Globalized distributionally robust optimization with multi core sets,"['Yueyao Li', 'Chenglong Bao', 'Wenxun Xing']","It is essential to capture the true probability distribution of uncertain data in the distributionally robust optimization (DRO). The uncertain data presents multimodality in numerous application scenarios, in the sense that the probability density function of the uncertain data has two or more modes (local maximums). In this paper, we propose a globalized distributionally robust optimization framework with multiple core sets (MGDRO) to handle the multimodal data. This framework captures the multimodal structure via a penalty function composed of the minimum distances from the random vector to all core sets. Under some assumptions, the MGDRO model can be reformulated as tractable semi-definite programs for both moment-based and metric-based ambiguity sets. We applied the MGDRO models to a multi-product newswendor problem with multimodal demands. The numerical results turn out that the MGDRO models outperform traditional DRO models and other multimodal models greatly."
https://arxiv.org/abs/2403.08168,2024-03-12,Collaborative Automotive Radar Sensing via Mixed-Precision Distributed Array Completion,"['Arian Eamaz', 'Farhang Yeganegi', 'Yunqiao Hu', 'Mojtaba Soltanalian', 'Shunqiao Sun']","This paper investigates the effects of coarse quantization with mixed precision on measurements obtained from sparse linear arrays, synthesized by a collaborative automotive radar sensing strategy. The mixed quantization precision significantly reduces the data amount that needs to be shared from radar nodes to the fusion center for coherent processing. We utilize the low-rank properties inherent in the constructed Hankel matrix of the mixed-precision array, to recover azimuth angles from quantized measurements. Our proposed approach addresses the challenge of mixed-quantized Hankel matrix completion, allowing for accurate estimation of the azimuth angles of interest. To evaluate the recovery performance of the proposed scheme, we establish a quasi-isometric embedding with a high probability for mixed-precision quantization. The effectiveness of our proposed scheme is demonstrated through numerical results, highlighting successful reconstruction."
https://arxiv.org/abs/2403.08167,2024-03-12,"MolBind: Multimodal Alignment of Language, Molecules, and Proteins","['Teng Xiao', 'Chao Cui', 'Huaisheng Zhu', 'Vasant G. Honavar']","Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-shot learning performance across a wide range of tasks, demonstrating its strong capability of capturing the underlying semantics of multiple modalities."
https://arxiv.org/abs/2403.08166,2024-03-12,A Darcy law with memory by homogenisation for evolving microstructure,"['David Wiedeman', 'Malte A. Peter']","We consider the homogenisation of the instationary Stokes equations in a porous medium with an a-priori given evolving microstructure. In order to pass to the homogenisation limit, we transform the Stokes equations to a domain with a fixed periodic microstructure. The homogenisation result is a Darcy-type equation with memory term and has the form of an integro-differential equation. The evolving microstructure leads to a time and space dependent permeability coefficient and the local change of the porosity causes an additional source term for the pressure."
https://arxiv.org/abs/2403.08165,2024-03-12,"SN 2023zaw: an ultra-stripped, nickel-poor supernova from a low-mass progenitor","['Kaustav K. Das', 'Christoffer Fremling', 'Mansi M. Kasliwal', 'Steve Schulze', 'Jesper Sollerman', 'Viraj Karambelkar', 'Sam Rose', 'Shreya Anand', 'Igor Andreoni', 'Marie Aubert', 'Sean J. Brennan', 'S. Bradley Cenko', 'Michael W. Coughlin', ""B. O'Connor"", 'Kishalay De', 'Jim Fuller', 'Matthew Graham', 'Erica Hammerstein', 'Annastasia Haynie', 'K-Ryan Hinds', 'Io Kleiser', 'S. R. Kulkarni', 'Zeren Lin', 'Chang Liu', 'Ashish A. Mahabal']","We present SN 2023zaw $-$ a sub-luminous ($\mathrm{M_r} = -16.7$ mag) and rapidly-evolving supernova ($\mathrm{t_{1/2,r}} = 4.9$ days), with the lowest nickel mass ($\approx0.002$ $\mathrm{M_\odot}$) measured among all stripped-envelope supernovae discovered to date. The photospheric spectra are dominated by broad He I and Ca NIR emission lines with velocities of $\sim10\ 000 - 12\ 000$ $\mathrm{km\ s^{-1}}$. The late-time spectra show prominent narrow He I emission lines at $\sim$1000$\ \mathrm{km\ s^{-1}}$, indicative of interaction with He-rich circumstellar material. SN 2023zaw is located in the spiral arm of a star-forming galaxy. We perform radiation-hydrodynamical and analytical modeling of the lightcurve by fitting with a combination of shock-cooling emission and nickel decay. The progenitor has a best-fit envelope mass of $\approx0.2$ $\mathrm{M_\odot}$ and an envelope radius of $\approx50$ $\mathrm{R_\odot}$. The extremely low nickel mass and low ejecta mass ($\approx0.5$ $\mathrm{M_\odot}$) suggest an ultra-stripped SN, which originates from a mass-losing low mass He-star (ZAMS mass $<$ 10 $\mathrm{M_\odot}$) in a close binary system. This is a channel to form double neutron star systems, whose merger is detectable with LIGO. SN 2023zaw underscores the existence of a previously undiscovered population of extremely low nickel mass ($< 0.005$ $\mathrm{M_\odot}$) stripped-envelope supernovae, which can be explored with deep and high-cadence transient surveys."
https://arxiv.org/abs/2403.08164,2024-03-12,EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech,"['Ziqi Liang', 'Haoxiang Shi', 'Jiawei Wang', 'Keda Lu']","Recently, deep learning-based Text-to-Speech (TTS) systems have achieved high-quality speech synthesis results. Recurrent neural networks have become a standard modeling technique for sequential data in TTS systems and are widely used. However, training a TTS model which includes RNN components requires powerful GPU performance and takes a long time. In contrast, CNN-based sequence synthesis techniques can significantly reduce the parameters and training time of a TTS model while guaranteeing a certain performance due to their high parallelism, which alleviate these economic costs of training. In this paper, we propose a lightweight TTS system based on deep convolutional neural networks, which is a two-stage training end-to-end TTS model and does not employ any recurrent units. Our model consists of two stages: Text2Spectrum and SSRN. The former is used to encode phonemes into a coarse mel spectrogram and the latter is used to synthesize the complete spectrum from the coarse mel spectrogram. Meanwhile, we improve the robustness of our model by a series of data augmentations, such as noise suppression, time warping, frequency masking and time masking, for solving the low resource mongolian problem. Experiments show that our model can reduce the training time and parameters while ensuring the quality and naturalness of the synthesized speech compared to using mainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for validation, which significantly reduces training time while maintaining a certain accuracy."
https://arxiv.org/abs/2403.08163,2024-03-12,Effective Underwater Glider Path Planning in Dynamic 3D Environments Using Multi-Point Potential Fields,"['Hanzhi Yang', 'Nina Mahmoudian']","Underwater gliders (UGs) have emerged as highly effective unmanned vehicles for ocean exploration. However, their operation in dynamic and complex underwater environments necessitates robust path-planning strategies. Previous studies have primarily focused on global energy or time-efficient path planning in explored environments, overlooking challenges posed by unpredictable flow conditions and unknown obstacles in varying and dynamic areas like fjords and near-harbor waters. This paper introduces and improves a real-time path planning method, Multi-Point Potential Field (MPPF), tailored for UGs operating in 3D space as they are constrained by buoyancy propulsion and internal actuation. The proposed MPPF method addresses obstacles, flow fields, and local minima, enhancing the efficiency and robustness of UG path planning. A low-cost prototype, the Research Oriented Underwater Glider for Hands-on Investigative Engineering (ROUGHIE), is utilized for validation. Through case studies and simulations, the efficacy of the enhanced MPPF method is demonstrated, highlighting its potential for real-world applications in underwater exploration."
https://arxiv.org/abs/2403.08162,2024-03-12,Iterative Learning for Joint Image Denoising and Motion Artifact Correction of 3D Brain MRI,"['Lintao Zhang', 'Mengqi Wu', 'Lihong Wang', 'David C. Steffens', 'Guy G. Potter', 'Mingxia Liu']","Image noise and motion artifacts greatly affect the quality of brain MRI and negatively influence downstream medical image analysis. Previous studies often focus on 2D methods that process each volumetric MR image slice-by-slice, thus losing important 3D anatomical information. Additionally, these studies generally treat image denoising and artifact correction as two standalone tasks, without considering their potential relationship, especially on low-quality images where severe noise and motion artifacts occur simultaneously. To address these issues, we propose a Joint image Denoising and motion Artifact Correction (JDAC) framework via iterative learning to handle noisy MRIs with motion artifacts, consisting of an adaptive denoising model and an anti-artifact model. In the adaptive denoising model, we first design a novel noise level estimation strategy, and then adaptively reduce the noise through a U-Net backbone with feature normalization conditioning on the estimated noise variance. The anti-artifact model employs another U-Net for eliminating motion artifacts, incorporating a novel gradient-based loss function designed to maintain the integrity of brain anatomy during the motion correction process. These two models are iteratively employed for joint image denoising and artifact correction through an iterative learning framework. An early stopping strategy depending on noise level estimation is applied to accelerate the iteration process. The denoising model is trained with 9,544 T1-weighted MRIs with manually added Gaussian noise as supervision. The anti-artifact model is trained on 552 T1-weighted MRIs with motion artifacts and paired motion-free images. Experimental results on a public dataset and a clinical study suggest the effectiveness of JDAC in both tasks of denoising and motion artifact correction, compared with several state-of-the-art methods."
https://arxiv.org/abs/2403.08161,2024-03-12,LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition,"['Zhonglin Sun', 'Chen Feng', 'Ioannis Patras', 'Georgios Tzimiropoulos']","In this work we focus on learning facial representations that can be adapted to train effective face recognition models, particularly in the absence of labels. Firstly, compared with existing labelled face datasets, a vastly larger magnitude of unlabeled faces exists in the real world. We explore the learning strategy of these unlabeled facial images through self-supervised pretraining to transfer generalized face recognition performance. Moreover, motivated by one recent finding, that is, the face saliency area is critical for face recognition, in contrast to utilizing random cropped blocks of images for constructing augmentations in pretraining, we utilize patches localized by extracted facial landmarks. This enables our method - namely LAndmark-based Facial Self-supervised learning LAFS), to learn key representation that is more critical for face recognition. We also incorporate two landmark-specific augmentations which introduce more diversity of landmark information to further regularize the learning. With learned landmark-based facial representations, we further adapt the representation for face recognition with regularization mitigating variations in landmark positions. Our method achieves significant improvement over the state-of-the-art on multiple face recognition benchmarks, especially on more challenging few-shot scenarios."
https://arxiv.org/abs/2403.08160,2024-03-12,Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime,"['Hong Hu', 'Yue M. Lu', 'Theodor Misiakiewicz']","Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data. It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities. This leaves open the question of understanding the impact of parametrization on the performance of these models. How does model complexity and generalization depend on the number of parameters $p$? How should we choose $p$ relative to the sample size $n$ to achieve optimal test error?"
https://arxiv.org/abs/2403.08159,2024-03-12,Exponential Stability of Parametric Optimization-Based Controllers via Lur'e Contractivity,"['Alexander Davydov', 'Francesco Bullo']","In this letter, we investigate sufficient conditions for the exponential stability of LTI systems driven by controllers derived from parametric optimization problems. Our primary focus is on parametric projection controllers, namely parametric programs whose objective function is the squared distance to a nominal controller. Leveraging the virtual system method of analysis and a novel contractivity result for Lur'e systems, we establish a sufficient LMI condition for the exponential stability of an LTI system with a parametric projection-based controller. Separately, we prove additional results for single-integrator systems. Finally, we apply our results to state-dependent saturated control systems and control barrier function-based control and provide numerical simulations."
https://arxiv.org/abs/2403.08158,2024-03-12,Dipolar BF theory and dipolar braiding statistics,['Jung Hoon Han'],"We analyze the recently proposed dipolar BF theory with couplings to charge and dipole currents. The quasiparticles of the theory are either charge-like or dipole-like, and the mutual braiding statistics between charge-like and dipole-like quasiparticles are dipolar, meaning that it depends on the position of the quasiparticle being encircled. The braiding statistics between two dipole-like quasiparticles is that of ordinary anyons. We further prove that the dipolar BF theory is equivalent to the rank-2 tensor BF theory developed earlier as an effective theory for the rank-2 toric code. Although the two theories are equivalent, the dipolar BF formulation embodies the dipole symmetry explicitly and gives a clean insight into the way the dipole symmetry manifests itself in various conservation laws and the dipolar braiding statistics."
https://arxiv.org/abs/2403.08157,2024-03-12,Multiscale Low-Frequency Memory Network for Improved Feature Extraction in Convolutional Neural Networks,"['Fuzhi Wu', 'Jiasong Wu', 'Youyong Kong', 'Chunfeng Yang', 'Guanyu Yang', 'Huazhong Shu', 'Guy Carrault', 'Lotfi Senhadji']","Deep learning and Convolutional Neural Networks (CNNs) have driven major transformations in diverse research areas. However, their limitations in handling low-frequency information present obstacles in certain tasks like interpreting global structures or managing smooth transition images. Despite the promising performance of transformer structures in numerous tasks, their intricate optimization complexities highlight the persistent need for refined CNN enhancements using limited resources. Responding to these complexities, we introduce a novel framework, the Multiscale Low-Frequency Memory (MLFM) Network, with the goal to harness the full potential of CNNs while keeping their complexity unchanged. The MLFM efficiently preserves low-frequency information, enhancing performance in targeted computer vision tasks. Central to our MLFM is the Low-Frequency Memory Unit (LFMU), which stores various low-frequency data and forms a parallel channel to the core network. A key advantage of MLFM is its seamless compatibility with various prevalent networks, requiring no alterations to their original core structure. Testing on ImageNet demonstrated substantial accuracy improvements in multiple 2D CNNs, including ResNet, MobileNet, EfficientNet, and ConvNeXt. Furthermore, we showcase MLFM's versatility beyond traditional image classification by successfully integrating it into image-to-image translation tasks, specifically in semantic segmentation networks like FCN and U-Net. In conclusion, our work signifies a pivotal stride in the journey of optimizing the efficacy and efficiency of CNNs with limited resources. This research builds upon the existing CNN foundations and paves the way for future advancements in computer vision. Our codes are available at https://github.com/AlphaWuSeu/ MLFM."
https://arxiv.org/abs/2403.08156,2024-03-12,NeRF-Supervised Feature Point Detection and Description,"['Ali Youssef', 'Francisco Vasconcelos']","Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition. While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability. This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation. We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry. Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches."
https://arxiv.org/abs/2403.08155,2024-03-12,Enhancing Space Situational Awareness to Mitigate Risk: A Case Study in the Misidentification of Starlink Satellites as UAP in Commercial Aviation,"['Douglas J. Buettner', 'Richard E. Griffiths', 'Nick Snell', 'John Stilley']","Over the past several years, the misidentification of SpaceX Starlink satellites as Unidentified Aerial Phenomena (UAP) by pilots and laypersons has generated unnecessary aviation risk and confusion. The many deployment and orbital evolution strategies, coupled with changing sun specular reflection angles, contribute to this gap in space situational awareness. In this paper we present a case analysis of an incident that generated multiple, corroborating reports of a UAP from five pilots on two commercial airline flights over the Pacific Ocean on August 10th, 2022. This incident included two cell phone photos and a video of an unrecognizable and possibly anomalous phenomenon. We then use supplemental two-line elements (TLEs) for the Starlink train of satellites launched that same day and Automatic Dependent Surveillance Broadcast (ADS-B) data from the flight with the photographs to reconstruct a view of these satellites from the cockpit at the time and place of the sighting. The success of this work demonstrates an approach that could, in principle, warn aviators about satellites that could be visible in unusual or novel illumination configurations, thus increasing space situational awareness and supporting aviation safety. We conclude with recommendations for governments and satellite operators to provide better a-priori data that can be used to create advisories to aviators and the public. The automated simulation of known specular reflection off constellations of satellites could also support researchers investigating sightings of unfamiliar aerial or aerospace objects as likely being from normal versus novel space events."
https://arxiv.org/abs/2403.08154,2024-03-12,The Effect of Different Optimization Strategies to Physics-Constrained Deep Learning for Soil Moisture Estimation,"['Jianxin Xie', 'Bing Yao', 'Zheyu Jiang']","Soil moisture is a key hydrological parameter that has significant importance to human society and the environment. Accurate modeling and monitoring of soil moisture in crop fields, especially in the root zone (top 100 cm of soil), is essential for improving agricultural production and crop yield with the help of precision irrigation and farming tools. Realizing the full sensor data potential depends greatly on advanced analytical and predictive domain-aware models. In this work, we propose a physics-constrained deep learning (P-DL) framework to integrate physics-based principles on water transport and water sensing signals for effective reconstruction of the soil moisture dynamics. We adopt three different optimizers, namely Adam, RMSprop, and GD, to minimize the loss function of P-DL during the training process. In the illustrative case study, we demonstrate the empirical convergence of Adam optimizers outperforms the other optimization methods in both mini-batch and full-batch training."
https://arxiv.org/abs/2403.08153,2024-03-12,The Runtime of Random Local Search on the Generalized Needle Problem,"['Benjamin Doerr', 'Andrew James Kelley']","In their recent work, C. Doerr and Krejca (Transactions on Evolutionary Computation, 2023) proved upper bounds on the expected runtime of the randomized local search heuristic on generalized Needle functions. Based on these upper bounds, they deduce in a not fully rigorous manner a drastic influence of the needle radius $k$ on the runtime."
https://arxiv.org/abs/2403.08152,2024-03-12,Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor Re-planning,"['Gilhyun Ryou', 'Geoffrey Wang', 'Sertac Karaman']","High-speed online trajectory planning for UAVs poses a significant challenge due to the need for precise modeling of complex dynamics while also being constrained by computational limitations. This paper presents a multi-fidelity reinforcement learning method (MFRL) that aims to effectively create a realistic dynamics model and simultaneously train a planning policy that can be readily deployed in real-time applications. The proposed method involves the co-training of a planning policy and a reward estimator; the latter predicts the performance of the policy's output and is trained efficiently through multi-fidelity Bayesian optimization. This optimization approach models the correlation between different fidelity levels, thereby constructing a high-fidelity model based on a low-fidelity foundation, which enables the accurate development of the reward model with limited high-fidelity experiments. The framework is further extended to include real-world flight experiments in reinforcement learning training, allowing the reward model to precisely reflect real-world constraints and broadening the policy's applicability to real-world scenarios. We present rigorous evaluations by training and testing the planning policy in both simulated and real-world environments. The resulting trained policy not only generates faster and more reliable trajectories compared to the baseline snap minimization method, but it also achieves trajectory updates in 2 ms on average, while the baseline method takes several minutes."
https://arxiv.org/abs/2403.08151,2024-03-12,Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations,"['Charles Edison Tripp', 'Jordan Perr-Sauer', 'Jamil Gafur', 'Amabarish Nag', 'Avi Purkayastha', 'Sagi Zisman', 'Erik A. Bensen']","Addressing the so-called ``Red-AI'' trend of rising energy consumption by large-scale neural networks, this study investigates the actual energy consumption, as measured by node-level watt-meters, of training various fully connected neural network architectures. We introduce the BUTTER-E dataset, an augmentation to the BUTTER Empirical Deep Learning dataset, containing energy consumption and performance data from 63,527 individual experimental runs spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU hardware collected using node-level watt-meters. This dataset reveals the complex relationship between dataset size, network structure, and energy use, and highlights the impact of cache effects. We propose a straightforward and effective energy model that accounts for network size, computing, and memory hierarchy. Our analysis also uncovers a surprising, hardware-mediated non-linear relationship between energy efficiency and network design, challenging the assumption that reducing the number of parameters or FLOPs is the best way to achieve greater energy efficiency. Highlighting the need for cache-considerate algorithm development, we suggest a combined approach to energy efficient network, algorithm, and hardware design. This work contributes to the fields of sustainable computing and Green AI, offering practical guidance for creating more energy-efficient neural networks and promoting sustainable AI."
https://arxiv.org/abs/2403.08150,2024-03-12,Probing Depth Variations of Solar Inertial Modes through Normal Mode Coupling,"['Krishnendu Mandal', 'Shravan M. Hanasoge']","Recently discovered inertial waves, observed on the solar surface, likely extend to the deeper layers of the Sun. Utilizing helioseismic techniques, we explore these motions, allowing us to discern inertial-mode eigenfunctions in both radial and latitudinal orientations. We analyze $8$ years of space-based observations ($2010 - 2017$) taken by the Helioseismic and Magnetic Imager (HMI) onboard the Solar dynamic observatory (SDO) using normal-mode coupling. Coupling between same and different-degree acoustic modes and different frequency bins are measured in order to capture the various length scales of inertial modes. We detect inertial modes at high latitude with azimuthal order $t=1$ and frequency $\sim -80$ nHz. This mode is present in the entire convection zone. The presence of Rossby modes may be seen down to a depth of $\sim 0.83R_\odot$ and the Rossby signal is indistinguishable from noise below that depth for high azimuthal order. We find that the amplitudes of these modes increase with depth down to around $0.92 R_\odot$ and decrease below that depth. We find that the latitudinal eigenfunctions of Rossby modes deviate from sectoral spherical harmonics if we use a similar approach as adopted in earlier studies. We found that spatial leakage and even pure noise in the measurements of non-sectoral components can also explain the above-mentioned characteristics of the latitudinal eigenfunctions. This realization underscores the necessity for careful interpretation when considering the latitudinal eigenfunctions of Rossby modes. Exploring the depth-dependent characteristics of these modes will enable us to capture interior dynamics distinctly, separate from p-mode seismology."
https://arxiv.org/abs/2403.08149,2024-03-12,On the Feasibility of EEG-based Motor Intention Detection for Real-Time Robot Assistive Control,"['Ho Jin Choi', 'Satyajeet Das', 'Shaoting Peng', 'Ruzena Bajcsy', 'Nadia Figueroa']","This paper explores the feasibility of employing EEG-based intention detection for real-time robot assistive control. We focus on predicting and distinguishing motor intentions of left/right arm movements by presenting: i) an offline data collection and training pipeline, used to train a classifier for left/right motion intention prediction, and ii) an online real-time prediction pipeline leveraging the trained classifier and integrated with an assistive robot. Central to our approach is a rich feature representation composed of the tangent space projection of time-windowed sample covariance matrices from EEG filtered signals and derivatives; allowing for a simple SVM classifier to achieve unprecedented accuracy and real-time performance. In pre-recorded real-time settings (160 Hz), a peak accuracy of 86.88% is achieved, surpassing prior works. In robot-in-the-loop settings, our system successfully detects intended motion solely from EEG data with 70% accuracy, triggering a robot to execute an assistive task. We provide a comprehensive evaluation of the proposed classifier."
https://arxiv.org/abs/2403.08148,2024-03-12,GCMe: Efficient implementation of Gaussian core model with smeared electrostatic interactions for molecular dynamics simulations of soft matter systems,"['Benjamin Bobin Ye', 'Shensheng Chen', 'Zhen-Gang Wang']","In recent years, molecular dynamics (MD) simulations have emerged as a pivotal tool for understanding the structure, dynamics, and phase behavior in charged soft matter systems. To explore phenomena across greater length and time scales in MD simulations, molecules are often coarse-grained for better computational performance. However, commonly-used force fields represent particles as hard-core interaction centers with point charges, which often overemphasizes the packing effect and short-range electrostatics, especially in systems with bulky deformable organic molecules and systems with strong coarse-graining. This underscores the need for an efficient soft-core model to physically capture the effective interactions between coarse-grained particles. To this end, we implement a soft-core model uniting the Gaussian core model with smeared electrostatic interactions that is phenomenologically equivalent to recent theoretical models. We first parameterize it generically using water as the model solvent. Then, we benchmark its performance in the OpenMM toolkit for different boundary conditions to highlight a computational speedup of up to $34\times$ compared to commonly used force fields and existing implementations. Finally, we demonstrate its utility by investigating how boundary polarizability affects the adsorption behavior of a polyelectrolyte solution on perfectly conducting and nonmetal electrodes."
https://arxiv.org/abs/2403.08147,2024-03-12,Representing Molecules as Random Walks Over Interpretable Grammars,"['Michael Sun', 'Minghao Guo', 'Weize Yuan', 'Veronika Thost', 'Crystal Elaine Owens', 'Aristotle Franklin Grosz', 'Sharvaa Selvan', 'Katelyn Zhou', 'Hassan Mohiuddin', 'Benjamin J Pedretti', 'Zachary P Smith', 'Jie Chen', 'Wojciech Matusik']","Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability."
https://arxiv.org/abs/2403.08146,2024-03-12,Nodal solutions to Paneitz-type equations,"['Jurgen Julio-Batalla', 'Jimmy Petean']","On a closed Riemannian manifold $(M^n ,g)$ with a proper isoparametric function $f$ we consider the equation $Δ^2 u -αΔu +βu = u^q$, where $α$ and $β$ are positive constants satisfying that $α^2 \geq 4 β$. We let ${\bf m}$ be the minimum of the dimensions of the focal varieties of $f$ and $q_f = \frac{n-{\bf m}+4}{n-{\bf m}-4}$, $q_f = \infty$ if $n\leq {\bf m}+4$. We prove the existence of infinitely many nodal solutions of the equation assuming that $1<q<q_f$. The solutions are $f$-invariant. To obtain the result, first we prove a $C^0-$estimate for positive $f$-invariant solutions of the equation. Then we prove the existence of mountain pass solutions with arbitrarily large energy."
https://arxiv.org/abs/2403.08145,2024-03-12,Algorithmic Information Disclosure in Optimal Auctions,"['Yang Cai', 'Yingkai Li', 'Jinzhao Wu']","This paper studies a joint design problem where a seller can design both the signal structures for the agents to learn their values, and the allocation and payment rules for selling the item. In his seminal work, Myerson (1981) shows how to design the optimal auction with exogenous signals. We show that the problem becomes NP-hard when the seller also has the ability to design the signal structures. Our main result is a polynomial-time approximation scheme (PTAS) for computing the optimal joint design with at most an $ε$ multiplicative loss in expected revenue. Moreover, we show that in our joint design problem, the seller can significantly reduce the information rent of the agents by providing partial information, which ensures a revenue that is at least $1 - \frac{1}{e}$ of the optimal welfare for all valuation distributions."
https://arxiv.org/abs/2403.08144,2024-03-12,"Prosody for Intuitive Robotic Interface Design: It's Not What You Said, It's How You Said It","['Elaheh Sanoubari', 'Atil Iscen', 'Leila Takayama', 'Stefano Saliceti', 'Corbin Cunningham', 'Ken Caluwaerts']","In this paper, we investigate the use of 'prosody' (the musical elements of speech) as a communicative signal for intuitive human-robot interaction interfaces. Our approach, rooted in Research through Design (RtD), examines the application of prosody in directing a quadruped robot navigation. We involved ten team members in an experiment to command a robot through an obstacle course using natural interaction. A human operator, serving as the robot's sensory and processing proxy, translated human communication into a basic set of navigation commands, effectively simulating an intuitive interface. During our analysis of interaction videos, when lexical and visual cues proved insufficient for accurate command interpretation, we turned to non-verbal auditory cues. Qualitative evidence suggests that participants intuitively relied on prosody to control robot navigation. We highlight specific distinct prosodic constructs that emerged from this preliminary exploration and discuss their pragmatic functions. This work contributes a discussion on the broader potential of prosody as a multifunctional communicative signal for designing future intuitive robotic interfaces, enabling lifelong learning and personalization in human-robot interaction."
https://arxiv.org/abs/2403.08143,2024-03-12,Ultra-long relaxation of a Kramers qubit formed in a bilayer graphene quantum dot,"['Artem O. Denisov', 'Veronika Reckova', 'Solenn Cances', 'Max J. Ruckriegel', 'Michele Masseroni', 'Christoph Adam', 'Chuyao Tong', 'Jonas D. Gerber', 'Wei Wister Huang', 'Kenji Watanabe', 'Takashi Taniguchi', 'Thomas Ihn', 'Klaus Ensslin', 'Hadrien Duprez']","The intrinsic valley degree of freedom makes bilayer graphene a unique platform for emerging types of semiconducting qubits. The single-carrier quantum dot ground state exhibits a two-fold degeneracy where the two states have opposite spin and valley quantum numbers. By breaking the time-reversal symmetry of this ground state with an out-of-plane magnetic field, a novel type of qubit (Kramers qubit), encoded in the two-dimensional spin-valley subspace, becomes accessible. The Kramers qubit is robust against known spin- and valley-mixing mechanisms, as it requires a simultaneous change of both quantum numbers, potentially resulting in long relaxation and coherence times. We measure the relaxation time of a single carrier in the excited states of a bilayer graphene quantum dot at small ($\sim \mathrm{mT}$) and zero magnetic fields. We demonstrate ultra-long spin-valley relaxation times of the Kramers qubit exceeding $30~\mathrm{s}$, which is about two orders of magnitude longer than the spin relaxation time of $400~\mathrm{ms}$. The demonstrated high-fidelity single-shot readout and long relaxation times are the foundation for novel, long-lived semiconductor qubits."
https://arxiv.org/abs/2403.08142,2024-03-12,ShadowRemovalNet: Efficient Real-Time Shadow Removal,"['Alzayat Saleh', 'Alex Olsen', 'Jake Wood', 'Bronson Philippa', 'Mostafa Rahimi Azghadi']","Shadows significantly impact computer vision tasks, particularly in outdoor environments. State-of-the-art shadow removal methods are typically too computationally intensive for real-time image processing on edge hardware. We propose ShadowRemovalNet, a novel method designed for real-time image processing on resource-constrained hardware. ShadowRemovalNet achieves significantly higher frame rates compared to existing methods, making it suitable for real-time computer vision pipelines like those used in field robotics. Beyond speed, ShadowRemovalNet offers advantages in efficiency and simplicity, as it does not require a separate shadow mask during inference. ShadowRemovalNet also addresses challenges associated with Generative Adversarial Networks (GANs) for shadow removal, including artefacts, inaccurate mask estimations, and inconsistent supervision between shadow and boundary pixels. To address these limitations, we introduce a novel loss function that substantially reduces shadow removal errors. ShadowRemovalNet's efficiency and straightforwardness make it a robust and effective solution for real-time shadow removal in outdoor robotics and edge computing applications."
https://arxiv.org/abs/2403.08141,2024-03-12,Parallel Diffusion Coefficient of Energetic Charged Particles in the Inner Heliosphere from the Turbulent Magnetic Fields Measured by Parker Solar Probe,"['Xiaohang Chen', 'Joe Giacalone', 'Fan Guo', 'Kristopher G. Klein']","Diffusion coefficients of energetic charged particles in turbulent magnetic fields are a fundamental aspect of diffusive transport theory but remain incompletely understood. In this work, we use quasi-linear theory to evaluate the spatial variation of the parallel diffusion coefficient $κ_\parallel$ from the measured magnetic turbulence power spectra in the inner heliosphere. We consider the magnetic field and plasma velocity measurements from Parker Solar Probe made during Orbits 5-13. The parallel diffusion coefficient is calculated as a function of radial distance from 0.062 to 0.8 AU, and the particle energy from 100 keV to 1GeV. We find that $κ_\parallel$ increases exponentially with both heliocentric distance and energy of particles. The fluctuations in $κ_\parallel$ are related to the episodes of large-scale magnetic structures in the solar wind. By fitting the results, we also provide an empirical formula of $κ_{\parallel}=(5.16\pm1.22) \times 10^{18} \: r^{1.17\pm0.08} \: E^{0.71\pm 0.02} \; (cm^2/s)$ in the inner heliosphere which can be used as a reference in studying the transport and acceleration of solar energetic particles as well as the modulation of cosmic rays."
https://arxiv.org/abs/2403.08140,2024-03-12,BAGEL: Bootstrapping Agents by Guiding Exploration with Language,"['Shikhar Murty', 'Christopher Manning', 'Peter Shaw', 'Mandar Joshi', 'Kenton Lee']","Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents. Unfortunately, LM agents often fail to generalize to new environments without human demonstrations. This work presents BAGEL, a method for bootstrapping LM agents without human supervision. BAGEL converts a seed set of randomly explored trajectories or synthetic instructions, into demonstrations, via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory. By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language. We use BAGEL demonstrations to adapt a zero shot LM agent at test time via in-context learning over retrieved demonstrations, and find improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x reduction in execution failures."
https://arxiv.org/abs/2403.08139,2024-03-12,Superspecial genus-$4$ double covers of elliptic curves,"['Takumi Ogasawara', 'Ryo Ohashi', 'Kosuke Sakata', 'Shushi Harashita']","In this paper we study genus-$4$ curves obtained as double covers of elliptic curves. Firstly we give explicit defining equations of such curves with explicit criterion for whether it is nonsingular, and show the irreducibility of the long polynomial determining whether the curve is nonsingular, in any characteristic $\ne 2,3$. Secondly as an application we enumerate superspecial genus-$4$ double covers of elliptic curves in small characteristic."
https://arxiv.org/abs/2403.08138,2024-03-12,"Toeplitz operators with symmetric, alternating and anti-symmetric separately radial symbols on the unit ball","['Armando Sánchez-Nungaray', 'José Rosales-Ortega', 'Carlos González-Flores']","We consider symmetric separately radial (with corresponding group $S_n\rtimes \mathbb{T}^n$) and alternating separately radial (with corresponding group $A_n\rtimes \mathbb{T}^n$) symbols, as well as the associated Toeplitz operators on the weighted Bergman spaces on the unit ball on $\mathbb{C}^n$. Using a purely representation theoretic approach we obtain that the $C^*$-algebras generated by each family of such Toeplitz operators is commutative. Furthermore, we show that the symmetric separately radial Toeplitz operators are more general than radial Toeplitz operators, i.e., every radial Toeplitz operator is a symmetric separately radial."
https://arxiv.org/abs/2403.08137,2024-03-12,From Paper to Card: Transforming Design Implications with Generative AI,"['Donghoon Shin', 'Lucy Lu Wang', 'Gary Hsieh']","Communicating design implications is common within the HCI community when publishing academic papers, yet these papers are rarely read and used by designers. One solution is to use design cards as a form of translational resource that communicates valuable insights from papers in a more digestible and accessible format to assist in design processes. However, creating design cards can be time-consuming, and authors may lack the resources/know-how to produce cards. Through an iterative design process, we built a system that helps create design cards from academic papers using an LLM and text-to-image model. Our evaluation with designers (N=21) and authors of selected papers (N=12) revealed that designers perceived the design implications from our design cards as more inspiring and generative, compared to reading original paper texts, and the authors viewed our system as an effective way of communicating their design implications. We also propose future enhancements for AI-generated design cards."
https://arxiv.org/abs/2403.08136,2024-03-12,RoboCertProb: Property Specification for Probabilistic RoboChart Models,"['Kangfeng Ye', 'Jim Woodcock']","RoboChart is a core notation in the RoboStar framework which brings modern modelling and formal verification technologies into software engineering for robotics. It is a timed and probabilistic domain-specific language for robotics and provides a UML-like architectural and state machine modelling. This work presents RoboCertProb for specifying quantitative properties of probabilistic robotic systems modelled in RoboChart. RoboCertProb's semantics is based on PCTL*. To interpret RoboCertProb over RoboChart models, we give a Markov semantics (DTMCs and MDPs) to RoboChart, derived from its existing transformation semantics to the PRISM language. In addition to property specification, RoboCertProb also entitles us to configure loose constants and unspecified functions and operations in RoboChart models. It allows us to set up environmental inputs to verify reactive probabilistic systems not directly supported in probabilistic model checkers like PRISM because they employ a closed-world assumption. We implement RoboCertProb in an accompanying tool of RoboChart, RoboTool, for specifying properties and automatically generating PRISM properties from them to formally verify RoboChart models using PRISM. We have used it to analyse the behaviour of software controllers for two real robots: an industrial painting robot and an agricultural robot for treating plants with UV lights."
https://arxiv.org/abs/2403.08135,2024-03-12,"Coupling between magnetic reconnection, energy release, and particle acceleration in the X17.2 2003 October 28 solar flare","['Victoria G. Kurt', 'Astrid M. Veronig', 'Gregory D. Fleishman', 'Jürgen Hinterreiter', 'Johannes Tschernitz', 'Alexandra L. Lysenko']","The 2003 October 28 (X17.2) eruptive flare was a unique event. The coronal electric field and the π-decay γ-ray emission flux had the highest values ever inferred in solar flares. This study reveals physical links between the magnetic reconnection process, the energy release, and the acceleration of electrons and ions to high energies in the chain of the magnetic energy transformations in the impulsive phase of the solar flare. The global reconnection rate and the local reconnection rate are calculated from flare ribbon separation in Hα filtergrams and photospheric magnetic field maps. Available results of INTEGRAL and CORONAS-F/SONG observations are combined with Konus-Wind data to quantify time behavior of electron and proton acceleration. Prompt γ-ray lines and delayed 2.2 MeV line temporal profiles observed with Konus-Wind and INTEGRAL/SPI used to detect and quantify the nuclei with energies of 10-70 MeV. The global and local reconnection rates reach their peaks at the end of the main rise phase of the flare. The spectral analysis of the high-energy γ-ray emission revealed a close association between the acceleration process efficiency and the reconnection rates. High-energy bremsstrahlung continuum and narrow γ-ray lines were observed in the main rise phase. In the main energy release phase, the upper energy of the bremsstrahlung spectrum was significantly reduced and the pion-decay γ-ray emission appeared abruptly. We discuss the reasons why the change of the acceleration regime occurred along with the large-scale magnetic field restructuration of this flare. We argue that the main energy release and proton acceleration up to subrelativistic energies began just when the reconnection rate was going through the maximum, i.e., after a major change of the flare topology."
https://arxiv.org/abs/2403.08134,2024-03-12,Evaluation of the AMOEBA force field for simulating metal halide perovskites in the solid state and in solution,"['P. V. G. M. Rathnayake', 'Stefano Bernardi', 'Asaph Widmer-Cooper']","In this work, we compare existing non-polarizable force fields developed to study the solid or solution phases of hybrid organic-inorganic halide perovskites with the AMOEBA polarizable force field. The aim is to test whether more computationally expensive polarizable force fields like AMOEBA offer better transferability between solution and solid phases, with the ultimate goal being the study of crystal nucleation, growth and other interfacial phenomena involving these ionic compounds. In the context of hybrid perovskites, AMOEBA force field parameters already exist for several elements in solution and we decided to leave them unchanged and to only parameterize the missing ones (Pb\textsuperscript{2+} and CH\textsubscript{3}NH\textsubscript{3}\textsuperscript{+} ions) in order to maximise transferability and avoid over-fitting to the specific examples studied here. Overall, we find that AMOEBA yields accurate hydration free energies (within 5\%) for typical ionic species while showing the correct ordering of stability for the different crystal polymorphs of CsPbI\textsubscript{3} and CH\textsubscript{3}NH\textsubscript{3}PbI\textsubscript{3}. While the existing parameters do not accurately reproduce all transition temperatures and lattice parameters, AMOEBA offers better transferability between solution and solid states than existing non-polarizable force fields."
https://arxiv.org/abs/2403.08133,2024-03-12,Physics-Inspired Deep Learning Anti-Aliasing Framework in Efficient Channel State Feedback,"['Yu-Chien Lin', 'Yan Xin', 'Ta-Sung Lee', ' Charlie', ' Zhang', 'Zhi Ding']","Acquiring downlink channel state information (CSI) at the base station is vital for optimizing performance in massive Multiple input multiple output (MIMO) Frequency-Division Duplexing (FDD) systems. While deep learning architectures have been successful in facilitating UE-side CSI feedback and gNB-side recovery, the undersampling issue prior to CSI feedback is often overlooked. This issue, which arises from low density pilot placement in current standards, results in significant aliasing effects in outdoor channels and consequently limits CSI recovery performance. To this end, this work introduces a new CSI upsampling framework at the gNB as a post-processing solution to address the gaps caused by undersampling. Leveraging the physical principles of discrete Fourier transform shifting theorem and multipath reciprocity, our framework effectively uses uplink CSI to mitigate aliasing effects. We further develop a learning-based method that integrates the proposed algorithm with the Iterative Shrinkage-Thresholding Algorithm Net (ISTA-Net) architecture, enhancing our approach for non-uniform sampling recovery. Our numerical results show that both our rule-based and deep learning methods significantly outperform traditional interpolation techniques and current state-of-the-art approaches in terms of performance."
https://arxiv.org/abs/2403.08132,2024-03-12,Information Leakage through Physical Layer Supply Voltage Coupling Vulnerability,"['Sahan Sanjaya', 'Aruna Jayasena', 'Prabhat Mishra']","Side-channel attacks exploit variations in non-functional behaviors to expose sensitive information across security boundaries. Existing methods leverage side-channels based on power consumption, electromagnetic radiation, silicon substrate coupling, and channels created by malicious implants. Power-based side-channel attacks are widely known for extracting information from data processed within a device while assuming that an attacker has physical access or the ability to modify the device. In this paper, we introduce a novel side-channel vulnerability that leaks data-dependent power variations through physical layer supply voltage coupling (PSVC). Unlike traditional power side-channel attacks, the proposed vulnerability allows an adversary to mount an attack and extract information without modifying the device. We assess the effectiveness of PSVC vulnerability through three case studies, demonstrating several end-to-end attacks on general-purpose microcontrollers with varying adversary capabilities. These case studies provide evidence for the existence of PSVC vulnerability, its applicability for on-chip as well as on-board side-channel attacks, and how it can eliminate the need for physical access to the target device, making it applicable to any off-the-shelf hardware. Our experiments also reveal that designing devices to operate at the lowest operational voltage significantly reduces the risk of PSVC side-channel vulnerability."
https://arxiv.org/abs/2403.08131,2024-03-12,Cost-Effective Methodology for Complex Tuning Searches in HPC: Navigating Interdependencies and Dimensionality,"['Adrian Perez Dieguez', 'Min Choi', 'Mahmut Okyay', 'Mauro Del Ben', 'Bryan M. Wong', 'Khaled Z. Ibrahim']","Tuning searches are pivotal in High-Performance Computing (HPC), addressing complex optimization challenges in computational applications. The complexity arises not only from finely tuning parameters within routines but also potential interdependencies among them, rendering traditional optimization methods inefficient. Instead of scrutinizing interdependencies among parameters and routines, practitioners often face the dilemma of conducting independent tuning searches for each routine, thereby overlooking interdependence, or pursuing a more resource-intensive joint search for all routines. This decision is driven by the consideration that some interdependence analysis and high-dimensional decomposition techniques in literature may be prohibitively expensive in HPC tuning searches. Our methodology adapts and refines these methods to ensure computational feasibility while maximizing performance gains in real-world scenarios. Our methodology leverages a cost-effective interdependence analysis to decide whether to merge several tuning searches into a joint search or conduct orthogonal searches. Tested on synthetic functions with varying levels of parameter interdependence, our methodology efficiently explores the search space. In comparison to Bayesian-optimization-based full independent or fully joint searches, our methodology suggested an optimized breakdown of independent and merged searches that led to final configurations up to 8% more accurate, reducing the search time by up to 95%. When applied to GPU-offloaded Real-Time Time-Dependent Density Functional Theory (RT-TDDFT), an application in computational materials science that challenges modern HPC autotuners, our methodology achieved an effective tuning search. Its adaptability and efficiency extend beyond RT-TDDFT, making it valuable for related applications in HPC."
https://arxiv.org/abs/2403.08130,2024-03-12,Imputation of Counterfactual Outcomes when the Errors are Predictable,"['Silvia Goncalves', 'Serena Ng']",A crucial input into causal inference is the imputed counterfactual outcome.
https://arxiv.org/abs/2403.08129,2024-03-12,Solvabilizer Numbers of Finite Groups,"['Banafsheh Akbari', 'Tuval Foguel', 'Jack Schmidt']","Consider a nonsolvable finite group G, where R(G) represents the solvable radical of G. For any element x in G, the solvabilizer of x in G, denoted by Sol_G(x), is defined as the set of all elements y in G such that the subgroup generated by x and y is solvable. Notably, the entirety of G can be expressed as the union over all x in G\R(G) of their respective solvabilizers: $G = \cup_{x\in G\R(G)} Sol_G(x). A solvabilizer covering of G is characterized by a subset X of G\R(G) such that G= \cup_{x\in X} Sol_G(x). The solvabilizer number of G is then defined as the minimum cardinality among all solvabilizer coverings of G. This paper delves into the exploration of the solvabilizer number for diverse nonsolvable finite groups G, shedding light on the interplay between solvability and the structure of these groups."
https://arxiv.org/abs/2403.08128,2024-03-12,Singular loci of algebras over ramified discrete valuation rings,['Nawaj KC'],"When $k$ is a field, the classical Jacobian criterion computes the singular locus of an equidimensional, finitely generated $k$-algebra as the closed subset of an ideal generated by appropriate minors of the so-called Jacobian matrix. Recently, Hochster-Jeffries and Saito have extended this result for algebras over any unramified discrete valuation ring of mixed characteristic via the use of $p$-derivations. Motivated by these results, in this paper, we state and prove an analogous Jacobian criterion for algebras over a ramified discrete valuation ring of mixed characteristic."
https://arxiv.org/abs/2403.08127,2024-03-12,Guidelines for the Creation of Analysis Ready Data,"['Harriette Phillips', 'Aiden Price', 'Owen Forbes', 'Claire Boulange', 'Kerrie Mengersen', 'Marketa Reeves', 'Rebecca Glauert']","Globally, there is an increased need for guidelines to produce high-quality data outputs for analysis. There is no framework currently exists providing guidelines for a comprehensive approach in producing analysis ready data (ARD). Through critically reviewing and summarising current literature, this paper proposes such guidelines for the creation of ARD. The guidelines proposed in this paper inform ten steps in the generation of ARD: ethics, project documentation, data governance, data management, data storage, data discovery and collection, data cleaning, quality assurance, metadata, and data dictionary. These steps are illustrated through a substantive case study which aimed to create ARD for a digital spatial platform: the Australian Child and Youth Wellbeing Atlas (ACYWA)."
https://arxiv.org/abs/2403.08126,2024-03-12,Quantum Channel Conditioning and Measurement Models,['Stan Gudder'],"If $H_1$ and $H_2$ are finite-dimensional Hilbert spaces, a channel from $H_1$ to $H_2$ is a completely positive, linear map $\mathcal{I}$ that takes the set of states $\mathcal{S}(H_1)$ for $H_1$ to the set of states $\mathcal{S}(H_2)$ for $H_2$. Corresponding to $\mathcal{I}$ there is a unique dual map $\mathcal{I}^*$ from the set of effects $\mathcal{E}(H_2)$ for $H_2$ to the set of effects $\mathcal{E}(H_1)$ for $H_1$. We call $\mathcal{I}^*(b)$ the effect $b$ conditioned by $\mathcal{I}$ and the set $\mathcal{I}^c = \mathcal{I}^*(\mathcal{E}(H_2))$ the conditioned set of $\mathcal{I}$. We point out that $\mathcal{I}^c$ is a convex subeffect algebra of the effect algebra $\mathcal{E}(H_1)$. We extend this definition to the conditioning $\mathcal{I}^*(B)$ for an observable $B$ on $H_2$ and say that an observable $A$ is in $\mathcal{I}^c$ if $A=\mathcal{I}^*(B)$ for some observable $B$. We show that $\mathcal{I}^c$ is closed under post-processing and taking parts. We also define the conditioning of instruments by channels. These concepts are illustrated using examples of Holevo instruments and channels. We next discuss measurement models and their corresponding observables and instruments. We show that calculations can be simplified by employing Kraus and Holevo separable channels. Such channels allow one to separate the components of a tensor product."
https://arxiv.org/abs/2403.08125,2024-03-12,Q-SLAM: Quadric Representations for Monocular SLAM,"['Chensheng Peng', 'Chenfeng Xu', 'Yue Wang', 'Mingyu Ding', 'Heng Yang', 'Masayoshi Tomizuka', 'Kurt Keutzer', 'Marco Pavone', 'Wei Zhan']","Monocular SLAM has long grappled with the challenge of accurately modeling 3D geometries. Recent advances in Neural Radiance Fields (NeRF)-based monocular SLAM have shown promise, yet these methods typically focus on novel view synthesis rather than precise 3D geometry modeling. This focus results in a significant disconnect between NeRF applications, i.e., novel-view synthesis and the requirements of SLAM. We identify that the gap results from the volumetric representations used in NeRF, which are often dense and noisy. In this study, we propose a novel approach that reimagines volumetric representations through the lens of quadric forms. We posit that most scene components can be effectively represented as quadric planes. Leveraging this assumption, we reshape the volumetric representations with million of cubes by several quadric planes, which leads to more accurate and efficient modeling of 3D scenes in SLAM contexts. Our method involves two key steps: First, we use the quadric assumption to enhance coarse depth estimations obtained from tracking modules, e.g., Droid-SLAM. This step alone significantly improves depth estimation accuracy. Second, in the subsequent mapping phase, we diverge from previous NeRF-based SLAM methods that distribute sampling points across the entire volume space. Instead, we concentrate sampling points around quadric planes and aggregate them using a novel quadric-decomposed Transformer. Additionally, we introduce an end-to-end joint optimization strategy that synchronizes pose estimation with 3D reconstruction."
https://arxiv.org/abs/2403.08124,2024-03-12,Towards Independence Criterion in Machine Unlearning of Features and Labels,"['Ling Han', 'Nanqing Luo', 'Hao Huang', 'Jing Chen', 'Mary-Anne Hartley']","This work delves into the complexities of machine unlearning in the face of distributional shifts, particularly focusing on the challenges posed by non-uniform feature and label removal. With the advent of regulations like the GDPR emphasizing data privacy and the right to be forgotten, machine learning models face the daunting task of unlearning sensitive information without compromising their integrity or performance. Our research introduces a novel approach that leverages influence functions and principles of distributional independence to address these challenges. By proposing a comprehensive framework for machine unlearning, we aim to ensure privacy protection while maintaining model performance and adaptability across varying distributions. Our method not only facilitates efficient data removal but also dynamically adjusts the model to preserve its generalization capabilities. Through extensive experimentation, we demonstrate the efficacy of our approach in scenarios characterized by significant distributional shifts, making substantial contributions to the field of machine unlearning. This research paves the way for developing more resilient and adaptable unlearning techniques, ensuring models remain robust and accurate in the dynamic landscape of data privacy and machine learning."
https://arxiv.org/abs/2403.08123,2024-03-12,6D Movable Antenna Based on User Distribution: Modeling and Optimization,"['Xiaodan Shao', 'Qijun Jiang', 'Rui Zhang']","In this paper, we propose a new six-dimensional (6D) movable antenna (6DMA) system for future wireless networks to improve the communication performance. Unlike the traditional fixed-position antenna (FPA) and existing fluid antenna/two-dimensional (2D) movable antenna (FA/2DMA) systems that adjust the positions of antennas only, the proposed 6DMA system consists of distributed antenna surfaces with independently adjustable three-dimensional (3D) positions as well as 3D rotations within a given space. In particular, this paper applies the 6DMA to the base station (BS) in wireless networks to provide full degrees of freedom (DoFs) for the BS to adapt to the dynamic user spatial distribution in the network. However, a challenging new problem arises on how to optimally control the 6D positions and rotations of all 6DMA surfaces at the BS to maximize the network capacity based on the user spatial distribution, subject to the practical constraints on 6D antennas' movement. To tackle this problem, we first model the 6DMA-enabled BS and the user channels with the BS in terms of 6D positions and rotations of all 6DMA surfaces. Next, we propose an efficient alternating optimization algorithm to search for the best 6D positions and rotations of all 6DMA surfaces by leveraging the Monte Carlo simulation technique. Specifically, we sequentially optimize the 3D position/3D rotation of each 6DMA surface with those of the other surfaces fixed in an iterative manner. Numerical results show that our proposed 6DMA-BS can significantly improve the network capacity as compared to the benchmark BS architectures with FPAs or MAs with limited/partial movability, especially when the user distribution is more spatially non-uniform."
https://arxiv.org/abs/2403.08122,2024-03-12,High energy dissipation rates from the impingement of free paper-thin sheets of liquids: Determination of the volume of the energy dissipation zone,['Robert J. Demyanovich'],"The micromixing time of impinging thin liquid sheets depends upon the energy dissipation rate. The kinetic energy released by the impingement has been previously studied and was found to be a function of the coefficient of restitution of the collision. In this work, the volume within which the released kinetic energy is dissipated was investigated. The volume of energy dissipation was determined by measuring the time required for the velocity of the single sheet (prior to the collision) to be reduced to the velocity in the mixed sheet (after the collision). Although different, the velocity in single sheets and the velocity in mixed sheets have been previously shown to be constant. High-speed video was used to measure the velocity of features, generated in the front single sheet, as they passed through the impingement zone and into the mixed sheet. The experimental results showed that the time required for the velocity change was approximately equal to the residence time of liquid in the impingement zone. A new equation for the energy dissipation rate was developed and compared with the energy dissipation rate derived from turbulence energy-cascade theory. This comparison showed that the large-eddy turnover time was approximately equal to the residence time in the impingement zone; a result that is in accordance with the notion from turbulence energy-cascade theory that large, energy-containing eddies lose their energy within approximately one large-eddy turnover time. Within the impingement zone, the large-eddy kinetic energy was found to decay exponentially with time."
https://arxiv.org/abs/2403.08121,2024-03-12,Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations,"['Akshay Kumar', 'Jarvis Haupt']","This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks, starting with small initializations. The present work considers neural networks that are assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two. This paper demonstrates that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points of the neural correlation function introduced in [1]. Additionally, for square loss and under a separability assumption on the weights of neural networks, a similar directional convergence of gradient flow dynamics is shown near certain saddle points of the loss function."
https://arxiv.org/abs/2403.08120,2024-03-12,Progressive and rushed Dyck paths,['Axel Bacher'],"We call progressive paths and rushed paths two families of Dyck paths studied by Asinowski and Jelínek, which have the same enumerating sequence (OEIS entry A287709). We present a bijection proving this fact. Rushed paths turn out to be in bijection with \emph{one-sided trees}, introduced by Durhuus and Ünel, which have an asymptotic enumeration involving a stretched exponential. We conclude by presenting several other classes of related lattice paths and directed animals that may have similar asymptotic properties."
https://arxiv.org/abs/2403.08119,2024-03-12,CMax-SLAM: Event-based Rotational-Motion Bundle Adjustment and SLAM System using Contrast Maximization,"['Shuang Guo', 'Guillermo Gallego']","Event cameras are bio-inspired visual sensors that capture pixel-wise intensity changes and output asynchronous event streams. They show great potential over conventional cameras to handle challenging scenarios in robotics and computer vision, such as high-speed and high dynamic range. This paper considers the problem of rotational motion estimation using event cameras. Several event-based rotation estimation methods have been developed in the past decade, but their performance has not been evaluated and compared under unified criteria yet. In addition, these prior works do not consider a global refinement step. To this end, we conduct a systematic study of this problem with two objectives in mind: summarizing previous works and presenting our own solution. First, we compare prior works both theoretically and experimentally. Second, we propose the first event-based rotation-only bundle adjustment (BA) approach. We formulate it leveraging the state-of-the-art Contrast Maximization (CMax) framework, which is principled and avoids the need to convert events into frames. Third, we use the proposed BA to build CMax-SLAM, the first event-based rotation-only SLAM system comprising a front-end and a back-end. Our BA is able to run both offline (trajectory smoothing) and online (CMax-SLAM back-end). To demonstrate the performance and versatility of our method, we present comprehensive experiments on synthetic and real-world datasets, including indoor, outdoor and space scenarios. We discuss the pitfalls of real-world evaluation and propose a proxy for the reprojection error as the figure of merit to evaluate event-based rotation BA methods. We release the source code and novel data sequences to benefit the community. We hope this work leads to a better understanding and fosters further research on event-based ego-motion estimation. Project page: https://github.com/tub-rip/cmax_slam"
https://arxiv.org/abs/2403.08118,2024-03-12,Characterising harmful data sources when constructing multi-fidelity surrogate models,"['Nicolau Andrés-Thió', 'Mario Andrés Muñoz', 'Kate Smith-Miles']","Surrogate modelling techniques have seen growing attention in recent years when applied to both modelling and optimisation of industrial design problems. These techniques are highly relevant when assessing the performance of a particular design carries a high cost, as the overall cost can be mitigated via the construction of a model to be queried in lieu of the available high-cost source. The construction of these models can sometimes employ other sources of information which are both cheaper and less accurate. The existence of these sources however poses the question of which sources should be used when constructing a model. Recent studies have attempted to characterise harmful data sources to guide practitioners in choosing when to ignore a certain source. These studies have done so in a synthetic setting, characterising sources using a large amount of data that is not available in practice. Some of these studies have also been shown to potentially suffer from bias in the benchmarks used in the analysis. In this study, we present a characterisation of harmful low-fidelity sources using only the limited data available to train a surrogate model. We employ recently developed benchmark filtering techniques to conduct a bias-free assessment, providing objectively varied benchmark suites of different sizes for future research. Analysing one of these benchmark suites with the technique known as Instance Space Analysis, we provide an intuitive visualisation of when a low-fidelity source should be used and use this analysis to provide guidelines that can be used in an applied industrial setting."
https://arxiv.org/abs/2403.08117,2024-03-12,On the resolution of the sign of gluon polarization in the proton,"['N. T. Hunt-Smith', 'C. Cocuzza', 'W. Melnitchouk', 'N. Sato', 'A. W Thomas', 'M. J. White']","Recently the possible existence of negative gluon helicity, $Δg$, has been observed to be compatible with existing empirical constraints, including from jet production in polarized proton-proton collisions at RHIC, and lattice QCD data on polarized gluon Ioffe time distributions. We perform a new global analysis of polarized parton distributions in the proton with new constraints from the high-$x$ region of deep-inelastic scattering (DIS). A dramatic reduction in the quality of the fit for the negative $Δg$ replicas compared to those with positive $Δg$ suggest that the negative $Δg$ solution cannot simultaneously account for high-$x$ polarized DIS data along with lattice and polarized jet data."
https://arxiv.org/abs/2403.08116,2024-03-12,Cyclic homology of categorical coalgebras and the free loop space,"['Manuel Rivera', 'Daniel Tolosa']","We prove that the cyclic chain complex of the categorical coalgebra of singular chains on an arbitrary topological space $X$ is naturally quasi-isomorphic to the $S^1$-equivariant chains of free loop space of $X$. This statement does not require any hypotheses on $X$ or on the commutative ring of coefficients. Along the way, we introduce a family of polytopes, coined as Goodwillie polytopes, that controls the combinatorics behind the relationship of the coHochschild complex of a categorical coalgebra and the Hochschild complex of its associated differential graded category."
https://arxiv.org/abs/2403.08115,2024-03-12,Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies,"['Vincent Freiberger', 'Erik Buchmann']","Privacy policies are expected to inform data subjects about their data protection rights. They should explain the data controller's data management practices, and make facts such as retention periods or data transfers to third parties transparent. Privacy policies only fulfill their purpose, if they are correctly perceived, interpreted, understood, and trusted by the data subject. Amongst others, this requires that a privacy policy is written in a fair way, e.g., it does not use polarizing terms, does not require a certain education, or does not assume a particular social background. In this work-in-progress paper, we outline our approach to assessing fairness in privacy policies. To this end, we identify from fundamental legal sources and fairness research, how the dimensions informational fairness, representational fairness and ethics/morality are related to privacy policies. We propose options to automatically assess policies in these fairness dimensions, based on text statistics, linguistic methods and artificial intelligence. Finally, we conduct initial experiments with German privacy policies to provide evidence that our approach is applicable. Our experiments indicate that there are indeed issues in all three dimensions of fairness. For example, our approach finds out if a policy discriminates against individuals with impaired reading skills or certain demographics, and identifies questionable ethics. This is important, as future privacy policies may be used in a corpus for legal artificial intelligence models."
https://arxiv.org/abs/2403.08114,2024-03-12,Guaranteeing Service in Connected Microgrids: Storage Planning and Optimal Power Sharing Policy,"['Arnab Dey', 'Vivek Khatana', 'Ankur Mani', 'Murti V. Salapaka']","The integration of renewable energy sources (RES) into power distribution grids poses challenges to system reliability due to the inherent uncertainty in their power production. To address this issue, battery energy sources (BESs) are being increasingly used as a promising solution to counter the uncertainty associated with RES power production. During the overall system planning stage, the optimal capacity of the BES has to be decided. In the operational phase, policies on when to charge the BESs and when to use them to support loads must be determined so that the BES remains within its operating range, avoiding depletion of charge on one hand and remaining within acceptable margins of maximum charge on the other. In this paper, a stochastic control framework is used to determine battery capacity, for microgrids, which ensures that during the operational phase, BESs' operating range is respected with pre-specified high probability. We provide an explicit analytical expression of the required BESs energy capacity for a single microgrid with RES as the main power source. Leveraging insights from the single microgrid case, the article focuses on the design and planning of BESs for the two-microgrid scenario. In this setting, microgrids are allowed to share power while respecting the capacity constraints imposed by the power lines. We characterize the optimal power transfer policy between the microgrids and the optimal BES capacity for multiple microgrids. This provides the BES savings arising from connecting the microgrids."
https://arxiv.org/abs/2403.08113,2024-03-12,Assessing the Influence of Toxic and Gender Discriminatory Communication on Perceptible Diversity in OSS Projects,"['Sayma Sultana', 'Gias Uddin', 'Amiangshu Bosu']","The presence of toxic and gender-identity derogatory language in open-source software (OSS) communities has recently become a focal point for researchers. Such comments not only lead to frustration and disengagement among developers but may also influence their leave from the OSS projects. Despite ample evidence suggesting that diverse teams enhance productivity, the existence of toxic or gender identity discriminatory communications poses a significant threat to the participation of individuals from marginalized groups and, as such, may act as a barrier to fostering diversity and inclusion in OSS projects. However, there is a notable lack of research dedicated to exploring the association between gender-based toxic and derogatory language with a perceptible diversity of open-source software teams. Consequently, this study aims to investigate how such content influences the gender, ethnicity, and tenure diversity of open-source software development teams. To achieve this, we extract data from active GitHub projects, assess various project characteristics, and identify instances of toxic and gender-discriminatory language within issue/pull request comments. Using these attributes, we construct a regression model to explore how they associate with the perceptible diversity of those projects."
https://arxiv.org/abs/2403.08112,2024-03-12,Ferrimagnetic Heusler tunnel junctions with fast spin-transfer torque switching enabled by low magnetization,"['Chirag Garg', 'Panagiotis Ch. Filippou', ' Ikhtiar', 'Yari Ferrante', 'See-Hun Yang', 'Brian Hughes', 'Charles T. Rettner', 'Timothy Phung', 'Sergey Faleev', 'Teya Topuria', 'Mahesh G. Samant', 'Jaewoo Jeong', 'Stuart S. P. Parkin']","Magnetic random access memory that uses magnetic tunnel junction memory cells is a high performance, non-volatile memory technology that goes beyond traditional charge-based memories. Today its speed is limited by the high magnetization of the memory storage layer. Here we show that fast and highly reliable switching is possible using a very low magnetization ferrimagnetic Heusler alloy, Mn3Ge. Moreover, the tunneling magnetoresistance is the highest yet achieved for a ferrimagnetic material at ambient temperature. Furthermore, the devices were prepared on technologically relevant amorphous substrates using a novel combination of a nitride seed layer and a chemical templating layer. These results show a clear path to the lowering of switching currents using ferrimagnetic Heusler materials and, therefore, to the scaling of high performance magnetic random access memories beyond those nodes possible with ferromagnetic devices."
https://arxiv.org/abs/2403.08111,2024-03-12,AI-Assisted Causal Pathway Diagram for Human-Centered Design,"['Ruican Zhong', 'Donghoon Shin', 'Rosemary Meza', 'Predrag Klasnja', 'Lucas Colusso', 'Gary Hsieh']","This paper explores the integration of causal pathway diagrams (CPD) into human-centered design (HCD), investigating how these diagrams can enhance the early stages of the design process. A dedicated CPD plugin for the online collaborative whiteboard platform Miro was developed to streamline diagram creation and offer real-time AI-driven guidance. Through a user study with designers (N=20), we found that CPD's branching and its emphasis on causal connections supported both divergent and convergent processes during design. CPD can also facilitate communication among stakeholders. Additionally, we found our plugin significantly reduces designers' cognitive workload and increases their creativity during brainstorming, highlighting the implications of AI-assisted tools in supporting creative work and evidence-based designs."
https://arxiv.org/abs/2403.08110,2024-03-12,Computing Generalized Ranks of Persistence Modules via Unfolding to Zigzag Modules,"['Tamal K. Dey', 'Aman Timalsina', 'Cheng Xin']","For a $P$-indexed persistence module ${\sf M}$, the (generalized) rank of ${\sf M}$ is defined as the rank of the limit-to-colimit map for ${\sf M}$ over the poset $P$. For $2$-parameter persistence modules, recently a zigzag persistence based algorithm has been proposed that takes advantage of the fact that generalized rank for $2$-parameter modules is equal to the number of full intervals in a zigzag module defined on the boundary of the poset. Analogous definition of boundary for $d$-parameter persistence modules or general $P$-indexed persistence modules does not seem plausible. To overcome this difficulty, we first unfold a given $P$-indexed module ${\sf M}$ into a zigzag module ${\sf M}_{ZZ}$ and then check how many full interval modules in a decomposition of ${\sf M}_{ZZ}$ can be folded back to remain full in ${\sf M}$. This number determines the generalized rank of ${\sf M}$. For special cases of degree-$d$ homology for $d$-complexes, we obtain a more efficient algorithm including a linear time algorithm for degree-$1$ homology in graphs."
https://arxiv.org/abs/2403.08109,2024-03-12,VANP: Learning Where to See for Navigation with Self-Supervised Vision-Action Pre-Training,"['Mohammad Nazeri', 'Junzhe Wang', 'Amirreza Payandeh', 'Xuesu Xiao']","Humans excel at efficiently navigating through crowds without collision by focusing on specific visual regions relevant to navigation. However, most robotic visual navigation methods rely on deep learning models pre-trained on vision tasks, which prioritize salient objects -- not necessarily relevant to navigation and potentially misleading. Alternative approaches train specialized navigation models from scratch, requiring significant computation. On the other hand, self-supervised learning has revolutionized computer vision and natural language processing, but its application to robotic navigation remains underexplored due to the difficulty of defining effective self-supervision signals. Motivated by these observations, in this work, we propose a Self-Supervised Vision-Action Model for Visual Navigation Pre-Training (VANP). Instead of detecting salient objects that are beneficial for tasks such as classification or detection, VANP learns to focus only on specific visual regions that are relevant to the navigation task. To achieve this, VANP uses a history of visual observations, future actions, and a goal image for self-supervision, and embeds them using two small Transformer Encoders. Then, VANP maximizes the information between the embeddings by using a mutual information maximization objective function. We demonstrate that most VANP-extracted features match with human navigation intuition. VANP achieves comparable performance as models learned end-to-end with half the training time and models trained on a large-scale, fully supervised dataset, i.e., ImageNet, with only 0.08% data."
https://arxiv.org/abs/2403.08108,2024-03-12,TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection,"['Hanning Chen', 'Wenjun Huang', 'Yang Ni', 'Sanggeon Yun', 'Fei Wen', 'Hugo Latapie', 'Mohsen Imani']","Task-oriented object detection aims to find objects suitable for accomplishing specific tasks. As a challenging task, it requires simultaneous visual data processing and reasoning under ambiguous semantics. Recent solutions are mainly all-in-one models. However, the object detection backbones are pre-trained without text supervision. Thus, to incorporate task requirements, their intricate models undergo extensive learning on a highly imbalanced and scarce dataset, resulting in capped performance, laborious training, and poor generalizability. In contrast, we propose TaskCLIP, a more natural two-stage design composed of general object detection and task-guided object selection. Particularly for the latter, we resort to the recently successful large Vision-Language Models (VLMs) as our backbone, which provides rich semantic knowledge and a uniform embedding space for images and texts. Nevertheless, the naive application of VLMs leads to sub-optimal quality, due to the misalignment between embeddings of object images and their visual attributes, which are mainly adjective phrases. To this end, we design a transformer-based aligner after the pre-trained VLMs to re-calibrate both embeddings. Finally, we employ a trainable score function to post-process the VLM matching results for object selection. Experimental results demonstrate that our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by 3.5% and only requires a single NVIDIA RTX 4090 for both training and inference."
https://arxiv.org/abs/2403.08107,2024-03-12,Simulation of a Diels-Alder Reaction on a Quantum Computer,"['Ieva Liepuoniute', 'Mario Motta', 'Thaddeus Pellegrini', 'Julia E. Rice', 'Tanvi P. Gujarati', 'Sofia Gil', 'Gavin O. Jones']","The simulation of chemical reactions is an anticipated application of quantum computers. Using a Diels-Alder reaction as a test case, in this study we explore the potential applications of quantum algorithms and hardware in investigating chemical reactions. Our specific goal is to calculate the activation barrier of a reaction between ethylene and cyclopentadiene forming a transition state. To achieve this goal, we use quantum algorithms for near-term quantum hardware (entanglement forging and quantum subspace expansion) and classical post-processing (many-body perturbation theory) in concert. We conduct simulations on IBM quantum hardware using up to 8 qubits, and compute accurate activation barriers in the reaction between cyclopentadiene and ethylene by accounting for both static and dynamic electronic correlation. This work illustrates a hybrid quantum-classical computational workflow to study chemical reactions on near-term quantum devices, showcasing the potential of quantum algorithms and hardware in accurately calculating activation barriers."
https://arxiv.org/abs/2403.08106,2024-03-12,V-PRISM: Probabilistic Mapping of Unknown Tabletop Scenes,"['Herbert Wright', 'Weiming Zhi', 'Matthew Johnson-Roberson', 'Tucker Hermans']","The ability to construct concise scene representations from sensor input is central to the field of robotics. This paper addresses the problem of robustly creating a 3D representation of a tabletop scene from a segmented RGB-D image. These representations are then critical for a range of downstream manipulation tasks. Many previous attempts to tackle this problem do not capture accurate uncertainty, which is required to subsequently produce safe motion plans. In this paper, we cast the representation of 3D tabletop scenes as a multi-class classification problem. To tackle this, we introduce \ourmethod{}, a framework and method for robustly creating probabilistic 3D segmentation maps of tabletop scenes. Our maps contain both occupancy estimates, segmentation information, and principled uncertainty measures. We evaluate the robustness of our method in (1) procedurally generated scenes using open-source object datasets, and (2) real-world tabletop data collected from a depth camera. Our experiments show that our approach outperforms alternative continuous reconstruction approaches that do not explicitly reason about objects in a multi-class formulation."
https://arxiv.org/abs/2403.08105,2024-03-12,Highway Preferential Attachment Models for Geographic Routing,"['Ofek Gila', 'Evrim Ozel', 'Michael T. Goodrich']","In the 1960s, the world-renowned social psychologist Stanley Milgram conducted experiments that showed that not only do there exist ``short chains'' of acquaintances between any two arbitrary people, but that these arbitrary strangers are able to find these short chains. This phenomenon, known as the \emph{small-world phenomenon}, is explained in part by any model that has a low diameter, such as the Barabási and Albert's \emph{preferential attachment} model, but these models do not display the same efficient routing that Milgram's experiments showed. In the year 2000, Kleinberg proposed a model with an efficient $\mathcal{O}(\log^2{n})$ greedy routing algorithm. In 2004, Martel and Nguyen showed that Kleinberg's analysis was tight, while also showing that Kleinberg's model had an expected diameter of only $Θ(\log{n})$ -- a much smaller value than the greedy routing algorithm's path lengths. In 2022, Goodrich and Ozel proposed the \emph{neighborhood preferential attachment} model (NPA), combining elements from Barabási and Albert's model with Kleinberg's model, and experimentally showed that the resulting model outperformed Kleinberg's greedy routing performance on U.S. road networks. While they displayed impressive empirical results, they did not provide any theoretical analysis of their model. In this paper, we first provide a theoretical analysis of a generalization of Kleinberg's original model and show that it can achieve expected $\mathcal{O}(\log{n})$ routing, a much better result than Kleinberg's model. We then propose a new model, \emph{windowed NPA}, that is similar to the neighborhood preferential attachment model but has provable theoretical guarantees w.h.p. We show that this model is able to achieve $\mathcal{O}(\log^{1 + ε}{n})$ greedy routing for any $ε> 0$."
https://arxiv.org/abs/2403.08104,2024-03-12,Minimal reconstructions of a coloring,"['Diego Gamboa', 'Carlos Uzcategui-Aylwin']","A coloring on a finite or countable set $X$ is a function $\varphi: [X]^{2} \to \{0,1\}$, where $[X]^{2}$ is the collection of unordered pairs of $X$. The collection of homogeneous sets for $\varphi$, denoted by $Hom(\varphi)$, consist of all $H \subseteq X$ such that $\varphi$ is constant on $[H]^2$; clearly, $Hom(\varphi) = Hom(1-\varphi)$. A coloring $\varphi$ is \textit{reconstructible} up to complementation from its homogeneous sets if, for any coloring $ψ$ on $X$ such that $Hom(\varphi) = Hom(ψ)$, either $ψ= \varphi$ or $ψ= 1-\varphi$. By $\mathcal{R}$ we denote the collection of all colorings reconstructible from their homogeneous sets. Let $\varphi$ and $ψ$ be colorings on $X$, and set \[ D(\varphi, ψ) = \{ \{x,y\} \in [X]^2: \; ψ\{x,y\} \neq \varphi\{x,y\}\}. \] If $\varphi\not\in \mathcal{R}$, let \[ r(\varphi) = \min\{|D(\varphi, ψ)|: \; Hom(\varphi) = Hom(ψ), \, ψ\neq \varphi, \, ψ\neq 1-\varphi\}. \] A coloring $ψ$ such that $Hom(\varphi)=Hom(ψ)$, $\varphi\neq ψ$ and $1-\varphi\neq ψ$ is called a {\em non trivial reconstruction} of $\varphi$. If, in addition, $r(\varphi) =|D(\varphi, ψ)|$, we call $ψ$ a {\em minimal reconstruction} of $\varphi$. The purpose of this article is to study the minimal reconstructions of a coloring. We show that, for large enough $X$, $r(\varphi)$ can only takes the values $1$ or $4$."
https://arxiv.org/abs/2403.08103,2024-03-12,Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data,['Ruslan Musaev'],"In the age of information abundance, the ability to provide users with contextually relevant and concise information is crucial. Keyword in Context (KIC) generation is a task that plays a vital role in and generation applications, such as search engines, personal assistants, and content summarization. In this paper, we present a novel approach to generating unambiguous and brief sentence-contexts for given keywords using the T5 transformer model, leveraging data obtained from the Context-Reverso API. The code is available at https://github.com/Rusamus/word2context/tree/main ."
https://arxiv.org/abs/2403.08102,2024-03-12,Tournament Auctions,"['Luca Anderlini', 'GaOn Kim']","We examine ``tournament'' second-price auctions in which $N$ bidders compete for the right to participate in a second stage and contend against bidder $N+1$. When the first $N$ bidders are committed so that their bids cannot be changed in the second stage, the analysis yields some unexpected results. The first $N$ bidders consistently bid above their values in equilibrium. When bidder $N+1$ is sufficiently stronger than the first $N$, overbidding leads to an increase in expected revenue in comparison to the standard second-price auction when $N$ is large."
https://arxiv.org/abs/2403.08101,2024-03-12,A systematic study of projection biases in the Weak Lensing analysis of cosmic shear and the combination of galaxy clustering and galaxy-galaxy lensing,"['P. R. V. Chintalapati', 'G. Gutierrez', 'M. H. L. S. Wang']","This paper presents the results of a systematic study of projection biases in the Weak Lensing analysis of cosmic shear and the combination of galaxy clustering and galaxy-galaxy lensing using data collected during the first-year of running the Dark Energy Survey experiment. The study uses $Λ$CDM as the cosmological model and two-point correlation functions for the WL analysis. The results in this paper show that, independent of the WL analysis, projection biases of more than $1σ$ exist, and are a function of the position of the true values of the parameters $h$, $n_{s}$, $Ω_{b}$, and $Ω_νh^{2}$ with respect to their prior probabilities. For cosmic shear, and the combination of galaxy clustering and galaxy-galaxy lensing, this study shows that the coverage probability of the $68.27\%$ credible intervals ranges from as high as $93\%$ to as low as $16\%$, and that these credible intervals are inflated, on average, by $29\%$ for cosmic shear and $20\%$ for the combination of galaxy clustering and galaxy-galaxy lensing. The results of the study also show that, in six out of nine tested cases, the reduction in error bars obtained by transforming credible intervals into confidence intervals is equivalent to an increase in the amount of data by a factor of three."
https://arxiv.org/abs/2403.08100,2024-03-12,Efficient Language Model Architectures for Differentially Private Federated Learning,"['Jae Hun Ro', 'Srinadh Bhojanapalli', 'Zheng Xu', 'Yanxiang Zhang', 'Ananda Theertha Suresh']","Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices. SGD is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency. However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance. In light of this, we ask if language models can be modified such that they can be efficiently trained with SGD client optimizers and answer this affirmatively."
https://arxiv.org/abs/2403.08099,2024-03-12,"Application of Distributed Arithmetic to Adaptive Filtering Algorithms: Trends, Challenges and Future",['Mohd. Tasleem Khan'],"The utilization of distributed arithmetic (DA) in AF algorithms has gained significant attention in recent years due to its potential to enhance computational efficiency and reduce resource requirements. This paper presents an exploration of the application of DA to adaptive filtering (AF) algorithms, analyzing trends, discussing challenges, and outlining future prospects. It begins by providing an overview of both DA and AF algorithms, highlighting their individual merits and established applications. Subsequently, the integration of DA into AF algorithms is explored, showcasing its ability to optimize multiply-accumulate operations and mitigate the computational burden associated with AF algorithms. Throughout the paper, the critical trends observed in the field are discussed, including advancements in DA-based hardware architectures. Moreover, the challenges encountered in implementing DA-based AF is also discussed. The continued evolution of DA techniques to cater to the demands of modern AF applications, including real-time processing, resource-constrained environments, and high-dimensional data streams is anticipated. In conclusion, this paper consolidates the current state of applying DA to AF algorithms, offering insights into prevailing trends, discussing challenges, and presenting future research and development in the field. The fusion of these two domains holds promise for achieving improved computational efficiency, reduced hardware complexity, and enhanced performance in various signal processing applications."
https://arxiv.org/abs/2403.08098,2024-03-12,VODKA-JWST: Synchronized growth of two SMBHs in a massive gas disk? A 3.8 kpc separation dual quasar at cosmic noon with JWST NIRSpec IFU,"['Yuzo Ishikawa', 'Nadia L. Zakamska', 'Yue Shen', 'Xin Liu', 'Yu-Ching Chen', 'Hsiang-Chih Hwang', 'Andrey Vayner', 'Sylvain Veilleux', 'David S. N. Rupke', 'Dominika Wylezalek', 'Arran C. Gross', 'Swetha Sankar', 'Nadiia Diachenko']","The search for dual supermassive black holes (SMBHs) is of immense interest in modern astrophysics. Galaxy mergers may be an important route to fuel and to produce SMBH pairs. Actively accreting SMBH pairs can be observed as a dual quasar, which are vital probes of SMBH growth. Gaia observations have enabled a novel technique to systematically search for such dual quasars at previously unreachable sub-kpc scales, based on the small jitters of the light centroid as the two quasars vary stochastically. Here we present the first detailed study of a 0.46'', 3.8 kpc separation, VODKA-selected dual quasar, J0749+2255, at $z=2.17$ using JWST/NIRSpec integral field unit spectroscopy. This is one of the most distant, small separation dual quasars identified today. Dual quasars at cosmic noon are not well characterized. We detect the faint ionized gas of the host galaxy, best traced by the narrow \ha\ emission. Line ratio diagnostics show a mix of ionization from the two quasars and intense star formation. The spatially-resolved spectra of the two quasars suggest that they have very similar black hole properties (two $M_{BH}\sim 10^9\ \textrm{M}_{\odot}$ with large Eddington ratio reaching $L/L_{Edd}\sim0.2$) hinting at the possible synchronized growth and accretion from the same gas supply. Surprisingly, the ionized gas kinematics suggest an extended, rotating disk rather than a disturbed system that would be expected in a major gas-rich galaxy merger. While it is unclear if J0749+2255 is representative of the dual quasar evolution, the observations with JWST revealed a major puzzle. It would be interesting to see what observations of other dual quasars will show."
https://arxiv.org/abs/2403.08097,2024-03-12,"EXCOGITO, an extensible coarse-graining toolbox for the investigation of biomolecules by means of low-resolution representation","['Marco Giulini', 'Raffaele Fiorentini', 'Luca Tubiana', 'Raffaello Potestio', 'Roberto Menichetti']","Bottom-up coarse-grained (CG) models proved to be essential to complement and sometimes even replace all-atom representations of soft matter systems and biological macromolecules. The development of low-resolution models takes the moves from the reduction of the degrees of freedom employed, that is, the definition of a mapping between a system's high-resolution description and its simplified counterpart. Even in absence of an explicit parametrisation and simulation of a CG model, the observation of the atomistic system in simpler terms can be informative: this idea is leveraged by the mapping entropy, a measure of the information loss inherent to the process of coarsening. Mapping entropy lies at the heart of the extensible coarse-graining toolbox, or EXCOGITO, developed to perform a number of operations and analyses on molecular systems pivoting around the properties of mappings. EXCOGITO can process an all-atom trajectory to compute the mapping entropy, identify the mapping that minimizes it, and establish quantitative relations between a low-resolution representation and geometrical, structural, and energetic features of the system. Here, the software, which is available free of charge under an open source licence, is presented and showcased to introduce potential users to its capabilities and usage."
https://arxiv.org/abs/2403.08096,2024-03-12,Generalizations of the Bassian and co-Bassian Properties for Abelian Groups,"['Andrey R. Chekhlov', 'Peter V. Danchev', 'Patrick W. Keef']","Trying to finalize in some way the present subject, this paper targets to generalize substantially the notions of Bassian and co-Bassian groups by introducing the so-called finitely (co-)Bassian groups, semi (co-)Bassian groups, fully generalized (co-)Bassian groups, absolutely generalized (co-)Bassian groups and establishing their crucial properties and characterizations. In fact, some of the concepts give nothing new by coinciding in the reduced case with the well-known (co-)Bassian property. However, in some of the definitions, the situation is slightly more complicated and we obtain a few new and interesting things by showing that the extensions of the Bassian and co-Bassian properties are totally distinct each other."
https://arxiv.org/abs/2403.08095,2024-03-12,Cohomologies of modified Rota-Baxter Lie algebras with derivations and applications,"['Basdouri Imed', 'Benabdelhafidh Sami', 'Sadraoui Mohamed Amin']","In this paper, first, we introduce a notion of modified Rota-Baxter Lie algebras of weight $\mathrmλ$ with derivations (or simply modified Rota-Baxter LieDer pairs) and their representations. Moreover, we investigate cohomologies of a modified Rota-Baxter LieDer pairs with coefficients in a suitable representation. As applications, we study formal one-parameter deformations and abelian extensions of modified Rota-Baxter LieDer pairs."
https://arxiv.org/abs/2403.08094,2024-03-12,Task and Motion Planning in Hierarchical 3D Scene Graphs,"['Aaron Ray', 'Christopher Bradley', 'Luca Carlone', 'Nicholas Roy']","Recent work in the construction of 3D scene graphs has enabled mobile robots to build large-scale hybrid metric-semantic hierarchical representations of the world. These detailed models contain information that is useful for planning, however how to derive a planning domain from a 3D scene graph that enables efficient computation of executable plans is an open question. In this work, we present a novel approach for defining and solving Task and Motion Planning problems in large-scale environments using hierarchical 3D scene graphs. We identify a method for building sparse problem domains which enable scaling to large scenes, and propose a technique for incrementally adding objects to that domain during planning time to avoid wasting computation on irrelevant elements of the scene graph. We test our approach in two hand crafted domains as well as two scene graphs built from perception, including one constructed from the KITTI dataset. A video supplement is available at https://youtu.be/63xuCCaN0I4."
https://arxiv.org/abs/2403.08093,2024-03-12,Preserving Automotive Heritage: A Blockchain-Based Solution for Secure Documentation of Classic Cars Restoration,"['José Murta', 'Vasco Amaral', 'Fernando Brito e Abreu']","Classic automobiles are an important part of the automotive industry and represent the historical and technological achievements of certain eras. However, to be considered masterpieces, they must be maintained in pristine condition or restored according to strict guidelines applied by expert services. Therefore, all data about restoration processes and other relevant information about these vehicles must be rigorously documented to ensure their verifiability and immutability. Here, we report on our ongoing research to adequately provide such capabilities to the classic car ecosystem."
https://arxiv.org/abs/2403.08092,2024-03-12,Mitigating the Impact of Attribute Editing on Face Recognition,"['Sudipta Banerjee', 'Sai Pranaswi Mullangi', 'Shruti Wagle', 'Chinmay Hegde', 'Nasir Memon']","Facial attribute editing using generative models can impair automated face recognition. This degradation persists even with recent identity-preserving models such as InstantID. To mitigate this issue, we propose two techniques that perform local and global attribute editing. Local editing operates on the finer details via a regularization-free method based on ControlNet conditioned on depth maps and auxiliary semantic segmentation masks. Global editing operates on coarser details via a regularization-based method guided by custom loss and regularization set. In this work, we empirically ablate twenty-six facial semantic, demographic and expression-based attributes altered using state-of-the-art generative models and evaluate them using ArcFace and AdaFace matchers on CelebA, CelebAMaskHQ and LFW datasets. Finally, we use LLaVA, a vision-language framework for attribute prediction to validate our editing techniques. Our methods outperform SoTA (BLIP, InstantID) at facial editing while retaining identity."
https://arxiv.org/abs/2403.08091,2024-03-12,Emergence of high-mass stars in complex fiber networks (EMERGE). I. Early ALMA Survey: observations and massive data reduction,"['A. Hacar', 'A. Socci', 'F. Bonanomi', 'D. Petry', 'M. Tafalla', 'D. Harsono', 'J. Forbrich', 'J. Alves', 'J. Grossschedl', 'J. R. Goicoechea', 'J. Pety', 'A. Burkert', 'G. X. Li']","(Abridged) Recent molecular surveys have revealed a rich gas organization of sonic-like fibers in all kind of environments prior to the formation of low- and high-mass stars. This paper introduces the EMERGE project aiming to investigate whether complex fiber arrangements could explain the origin of high-mass stars and clusters. We analyzed the EMERGE Early ALMA Survey including 7 star-forming regions in Orion (OMC-1/2/3/4 South, L1641N, NGC2023, and Flame Nebula) homogeneously surveyed in both molecular lines (N$_2$H$^+$ J=1-0, HNC J=1-0, plus HC3N J=10-9) and 3mm-continuum using a combination of interferometric ALMA mosaics and IRAM-30m single-dish (SD) maps. Based on our low-resolution (SD) observations, we describe the global properties of our sample covering a wide range of physical conditions including low-, intermediate, and high-mass star-forming regions in different evolutionary stages. Their comparison with ancillary YSO catalogs denotes N$_2$H$^+$ as the best proxy for the dense, star-forming gas in our targets showing a constant star formation efficiency and a fast time evolution of <1 Myr. While apparently clumpy and filamentary in our SD data, all targets show a much more complex fibrous substructure at the enhanced resolution of our ALMA+IRAM-30m maps. A large number of filamentary features at sub-parsec scales are clearly recognized in the high-density gas traced by N$_2$H$^+$ directly connected to the formation of individual protostars. This complex gas organization appears to extend further into the more diffuse gas traced by HNC. This paper presents the EMERGE Early ALMA survey including a first data release of continuum maps and spectral products for this project to be analysed in future papers of this series. A first look at these results illustrates the need of advanced data combination techniques to investigate the intrinsic multi-scale, gas structure of the ISM."
https://arxiv.org/abs/2403.08090,2024-03-12,Controllability of shapes through Landmark Manifolds,"['Erlend Grong', 'Sylvie Vega-Molino']","Landmark manifolds consist of distinct points that are often used to describe shapes. We show that in the Euclidean space, we can preselect two vector fields such that their flows will be able to take any collection of landmarks to another, regardless of the number of landmarks we choose."
https://arxiv.org/abs/2403.08089,2024-03-12,X-ray induced grain structure dynamics in Bi2Se3,"['Kento Katagiri', 'Bernard Kozioziemski', 'Eric Folsom', 'Yifan Wang', 'Karen Appel', 'Philip K. Cook', 'Jon Eggert', 'Sebastian Göde', 'Marylesa Howard', 'Sungwon Kim', 'Mikako Matsuda', 'Motoaki Nakatsutsumi', 'Martin M. Nielsen', 'Henning F. Poulsen', 'Frank Seiboth', 'Hugh Simons', 'Bihan Wang', 'Wenge Yang', 'Ulf Zastrau', 'Hyunjung Kim', 'Leora E. Dresselhaus-Marais']","Grain rotation in crystals often results in coarsening or refinement of the grains that modify the mechanical and thermal properties of materials. While many studies have explored how externally applied stress and temperature drive grain structure dynamics in nano-polycrystalline materials, the analogous studies on colossal grains have been limited, especially in the absence of external force. In this work, we used X-ray free electron laser pulses to irradiate single-crystalline bismuth selenide (Bi2Se3) and observed grain boundary formation and subsequent grain rotation in response to the X-ray radiation. Our observations with simultaneous X-ray diffraction and transmission X-ray microscopy demonstrate how intense X-ray radiation can rapidly change grain morphologies of initially single-crystalline material."
https://arxiv.org/abs/2403.08088,2024-03-12,Nucleon charge and magnetisation distributions: flavour separation and zeroes,"['Zhao-Qian Yao', 'Daniele Binosi', 'Zhu-Fang Cui', 'Craig D. Roberts']","A symmetry-preserving truncation of the quantum field equations describing hadron properties is used to deliver parameter-free predictions for all nucleon elastic electromagnetic form factors and their flavour separation to large values of momentum transfer, $Q^2$. The proton electric form factor, $G_E^p$, possesses a zero, whereas that of the neutron, $G_E^n$, does not. The difference owes to the behaviour of the Pauli form factor of the proton's singly-represented valence $d$-quark. Consequently, $G_E^n>G_E^p$ on a material large-$Q^2$ domain. These predictions can be tested in modern experiments."
https://arxiv.org/abs/2403.08087,2024-03-12,Hochschild cohomology in toposes,"['Cameron Michie', 'Ivan Tomasic']","We develop a theory of internal Hochschild cohomology in a ringed topos. We construct it via the internal Hochschild cochain complex, as well as through derived functor/topos cohomology theory, and discuss its relationship to the absolute Hochschild cohomology."
https://arxiv.org/abs/2403.08086,2024-03-12,Flow-Based Visual Stream Compression for Event Cameras,"['Daniel C. Stumpp', 'Himanshu Akolkar', 'Alan D. George', 'Ryad Benosman']","As the use of neuromorphic, event-based vision sensors expands, the need for compression of their output streams has increased. While their operational principle ensures event streams are spatially sparse, the high temporal resolution of the sensors can result in high data rates from the sensor depending on scene dynamics. For systems operating in communication-bandwidth-constrained and power-constrained environments, it is essential to compress these streams before transmitting them to a remote receiver. Therefore, we introduce a flow-based method for the real-time asynchronous compression of event streams as they are generated. This method leverages real-time optical flow estimates to predict future events without needing to transmit them, therefore, drastically reducing the amount of data transmitted. The flow-based compression introduced is evaluated using a variety of methods including spatiotemporal distance between event streams. The introduced method itself is shown to achieve an average compression ratio of 2.81 on a variety of event-camera datasets with the evaluation configuration used. That compression is achieved with a median temporal error of 0.48 ms and an average spatiotemporal event-stream distance of 3.07. When combined with LZMA compression for non-real-time applications, our method can achieve state-of-the-art average compression ratios ranging from 10.45 to 17.24. Additionally, we demonstrate that the proposed prediction algorithm is capable of performing real-time, low-latency event prediction."
https://arxiv.org/abs/2403.08085,2024-03-12,Lessons from a Pioneering Software Engineering Environment: Design Principles of Software through Pictures,"['Anthony I.', ' Wasserman']","This paper describes the historical background that led to the development of the innovative Software through Pictures multi-user development environment, and the principles for its integration with other software products to create a software engineering environment covering multiple tasks in the software development lifecycle."
https://arxiv.org/abs/2403.08084,2024-03-12,Extending Irksome: improvements in automated Runge--Kutta time stepping for finite element methods,"['Robert C. Kirby', 'Scott P. MacLachlan']","Irksome is a library based on the Unified Form Language (UFL) that enables automated generation of Runge--Kutta methods for time-stepping finite element spatial discretizations of partial differential equations (PDE). Allowing users to express semidiscrete forms of PDE, it generates UFL representations for the stage-coupled variational problems to be solved at each time step. The Firedrake package then generates efficient code for evaluating these variational problems and allows users a wide range of options to deploy efficient algebraic solvers in PETSc."
https://arxiv.org/abs/2403.08083,2024-03-12,On the development of an RFSoC-based ultra-fast Phasemeter with GHz bandwidth,"['Shreevathsa Chalathadka Subrahmanya', 'Christian Darsow-Fromm', 'Oliver Gerberding']","Precise measurements of the frequency and phase of an electrical or optical signal play a key role in various branches of science and engineering. Tracking changing laser frequencies is specifically demanding when the lasers themselves are noisy or if the frequencies rapidly change because they encode highly dynamic signals in, e.g., Doppler-ranging or dynamic cavity readout. Here, we report the development of a high signal bandwidth (> 2 GHz) and high tracking bandwidth (2 MHz) multi-channel Phasemeter. The implementation utilizes an all-digital phase-locked loop realized within the field programmable gate array (FPGA) part of a Radio Frequency System-on-Chip (RFSoC), the programmable logic. We present performance measurements, discuss the role of the high tracking bandwidth for tracking highly dynamic signals, and demonstrate ultra-stable phase locking of a beat note between two widely tunable external cavity diode lasers. We achieve a phase-noise floor in the sub-milli radian regime, even for GHz signals, and demonstrate stable tracking of signals with a frequency change rate of 240 GHz/s."
https://arxiv.org/abs/2403.08082,2024-03-12,Data Monetization Pathways and Complex Dynamic Game Equilibrium Analysis in the Energy Industry,"['Zongxian Wang', 'Jie Song']","As the most critical production factor in the era of the digital economy, data will have a significant impact on social production and development. Energy enterprises possess data that is interconnected with multiple industries, characterized by diverse needs, sensitivity, and long-term nature. The path to monetizing energy enterprises' data is challenging yet crucial. This paper explores the game-theoretic aspects of the data monetization process in energy enterprises by considering the relationships between enterprises and trading platforms. We construct a class of game decision models and study their equilibrium strategies. Our analysis shows that enterprises and platforms can adjust respective benefits by regulating the wholesale price of data and the intensity of data value mining to form a benign equilibrium state. Furthermore, by integrating nonlinear dynamical theory, we discuss the dynamic characteristics present in multi-period repeated game processes. We find that decision-makers should keep the adjustment parameters and initial states within reasonable ranges in multi-period dynamic decision-making to avoid market failure. Finally, based on the theoretical and numerical analysis, we provide decision insights and recommendations for enterprise decision-making to facilitate data monetization through strategic interactions with trading platforms."
https://arxiv.org/abs/2403.08081,2024-03-12,Mechanics of Next Token Prediction with Self-Attention,"['Yingcong Li', 'Yixiao Huang', 'M. Emrullah Ildiz', 'Ankit Singh Rawat', 'Samet Oymak']","Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: $\textit{What}$ $\textit{does}$ $\textit{a}$ $\textit{single}$ $\textit{self-attention}$ $\textit{layer}$ $\textit{learn}$ $\textit{from}$ $\textit{next-token}$ $\textit{prediction?}$ We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: $\textbf{(1)}$ $\textbf{Hard}$ $\textbf{retrieval:}$ Given input sequence, self-attention precisely selects the $\textit{high-priority}$ $\textit{input}$ $\textit{tokens}$ associated with the last input token. $\textbf{(2)}$ $\textbf{Soft}$ $\textbf{composition:}$ It then creates a convex combination of the high-priority tokens from which the next token can be sampled. Under suitable conditions, we rigorously characterize these mechanics through a directed graph over tokens extracted from the training data. We prove that gradient descent implicitly discovers the strongly-connected components (SCC) of this graph and self-attention learns to retrieve the tokens that belong to the highest-priority SCC available in the context window. Our theory relies on decomposing the model weights into a directional component and a finite component that correspond to hard retrieval and soft composition steps respectively. This also formalizes a related implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that these findings shed light on how self-attention processes sequential data and pave the path toward demystifying more complex architectures."
https://arxiv.org/abs/2403.08080,2024-03-12,The Randomized Block Coordinate Descent Method in the Hölder Smooth Setting,"['Leandro Farias Maia', 'David Huckleberry Gutman']","This work provides the first convergence analysis for the Randomized Block Coordinate Descent method for minimizing a function that is both Hölder smooth and block Hölder smooth. Our analysis applies to objective functions that are non-convex, convex, and strongly convex. For non-convex functions, we show that the expected gradient norm reduces at an $O\left(k^{\fracγ{1+γ}}\right)$ rate, where $k$ is the iteration count and $γ$ is the Hölder exponent. For convex functions, we show that the expected suboptimality gap reduces at the rate $O\left(k^{-γ}\right)$. In the strongly convex setting, we show this rate for the expected suboptimality gap improves to $O\left(k^{-\frac{2γ}{1-γ}}\right)$ when $γ>1$ and to a linear rate when $γ=1$. Notably, these new convergence rates coincide with those furnished in the existing literature for the Lipschitz smooth setting."
https://arxiv.org/abs/2403.08079,2024-03-12,BayesFLo: Bayesian fault localization of complex software systems,"['Yi Ji', 'Simon Mak', 'Ryan Lekivetz', 'Joseph Morgan']","Software testing is essential for the reliable development of complex software systems. A key step in software testing is fault localization, which uses test data to pinpoint failure-inducing combinations for further diagnosis. Existing fault localization methods, however, are largely deterministic, and thus do not provide a principled approach for assessing probabilistic risk of potential root causes, or for integrating domain and/or structural knowledge from test engineers. To address this, we propose a novel Bayesian fault localization framework called BayesFLo, which leverages a flexible Bayesian model on potential root cause combinations. A key feature of BayesFLo is its integration of the principles of combination hierarchy and heredity, which capture the structured nature of failure-inducing combinations. A critical challenge, however, is the sheer number of potential root cause scenarios to consider, which renders the computation of posterior root cause probabilities infeasible even for small software systems. We thus develop new algorithms for efficient computation of such probabilities, leveraging recent tools from integer programming and graph representations. We then demonstrate the effectiveness of BayesFLo over state-of-the-art fault localization methods, in a suite of numerical experiments and in two motivating case studies on the JMP XGBoost interface."
https://arxiv.org/abs/2403.08078,2024-03-12,Automated discovery of reprogrammable nonlinear dynamic metamaterials,"['Giovanni Bordiga', 'Eder Medina', 'Sina Jafarzadeh', 'Cyrill Boesch', 'Ryan P. Adams', 'Vincent Tournat', 'Katia Bertoldi']","Harnessing the rich nonlinear dynamics of highly-deformable materials has the potential to unlock the next generation of functional smart materials and devices. However, unlocking such potential requires effective strategies to spatially design optimal material architectures for desired nonlinear dynamic responses such as guiding of nonlinear elastic waves, energy focusing, and cloaking. Here, we introduce an inverse-design framework for the discovery of flexible mechanical metamaterials with a target nonlinear dynamic response. The desired dynamic task is encoded via optimal tuning of the full-scale metamaterial geometry through an inverse-design approach powered by a custom-developed fully-differentiable simulation environment. By deploying such strategy, we design mechanical metamaterials tailored for energy focusing, energy splitting, dynamic protection, and nonlinear motion conversion. Furthermore, we illustrate that our design framework can be expanded to automatically discover reprogrammable architectures capable of switching between different dynamic tasks. For instance, we encode two strongly competing tasks -- energy focusing and dynamic protection -- within a single architecture, utilizing static pre-compression to switch between these behaviors. The discovered designs are physically realized and experimentally tested, demonstrating the robustness of the engineered tasks. All together, our approach opens an untapped avenue towards designer materials with tailored robotic-like reprogrammable functionalities."
https://arxiv.org/abs/2403.08077,2024-03-12,A Multimodal Intermediate Fusion Network with Manifold Learning for Stress Detection,"['Morteza Bodaghi', 'Majid Hosseini', 'Raju Gottumukkala']","Multimodal deep learning methods capture synergistic features from multiple modalities and have the potential to improve accuracy for stress detection compared to unimodal methods. However, this accuracy gain typically comes from high computational cost due to the high-dimensional feature spaces, especially for intermediate fusion. Dimensionality reduction is one way to optimize multimodal learning by simplifying data and making the features more amenable to processing and analysis, thereby reducing computational complexity. This paper introduces an intermediate multimodal fusion network with manifold learning-based dimensionality reduction. The multimodal network generates independent representations from biometric signals and facial landmarks through 1D-CNN and 2D-CNN. Finally, these features are fused and fed to another 1D-CNN layer, followed by a fully connected dense layer. We compared various dimensionality reduction techniques for different variations of unimodal and multimodal networks. We observe that the intermediate-level fusion with the Multi-Dimensional Scaling (MDS) manifold method showed promising results with an accuracy of 96.00\% in a Leave-One-Subject-Out Cross-Validation (LOSO-CV) paradigm over other dimensional reduction methods. MDS had the highest computational cost among manifold learning methods. However, while outperforming other networks, it managed to reduce the computational cost of the proposed networks by 25\% when compared to six well-known conventional feature selection methods used in the preprocessing step."
https://arxiv.org/abs/2403.08076,2024-03-12,Recovery of contextuality based on mirror-like state discrimination in PT- and anti-PT-symmetric systems,"['Xuan Fan', 'Ya Xiao', 'Yongjian Gu']","In the past decades, researches on parity-time (PT) and anti-parity-time(APT) systems have garnered unprecedented attention, showcasing their various intriguing characteristics and promising potentiality in extending canonical Hermitian quantum mechanics. However, despite significant endeavors devoted to this new field of physics, non-Hermitian dynamics of contextuality still remains an uncharted region, either in PT-symmetry or APT-symmetry systems. Since contextuality has also been proven to be the core resource for quantum state discrimination (QSD) tasks, here we systematically investigate the novel performance of contextuality through QSD in both systems, taking mirror-symmetric three-state minimum error discrimination (MED) and maximum confidence discrimination (MCD) scenarios as two examples. The time evolution of contextuality in two scenarios and eight regimes (four regimes for each scenario) are comprehensively compared and analyzed, with the difference of initial states also considered. In the symmetry-unbroken regimes, our simulation shows periodic oscillations of contextuality for both MED and MCD scenarios, the period of which is state-independent but related to non-Hermiticity of the system. Both MED and MCD shows non-trivial recovery of contextuality exceeding its initial value in PT system, which is only existent for MCD in APT system. In the symmetry-broken regimes, the success probabilities of both scenarios start from a prompt decay at first, ending up with a stable value which is constantly 1/3. Non-triviality is found only for MCD scenario in PT system, where the recovered contextuality exceeds its initial value."
https://arxiv.org/abs/2403.08075,2024-03-12,Several isoperimetric inequalities of Dirichlet and Neumann eigenvalues of the Witten-Laplacian,"['Ruifeng Chen', 'Jing Mao']","In this paper, by mainly using the rearrangement technique and suitably constructing trial functions, under the constraint of fixed weighted volume, we can successfully obtain several isoperimetric inequalities for the first and the second Dirichlet eigenvalues, the first nonzero Neumann eigenvalue of the Witten-Laplacian on bounded domains in space forms. These spectral isoperimetric inequalities extend those classical ones (i.e. the Faber-Krahn inequality, the Hong-Krahn-Szegő inequality and the Szegő-Weinberger inequality) of the Laplacian."
https://arxiv.org/abs/2403.08074,2024-03-12,Minimal-Ambiguity Scattering Matrix Estimation with Load-Tunable Ports,['Philipp del Hougne'],"We address the following generic wave problem: is the estimation of an arbitrarily complex linear $N$-port system's scattering matrix possible if waves can be input and output only via $N_\mathrm{A}<N$ ports while the remaining $N_\mathrm{S}=N-N_\mathrm{A}$ ports are terminated with tunable loads? Fundamentally, this problem is intriguing because it ultimately probes to what extent inherent structure in Maxwell's equations constrains the scattering coefficients. Various limited versions of the problem are of temporary scientific and technological interest, ranging from optimal non-invasive focusing on perturbation-inducing targets in complex media, via the characterization of miniaturized, embedded, receive-only and/or multi-element antenna systems to physics-compliant end-to-end channel models for complex metasurface-programmable ""smart radio environments"". More generally, solutions to the problem may yield promising measurement techniques to characterize an arbitrary linear $N$-port system with an $N_\mathrm{A}$-port measurement device, where $N_\mathrm{A} \ll N$. We show theoretically that if $N_\mathrm{A}\geq 2$ and at least three distinct tunable loads are available, the problem can be solved except for sign ambiguities on the off-diagonal scattering coefficients involving the $N_\mathrm{S}$ not-directly-accessible (NDA) ports. If the transmission from at least one accessible port to the NDA ports can be measured, the sign ambiguity can be lifted. We corroborate our results with microwave experiments on an 8-port chaotic cavity with $N_\mathrm{A}=N_\mathrm{S}=4$. Moreover, we reveal additional constraining structure in Maxwell's equations by showing that a limitation to phase-insensitive measurements only results in a mild additional blockwise phase ambiguity that can be lifted simultaneously with the sign ambiguity."
https://arxiv.org/abs/2403.08073,2024-03-12,Experimental demonstration of Contextual Advantage in minimum error and maximum confidence mirror-state discrimination,"['Xuan Fan', 'Ya Xiao', 'Yongjian Gu']","Contextuality is well known as a vital resource for locating the boundary between classical and quantum theories, as well as identifying tasks showing quantum advantage. In a surge of recent works [Schmid and Spekkens, Phys.Rev.X 8, 011015 (2018); Mukherjee, Naonit and Pan, Phys.Rev.A 106, 012216 (2022); Flatt, Lee, Carceller, Brask and Bae, PRX QUANTUM 3, 030337 (2022)], it has also been shown that contextuality is the crucial resource in quantum state discrimination (QSD) tasks, including minimum error discrimination (MED) and maximum confidence discrimination (MCD), together with many other figure-of-merits. Despite the fundamental progress made by those aforementioned works, none of them mention about how to realize their fancy proposals, which is doubtlessly necessary for the final goal of applying this resource in real QSD tasks. In this paper, we report the first experimental demonstration of contextual advantage in both MED and MCD for three mirror-symmetric states using interferometric quantum walk, which can be easily generalized to any figure-of-merit in QSD. Our experiment agrees well with the result of theoretical simulation, and also shows the great potentiality of leveraging this method to explore a simpler version for the witness of contextuality, as well as demonstrating quanutm advantage of various tasks that require QSD."
https://arxiv.org/abs/2403.08072,2024-03-12,On conjugacy and perturbation of subalgebras,"['David Gao', 'Srivatsav Kunnawalkam Elayavalli', 'Gregory Patchell', 'Hui Tan']","We study conjugacy orbits of certain types of subalgebras in tracial von Neumann algebras. For any separable II$_1$ factor $N_0$ we construct a highly indecomposable non Gamma II$_1$ factor $N$ such that $N_0 \subset N$ and moreover every von Neumann subalgebra of $N$ with Haagerup's property admits a unique embedding up to unitary conjugation. Such a factor necessarily has to be non separable, but we show that it can be taken of density character $2^{\aleph_0}$. On the other hand we are able to construct for any separable II$_1$ factor $M_0$, a separable II$_1$ factor $M$ containing $M_0$ such that every property (T) subfactor admits a unique embedding into $M$ up to uniformly approximate unitary equivalence; i.e., any pair of embeddings can be conjugated up to a small uniform $2$-norm perturbation."
https://arxiv.org/abs/2403.08071,2024-03-12,Schubert valuations on Grassmann varieties,"['Rocco Chirivì', 'Xin Fang', 'Peter Littelmann']","The goal of the paper is twofold: on one side it provides an order structure on the set of all maximal chains in the Bruhat poset of Schubert varieties in a Grassmann variety; on the other hand, using this order structure, it works out explicit formulae for the valuation and the Newton-Okounkov body associated to each maximal chain appearing in the framework of Seshadri stratification."
https://arxiv.org/abs/2403.08070,2024-03-12,On the Ashbaugh-Benguria type conjecture about lower-order Neumann eigenvalues of the Witten-Laplacian,"['Ruifeng Chen', 'Jing Mao']","An isoperimetric inequality for lower order nonzero Neumann eigenvalues of the Witten-Laplacian on bounded domains in a Euclidean space or a hyperbolic space has been proven in this paper. About this conclusion, we would like to point out two things:"
https://arxiv.org/abs/2403.08069,2024-03-12,Noncentrosymmetric Triangular Magnet CaMnTeO$_6$: Strong Quantum Fluctuations and Role of s0 vs. s2 Electronic States in Competing Exchange Interactions,"['Xudong Huai', 'Emmanuel Acheampong', 'Erich Delles', 'Michał J. Winiarski', 'Maurice Sorolla II', 'Lila Nassar', 'Mingli Liang', 'Caleb Ramette', 'Huiwen Ji', 'Allen Scheie', 'Stuart Calder', 'Martin Mourigal', 'Thao T. Tran']","Noncentrosymmetric triangular magnets offer a unique platform for realizing strong quantum fluctuations. However, designing these quantum materials remains an open challenge attributable to a knowledge gap in the tunability of competing exchange interactions at the atomic level. Here, we create a new noncentrosymmetric triangular S = 3/2 magnet CaMnTeO$_6$ based on careful chemical and physical considerations. The model material displays competing magnetic interactions and features nonlinear optical responses with the capability of generating coherent photons. The incommensurate magnetic ground state of CaMnTeO$_6$ with an unusually large spin rotation angle of 127 deg.(1) indicates that the anisotropic interlayer exchange is strong and competing with the isotropic interlayer Heisenberg interaction. The moment of 1.39(1) $μ$B, extracted from low-temperature heat capacity and neutron diffraction measurements, is only 46% of the expected value of the static moment 3 $μ$B. This reduction indicates the presence of strong quantum fluctuations in the half-integer spin S = 3/2 CaMnTeO$_6$ magnet, which is rare. By comparing the spin-polarized band structure, chemical bonding, and physical properties of AMnTeO$_6$ (A = Ca, Sr, Pb), we demonstrate how quantum-chemical interpretation can illuminate insights into the fundamentals of magnetic exchange interactions, providing a powerful tool for modulating spin dynamics with atomically precise control."
https://arxiv.org/abs/2403.08068,2024-03-12,SCALHEALTH: Scalable Blockchain Integration for Secure IoT Healthcare Systems,"['Mehrzad Mohammadi', 'Reza Javan', 'Mohammad Beheshti-Atashgah', 'Mohammad Reza Aref']","Internet of Things (IoT) devices are capable of allowing for far-reaching access to and evaluation of patient data to monitor health and diagnose from a distance. An electronic healthcare system that checks patient data, prepares medicines and provides financial assistance is necessary. Providing safe data transmission, monitoring, decentralization, preserving patient privacy, and maintaining confidentiality are essential to an electronic healthcare system. In this study, we introduce (SCALHEALTH) which is a blockchain-based scheme of the Hyperledger Fabric consortium. In this study, we use authentication to agree on a common key for data encryption to send data confidentially. Also, sending data through IPFS is decentralized. Non-fungible token (NFT) is used to send patient prescriptions to pharmacies and insurance companies to ensure the authenticity of patient prescriptions. As the system's main body, blockchain creates authorization and validation for all devices and institutions. Also, all metadata in the system is recorded on the blockchain to maintain integrity, transparency, and timely data monitoring. The proposed study uses two types of blockchain: a health blockchain and a financial blockchain. The financial blockchain is for financial transactions and is based on Ethereum. The health blockchain also introduces a mechanism that allows several blockchains to be active in parallel, instead of only one blockchain. The prototype of this mechanism is simulated in two scenarios. In comparison to the normal state, the proposed plan has superior results."
https://arxiv.org/abs/2403.08067,2024-03-12,"Chemical Cartography with APOGEE: Two-process Parameters and Residual Abundances for 288,789 Stars from Data Release 17","['Tawny Sit', 'David H. Weinberg', 'Adam Wheeler', 'Christian R. Hayes', 'Sten Hasselquist', 'Thomas Masseron', 'Jennifer Sobeck']","Stellar abundance measurements are subject to systematic errors that induce extra scatter and artificial correlations in elemental abundance patterns. We derive empirical calibration offsets to remove systematic trends with surface gravity $\log(g)$ in 17 elemental abundances of 288,789 evolved stars from the SDSS APOGEE survey. We fit these corrected abundances as the sum of a prompt process tracing core-collapse supernovae and a delayed process tracing Type Ia supernovae, thus recasting each star's measurements into the amplitudes $A_{\text{cc}}$ and $A_{\text{Ia}}$ and the element-by-element residuals from this two-parameter fit. As a first application of this catalog, which is $8\times$ larger than that of previous analyses that used a restricted $\log(g)$ range, we examine the median residual abundances of 14 open clusters, nine globular clusters, and four dwarf satellite galaxies. Relative to field Milky Way disk stars, the open clusters younger than 2 Gyr show $\approx 0.1-0.2$ dex enhancements of the neutron-capture element Ce, and the two clusters younger than 0.5 Gyr also show elevated levels of C+N, Na, S, and Cu. Globular clusters show elevated median abundances of C+N, Na, Al, and Ce, and correlated abundance residuals that follow previously known trends. The four dwarf satellites show similar residual abundance patterns despite their different star formation histories, with $\approx 0.2-0.3$ dex depletions in C+N, Na, and Al and $\approx 0.1$ dex depletions in Ni, V, Mn, and Co. We provide our catalog of corrected APOGEE abundances, two-process amplitudes, and residual abundances, which will be valuable for future studies of abundance patterns in different stellar populations and of additional enrichment processes that affect galactic chemical evolution."
https://arxiv.org/abs/2403.08066,2024-03-12,"Zero-Rating, One Big Mess: Analyzing Differential Pricing Practices of European MNOs","['Gabriel Karl Gegenhuber', 'Wilfried Mayer', 'Edgar Weippl']","Zero-rating, the practice of not billing data traffic that belongs to certain applications, has become popular within the mobile ecosystem around the globe. There is an ongoing debate whether mobile operators should be allowed to differentiate traffic or whether net neutrality regulations should prevent this. Despite the importance of this issue, we know little about the technical aspects of zero-rating offers since the implementation is kept secret by mobile operators and therefore is opaque to end-users and regulatory agencies."
https://arxiv.org/abs/2403.08065,2024-03-12,System Design Approach for Control of Differentially Private Dynamical Systems,"['Raman Goyal', 'Dhrubajit Chowdhury', 'Shantanu Rane']","This paper introduces a novel approach to concurrently design dynamic controllers and correlated differential privacy noise in dynamic control systems. An increase in privacy noise increases the system's privacy but adversely affects the system's performance. Our approach optimizes the noise distribution while shaping closed-loop system dynamics such that the privacy noise has the least impact on system performance and the most effect on system privacy. We further add privacy noise to both control input and system output to privatize the system's state for an adversary with access to both communication channels and direct output measurements. The study also suggests tailored privacy bounds for different states, providing a comprehensive framework for jointly optimizing system performance and privacy in the context of differential privacy."
https://arxiv.org/abs/2403.08064,2024-03-12,Low degree rational curves on quasi-polarized K3 surfaces,"['Sławomir Rams', 'Matthias Schütt']","We prove that there are at most $(24-r_0)$ low-degree rational curves on high-degree models of K3 surfaces with at most Du Val singularities, where $r_0$ is the number of exceptional divisors on the minimal resolution. We also provide several existence results in the above setting (i.e. for rational curves on quasi-polarized K3 surfaces), which imply that for various values of $r_0$ our bound cannot be improved."
https://arxiv.org/abs/2403.08063,2024-03-12,Towards Code Generation for Octree-Based Multigrid Solvers,"['Richard Angersbach', 'Sebastian Kuckuck', 'Harald Köstler']","This paper presents a novel method designed to generate multigrid solvers optimized for octree-based software frameworks. Our approach focuses on accurately capturing local features within a domain while leveraging the efficiency inherent in multigrid techniques. We outline the essential steps involved in generating specialized kernels for local refinement and communication routines, integrating on-the-fly interpolations to seamlessly transfer information between refinement levels. For this purpose, we established a software coupling via an automatic fusion of generated multigrid solvers and communication kernels with manual implementations of complex octree data structures and algorithms often found in established software frameworks. We demonstrate the effectiveness of our method through numerical experiments with different interpolation orders. Large-scale benchmarks conducted on the SuperMUC-NG CPU cluster underscore the advantages of our approach, offering a comparison against a reference implementation to highlight the benefits of our method and code generation in general."
https://arxiv.org/abs/2403.08062,2024-03-12,Efficient Fault Tolerance for Pipelined Query Engines via Write-ahead Lineage,"['Ziheng Wang', 'Alex Aiken']","Modern distributed pipelined query engines either do not support intra-query fault tolerance or employ high-overhead approaches such as persisting intermediate outputs or checkpointing state. In this work, we present write-ahead lineage, a novel fault recovery technique that combines Spark's lineage-based replay and write-ahead logging. Unlike Spark, where the lineage is determined before query execution, write-ahead lineage persistently logs lineage at runtime to support dynamic task dependencies in pipelined query engines. Since only KB-sized lineages are persisted instead of MB-sized intermediate outputs, the normal execution overhead is minimal compared to spooling or checkpointing based approaches. To ensure fast fault recovery times, tasks only consume intermediate outputs with persisted lineage, preventing global rollbacks upon failure. In addition, lost tasks from different stages can be recovered in a pipelined parallel manner. We implement write-ahead lineage in a distributed pipelined query engine called Quokka. We show that Quokka is around 2x faster than SparkSQL on the TPC-H benchmark with similar fault recovery performance."
https://arxiv.org/abs/2403.08061,2024-03-12,Gaze-based Human-Robot Interaction System for Infrastructure Inspections,"['Sunwoong Choi', 'Zaid Abbas Al-Sabbag', 'Sriram Narasimhan', 'Chul Min Yeum']","Routine inspections for critical infrastructures such as bridges are required in most jurisdictions worldwide. Such routine inspections are largely visual in nature, which are qualitative, subjective, and not repeatable. Although robotic infrastructure inspections address such limitations, they cannot replace the superior ability of experts to make decisions in complex situations, thus making human-robot interaction systems a promising technology. This study presents a novel gaze-based human-robot interaction system, designed to augment the visual inspection performance through mixed reality. Through holograms from a mixed reality device, gaze can be utilized effectively to estimate the properties of the defect in real-time. Additionally, inspectors can monitor the inspection progress online, which enhances the speed of the entire inspection process. Limited controlled experiments demonstrate its effectiveness across various users and defect types. To our knowledge, this is the first demonstration of the real-time application of eye gaze in civil infrastructure inspections."
https://arxiv.org/abs/2403.08060,2024-03-12,Classical Limits of Hilbert Bimodules as Symplectic Dual Pairs,"['Benjamin H. Feintzeig', 'Jer Steeger']","Hilbert bimodules are morphisms between C*-algebraic models of quantum systems, while symplectic dual pairs are morphisms between Poisson geometric models of classical systems. Both of these morphisms preserve representation-theoretic structures of the relevant types of models. Previously, it has been shown that one can functorially associate certain symplectic dual pairs to Hilbert bimodules through strict deformation quantization. We show that, in the inverse direction, strict deformation quantization also allows one to functorially take the classical limit of a Hilbert bimodule to reconstruct a symplectic dual pair."
https://arxiv.org/abs/2403.08059,2024-03-12,FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation,"['Benjamin D. Killeen', 'Liam J. Wang', 'Han Zhang', 'Mehran Armand', 'Russell H. Taylor', 'Greg Osgood', 'Mathias Unberath']","Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D structures are projected in transmission onto the imaging plane, resulting in overlapping features of varying opacity and shape. To pave the way toward an FM for comprehensive and automated analysis of arbitrary medical X-ray images, we develop FluoroSAM, a language-aligned variant of the Segment-Anything Model, trained from scratch on 1.6M synthetic X-ray images. FluoroSAM is trained on data including masks for 128 organ types and 464 non-anatomical objects, such as tools and implants. In real X-ray images of cadaveric specimens, FluoroSAM is able to segment bony anatomical structures based on text-only prompting with 0.51 and 0.79 DICE with point-based refinement, outperforming competing SAM variants for all structures. FluoroSAM is also capable of zero-shot generalization to segmenting classes beyond the training set thanks to its language alignment, which we demonstrate for full lung segmentation on real chest X-rays."
https://arxiv.org/abs/2403.08058,2024-03-12,CHAI: Clustered Head Attention for Efficient LLM Inference,"['Saurabh Agarwal', 'Bilge Acun', 'Basil Homer', 'Mostafa Elhoushi', 'Yejin Lee', 'Shivaram Venkataraman', 'Dimitris Papailiopoulos', 'Carole-Jean Wu']","Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets."
https://arxiv.org/abs/2403.08057,2024-03-12,MineXR: Mining Personalized Extended Reality Interfaces,"['Hyunsung Cho', 'Yukang Yan', 'Kashyap Todi', 'Mark Parent', 'Missie Smith', 'Tanya R. Jonker', 'Hrvoje Benko', 'David Lindlbauer']","Extended Reality (XR) interfaces offer engaging user experiences, but their effective design requires a nuanced understanding of user behavior and preferences. This knowledge is challenging to obtain without the widespread adoption of XR devices. We introduce MineXR, a design mining workflow and data analysis platform for collecting and analyzing personalized XR user interaction and experience data. MineXR enables elicitation of personalized interfaces from participants of a data collection: for any particular context, participants create interface elements using application screenshots from their own smartphone, place them in the environment, and simultaneously preview the resulting XR layout on a headset. Using MineXR, we contribute a dataset of personalized XR interfaces collected from 31 participants, consisting of 695 XR widgets created from 178 unique applications. We provide insights for XR widget functionalities, categories, clusters, UI element types, and placement. Our open-source tools and data support researchers and designers in developing future XR interfaces."
https://arxiv.org/abs/2403.08056,2024-03-12,Improving Memory Dependence Prediction with Static Analysis,"['Luke Panayi', 'Rohan Gandhi', 'Jim Whittaker', 'Vassilios Chouliaras', 'Martin Berger', 'Paul Kelly']","This paper explores the potential of communicating information gained by static analysis from compilers to Out-of-Order (OoO) machines, focusing on the memory dependence predictor (MDP). The MDP enables loads to issue without all in-flight store addresses being known, with minimal memory order violations. We use LLVM to find loads with no dependencies and label them via their opcode. These labelled loads skip making lookups into the MDP, improving prediction accuracy by reducing false dependencies. We communicate this information in a minimally intrusive way, i.e.~without introducing additional hardware costs or instruction bandwidth, providing these improvements without any additional overhead in the CPU. We find that in select cases in Spec2017, a significant number of load instructions can skip interacting with the MDP and lead to a performance gain. These results point to greater possibilities for static analysis as a source of near zero cost performance gains in future CPU designs."
https://arxiv.org/abs/2403.08055,2024-03-12,DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction,"['Mohamed Elrefaie', 'Angela Dai', 'Faez Ahmed']","This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of 3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional neural network model, both aimed at aerodynamic car design through machine learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million surface mesh faces and comprehensive aerodynamic performance data comprising of full 3D pressure, velocity fields, and wall-shear stresses, addresses the critical need for extensive datasets to train deep learning models in engineering applications. It is 60\% larger than the previously available largest public dataset of cars, and is the only open-source dataset that also models wheels and underbody. RegDGCNN leverages this large-scale dataset to provide high-precision drag estimates directly from 3D meshes, bypassing traditional limitations such as the need for 2D image rendering or Signed Distance Fields (SDF). By enabling fast drag estimation in seconds, RegDGCNN facilitates rapid aerodynamic assessments, offering a substantial leap towards integrating data-driven methods in automotive design. Together, DrivAerNet and RegDGCNN promise to accelerate the car design process and contribute to the development of more efficient vehicles. To lay the groundwork for future innovations in the field, the dataset and code used in our study are publicly accessible at \url{https://github.com/Mohamedelrefaie/DrivAerNet}"
https://arxiv.org/abs/2403.08054,2024-03-12,Learning-based Prescribed-Time Safety for Control of Unknown Systems with Control Barrier Functions,"['Tzu-Yuan Huang', 'Xiaobing Dai', 'Sihua Zhang', 'Alexandre Capone', 'Velimir Todorovski', 'Stefan Sosnowski', 'Sandra Hirche']","In many control system applications, state constraint satisfaction needs to be guaranteed within a prescribed time. While this issue has been partially addressed for systems with known dynamics, it remains largely unaddressed for systems with unknown dynamics. In this paper, we propose a Gaussian process-based time-varying control method that leverages backstepping and control barrier functions to achieve safety requirements within prescribed time windows. It can be used to keep a system within a safe region or to make it return to a safe region within a limited time window. These properties are cemented by rigorous theoretical results. The effectiveness of the proposed controller is demonstrated in a simulation of a robotic manipulator."
https://arxiv.org/abs/2403.08053,2024-03-12,Generating Clarification Questions for Disambiguating Contracts,"['Anmol Singhal', 'Chirag Jain', 'Preethu Rose Anish', 'Arkajyoti Chakraborty', 'Smita Ghaisas']","Enterprises frequently enter into commercial contracts that can serve as vital sources of project-specific requirements. Contractual clauses are obligatory, and the requirements derived from contracts can detail the downstream implementation activities that non-legal stakeholders, including requirement analysts, engineers, and delivery personnel, need to conduct. However, comprehending contracts is cognitively demanding and error-prone for such stakeholders due to the extensive use of Legalese and the inherent complexity of contract language. Furthermore, contracts often contain ambiguously worded clauses to ensure comprehensive coverage. In contrast, non-legal stakeholders require a detailed and unambiguous comprehension of contractual clauses to craft actionable requirements. In this work, we introduce a novel legal NLP task that involves generating clarification questions for contracts. These questions aim to identify contract ambiguities on a document level, thereby assisting non-legal stakeholders in obtaining the necessary details for eliciting requirements. This task is challenged by three core issues: (1) data availability, (2) the length and unstructured nature of contracts, and (3) the complexity of legal text. To address these issues, we propose ConRAP, a retrieval-augmented prompting framework for generating clarification questions to disambiguate contractual text. Experiments conducted on contracts sourced from the publicly available CUAD dataset show that ConRAP with ChatGPT can detect ambiguities with an F2 score of 0.87. 70% of the generated clarification questions are deemed useful by human evaluators."
https://arxiv.org/abs/2403.08052,2024-03-12,A Computational Method for $H_2$-optimal Estimator and State Feedback Controller Synthesis for PDEs,"['Sachin Shivakumar', 'Matthew Peet']","In this paper, we present solvable, convex formulations of $H_2$-optimal state estimation and state-feedback control problems for a general class of linear Partial Differential Equations (PDEs) with one spatial dimension. These convex formulations are derived by using an analysis and control framework called the `Partial Integral Equation' (PIE) framework, which utilizes the PIE representation of infinite-dimensional systems. Since PIEs are parameterized by Partial Integral (PI) operators that form an algebra, $H_2$-optimal estimation and control problems for PIEs can be formulated as Linear PI Inequalities (LPIs). Furthermore, if a PDE admits a PIE representation, then the stability and $H_2$ performance of the PIE system implies that of the PDE system. Consequently, the optimal estimator and controller obtained for a PIE using LPIs provide the same stability and performance when applied to the corresponding PDE. These LPI optimization problems can be solved computationally using semi-definite programming solvers because such problems can be formulated using Linear Matrix Inequalities by using positive matrices to parameterize a cone of positive PI operators. We illustrate the application of these methods by constructing observers and controllers for some standard PDE examples."
https://arxiv.org/abs/2403.08051,2024-03-12,Multi-Apartment Rent Division,"['Ariel D. Procaccia', 'Benjamin Schiffer', 'Shirley Zhang']","Rent division is the well-studied problem of fairly assigning rooms and dividing rent among a set of roommates within a single apartment. A shortcoming of existing solutions is that renters are assumed to be considering apartments in isolation, whereas in reality, renters can choose among multiple apartments. In this paper, we generalize the rent division problem to the multi-apartment setting, where the goal is to both fairly choose an apartment among a set of alternatives and fairly assign rooms and rents within the chosen apartment. Our main contribution is a generalization of envy-freeness called rearrangeable envy-freeness. We show that a solution satisfying rearrangeable envy-freeness is guaranteed to exist and that it is possible to optimize over all rearrangeable envy-free solutions in polynomial time. We also define an even stronger fairness notion called universal envy-freeness and study its existence when values are drawn randomly."
https://arxiv.org/abs/2403.08050,2024-03-12,Experimental demonstration of the shadow of a laser beam,"['Raphael A. Abrahao', 'Henri P N Morin', 'Jordan T R Page', 'Akbar Safari', 'Robert W Boyd', 'Jeff S Lundeen']","Light, being massless, casts no shadow; under ordinary circumstances, photons pass right through each other unimpeded. Here, we demonstrate a laser beam acting like an object - the beam casts a shadow upon a surface when the beam is illuminated by another light source. We observe a regular shadow in the sense it can be seen by the naked eye, it follows the contours of the surface it falls on, and it follows the position and shape of the object (the laser beam). Specifically, we use a nonlinear optical process involving four atomic levels of ruby. We find a maximum contrast of approximately 22 percent, similar to that of a shadow of a tree on a sunny day. Making light itself cast a shadow opens new possibilities for fabrication, imaging, and illumination."
https://arxiv.org/abs/2403.08049,2024-03-12,TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks,"['Yuexi Chen', 'Vlad I. Morariu', 'Anh Truong', 'Zhicheng Liu']","Mixed-media tutorials, which integrate videos, images, text, and diagrams to teach procedural skills, offer more browsable alternatives than timeline-based videos. However, manually creating such tutorials is tedious, and existing automated solutions are often restricted to a particular domain. While AI models hold promise, it is unclear how to effectively harness their powers, given the multi-modal data involved and the vast landscape of models. We present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks. First, we distill common tutorial components by surveying existing work; then, we present an approach to identify, assemble, and evaluate AI models for component extraction; finally, we propose guidelines for designing user interfaces (UI) that support tutorial creation based on AI-generated components. We show that TutoAI has achieved higher or similar quality compared to a baseline model in preliminary user studies."
https://arxiv.org/abs/2403.08048,2024-03-12,A Spectroscopic Hunt for Post-Red Supergiants in the Large Magellanic Cloud I: Preliminary Results,"['Kaitlyn M. Chen', 'Trevor Z. Dorn-Wallenstein']","Yellow supergiants (YSGs) are rare and poorly understood, and studying them is critical to constraining massive star evolution. We obtained flux-calibrated Magellan Inamori Kyocera Echelle (MIKE) high-resolution spectra of 40 YSGs in the Large Magellanic Cloud (LMC); this sample likely contains post-red supergiants (RSGs). Fitting these data with ATLAS9 model atmospheres, we determined fundamental parameters for these stars. We measure the first spectroscopic luminosities for YSGs above 20 $M_\odot$, providing us a novel probe of the luminosity-to-mass ratio. Many stars in our sample appear to have anomalously high surface gravities, despite being confirmed LMC supergiants. We manually inspected our data finding evidence for binary companions and ongoing mass loss. Our work demonstrates the valuable role of high-resolution spectroscopy in interpreting the evolutionary status of cool supergiants."
https://arxiv.org/abs/2403.08047,2024-03-12,Transition to a weaker Sun: Changes in the solar atmosphere during the decay of the Modern Maximum,"['K. Mursula', 'A. A. Pevtsov', 'T. Asikainen', 'I. Tähtinen', 'A. R. Yeates']","The Sun experienced a period of unprecedented activity during the 20th century, now called the Modern Maximum (MM). The decay of the MM after cycle 19 has changed the Sun, the heliosphere, and the planetary environments in many ways. However, studies disagree on whether this decay has proceeded synchronously in different solar parameters or not. One key issue is if the relation between two long parameters of solar activity, the sunspot number and the solar 10.7cm radio flux, has remained the same during this decay. A recent study argues that there is an inhomogeneity in the 10.7cm radio flux in 1980, which leads to a step-like jump (""1980 jump"") in this relation. Here we show that the relation between sunspot number and 10.7cm radio flux varies in time, not due to an inhomogeneous radio flux but due to physical changes in the solar atmosphere. We used radio fluxes at four different wavelengths measured in Japan, and studied their long-term relation with the sunspot number and the 10.7cm radio flux. We also used two other solar parameters, the MgII index and the number of active regions. We find that the 1980 jump is only the first of a series of 1-2-year ""humps"" that mainly occur during solar maxima. All radio fluxes increase with respect to the sunspot number from the 1970s to 2010s. These results reestablish the 10.7cm flux as a homogeneous measure of solar activity. The fluxes of the longer radio waves are found to increase with respect to the shorter waves, which suggests a long-term change in the solar radio spectrum. We also find that the MgII index and the number of active regions also increased with respect to the sunspot number, further verifying the difference in the long-term evolution in chromospheric and photospheric parameters. Our results provide evidence for important structural changes in solar magnetic fields and the solar atmosphere during the decay of the MM."
https://arxiv.org/abs/2403.08046,2024-03-12,Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models,"['Charlie Campanella', 'Rob van der Goot']","Large language models (LLMs) have emerged as a useful technology for job matching, for both candidates and employers. Job matching is often based on a particular geographic location, such as a city or region. However, LLMs have known biases, commonly derived from their training data. In this work, we aim to quantify the metropolitan size bias encoded within large language models, evaluating zero-shot salary, employer presence, and commute duration predictions in 384 of the United States' metropolitan regions. Across all benchmarks, we observe negative correlations between the metropolitan size and the performance of the LLMS, indicating that smaller regions are indeed underrepresented. More concretely, the smallest 10 metropolitan regions show upwards of 300% worse benchmark performance than the largest 10."
https://arxiv.org/abs/2403.08045,2024-03-12,What Can Quantum Information Theory Offer to Quantum Chemistry?,"['Damiano Aliverti-Piuri', 'Kaustav Chatterjee', 'Lexin Ding', 'Ke Liao', 'Julia Liebert', 'Christian Schilling']","It is the ultimate goal of this work to foster synergy between quantum chemistry and the flourishing field of quantum information theory. For this, we first translate quantum information concepts such as entanglement and correlation into the context of quantum chemical systems. In particular, we establish two conceptually distinct perspectives on `electron correlation' leading to a notion of orbital and particle correlation. We then demonstrate that particle correlation equals total orbital correlation minimized over all orbital bases. Accordingly, particle correlation resembles the minimal, thus intrinsic, complexity of many-electron wave functions while orbital correlation quantifies their complexity relative to a basis. We illustrate these concepts of intrinsic and extrinsic correlation complexity in molecular systems, which also manifests the crucial link between the two correlation pictures. Our results provide theoretical justification for the long-favored natural orbitals for simplifying electronic structures, and open new pathways for developing more efficient approaches towards the electron correlation problem."
https://arxiv.org/abs/2403.08044,2024-03-12,"Neural, Muscular, and Perceptual responses with shoulder exoskeleton use over Days","['Tiash Rana Mukherjee', 'Oshin Tyagi', 'Jingkun Wang', 'John Kang', 'Ranjana Mehta']","Passive shoulder exoskeletons have been widely introduced in the industry to aid upper extremity movements during repetitive overhead work. As an ergonomic intervention, it is important to understand how users adapt to these devices over time and if these induce external stress while working. The study evaluated the use of an exoskeleton over a period of 3 days by assessing the neural, physiological, and perceptual responses of twenty-four participants by comparing a physical task against the same task with an additional cognitive workload. Over days adaptation to task irrespective of task and group were identified. Electromyography (EMG) analysis of shoulder and back muscles reveals lower muscle activity in the exoskeleton group irrespective of task. Functional connectivity analysis using functional near infrared spectroscopy (fNIRS) reveals that exoskeletons benefit users by reducing task demands in the motor planning and execution regions. Sex-based differences were also identified in these neuromuscular assessments."
https://arxiv.org/abs/2403.08043,2024-03-12,Authorship Style Transfer with Policy Optimization,"['Shuai Liu', 'Shantanu Agarwal', 'Jonathan May']","Authorship style transfer aims to rewrite a given text into a specified target while preserving the original meaning in the source. Existing approaches rely on the availability of a large number of target style exemplars for model training. However, these overlook cases where a limited number of target style examples are available. The development of parameter-efficient transfer learning techniques and policy optimization (PO) approaches suggest lightweight PO is a feasible approach to low-resource style transfer. In this work, we propose a simple two step tune-and-optimize technique for low-resource textual style transfer. We apply our technique to authorship transfer as well as a larger-data native language style task and in both cases find it outperforms state-of-the-art baseline models."
https://arxiv.org/abs/2403.08042,2024-03-12,CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions,"['Amel Imene Hadj Bouzid', 'Baudouin Denis de Senneville', 'Fabien Baldacci', 'Pascal Desbarats', 'Patrick Berger', 'Ilyes Benlala', 'Gaël Dournes']","This research embarked on a comparative exploration of the holistic segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized data from two CF reference centers, covering five major CF structural changes. Initially, it compared the 2D and 3D models, highlighting the 3D model's superior capability in capturing complex features like mucus plugs and consolidations. To improve the 2D model's performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model's performance. The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings. Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models' interpretability and reliability, providing valuable insights for their clinical application."
https://arxiv.org/abs/2403.08041,2024-03-12,What would Plato say? Concepts and notions from Greek philosophy applied to gamification mechanics for a meaningful and ethical gamification,['Kostas Karpouzis'],"Gamification, the integration of game mechanics in non-game settings, has become increasingly prevalent in various digital platforms; however, its ethical and societal impacts are often overlooked. This paper delves into how Platonic and Aristotelian philosophies can provide a critical framework for understanding and evaluating the ethical dimensions of gamification. Plato's allegory of the cave and theory of forms are used to analyse the perception of reality in gamified environments, questioning their authenticity and the value of virtual achievements, while Aristotle's virtue ethics, with its emphasis on moderation, virtue, and eudaimonia (true and full happiness), can help assess how gamification influences user behaviour and ethical decision-making. The paper critically examines various gamification elements, such as the hero's journey, altruistic actions, badge levels, and user autonomy, through these philosophical lenses, and addresses the ethical responsibilities of gamification designers, advocating for a balanced approach that prioritizes user well-being and ethical development over commercial interests. By bridging ancient philosophical insights with modern digital culture, this research contributes to a deeper understanding of the ethical implications of gamification, emphasizing the need for responsible and virtuous design in digital applications."
https://arxiv.org/abs/2403.08040,2024-03-12,MicroT: Low-Energy and Adaptive Models for MCUs,"['Yushan Huang', 'Ranya Aloufi', 'Xavier Cadet', 'Yuchen Zhao', 'Payam Barnaghi', 'Hamed Haddadi']","We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT can improve accuracy by up to 9.87%. On MCUs, compared to the standard full model inference, MicroT can save up to about 29.13% in energy consumption. MicroT also allows users to adaptively adjust the stage-decision ratio as needed, better balancing model performance and energy consumption. Under the standard stage-decision ratio configuration, MicroT can increase accuracy by 5.91% and save about 14.47% of energy consumption."
https://arxiv.org/abs/2403.08039,2024-03-12,On closed definable subsets in Hensel minimal structures,['Krzysztof Jan Nowak'],"This paper deals with Hensel minimal structures on non-trivially valued fields $K$. The main aim is to establish the following two properties of closed 0-definable subsets $A$ in the affine spaces $K^{n}$. Every such subset $A$ is the zero locus of a continuous 0-definable function $f:K^{n} \to K$, and there exists a 0-definable retraction $r: K^{n} \to A$. While the former property is a non-Archimedean counterpart of the one from o-minimal geometry, the former does not hold in real geometry in general. The proofs make use of a model-theoretic compactness argument and ubiquity of clopen sets in non-Archimedean geometry."
https://arxiv.org/abs/2403.08038,2024-03-12,Bus Factor Explorer,"['Egor Klimov', 'Muhammad Umair Ahmed', 'Nikolai Sviridov', 'Pouria Derakhshanfar', 'Eray Tüzün', 'Vladimir Kovalenko']","Bus factor (BF) is a metric that tracks knowledge distribution in a project. It is the minimal number of engineers that have to leave for a project to stall. Despite the fact that there are several algorithms for calculating the bus factor, only a few tools allow easy calculation of bus factor and convenient analysis of results for projects hosted on Git-based providers."
https://arxiv.org/abs/2403.08037,2024-03-12,Giant radio galaxies in the LOFAR deep fields,"['M. Simonte', 'H. Andernach', 'M. Brueggen', 'G. K. Miley', 'P. Barthel']","In this study, we compare the radio, optical and environmental properties of GRGs with those of a control sample of smaller RGs we found in the three LOw-Frequency ARray (LOFAR) deep fields, namely the Bootes, ELAIS-N1, Lockman Hole, for a total area of about 95 deg^2. We inspected the LOFAR deep fields and created a catalogue of 1609 extended radio galaxies (ERGs). By visual inspection, we identified their host galaxies and spectroscopically or photometrically classified 280 of these as GRGs. We studied their properties, such as their accretion state, stellar mass and star formation rate (SFR) using deep optical and infrared survey data. Moreover, we explored the environment in terms of the surface number density of neighbouring galaxies within these surveys. Integrated flux densities and radio luminosities were also determined for a subset of ERGs through available survey images at 50, 150, 610, and 1400 MHz to compute integrated spectral indices. Considering the fraction of GRGs displaying an FRII morphology alongside the host galaxy properties, we suggest that GRGs consistently possess sufficient power to overcome jet frustration caused by the interstellar medium. Moreover, clear differences emerge in the environmental densities between GRGs and smaller RGs, using the number of neighbouring galaxies within 10 Mpc from the host galaxy as a proxy. GRGs preferentially reside in sparser environments compared to their smaller counterparts. In particular, only 3.6% of the GRGs reside within a 3D comoving distance of 5 Mpc from a previously reported galaxy cluster. We found that larger sources exhibit steeper integrated spectral indices, suggesting that GRGs are late-stage versions of RGs. These results suggest that GRGs are amongst the oldest radio sources with the most stable nuclear activity that reside in sparse environments."
https://arxiv.org/abs/2403.08036,2024-03-12,A Review of Cybersecurity Incidents in the Food and Agriculture Sector,"['Ajay Kulkarni', 'Yingjie Wang', 'Munisamy Gopinath', 'Dan Sobien', 'Abdul Rahman', 'Feras A. Batarseh']","The increasing utilization of emerging technologies in the Food & Agriculture (FA) sector has heightened the need for security to minimize cyber risks. Considering this aspect, this manuscript reviews disclosed and documented cybersecurity incidents in the FA sector. For this purpose, thirty cybersecurity incidents were identified, which took place between July 2011 and April 2023. The details of these incidents are reported from multiple sources such as: the private industry and flash notifications generated by the Federal Bureau of Investigation (FBI), internal reports from the affected organizations, and available media sources. Considering the available information, a brief description of the security threat, ransom amount, and impact on the organization are discussed for each incident. This review reports an increased frequency of cybersecurity threats to the FA sector. To minimize these cyber risks, popular cybersecurity frameworks and recent agriculture-specific cybersecurity solutions are also discussed. Further, the need for AI assurance in the FA sector is explained, and the Farmer-Centered AI (FCAI) framework is proposed. The main aim of the FCAI framework is to support farmers in decision-making for agricultural production, by incorporating AI assurance. Lastly, the effects of the reported cyber incidents on other critical infrastructures, food security, and the economy are noted, along with specifying the open issues for future development."
https://arxiv.org/abs/2403.08035,2024-03-12,Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection,"['Tharindu Kumarage', 'Amrita Bhattacharjee', 'Joshua Garland']","Large language models (LLMs) excel in many diverse applications beyond language generation, e.g., translation, summarization, and sentiment analysis. One intriguing application is in text classification. This becomes pertinent in the realm of identifying hateful or toxic speech -- a domain fraught with challenges and ethical dilemmas. In our study, we have two objectives: firstly, to offer a literature review revolving around LLMs as classifiers, emphasizing their role in detecting and classifying hateful or toxic content. Subsequently, we explore the efficacy of several LLMs in classifying hate speech: identifying which LLMs excel in this task as well as their underlying attributes and training. Providing insight into the factors that contribute to an LLM proficiency (or lack thereof) in discerning hateful content. By combining a comprehensive literature review with an empirical analysis, our paper strives to shed light on the capabilities and constraints of LLMs in the crucial domain of hate speech detection."
https://arxiv.org/abs/2403.08034,2024-03-12,Vibronic correlations in molecular strong field dynamics,"['Marie Labeye', 'Camille Lévêque', 'François Risoud', 'Alfred Maquet', 'Jérémie Caillat', 'Richard Taïeb']","We investigate ultrafast vibronic dynamics triggered by intense femtosecond infrared pulses in small molecules. Our study is based on numerical simulations performed with 2D model molecules, and analyzed in the perspective of the renown Lochfrass and Bond-Softening models. We give a new interpretation of the observed nuclear wave packet dynamics, with a focus on the phase of the bond oscillations. Our simulations also reveal intricate features in the field-induced nuclear motion that are not accounted for by existing models. Our analyses assign these features to strong dynamic correlations between the active electron and the nuclei, which significantly depend on the carrier envelope phase of the pulse, even for relatively ``long'' pulses, which should make them experimentally observable."
https://arxiv.org/abs/2403.08033,2024-03-12,Navigating the Quantum Divide(s),"['A. Ayda Gercek', 'Zeki C. Seskir']","This article explores the possible divides resulting from the introduction of emerging quantum technologies (QT) to society. It provides the multidirectional impacts of QT on science, technology, geopolitics, and societal structures. We aim to challenge the idea of a singular ""quantum divide"" by presenting a more comprehensive perspective. To complement the existing literature on the quantum divide, we propose four distinct divides that could result from the emergence of QT. Firstly, we examine the ""Quantum Divide in Science"", representing the paradigmatic gap among scientists and inequalities in access to knowledge/resources within research communities. We suggest the ""Quantum Divide in Technologies through Path-dependency"" as the second possible divide, examining the adoption processes of certain technologies to be developed by nations, firms, and research communities. The discussion extends internationally, focusing on the ""Quantum Divide between Countries,"" by dealing with the reasons and outcomes of the adoption processes between countries of different development levels (economically, industrially, and technologically). As the final divide, we put forth our perspective on the ""Quantum Divide within Societies"", one of the most explored ones in the literature, addressing societal implications. For each type of the divide, we propose several directions to navigate them, some complementary, some incompatible. Finally, we discuss the interconnectedness and distinctness of different types of divides and how they impact the directions to navigate them. This study serves as a guidance for those interested in a more in-depth investigation of the concept of quantum divide, possible directions of navigating the divides, and how the introduction of QT might affect the innovation ecosystems by impacting the scientific, technological, international, and societal institutions."
https://arxiv.org/abs/2403.08032,2024-03-12,LG-Traj: LLM Guided Pedestrian Trajectory Prediction,"['Pranav Singh Chib', 'Pravendra Singh']","Accurate pedestrian trajectory prediction is crucial for various applications, and it requires a deep understanding of pedestrian motion patterns in dynamic environments. However, existing pedestrian trajectory prediction methods still need more exploration to fully leverage these motion patterns. This paper investigates the possibilities of using Large Language Models (LLMs) to improve pedestrian trajectory prediction tasks by inducing motion cues. We introduce LG-Traj, a novel approach incorporating LLMs to generate motion cues present in pedestrian past/observed trajectories. Our approach also incorporates motion cues present in pedestrian future trajectories by clustering future trajectories of training data using a mixture of Gaussians. These motion cues, along with pedestrian coordinates, facilitate a better understanding of the underlying representation. Furthermore, we utilize singular value decomposition to augment the observed trajectories, incorporating them into the model learning process to further enhance representation learning. Our method employs a transformer-based architecture comprising a motion encoder to model motion patterns and a social decoder to capture social interactions among pedestrians. We demonstrate the effectiveness of our approach on popular pedestrian trajectory prediction benchmarks, namely ETH-UCY and SDD, and present various ablation experiments to validate our approach."
https://arxiv.org/abs/2403.08031,2024-03-12,Score-based mechanisms,"['Eduardo Perez-Richet', 'Vasiliki Skreta']","We propose a mechanism design framework that incorporates both soft information, which can be freely manipulated, and semi-hard information, which entails a cost for falsification. The framework captures various contexts such as school choice, public housing, organ transplant and manipulations of classification algorithms. We first provide a canonical class of mechanisms for these settings. The key idea is to treat the submission of hard information as an observable and payoff-relevant action and the contractible part of the mechanism as a mapping from submitted scores to a distribution over decisions (a score-based decision rule). Each type report triggers a distribution over score submission requests and a distribution over decision rules. We provide conditions under which score-based mechanisms are without loss of generality. In other words, situations under which the agent does not make any type reports and decides without a mediator what score to submit in a score-based decision rule. We proceed to characterize optimal approval mechanisms in the presence of manipulable hard information. In several leading settings optimal mechanisms are score-based (and thus do not rely on soft information) and involve costly screening. The solution methodology we employ is suitable both for concave cost functions and quadratic costs and is applicable to a wide range of contexts in economics and in computer science."
https://arxiv.org/abs/2403.08030,2024-03-12,2-stacks over bisites,['Elena Caviglia'],"We generalize the concept of stack one dimension higher, introducing a notion of 2-stack suitable for a trihomomorphism from a 2-category equipped with a bitopology into the tricategory of bicategories. Moreover, we give a characterization of 2-stacks in terms of explicit conditions, that are easier to use in practice. These explicit conditions are effectiveness conditions for appropriate data of descent on objects, morphisms and 2-cells, generalizing the usual stacky gluing conditions one dimension higher. Furthermore, we prove some new results on bitopologies. The main one is that every object of a subcanonical bisite can be seen as the sigma-bicolimit of each covering bisieve over it. This generalizes one dimension higher a well-know result for subcanonical Grothendieck sites."
https://arxiv.org/abs/2403.08029,2024-03-12,HOLISMOKES -- XII. Time-delay Measurements of Strongly Lensed Type Ia Supernovae using a Long Short-Term Memory Network,"['S. Huber', 'S. H. Suyu']","Strongly lensed Type Ia supernovae (LSNe Ia) are a promising probe to measure the Hubble constant ($H_0$) directly. To use LSNe Ia for cosmography, a time-delay measurement between the multiple images, a lens-mass model, and a mass reconstruction along the line of sight are required. In this work, we present the machine learning network LSTM-FCNN which is a combination of a Long Short-Term Memory Network (LSTM) and a fully-connected neural network (FCNN). The LSTM-FCNN is designed to measure time delays on a sample of LSNe Ia spanning a broad range of properties, which we expect to find with the upcoming Rubin Observatory Legacy Survey of Space and Time (LSST) and for which follow-up observations are planned. With follow-up observations in $i$ band (cadence of one to three days with a single-epoch $5σ$ depth of 24.5 mag), we reach a bias-free delay measurement with a precision around 0.7 days over a large sample of LSNe Ia. The LSTM-FCNN is far more general than previous machine learning approaches such as the Random Forest (RF), where a RF has to be trained for each observational pattern separately, and yet the LSTM-FCNN outperforms the RF by a factor of roughly three. Therefore, the LSTM-FCNN is a very promising approach to achieve robust time delays in LSNe Ia, which is important for a precise and accurate constraint on $H_0$"
https://arxiv.org/abs/2403.08028,2024-03-12,Anomalous Shiba spectrum and superconductivity induced magnetic interactions in materials with topological band inversion,"['Didier Ndengeyintwali', 'Shiva Heidari', 'Cody Youmans', 'Pavan Hosur', 'Pouyan Ghaemi']","We study the Shiba states in materials with bulk band inversion such as iron-based topological superconductors or doped topological insulators. We show that the structure of the Shiba state spectrum depends on the doping level relative to the chemical potential at which the band-inversion occurs. Moreover, we demonstrate that the transition from ferro-magnetic to antiferromagnetic coupling and vice versa, which is caused by the coupling of magnetic impurities through the overlap of Shiba states, is highly dependent on the doping level. Additionally, topological edge states may have a substantial impact on the Shiba states, leading to a decrease in Shiba state energies and the creation of new states when the magnetic impurity approaches the boundary."
https://arxiv.org/abs/2403.08027,2024-03-12,McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets,"['Braulio V. Sánchez Vinces', 'Robson L. F. Cordeiro', 'Christos Faloutsos']","How could we have an outlier detector that works even with nondimensional data, and ranks together both singleton microclusters ('one-off' outliers) and nonsingleton microclusters by their anomaly scores? How to obtain scores that are principled in one scalable and 'hands-off' manner? Microclusters of outliers indicate coalition or repetition in fraud activities, etc.; their identification is thus highly desirable. This paper presents McCatch: a new algorithm that detects microclusters by leveraging our proposed 'Oracle' plot (1NN Distance versus Group 1NN Distance). We study 31 real and synthetic datasets with up to 1M data elements to show that McCatch is the only method that answers both of the questions above; and, it outperforms 11 other methods, especially when the data has nonsingleton microclusters or is nondimensional. We also showcase McCatch's ability to detect meaningful microclusters in graphs, fingerprints, logs of network connections, text data, and satellite imagery. For example, it found a 30-elements microcluster of confirmed 'Denial of Service' attacks in the network logs, taking only ~3 minutes for 222K data elements on a stock desktop."
https://arxiv.org/abs/2403.08026,2024-03-12,Bending Mechanics of Biomimetic Scale Plates,"['Pranta Rahman Sarkar', 'Hossein Ebrahimi', 'Md Shahjahan Hossain', 'Ranajay Ghosh']","We develop the fundamentals of nonlinear and anisotropic bending behavior of biomimetic scale plates using a combination of analytical modeling, finite element (FE) computations, and motivational experiments. The analytical architecture-property relationships are derived for both synclastic and anticlastic curvatures. The results show that, as the scales engage, both synclastic and anticlastic deformations show non-linear scale contact kinematics and cross-curvature sensitivity of moments resulting in strong curvature-dependent elastic nonlinearity and emergent anisotropy. The anisotropy of bending rigidities and their evolution with curvature are affected by both the direction and magnitude of bending as well as scale geometry parameters, and their distribution on the substrate. Like earlier beam-like substrates, kinematic locked states were found to occur; however, their existence and evolution are also strongly determined by scale geometry and imposed cross-curvatures. This validated model helps us to quantify bending response, locking behavior, and their geometric dependence, paving the way for a deeper understanding of the nature of nonlinearity and anisotropy of these systems."
https://arxiv.org/abs/2403.08025,2024-03-12,Stability analysis of multiple solutions of three wave interaction with group velocity dispersion and wave number mismatch,"['Niladri Ghosh', 'Amiya Das', 'Debraj Nath']","This paper explores the analytical approach for obtaining the multiple solutions of three-wave interacting system in (1+1) dimensions. We present a novel approach by expressing the wave solutions in terms of Jacobi elliptic functions and delve into specific cases involving hyperbolic functions. Additionally, the paper focuses on analysing the linear stability of two kinds of solutions: (a) periodic and (b) one or two-hump bright solitons due to group velocity and group velocity dispersion. For linear stability, we solve the eigenvalue problem by Fourier collocation method, where Fourier coefficients are defined analytically and compared numerically. On the other hand, we check the linear stability by direct numerical simulations with Pseudospectral method along special derivatives $(t)$ and 4th order Runge-Kutta method in temporal direction $(z)$. Then it is confirmed by Crank-Nicholson finite difference method. Furthermore, we introduce a special case known as constant magnitude wave solution and examine its modulational instability in presence of group velocity dispersion. In addition, the influence of group velocities and wave vector mismatch are investigated."
https://arxiv.org/abs/2403.08024,2024-03-12,xMLP: Revolutionizing Private Inference with Exclusive Square Activation,"['Jiajie Li', 'Jinjun Xiong']","Private Inference (PI) enables deep neural networks (DNNs) to work on private data without leaking sensitive information by exploiting cryptographic primitives such as multi-party computation (MPC) and homomorphic encryption (HE). However, the use of non-linear activations such as ReLU in DNNs can lead to impractically high PI latency in existing PI systems, as ReLU requires the use of costly MPC computations, such as Garbled Circuits. Since square activations can be processed by Beaver's triples hundreds of times faster compared to ReLU, they are more friendly to PI tasks, but using them leads to a notable drop in model accuracy. This paper starts by exploring the reason for such an accuracy drop after using square activations, and concludes that this is due to an ""information compounding"" effect. Leveraging this insight, we propose xMLP, a novel DNN architecture that uses square activations exclusively while maintaining parity in both accuracy and efficiency with ReLU-based DNNs. Our experiments on CIFAR-100 and ImageNet show that xMLP models consistently achieve better performance than ResNet models with fewer activation layers and parameters while maintaining consistent performance with its ReLU-based variants. Remarkably, when compared to state-of-the-art PI Models, xMLP demonstrates superior performance, achieving a 0.58% increase in accuracy with 7x faster PI speed. Moreover, it delivers a significant accuracy improvement of 4.96% while maintaining the same PI latency. When offloading PI to the GPU, xMLP is up to 700x faster than the previous state-of-the-art PI model with comparable accuracy."
https://arxiv.org/abs/2403.08023,2024-03-12,51% Attack via Difficulty Increase with a Small Quantum Miner,"['Bolton Bailey', 'Or Sattath']","We present a strategy for a single quantum miner with relatively low hashing power, with the same ramifications as a 51% attack. Bitcoin nodes consider the chain with the highest cumulative proof-of-work to be the valid chain. A quantum miner can manipulate the block timestamps to multiply the difficulty by $c$. The fork-choice rule counts every block with increased difficulty with weight $c$. By using Grover's algorithm, it is only $O(\sqrt c)$ harder for the quantum miner to mine such blocks. By picking a high enough $c$, the single quantum miner can create a competing chain with fewer blocks, but more cumulative proof-of-work. The time required is $O(\frac{1}{r^2})$ epochs, where $r$ is the fraction of the block rewards that the quantum miner would have received if they mined honestly."
https://arxiv.org/abs/2403.08022,2024-03-12,An asymptotic Grad-Shafranov equation for quasisymmetric stellarators,"['Nikita Nikulsin', 'Wrick Sengupta', 'Rogerio Jorge', 'Amitava Bhattacharjee']","A first-order model is derived for quasisymmetric stellarators where the vacuum field due to coils is dominant, but plasma-current-induced terms are not negligible and can contribute to magnetic differential equations, with $β$ of the order of the ratio of induced to vacuum fields. Under these assumptions, it is proven that the aspect ratio must be large and a simple expression can be obtained for the lowest-order vacuum field. The first-order correction, which involves both vacuum and current-driven fields, is governed by a Grad-Shafranov equation and the requirement that flux surfaces exist. These two equations are not always consistent, and so this model is generally overconstrained, but special solutions exist that satisfy both equations simultaneously. One family of such solutions are the first-order near-axis solutions. Thus, the first-order near-axis model is a subset of the model presented here. Several other solutions outside the scope of the near-axis model are also found. A case study comparing one such solution to a VMEC-generated solution shows good agreement."
https://arxiv.org/abs/2403.08021,2024-03-12,Top anomalous chromomagnetic dipole moment in the Bestest Little Higgs Model,"['T. Cisneros-Pérez', 'M. A. Hernández-Ruíz', 'A. Ramirez-Morales', 'A. Gutiérrez-Rodríguez', 'J. Montaño-Domínguez']","We investigate the anomalous Chromomagnetic Dipole Moment (CMDM), $\hatμ^{BLHM}_t$ of the top quark in the Bestest Little Higgs Model (BLHM). We include new interactions with the involvement of the extended CKM matrix of the BLHM and we explore most of the allowed parameter space, obtaining multiple CMDM in the range of $10^{-4}-10^{-2}$. We consider experimental and model parameter uncertainties to integrate them into all our calculations using a Monte Carlo method. This enables us to determine the extent to which deviations arising from experimental errors can be accommodated within the statistical errors of the model and which relate to the physics framework of the BLHM, guiding future theory, phenomenological, and experimental research."
https://arxiv.org/abs/2403.08020,2024-03-12,"Epidemiology, Trajectories and Outcomes of Acute Kidney Injury Among Hospitalized Patients: A Retrospective Multicenter Large Cohort Study","['Esra Adiyeke', 'Yuanfang Ren', 'Shmuel Fogel', 'Parisa Rashidi', 'Mark Segal', 'Elizabeth A. Shenkman', 'Azra Bihorac', 'Tezcan Ozrazgat-Baslanti']","Background: Acute kidney injury (AKI) is a clinical syndrome affecting almost one fifth of hospitalized patients, as well as more than half of the patients who are admitted to the intensive care unit (ICU). Stratifying AKI patients into groups based on severity and duration would facilitate more targeted efforts for treating AKI. Methods: In a retrospective, multicenter and longitudinal cohort study of 935,679 patients who were admitted between 2012 and 2020 to health centers included in OneFlorida+ Network, we analyzed the impact of AKI trajectories which are rapidly reversed AKI, persistent AKI with renal recovery, and persistent AKI without renal recovery on patients' clinical outcomes, including hospital, 30-day, 1-year, and 3-year mortality, kidney replacement therapy, new chronic kidney disease (CKD) within 90 days or 1-year of discharge, CKD progression within 1-year of discharge, resource utilization, hospital disposition, and major complications during hospitalization. As analytical approaches, Kaplan-Meier estimators and survival curves, Cox proportional-hazards regression model, logistic regression model, Kruskal-Wallis test, analysis of variance, chi-square, Fisher's exact test were used. Results: Among 2,187,254 encounters, 14% had AKI, of which 63%, 21%, and 16% had Stage 1, 2, and 3, respectively, as the worst AKI stage. Fraction of patients with persistent AKI was 31%. Patients with AKI had worse clinical outcomes and increased resource utilization compared to patients without the condition. One-year mortality was 5 times greater for patients with persistent AKI compared to those without AKI. Conclusions: Persistent AKI was associated with prolonged hospitalization, increased ICU admission and mortality compared to the other groups. This may emphasize the critical need for devising strategies targeting effective management of AKI and prevention of persisting AKI."
https://arxiv.org/abs/2403.08019,2024-03-12,MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation,"['Yuelong Li', 'Yafei Mao', 'Raja Bala', 'Sunil Hadap']","We propose a single-shot approach to determining 6-DoF pose of an object with available 3D computer-aided design (CAD) model from a single RGB image. Our method, dubbed MRC-Net, comprises two stages. The first performs pose classification and renders the 3D object in the classified pose. The second stage performs regression to predict fine-grained residual pose within class. Connecting the two stages is a novel multi-scale residual correlation (MRC) layer that captures high-and-low level correspondences between the input image and rendering from first stage. MRC-Net employs a Siamese network with shared weights between both stages to learn embeddings for input and rendered images. To mitigate ambiguity when predicting discrete pose class labels on symmetric objects, we use soft probabilistic labels to define pose class in the first stage. We demonstrate state-of-the-art accuracy, outperforming all competing RGB-based methods on four challenging BOP benchmark datasets: T-LESS, LM-O, YCB-V, and ITODD. Our method is non-iterative and requires no complex post-processing."
https://arxiv.org/abs/2403.08018,2024-03-12,Learning Data Association for Multi-Object Tracking using Only Coordinates,"['Mehdi Miah', 'Guillaume-Alexandre Bilodeau', 'Nicolas Saunier']","We propose a novel Transformer-based module to address the data association problem for multi-object tracking. From detections obtained by a pretrained detector, this module uses only coordinates from bounding boxes to estimate an affinity score between pairs of tracks extracted from two distinct temporal windows. This module, named TWiX, is trained on sets of tracks with the objective of discriminating pairs of tracks coming from the same object from those which are not. Our module does not use the intersection over union measure, nor does it requires any motion priors or any camera motion compensation technique. By inserting TWiX within an online cascade matching pipeline, our tracker C-TWiX achieves state-of-the-art performance on the DanceTrack and KITTIMOT datasets, and gets competitive results on the MOT17 dataset. The code will be made available upon publication."
https://arxiv.org/abs/2403.08017,2024-03-12,Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI,"['Vladimir Zaigrajew', 'Hubert Baniecki', 'Lukasz Tulczyjew', 'Agata M. Wijata', 'Jakub Nalepa', 'Nicolas Longépé', 'Przemyslaw Biecek']","Remote sensing (RS) applications in the space domain demand machine learning (ML) models that are reliable, robust, and quality-assured, making red teaming a vital approach for identifying and exposing potential flaws and biases. Since both fields advance independently, there is a notable gap in integrating red teaming strategies into RS. This paper introduces a methodology for examining ML models operating on hyperspectral images within the HYPERVIEW challenge, focusing on soil parameters' estimation. We use post-hoc explanation methods from the Explainable AI (XAI) domain to critically assess the best performing model that won the HYPERVIEW challenge and served as an inspiration for the model deployed on board the INTUITION-1 hyperspectral mission. Our approach effectively red teams the model by pinpointing and validating key shortcomings, constructing a model that achieves comparable performance using just 1% of the input features and a mere up to 5% performance loss. Additionally, we propose a novel way of visualizing explanations that integrate domain-specific information about hyperspectral bands (wavelengths) and data transformations to better suit interpreting models for hyperspectral image analysis."
https://arxiv.org/abs/2403.08016,2024-03-12,Aedes aegypti Egg Counting with Neural Networks for Object Detection,"['Micheli Nayara de Oliveira Vicente', 'Gabriel Toshio Hirokawa Higa', 'João Vitor de Andrade Porto', 'Higor Henrique', 'Picoli Nucci', 'Asser Botelho Santana', 'Karla Rejane de Andrade Porto', 'Antonia Railda Roel', 'Hemerson Pistori']","Aedes aegypti is still one of the main concerns when it comes to disease vectors. Among the many ways to deal with it, there are important protocols that make use of egg numbers in ovitraps to calculate indices, such as the LIRAa and the Breteau Index, which can provide information on predictable outbursts and epidemics. Also, there are many research lines that require egg numbers, specially when mass production of mosquitoes is needed. Egg counting is a laborious and error-prone task that can be automated via computer vision-based techniques, specially deep learning-based counting with object detection. In this work, we propose a new dataset comprising field and laboratory eggs, along with test results of three neural networks applied to the task: Faster R-CNN, Side-Aware Boundary Localization and FoveaBox."
https://arxiv.org/abs/2403.08015,2024-03-12,Eigenvalues of Product of Ginibre Ensembles and Their Inverses and that of Truncated Haar Unitary Matrices and Their Inverses,"['Shuhua Chang', 'Tiefeng Jiang', 'Yongcheng Qi']","Consider two types of products of independent random matrices, including products of Ginibre matrices and inverse Ginibre matrices and products of truncated Haar unitary matrices and inverse truncated Haar matrices. Each product matrix has $m$ multiplicands of $n$ by $n$ square matrices, and the empirical distribution based on the $n$ eigenvalues of the product matrix is called empirical spectral distribution of the matrix. In this paper, we investigate the limiting empirical spectral distribution of the product matrices when $n$ tends to infinity and $m$ changes with $n$. For properly rescaled eigenvalues for two types of the product matrices, we obtain the necessary and sufficient conditions for the convergence of the empirical spectral distributions."
https://arxiv.org/abs/2403.08014,2024-03-12,The intermittently-resonant coevolution of migrating planets and their pulsating stars,"['Jared Bryan', 'Julien de Wit', 'Meng Sun', 'Zoe L. de Beurs', 'Richard H. D. Townsend']","Hot Jupiters are expected to form far from their host star and move toward close-in, circular orbits via a smooth, monotonic decay due to mild and constant tidal dissipation. Yet, three systems have recently been found exhibiting planet-induced stellar pulsations suggesting unexpectedly strong tidal interactions. Here we combine stellar evolution and tide models to show that dynamical tides raised by eccentric gas giants can give rise to chains of resonance locks with multiple modes, enriching the dynamics seen in single-mode resonance locking of circularized systems. These series of resonance locks yield orders-of-magnitude larger changes in eccentricity and harmonic pulsations relative to those expected from a single episode of resonance locking or nonresonant tidal interactions. Resonances become more frequent as a star evolves off the main sequence providing an alternative explanation to the origin of some stellar pulsators and yielding the concept of ""dormant migrating giants"". Evolution trajectories are characterized by competing episodes of inward/outward migration and spin-up/-down of the star which are sensitive to the system parameters, revealing a new challenge in modeling migration paths and in contextualizing the observed populations of giant exoplanets and stellar binaries. This sensitivity however offers a new window to constrain the stellar properties of planetary hosts via tidal asteroseismology."
https://arxiv.org/abs/2403.08013,2024-03-12,Supervised Time Series Classification for Anomaly Detection in Subsea Engineering,"['Ergys Çokaj', 'Halvor Snersrud Gustad', 'Andrea Leone', 'Per Thomas Moe', 'Lasse Moldestad']","Time series classification is of significant importance in monitoring structural systems. In this work, we investigate the use of supervised machine learning classification algorithms on simulated data based on a physical system with two states: Intact and Broken. We provide a comprehensive discussion of the preprocessing of temporal data, using measures of statistical dispersion and dimension reduction techniques. We present an intuitive baseline method and discuss its efficiency. We conclude with a comparison of the various methods based on different performance metrics, showing the advantage of using machine learning techniques as a tool in decision making."
https://arxiv.org/abs/2403.08012,2024-03-12,doped: Python toolkit for robust and repeatable charged defect supercell calculations,"['Seán R. Kavanagh', 'Alexander G. Squires', 'Adair Nicolson', 'Irea Mosquera-Lois', 'Alex M. Ganose', 'Bonan Zhu', 'Katarina Brlec', 'Aron Walsh', 'David O. Scanlon']","Defects are a universal feature of crystalline solids, dictating the key properties and performance of many functional materials. Given their crucial importance yet inherent difficulty in measuring experimentally, computational methods (such as DFT and ML/classical force-fields) are widely used to predict defect behaviour at the atomic level and the resultant impact on macroscopic properties. Here we report `doped`, a Python package for the generation, pre-/post-processing and analysis of defect supercell calculations. `doped` has been built to implement the defect simulation workflow in an efficient, user-friendly yet powerful and fully-flexible manner, with the goal of providing a robust general-purpose platform for conducting reproducible calculations of solid-state defect properties."
https://arxiv.org/abs/2403.08011,2024-03-12,Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language,"['Yash Sharma', 'Basil Abraham', 'Preethi Jyothi']","An important and difficult task in code-switched speech recognition is to recognize the language, as lots of words in two languages can sound similar, especially in some accents. We focus on improving performance of end-to-end Automatic Speech Recognition models by conditioning transformer layers on language ID of words and character in the output in an per layer supervised manner. To this end, we propose two methods of introducing language specific parameters and explainability in the multi-head attention mechanism, and implement a Temporal Loss that helps maintain continuity in input alignment. Despite being unable to reduce WER significantly, our method shows promise in predicting the correct language from just spoken data. We introduce regularization in the language prediction by dropping LID in the sequence, which helps align long repeated output sequences."
https://arxiv.org/abs/2403.08010,2024-03-12,Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM,"['Jingcong Liang', 'Rong Ye', 'Meng Han', 'Ruofei Lai', 'Xinyu Zhang', 'Xuanjing Huang', 'Zhongyu Wei']","How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments. At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate. In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration. To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system's performance to actual debate outcomes. The findings indicate a notable enhancement over directly using LLMs for debate evaluation. Source code and benchmark data are available online at https://github.com/ljcleo/Debatrix ."
https://arxiv.org/abs/2403.08009,2024-03-12,An Improved Framework for Computing Waveforms,"['Giacomo Brunello', 'Stefano De Angelis']","We combine the observable-based formalism (KMOC), the analytic properties of the scattering amplitude, generalised unitarity and the heavy-mass expansion with a newly introduced IBP reduction for Fourier integrals, to provide an efficient framework for computing scattering waveforms. We apply this framework to the scattering of two charged massive bodies in classical electrodynamics. Our work paves the way for the computation of the analytic one-loop waveform in General Relativity."
https://arxiv.org/abs/2403.08008,2024-03-12,Distribution and Properties of Molecular Gas Toward the Monoceros OB1 Region,"['Zi Zhuang', 'Yang Su', 'Shiyu Zhang', 'Xuepeng Chen', 'Qing-Zeng Yan', 'Haoran Feng', 'Li Sun', 'Xiaoyun Xu', 'Yan Sun', 'Xin Zhou', 'Hongchi Wang', 'Ji Yang']","We perform a comprehensive CO study toward the Monoceros OB1 (Mon OB1) region based on the MWISP survey at an angular resolution of about $50''$. The high-sensitivity data, together with the high dynamic range, shows that molecular gas in the $\rm 8^{\circ}\times4^{\circ}$ region displays complicated hierarchical structures and various morphology (e.g., filamentary, cavity-like, shell-like, and other irregular structures). Based on Gaussian decomposition and clustering for $\mathrm{^{13}CO}$ data, a total of 263 $\mathrm{^{13}CO}$ structures are identified in the whole region, and 88\% of raw data flux is recovered. The dense gas with relatively high column density from the integrated CO emission is mainly concentrated in the region where multiple $\rm ^{13}CO$ structures are overlapped. Combining the results of 32 large $\mathrm{^{13}CO}$ structures with distances from Gaia DR3, we estimate an average distance of $\rm 729^{+45}_{-45}~pc$ for the GMC complex. The total mass of the GMC Complex traced by $\mathrm{^{12}CO}$, $\mathrm{^{13}CO}$, and $\mathrm{C^{18}O}$ are $1.1\times10^5~M_\odot$, $4.3\times10^4~M_\odot$, and $8.4\times10^3~M_\odot$, respectively. The dense gas fraction shows a clear difference between Mon OB1 GMC East (12.4\%) and Mon OB1 GMC West (3.3\%). Our results show that the dense gas environment is closely linked to the nearby star-forming regions. On the other hand, star-forming activities have a great influence on the physical properties of the surrounding molecular gas (e.g., greater velocity dispersion, higher temperatures, and more complex velocity structures, etc.). We also discuss the distribution/kinematics of molecular gas associated with nearby star-forming activities."
https://arxiv.org/abs/2403.08007,2024-03-12,IndicSTR12: A Dataset for Indic Scene Text Recognition,"['Harsh Lunia', 'Ajoy Mondal', 'C V Jawahar']","The importance of Scene Text Recognition (STR) in today's increasingly digital world cannot be overstated. Given the significance of STR, data intensive deep learning approaches that auto-learn feature mappings have primarily driven the development of STR solutions. Several benchmark datasets and substantial work on deep learning models are available for Latin languages to meet this need. On more complex, syntactically and semantically, Indian languages spoken and read by 1.3 billion people, there is less work and datasets available. This paper aims to address the Indian space's lack of a comprehensive dataset by proposing the largest and most comprehensive real dataset - IndicSTR12 - and benchmarking STR performance on 12 major Indian languages. A few works have addressed the same issue, but to the best of our knowledge, they focused on a small number of Indian languages. The size and complexity of the proposed dataset are comparable to those of existing Latin contemporaries, while its multilingualism will catalyse the development of robust text detection and recognition models. It was created specifically for a group of related languages with different scripts. The dataset contains over 27000 word-images gathered from various natural scenes, with over 1000 word-images for each language. Unlike previous datasets, the images cover a broader range of realistic conditions, including blur, illumination changes, occlusion, non-iconic texts, low resolution, perspective text etc. Along with the new dataset, we provide a high-performing baseline on three models - PARSeq, CRNN, and STARNet."
https://arxiv.org/abs/2403.08006,2024-03-12,Quantum tunneling of the magnetization in systems with anisotropic 4f ion pairs: Rates from low temperature zero field relaxation,['Thomas Greber'],"Anisotropic open shell 4f ions have magnetic moments that can be read and written as atomic bits. If it comes to qbits where the phase of the wave function has to be written, controlled and read, it is of advantage to rely on more than one atom that carries the quantum information of the system because states with different susceptibilities may be addressed. Such systems are realized for pairs of lanthanides in single molecule magnets, where four pseudospin states are found and mixed in quantum tunneling processes. For the case of endohedral fullerenes like Dy2S@C82 or Tb2ScN@C80 the quantum tunneling of the magnetisation is imprinted in the magnetisation lifetimes at sub-Kelvin temperatures. A Hamiltonian that includes quantum tunneling of the magnetisation predicts the lifting of the zero field ground state degeneracy and non-linear coupling to magnetic fields in such systems."
https://arxiv.org/abs/2403.08005,2024-03-12,Stable free boundary minimal hypersurfaces in locally wedge-shaped manifolds,['Zetian Yan'],"We prove that a stable $C^{1,1}$-to-edge properly embedded free boundary minimal hypersurface $Σ^3$ of a $4$-dimensional wedge domain $Ω^4_θ$ with angle $θ\in (0,π]$ is flat."
https://arxiv.org/abs/2403.08004,2024-03-12,Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing,"['Rodrigo Santos', 'João Silva', 'António Branco']","The combination of language processing and image processing keeps attracting increased interest given recent impressive advances that leverage the combined strengths of both domains of research. Among these advances, the task of editing an image on the basis solely of a natural language instruction stands out as a most challenging endeavour. While recent approaches for this task resort, in one way or other, to some form of preliminary preparation, training or fine-tuning, this paper explores a novel approach: We propose a preparation-free method that permits instruction-guided image editing on the fly. This approach is organized along three steps properly orchestrated that resort to image captioning and DDIM inversion, followed by obtaining the edit direction embedding, followed by image editing proper. While dispensing with preliminary preparation, our approach demonstrates to be effective and competitive, outperforming recent, state of the art models for this task when evaluated on the MAGICBRUSH dataset."
https://arxiv.org/abs/2403.08003,2024-03-12,Real-time Surgical Instrument Segmentation in Video Using Point Tracking and Segment Anything,"['Zijian Wu', 'Adam Schmidt', 'Peter Kazanzides', 'Septimiu E. Salcudean']","The Segment Anything Model (SAM) is a powerful vision foundation model that is revolutionizing the traditional paradigm of segmentation. Despite this, a reliance on prompting each frame and large computational cost limit its usage in robotically assisted surgery. Applications, such as augmented reality guidance, require little user intervention along with efficient inference to be usable clinically. In this study, we address these limitations by adopting lightweight SAM variants to meet the speed requirement and employing fine-tuning techniques to enhance their generalization in surgical scenes. Recent advancements in Tracking Any Point (TAP) have shown promising results in both accuracy and efficiency, particularly when points are occluded or leave the field of view. Inspired by this progress, we present a novel framework that combines an online point tracker with a lightweight SAM model that is fine-tuned for surgical instrument segmentation. Sparse points within the region of interest are tracked and used to prompt SAM throughout the video sequence, providing temporal consistency. The quantitative results surpass the state-of-the-art semi-supervised video object segmentation method on the EndoVis 2015 dataset, with an over 25 FPS inference speed running on a single GeForce RTX 4060 GPU."
https://arxiv.org/abs/2403.08002,2024-03-12,Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging,"['Juan Manuel Zambrano Chaves', 'Shih-Cheng Huang', 'Yanbo Xu', 'Hanwen Xu', 'Naoto Usuyama', 'Sheng Zhang', 'Fei Wang', 'Yujia Xie', 'Mahmoud Khademi', 'Ziyi Yang', 'Hany Awadalla', 'Julia Gong', 'Houdong Hu', 'Jianwei Yang', 'Chunyuan Li', 'Jianfeng Gao', 'Yu Gu', 'Cliff Wong', 'Mu Wei', 'Tristan Naumann', 'Muhao Chen', 'Matthew P. Lungren', 'Serena Yeung-Levy', 'Curtis P. Langlotz', 'Sheng Wang']","The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space. We conduct a comprehensive study of this approach on radiology imaging. For training, we assemble a large dataset with over 1 million image-text pairs. For evaluation, we propose a clinically driven novel approach using GPT-4 and demonstrate its parity with expert evaluation. We also study grounding qualitatively using attention. For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training. The resulting LLaVA-Rad (7B) model attains state-of-the-art results on radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications."
https://arxiv.org/abs/2403.08001,2024-03-12,Existence and uniqueness of weak solutions for the generalized stochastic Navier-Stokes-Voigt equations,"['Ankit Kumar', 'Hermenegildo Borges de Oliveira', 'Manil T. Mohan']","In this work, we consider the incompressible generalized Navier-Stokes-Voigt equations in a bounded domain $\mathcal{O}\subset\mathbb{R}^d$, $d\geq 2$, driven by a multiplicative Gaussian noise. The considered momentum equation is given by:"
https://arxiv.org/abs/2403.08000,2024-03-12,Overlapping community detection algorithms using Modularity and the cosine,"['Do Duy Hieu', 'Phan Thi Ha Duong']","The issue of network community detection has been extensively studied across many fields. Most community detection methods assume that nodes belong to only one community. However, in many cases, nodes can belong to multiple communities simultaneously.This paper presents two overlapping network community detection algorithms that build on the two-step approach, using the extended modularity and cosine function. The applicability of our algorithms extends to both undirected and directed graph structures. To demonstrate the feasibility and effectiveness of these algorithms, we conducted experiments using real data."
https://arxiv.org/abs/2403.07999,2024-03-12,Nodal d-wave pairing from spin fluctuations in a thermally disordered anti-ferromagnet,['Nick Bultinck'],"We consider electron pairing in a two-dimensional thermally disordered itinerant anti-ferromagnet. It is shown that transverse spin fluctuations in such a state can give rise to superconductivity with a sizeable critical temperature $T_c$. Below $T_c$ there is quasi-long-range spin-singlet and $d_{x^2-y^2}$ superconducting order, together with fluctuating triplet order at momentum $(π,π)$. The singlet pairs we find are tightly bound together, and the pair wavefunction has a purely inter-sublattice structure due to the U(1) spin rotation symmetry of the anti-ferromagnet."
https://arxiv.org/abs/2403.07998,2024-03-12,Pairs Trading Using a Novel Graphical Matching Approach,"['Khizar Qureshi', 'Tauhid Zaman']","Pairs trading, a strategy that capitalizes on price movements of asset pairs driven by similar factors, has gained significant popularity among traders. Common practice involves selecting highly cointegrated pairs to form a portfolio, which often leads to the inclusion of multiple pairs sharing common assets. This approach, while intuitive, inadvertently elevates portfolio variance and diminishes risk-adjusted returns by concentrating on a small number of highly cointegrated assets. Our study introduces an innovative pair selection method employing graphical matchings designed to tackle this challenge. We model all assets and their cointegration levels with a weighted graph, where edges signify pairs and their weights indicate the extent of cointegration. A portfolio of pairs is a subgraph of this graph. We construct a portfolio which is a maximum weighted matching of this graph to select pairs which have strong cointegration while simultaneously ensuring that there are no shared assets within any pair of pairs. This approach ensures each asset is included in just one pair, leading to a significantly lower variance in the matching-based portfolio compared to a baseline approach that selects pairs purely based on cointegration. Theoretical analysis and empirical testing using data from the S\&P 500 between 2017 and 2023, affirm the efficacy of our method. Notably, our matching-based strategy showcases a marked improvement in risk-adjusted performance, evidenced by a gross Sharpe ratio of 1.23, a significant enhancement over the baseline value of 0.48 and market value of 0.59. Additionally, our approach demonstrates reduced trading costs attributable to lower turnover, alongside minimized single asset risk due to a more diversified asset base."
https://arxiv.org/abs/2403.07997,2024-03-12,Fast-Forward Reality: Authoring Error-Free Context-Aware Policies with Real-Time Unit Tests in Extended Reality,"['Xun Qian', 'Tianyi Wang', 'Xuhai Xu', 'Tanya R Jonker', 'Kashyap Todi']","Advances in ubiquitous computing have enabled end-user authoring of context-aware policies (CAPs) that control smart devices based on specific contexts of the user and environment. However, authoring CAPs accurately and avoiding run-time errors is challenging for end-users as it is difficult to foresee CAP behaviors under complex real-world conditions. We propose Fast-Forward Reality, an Extended Reality (XR) based authoring workflow that enables end-users to iteratively author and refine CAPs by validating their behaviors via simulated unit test cases. We develop a computational approach to automatically generate test cases based on the authored CAP and the user's context history. Our system delivers each test case with immersive visualizations in XR, facilitating users to verify the CAP behavior and identify necessary refinements. We evaluated Fast-Forward Reality in a user study (N=12). Our authoring and validation process improved the accuracy of CAPs and the users provided positive feedback on the system usability."
https://arxiv.org/abs/2403.07996,2024-03-12,TriOS Schwarzschild Orbit Modeling: Robustness of Parameter Inference for Masses and Shapes of Triaxial Galaxies with Supermassive Black Holes,"['Jacob Pilawa', 'Emily R. Liepold', 'Chung-Pei Ma']","Evidence for the majority of the supermassive black holes in the local universe has been obtained dynamically from stellar motions with the Schwarzschild orbit superposition method. However, there have been only a handful of studies using simulated data to examine the ability of this method to reliably recover known input black hole masses $M_{BH}$ and other galaxy parameters. Here we conduct a comprehensive assessment of the reliability of the triaxial Schwarzschild method at $\textit{simultaneously}$ determining $M_{BH}$, stellar mass-to-light ratio $M^{*}/L$, dark matter mass, and three intrinsic triaxial shape parameters of simulated galaxies. For each of 25 rounds of mock observations using simulated stellar kinematics and the $\texttt{TriOS}$ code, we derive best-fitting parameters and confidence intervals after a full search in the 6D parameter space with our likelihood-based model inference scheme. The two key mass parameters, $M_{BH}$ and $M^{*}/L$, are recovered within the 68% confidence interval, and other parameters are recovered between 68% and 95% confidence intervals. The spatially varying velocity anisotropy of the stellar orbits is also well recovered. We explore whether the goodness-of-fit measure used for galaxy model selection in our pipeline is biased by variable complexity across the 6D parameter space. In our tests, adding a penalty term to the likelihood measure either makes little difference, or worsens the recovery in some cases."
https://arxiv.org/abs/2403.07995,2024-03-12,"Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic Music Generation","['Keshav Bhandari', 'Simon Colton']","Modelling musical structure is vital yet challenging for artificial intelligence systems that generate symbolic music compositions. This literature review dissects the evolution of techniques for incorporating coherent structure, from symbolic approaches to foundational and transformative deep learning methods that harness the power of computation and data across a wide variety of training paradigms. In the later stages, we review an emerging technique which we refer to as ""sub-task decomposition"" that involves decomposing music generation into separate high-level structural planning and content creation stages. Such systems incorporate some form of musical knowledge or neuro-symbolic methods by extracting melodic skeletons or structural templates to guide the generation. Progress is evident in capturing motifs and repetitions across all three eras reviewed, yet modelling the nuanced development of themes across extended compositions in the style of human composers remains difficult. We outline several key future directions to realize the synergistic benefits of combining approaches from all eras examined."
https://arxiv.org/abs/2403.07994,2024-03-12,Challenges in certifying quantum teleportation: moving beyond conventional fidelity benchmark,"['D. G. Bussandri', 'G. M. Bosyk', 'F. Toscano']","The conventional certification method for quantum teleportation protocols relies on surpassing the highest achievable classical average fidelity between target and teleported states. Our investigation highlights the limitations of this approach: inconsistent conclusions can be obtained when it is considered different distance measures in the quantum state space, leading to contradictory interpretations. In particular, this behavior is manifested when modeling a very common noisy experimental scenario, in which the resource state takes the form of a Werner state generated by the influence of a depolarizing channel acting on the Bell state resource. Two additional noise models, based on amplitude-damping channel, are also analyzed. Our work, therefore, stresses the necessity of new certification methods for quantum teleportation."
https://arxiv.org/abs/2403.07993,2024-03-12,Applications of $\mathrm{C}^*$-classification,['Bhishan Jacelon'],"We provide some background on the category of classifiable $\mathrm{C}^*$-algebras, whose objects are infinite-dimensional, simple, separable, unital $\mathrm{C}^*$-algebras that have finite nuclear dimension and satisfy the universal coefficient theorem, and describe some applications of the classification of objects in, and morphisms into, this category."
https://arxiv.org/abs/2403.07992,2024-03-12,Dynamic Field of View Reduction Related to Subjective Sickness Measures in an HMD-based Data Analysis Task,"['Daniel Zielasko', 'Alexander Meißner', 'Sebastian Freitag', 'Benjamin Weyers', 'Torsten W. Kuhlen']","Various factors influence the degree of cybersickness a user can suffer in an immersive virtual environment, some of which can be controlled without adapting the virtual environment itself. When using HMDs, one example is the size of the field of view. However, the degree to which factors like this can be manipulated without affecting the user negatively in other ways is limited. Another prominent characteristic of cybersickness is that it affects individuals very differently. Therefore, to account for both the possible disruptive nature of alleviating factors and the high interpersonal variance, a promising approach may be to intervene only in cases where users experience discomfort symptoms, and only as much as necessary. Thus, we conducted a first experiment, where the field of view was decreased when people feel uncomfortable, to evaluate the possible positive impact on sickness and negative influence on presence. While we found no significant evidence for any of these possible effects, interesting further results and observations were made."
https://arxiv.org/abs/2403.07991,2024-03-12,Dissipative frequency converter: from Lindblad dynamics to non-Hermitian topology,"['Florian Koch', 'Jan Carl Budich']","A topological frequency converter represents a dynamical counterpart of the integer quantum Hall effect, where a two-level system enacts a quantized time-averaged power transfer between two driving modes of incommensurate frequency. Here, we investigate as to what extent temporal coherence in the quantum dynamics of the two-level system is important for the topological quantization of the converter. To this end, we consider dissipative channels corresponding to spontaneous decay and dephasing in the instantaneous eigenbasis of the Hamiltonian as well as spontaneous decay in a fixed basis. The dissipation is modelled using both a full Lindblad and an effective non-Hermitian (NH) Hamiltonian description. For all three dissipation channels we find a transition from the unperturbed dynamics to a quantum watchdog effect, which destroys any power transfer in the strong coupling limit. This is striking because the watchdog effect leads to perfectly adiabatic dynamics in the instantaneous eigenbasis, at first glance similar to the unperturbed case. Furthermore, it is found that dephasing immediately leads to an exponential decay of the power transfer in time due to loss of polarisation in the mixed quantum state. Finally, we discuss the appearance in the effective NH trajectory description of non-adiabatic processes, which are suppressed in the full Lindblad dynamics."
https://arxiv.org/abs/2403.07990,2024-03-12,Latest Evolution of the X-Ray Remnant of SN 1987A: Beyond the Inner Ring,"['Aravind P. Ravi', 'Sangwook Park', 'Svetozar A. Zhekov', 'Salvatore Orlando', 'Marco Miceli', 'Kari A. Frank', 'Patrick S. Broos', 'David N. Burrows']","Based on our Chandra imaging-spectroscopic observations, we present the latest evolution of the X-ray remnant of SN 1987A. Recent changes in the electron temperatures and volume emission measures suggest that the blast wave in SN 1987A is moving out of the dense inner ring structure, also called the equatorial ring (ER). The 0.5-2.0 keV X-ray light curve shows a linearly declining trend (by $\sim$4.5 % yr$^{-1}$) between 2016 and 2020, as the blast wave heats the hitherto unknown circumstellar medium (CSM) outside the ER. While the peak X-ray emission in the latest 0.3-8.0 keV image is still within the ER, the radial expansion rate in the 3.0-8.0 keV images suggests an increasing contribution of the X-ray emission from less dense CSM since 2012, at least partly from beyond the ER. It is remarkable that, since 2020, the declining soft X-ray flux has stabilized around $\sim$7 $\times$ 10$^{-12}$ erg s$^{-1}$ cm$^{-2}$, which may signal a contribution from the reverse-shocked outer layers of ejecta as predicted by the 3-D magneto-hydrodynamic (MHD) models. In the latest ACIS spectrum of supernova remnant (SNR) 1987A in 2022 we report a significant detection of the Fe K line at $\sim$6.7 keV, which may be due to changing thermal conditions of the X-ray emitting CSM and/or the onset of reverse shock interactions with the Fe-ejecta."
https://arxiv.org/abs/2403.07989,2024-03-12,Boolean intervals in the weak Bruhat order of a finite Coxeter group,"['Ben Adenbaum', 'Jennifer Elder', 'Pamela E. Harris', 'J. Carlos Martínez Mori']","Given a Coxeter group $W$ with Coxeter system $(W,S)$, where $S$ is finite. We provide a complete characterization of Boolean intervals in the weak order of $W$ uniformly for all Coxeter groups in terms of independent sets of the Coxeter graph. Moreover, we establish that the number of Boolean intervals of rank $k$ in the weak order of $W$ is ${i_k(Γ_W)\cdot|W|}\,/\,2^{k}$, where $Γ_W$ is the Coxeter graph of $W$ and $i_k(Γ_W)$ is the number of independent sets of size $k$ of $Γ_W$ when $W$ is finite. Specializing to $A_n$, we recover the characterizations and enumerations of Boolean intervals in the weak order of $A_n$ given in arXiv:2306.14734. We provide the analogous results for types $C_n$ and $D_n$, including the related generating functions and additional connections to well-known integer sequences."
https://arxiv.org/abs/2403.07988,2024-03-12,Configuration and EMT Simulation of the 240-bus MiniWECC System Integrating Offshore Wind Farms (OWFs),"['Buxin She', 'Hisham Mahmood', 'Marcelo Elizondo', 'Veronica Adetola', 'Yuqing Dong']","As offshore wind farms (OWFs) become increasingly prevalent in Northern California and Southern Oregon, they introduce faster dynamics into the Western Electricity Coordinating Council (WECC) system, reshaping its dynamic behavior. Accordingly, electromagnetic transient (EMT) simulation is essential to assess high frequency dynamics of the WECC system with integrated OWFs. Against this background, this paper presents the integration of detailed dynamic models of OWFs into a 240-bus miniWECC system in PSCAD software. The sequential initialization technique is employed to facilitate the smooth initiation of a large-scale system in an EMT simulation. The performance of the configured model is assessed under wind speed variations and grounded faults, demonstrating the effectiveness of the miniWECC system with OWFs. This system serves as a valuable basic use case for validating the fast dynamic performance of future WECC systems with high penetration of wind energy."
https://arxiv.org/abs/2403.07987,2024-03-12,Nucleation Rate in a High-Temperature Quantum Field Theory with Hard Particles,['Joonas Hirvonen'],"We study the effects of thermal, hard particles on nucleation rates in high-temperature quantum field theories (QFTs). The hard particles, which form the heat bath for nucleating fields, can be regarded as the most qualitatively distinct aspect of high-temperature QFTs in comparison with Langer's classical nucleation theory: They have their own dynamics and interactions with the nucleating fields, and consequently they do not obey a purely equilibrium distribution during nucleation. We focus on the situation where the hard particles are correctly described by a Vlasov equation, and show that the hard particles can be included into Langer's framework, with only the dynamical part of the nucleation rate changing. The statistical part retains its form and is in accordance with the effective field theory approach to thermal nucleation. It is also shown that the dominant damping from a light bosonic quantum field comes from the corresponding long-wavelength classical field instead of the corresponding bosonic hard particles."
https://arxiv.org/abs/2403.07986,2024-03-12,"EIGER VI. The Correlation Function, Host Halo Mass and Duty Cycle of Luminous Quasars at $z\gtrsim6$","['Anna-Christina Eilers', 'Ruari Mackenzie', 'Elia Pizzati', 'Jorryt Matthee', 'Joseph F. Hennawi', 'Haowen Zhang', 'Rongmon Bordoloi', 'Daichi Kashino', 'Simon J. Lilly', 'Rohan P. Naidu', 'Robert A. Simcoe', 'Minghao Yue', 'Carlos S. Frenk', 'John C. Helly', 'Matthieu Schaller', 'Joop Schaye']","We expect luminous ($M_{1450}\lesssim-26.5$) high-redshift quasars to trace the highest density peaks in our universe, and therefore to reside in proto-clusters encompassing an abundance of galaxies in close vicinity. Here, we present observations of four $z\gtrsim6$ quasar fields using JWST/NIRCam in imaging and widefield slitless spectroscopy mode and report a wide range in the number of detected [OIII]-emitting galaxies in the quasars' environments, ranging between a density enhancement of $δ>100$ within a $2$ cMpc radius - one of the largest proto-clusters during the Epoch of Reionization discovered to date - to a density contrast consistent with zero, indicating the presence of a UV-luminous quasar in a region comparable to the average density of the universe. By measuring the two-point cross-correlation function of quasars and their surrounding galaxies, as well as the galaxy auto-correlation function, we infer a correlation length of quasars at $\langle z\rangle=6.25$ of $r_0^{\rm QQ}=21.3^{+2.7}_{-2.6}~{\rm cMpc}\,h^{-1}$, while we obtain a correlation length of the [OIII]-emitting galaxies of $r_0^{\rm GG}=4.2\pm0.1~{\rm cMpc}\,h^{-1}$. By comparing the correlation functions to dark-matter-only simulations we estimate the minimum mass of the quasars' host dark matter halos to be $\log_{10}(M_{\rm halo, min}/M_\odot)=12.30\pm0.14$ (and $\log_{10}(M_{\rm halo, min}^{\rm [OIII]}/M_\odot) = 10.72\pm0.03$ for the [OIII]-emitters), indicating that (a) luminous quasars do not necessarily reside within the most overdense regions in the early universe, and that (b) the UV-luminous duty cycle of quasar activity at these redshifts is $f_{\rm duty}\ll1$. Such short quasar activity timescales challenge our understanding of early supermassive black hole growth and provide evidence for highly dust-obscured growth phases or episodic, radiatively inefficient accretion rates."
https://arxiv.org/abs/2403.07985,2024-03-12,The white dwarf binary pathways survey -- X. Gaia orbits for known UV excess binaries,"['J. A. Garbutt', 'S. G. Parsons', 'O. Toloza', 'B. T. Gänsicke', 'M. S. Hernandez', 'D. Koester', 'F. Lagos', 'R. Raddi', 'A. Rebassa-Mansergas', 'J. J. Ren', 'M. R. Schreiber', 'M. Zorotovic']","White dwarfs with a F, G or K type companion represent the last common ancestor for a plethora of exotic systems throughout the galaxy, though to this point very few of them have been fully characterised in terms of orbital period and component masses, despite the fact several thousand have been identified. Gaia data release 3 has examined many hundreds of thousands of systems, and as such we can use this, in conjunction with our previous UV excess catalogues, to perform spectral energy distribution fitting in order to obtain a sample of 206 binaries likely to contain a white dwarf, complete with orbital periods, and either a direct measurement of the component masses for astrometric systems, or a lower limit on the component masses for spectroscopic systems. Of this sample of 206, four have previously been observed with Hubble Space Telescope spectroscopically in the ultraviolet, which has confirmed the presence of a white dwarf, and we find excellent agreement between the dynamical and spectroscopic masses of the white dwarfs in these systems. We find that white dwarf plus F, G or K binaries can have a wide range of orbital periods, from less than a day to many hundreds of days. A large number of our systems are likely post-stable mass transfer systems based on their mass/period relationships, while others are difficult to explain either via stable mass transfer or standard common envelope evolution."
https://arxiv.org/abs/2403.07984,2024-03-12,Final state radiation from high and ultrahigh energy neutrino interactions,"['Ryan Plestid', 'Bei Zhou']","Charged leptons produced by high-energy and ultrahigh-energy neutrinos have a substantial probability of emitting prompt internal bremsstrahlung $ν_\ell + N \rightarrow \ell + X + γ$. This can have important consequences for neutrino detection. We discuss observable consequences at high- and ultrahigh-energy neutrino telescopes and LHC's Forward Physics Facility. Logarithmic enhancements can be substantial (e.g.\ $\sim 20\%$) when either the charged lepton's energy, or the rest of the cascade, is measured. We comment on applications involving the inelasticity distribution including measurements of the $ν/\barν$ flux ratio, throughgoing muons, and double-bang signatures for high-energy neutrino observation. Furthermore, for ultrahigh-energy neutrino observation, we find that final state radiation affects flavor measurements and decreases the energy of both Earth-emergent tau leptons and regenerated tau neutrinos. Finally, for LHC's Forward Physics Facility, we find that final state radiation will impact future extractions of strange quark parton distribution functions. Final state radiation should be included in future analyses at neutrino telescopes and the Forward Physics Facility."
https://arxiv.org/abs/2403.07983,2024-03-12,A sub-solar metallicity on the ultra-short period planet HIP 65Ab,"['Luc Bazinet', 'Stefan Pelletier', 'Björn Benneke', 'Ricardo Salinas', 'Gregory N. Mace']","Studying and understanding the physical and chemical processes that govern hot Jupiters gives us insights on the formation of these giant planets. Having a constraint on the molecular composition of their atmosphere can help us pinpoint their evolution timeline. Namely, the metal enrichment and carbon-to-oxygen ratio can give us information about where in the protoplanetary disk a giant planet may have accreted its envelope, and subsequently, indicate if it went through migration. Here we present the first analysis of the atmosphere of the hot Jupiter HIP 65Ab. Using near-infrared high-resolution observations from the IGRINS spectrograph, we detect H$_2$O and CO absorption in the dayside atmosphere of HIP 65Ab. Using a high-resolution retrieval framework, we find a CO abundance of log(CO) = $-3.85^{+0.33}_{-0.36}$, which is slightly under abundant with expectation from solar composition models. We also recover a low water abundance of log(H$_2$O) = $-4.42\pm{0.18}$, depleted by 1 order of magnitude relative to a solar-like composition. Upper limits on the abundance of all other relevant major carbon- and oxygen-bearing molecules are also obtained. Overall, our results are consistent with a sub-stellar metallicity but slightly elevated C/O. Such a composition may indicate that HIP 65Ab accreted its envelope from beyond the water snowline and underwent a disk-free migration to its current location. Alternatively, some of the oxygen on HIP 65Ab could be condensed out of the atmosphere, in which case the observed gas-phase abundances would not reflect the true bulk envelope composition."
https://arxiv.org/abs/2403.07982,2024-03-12,"The XMM Cluster Survey: Automating the estimation of hydrostatic mass for large samples of galaxy clusters I -- Methodology, Validation, & Application to the SDSSRM-XCS sample","['D. J. Turner', 'P. A. Giles', 'A. K. Romer', 'J. Pilling', 'T. K. Lingard', 'R. Wilkinson', 'M. Hilton', 'E. W. Upsdell', 'R. Al-Serkal', 'T. Cheng', 'R. Eappen', 'P. J. Rooney', 'S. Bhargava', 'C. A. Collins', 'J. Mayers', 'C. Miller', 'R. C. Nichol', 'M. Sahén', 'P. T. P. Viana']","We describe features of the X-ray: Generate and Analyse (XGA) open-source software package that have been developed to facilitate automated hydrostatic mass ($M_{\rm hydro}$) measurements from XMM X-ray observations of clusters of galaxies. This includes describing how XGA measures global, and radial, X-ray properties of galaxy clusters. We then demonstrate the reliability of XGA by comparing simple X-ray properties, namely the X-ray temperature and gas mass, with published values presented by the XMM Cluster Survey (XCS), the Ultimate XMM eXtragaLactic survey project (XXL), and the Local Cluster Substructure Survey (LoCuSS). XGA measured values for temperature are, on average, within 1% of the values reported in the literature for each sample. XGA gas masses for XXL clusters are shown to be ${\sim}$10% lower than previous measurements (though the difference is only significant at the $\sim$1.8$σ$ level), LoCuSS $R_{2500}$ and $R_{500}$ gas mass re-measurements are 3% and 7% lower respectively (representing a 1.5$σ$ and 3.5$σ$ difference). Like-for-like comparisons of hydrostatic mass are made to LoCuSS results, which show that our measurements are $10{\pm}3%$ ($19{\pm}7%$) higher for $R_{2500}$ ($R_{500}$). The comparison between $R_{500}$ masses shows significant scatter. Finally, we present new $M_{\rm hydro}$ measurements for 104 clusters from the SDSS DR8 redMaPPer XCS sample (SDSSRM-XCS). Our SDSSRM-XCS hydrostatic mass measurements are in good agreement with multiple literature estimates, and represent one of the largest samples of consistently measured hydrostatic masses. We have demonstrated that XGA is a powerful tool for X-ray analysis of clusters; it will render complex-to-measure X-ray properties accessible to non-specialists."
https://arxiv.org/abs/2403.07981,2024-03-12,SPINAS: Spinor Amplitude Subroutines for Constructive Diagram Evaluations,['Neil Christensen'],"SPINAS is a C++ package created for the implementation and numerical computation of phase-space points of constructive amplitudes in particle physics. This package contains a suite of classes and methods for handling particles, propagators, spinor products, and processes. SPINAS is structured to offer straightforward usability while ensuring maximum efficiency. This is achieved through a design that emphasizes the storage and reuse of intermediate results within amplitude calculations for each phase-space point. We include a user guide describing how to use the components, a complete example of how to use SPINAS for a scattering amplitude, a discussion of the design and implementation useful for those wishing to contribute, and a discussion of our validation of this package, including both a validation of individual components of the package and a comparison of a complete set of Standard Model processes with Feynman diagrams."
https://arxiv.org/abs/2403.07980,2024-03-12,Angular momentum sensitivities in scalar-tensor theories,"['Adrien Kuntz', 'Enrico Barausse']","Scalar-tensor theories have a long history as possible phenomenological alternatives to General Relativity, but are known to potentially produce deviations from the (strong) equivalence principle in systems involving self-gravitating objects, as a result of the presence of an additional gravitational scalar field besides the tensor modes of General Relativity. We describe here a novel mechanism whereby the equivalence principle is violated for an isolated rotating neutron star, if the gravitational scalar field is changing in time far from the system. We show that the neutron star rotational period changes due to an effective coupling (""angular momentum sensitivity"") to the gravitational scalar, and compute that coupling for viable equations of state for nuclear matter. We comment on the relevance of our findings for testing scalar-tensor theories and models of ultralight dark matter with pulsar timing observations, a topic that we tackle in a companion paper."
https://arxiv.org/abs/2403.07979,2024-03-12,Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning,"['Giorgio Franceschelli', 'Mirco Musolesi']","The Overfitted Brain hypothesis suggests dreams happen to allow generalization in the human brain. Here, we ask if the same is true for reinforcement learning agents as well. Given limited experience in a real environment, we use imagination-based reinforcement learning to train a policy on dream-like episodes, where non-imaginative, predicted trajectories are modified through generative augmentations. Experiments on four ProcGen environments show that, compared to classic imagination and offline training on collected experience, our method can reach a higher level of generalization when dealing with sparsely rewarded environments."
https://arxiv.org/abs/2403.07978,2024-03-12,A Complete Set of 4-Point Amplitudes in the Constructive Standard Model,['Neil Christensen'],"We present a complete set of 4-point amplitudes in the constructive Standard Model at tree level. Any 4-point amplitude can be obtained from the results presented here by a suitable choice of masses, a permutation of the particles (by crossing symmetry), and a reversal of the momenta of the outgoing particles. We have validated all of these amplitudes by comparing with Feynman diagrams for a variety of masses, scattering energy and angles, and helicities of the photons and gluons, when they are in the initial states. The standard constructive techniques work for these amplitudes without the need for any contact terms and indeed, contact terms are not allowed. Only five 4-point vertices are used (allowed), involving the Higgs boson and the W and Z bosons. When external photons or gluons are present, the amplitude simplifies to a single spinor-product structure, present in the numerator. In a few cases, however, the propagator structure is more complex, with different terms depending on the charge or color structure."
https://arxiv.org/abs/2403.07977,2024-03-12,Perturbative Unitarity and the 4-Point Vertices in the Constructive Standard Model,['Neil Christensen'],"We find a complete set of 4-point vertices in the Constructive Standard Model (CSM). This set is smaller than in Feynman diagrams as the CSM does not need or allow any additional 4-point vertices (or contact terms) beyond what is present in Feynman diagrams and, furthermore, it does not need or allow a 4-point vertex for A,Z,W+,W- or A,A,W+,W-, in addition to the already known absence of the 4-gluon vertex. We show that with this set of 4-point vertices, perturbative unitarity is satisfied in the CSM. Additionally, we show that many constructive diagrams are not Feynman diagrams rewritten in spinor form. In fact, we show that there is a significant rearrangement of contributions from the diagrams in constructive calculations relative to Feynman diagrams, for some processes. In addition to the already known or expected rearrangement in diagrams involving external photons, we also find that diagrams involving a 4-vector-boson vertex are also significantly different than their Feynman counterparts."
https://arxiv.org/abs/2403.07976,2024-03-12,Cosmic Ray Feedback on Bi-stable ISM Turbulence,"['Roark Habegger', 'Ka Wai Ho', 'Ka Ho Yuen', 'Ellen G. Zweibel']","Despite being energetically important, the effect of cosmic rays on the dynamics of the interstellar medium (ISM) is assumed to be negligible because the cosmic ray energy diffusion coefficient parallel to the magnetic field is relatively large. Using numerical simulations, we explore how variation of the cosmic ray diffusion coefficient as a function of gas temperature could impact the dynamics of the ISM. We create a two-zone model of cosmic ray transport, reflecting the strong damping of the small scale magnetic field fluctuations, which scatter the cosmic rays, in a gas with low ionization. The variable diffusion coefficient allows more cold gas to form. However, setting the diffusion coefficient at a critical value in the warm phase allows the cosmic rays to adjust the kinetic energy cascade. Specifically, we show the slope of the cascade changes for motion perpendicular to the mean magnetic field, whereas kinetic energy parallel to the magnetic field is reduced equally across inertial scales. We show that cosmic ray energization (or reacceleration) comes at the expense of total radiated energy generated during the formation of a cold cloud. We also show that our two-zone model of cosmic ray transport is capable of matching estimates of the grammage for some paths through the simulation, but full comparison of the grammage requires simulating turbulence in a larger volume."
https://arxiv.org/abs/2403.07975,2024-03-12,Superphot+: Realtime Fitting and Classification of Supernova Light Curves,"['Kaylee M. de Soto', 'Ashley Villar', 'Edo Berger', 'Sebastian Gomez', 'Griffin Hosseinzadeh', 'Doug Branton', 'Sandro Campos', 'Melissa DeLucchi', 'Jeremy Kubica', 'Olivia Lynn', 'Konstantin Malanchev', 'Alex I. Malz']","Photometric classifications of supernova (SN) light curves have become necessary to utilize the full potential of large samples of observations obtained from wide-field photometric surveys, such as the Zwicky Transient Facility (ZTF) and the Vera C. Rubin Observatory. Here, we present a photometric classifier for SN light curves that does not rely on redshift information and still maintains comparable accuracy to redshift-dependent classifiers. Our new package, Superphot+, uses a parametric model to extract meaningful features from multiband SN light curves. We train a gradient-boosted machine with fit parameters from 6,061 ZTF SNe that pass data quality cuts and are spectroscopically classified as one of five classes: SN Ia, SN II, SN Ib/c, SN IIn, and SLSN-I. Without redshift information, our classifier yields a class-averaged F1-score of 0.61 +/- 0.02 and a total accuracy of 0.83 +/- 0.01. Including redshift information improves these metrics to 0.71 +/- 0.02 and 0.88 +/- 0.01, respectively. We assign new class probabilities to 3,558 ZTF transients that show SN-like characteristics (based on the ALeRCE Broker light curve and stamp classifiers), but lack spectroscopic classifications. Finally, we compare our predicted SN labels with those generated by the ALeRCE light curve classifier, finding that the two classifiers agree on photometric labels for 82 +/- 2% of light curves with spectroscopic labels and 72% of light curves without spectroscopic labels. Superphot+ is currently classifying ZTF SNe in real time via the ANTARES Broker, and is designed for simple adaptation to six-band Rubin light curves in the future."
https://arxiv.org/abs/2403.07974,2024-03-12,LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code,"['Naman Jain', 'King Han', 'Alex Gu', 'Wen-Ding Li', 'Fanjia Yan', 'Tianjun Zhang', 'Sida Wang', 'Armando Solar-Lezama', 'Koushik Sen', 'Ion Stoica']","Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model"
https://arxiv.org/abs/2403.07973,2024-03-12,Flexible Non-intrusive Dynamic Instrumentation for WebAssembly,"['Ben L. Titzer', 'Elizabeth Gilbert', 'Bradley Wei Jie Teo', 'Yash Anand', 'Kazuyuki Takayama', 'Heather Miller']","A key strength of managed runtimes over hardware is the ability to gain detailed insight into the dynamic execution of programs with instrumentation. Analyses such as code coverage, execution frequency, tracing, and debugging, are all made easier in a virtual setting. As a portable, low-level bytecode, WebAssembly offers inexpensive in-process sandboxing with high performance. Yet to date, Wasm engines have not offered much insight into executing programs, supporting at best bytecode-level stepping and basic source maps, but no instrumentation capabilities. In this paper, we show the first non-intrusive dynamic instrumentation system for WebAssembly in the open-source Wizard Research Engine. Our innovative design offers a flexible, complete hierarchy of instrumentation primitives that support building high-level, complex analyses in terms of low-level, programmable probes. In contrast to emulation or machine code instrumentation, injecting probes at the bytecode level increases expressiveness and vastly simplifies the implementation by reusing the engine's JIT compiler, interpreter, and deoptimization mechanism rather than building new ones. Wizard supports both dynamic instrumentation insertion and removal while providing consistency guarantees, which is key to composing multiple analyses without interference. We detail a fully-featured implementation in a high-performance multi-tier Wasm engine, show novel optimizations specifically designed to minimize instrumentation overhead, and evaluate performance characteristics under load from various analyses. This design is well-suited for production engine adoption as probes can be implemented to have no impact on production performance when not in use."
https://arxiv.org/abs/2403.07972,2024-03-12,Non-perturbative correction on the black hole geometry,"['Behnam Pourhassan', 'Hoda Farahani', 'Farideh Kazemian', 'İzzet Sakallı', 'Sudhaker Upadhyay', 'Dharm Veer Singh']","In this paper, we use the holographic principle to obtain a modified metric of black holes that reproduces the exponentially corrected entropy. The exponential correction of the black hole entropy comes from non-perturbative corrections. It interprets as a quantum effect which affects black hole thermodynamics especially in the infinitesimal scales. Hence, it may affect black hole stability at the final stage. Then, we study modified thermodynamics due to the non-perturbative corrections and calculate thermodynamics quantities of several non-rotating black holes."
https://arxiv.org/abs/2403.07971,2024-03-12,Evaluating the Impact of Vaccine Hesitancy on the Allocation of Vital Resources During COVID-19 Pandemic,"['Hieu Bui', 'Sandra Eksioglu', 'Ruben Proano']","The COVID-19 pandemic highlighted significant challenges in the allocation of vital healthcare resources. Existing epidemiological models, specifically compartmental models, aimed to predict the spread of the COVID-19 virus and its impact on the population, but they overlooked the influence of \ac{VH} on disease dynamics, including the expected number of hospitalizations and fatalities. We propose improvements to the \ac{SEIR} model for COVID-19 by incorporating the influence of vaccination, \ac{VH}, and resource availability on the disease dynamics. We collect publicly available data and perform data analysis to capture \ac{VH} dynamic changes over time and develop scenario paths for \ac{VH}. We simulate the proposed compartmental model for each \ac{VH} path to explain the impacts of public attitudes toward vaccination, the impacts of healthcare resources on patient outcomes, and the timing of vaccination rollout on the progression and severity of the epidemic. Our analysis demonstrates that reducing \ac{VH} improves health outcomes, reinforcing the importance of addressing \ac{VH} to curb the spread of infectious diseases. Our results show that adequate levels of critical healthcare resources are crucial for minimizing fatalities and also highlight the life-saving impact of timely and effective vaccination programs."
https://arxiv.org/abs/2403.07970,2024-03-12,Zitterbewegung CP violation in a Schwarzschild spacetime,['Jean-Marcel Rax'],"Neutral kaons oscillations in a Schwarzschild spacetime are analyzed. The interplay between two oscillations: (i) mixing associated with second order weak coupling and (ii) strange quark's zitterbewegung, introduces a coupling responsible for the observed CP violation. This curvature induced violation is a CPT violation with T conservation rather than a T violation with CPT conservation. The non-dissipative Hermitian evolution of the kaons system leads to the identification of this CPT violation. Then, the finite lifetime of the short-lived kaons induces a dissipative rotation of the imaginary violation parameter such that it becomes real and appears as a T violation. The consequences of this elucidation of the origin of neutral kaons CP violation are then discussed and some open perspectives are identified."
https://arxiv.org/abs/2403.07969,2024-03-12,KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction,"['Zixuan Li', 'Yutao Zeng', 'Yuxin Zuo', 'Weicheng Ren', 'Wenxuan Liu', 'Miao Su', 'Yucan Guo', 'Yantao Liu', 'Xiang Li', 'Zhilei Hu', 'Long Bai', 'Wei Li', 'Yidan Liu', 'Pan Yang', 'Xiaolong Jin', 'Jiafeng Guo', 'Xueqi Cheng']","In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over $\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning. After code pretraining on around $1.5$B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by $\textbf{49.8\%}$ F1, compared to LLaMA2, under the few-shot setting. After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to $\textbf{12.5\%}$ and $\textbf{21.9\%}$, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively. Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to $\textbf{7.5\%}$ under the supervised setting."
https://arxiv.org/abs/2403.07968,2024-03-12,Do Deep Neural Network Solutions Form a Star Domain?,"['Ankit Sonthalia', 'Alexander Rubinstein', 'Ehsan Abbasnejad', 'Seong Joon Oh']","Entezari et al. (2022) conjectured that neural network solution sets reachable via stochastic gradient descent (SGD) are convex, considering permutation invariances. This means that two independent solutions can be connected by a linear path with low loss, given one of them is appropriately permuted. However, current methods to test this theory often fail to eliminate loss barriers between two independent solutions (Ainsworth et al., 2022; Benzing et al., 2022). In this work, we conjecture that a more relaxed claim holds: the SGD solution set is a star domain that contains a star model that is linearly connected to all the other solutions via paths with low loss values, modulo permutations. We propose the Starlight algorithm that finds a star model of a given learning task. We validate our claim by showing that this star model is linearly connected with other independently found solutions. As an additional benefit of our study, we demonstrate better uncertainty estimates on Bayesian Model Averaging over the obtained star domain. Code is available at https://github.com/aktsonthalia/starlight."
https://arxiv.org/abs/2403.07967,2024-03-12,Feasibility of machine learning-based rice yield prediction in India at the district level using climate reanalysis data,"['Djavan De Clercq', 'Adam Mahdi']","Yield forecasting, the science of predicting agricultural productivity before the crop harvest occurs, helps a wide range of stakeholders make better decisions around agricultural planning. This study aims to investigate whether machine learning-based yield prediction models can capably predict Kharif season rice yields at the district level in India several months before the rice harvest takes place. The methodology involved training 19 machine learning models such as CatBoost, LightGBM, Orthogonal Matching Pursuit, and Extremely Randomized Trees on 20 years of climate, satellite, and rice yield data across 247 of Indian rice-producing districts. In addition to model-building, a dynamic dashboard was built understand how the reliability of rice yield predictions varies across districts. The results of the proof-of-concept machine learning pipeline demonstrated that rice yields can be predicted with a reasonable degree of accuracy, with out-of-sample R2, MAE, and MAPE performance of up to 0.82, 0.29, and 0.16 respectively. These results outperformed test set performance reported in related literature on rice yield modeling in other contexts and countries. In addition, SHAP value analysis was conducted to infer both the importance and directional impact of the climate and remote sensing variables included in the model. Important features driving rice yields included temperature, soil water volume, and leaf area index. In particular, higher temperatures in August correlate with increased rice yields, particularly when the leaf area index in August is also high. Building on the results, a proof-of-concept dashboard was developed to allow users to easily explore which districts may experience a rise or fall in yield relative to the previous year."
https://arxiv.org/abs/2403.07966,2024-03-12,Applying ranking techniques for estimating influence of Earth variables on temperature forecast error,"['M. Julia Flores', 'Melissa Ruiz-Vásquez', 'Ana Bastos', 'René Orth']","This paper describes how to analyze the influence of Earth system variables on the errors when providing temperature forecasts. The initial framework to get the data has been based on previous research work, which resulted in a very interesting discovery. However, the aforementioned study only worked on individual correlations of the variables with respect to the error. This research work is going to re-use the main ideas but introduce three main novelties: (1) applying a data science approach by a few representative locations; (2) taking advantage of the rankings created by Spearman correlation but enriching them with other metrics looking for a more robust ranking of the variables; (3) evaluation of the methodology by learning random forest models for regression with the distinct experimental variations. The main contribution is the framework that shows how to convert correlations into rankings and combine them into an aggregate ranking. We have carried out experiments on five chosen locations to analyze the behavior of this ranking-based methodology. The results show that the specific performance is dependent on the location and season, which is expected, and that this selection technique works properly with Random Forest models but can also improve simpler regression models such as Bayesian Ridge. This work also contributes with an extensive analysis of the results. We can conclude that this selection based on the top-k ranked variables seems promising for this real problem, and it could also be applied in other domains."
https://arxiv.org/abs/2403.07965,2024-03-12,Conditional computation in neural networks: principles and research trends,"['Simone Scardapane', 'Alessandro Baiocchi', 'Alessio Devoto', 'Valerio Marsocci', 'Pasquale Minervini', 'Jary Pomponi']","This article summarizes principles and ideas from the emerging area of applying \textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter). We first provide a general formalism to describe these techniques in an uniform way. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on emerging applicative areas ranging from automated scientific discovery to semantic communication."
https://arxiv.org/abs/2403.07964,2024-03-12,Optimal Design and Implementation of an Open-source Emulation Platform for User-Centric Shared E-mobility Services,"['Maqsood Hussain Shah', 'Yue Ding', 'Shaoshu Zhu', 'Yingqi Gu', 'Mingming Liu']","In response to the escalating global challenge of increasing emissions and pollution in transportation, shared electric mobility services, encompassing e-cars, e-bikes, and e-scooters, have emerged as a popular strategy. However, existingshared electric mobility services exhibit critical design deficiencies, including insufficient service integration, imprecise energy consumption forecasting, limited scalability and geographical coverage, and a notable absence of a user-centric perspective, particularly in the context of multi-modal transportation. More importantly, there is no consolidated open-source framework which could benefit the e-mobility research community. This paper aims to bridge this gap by providing a pioneering open-source framework for shared e-mobility. The proposed framework, with an agent-in-the-loop approach and modular architecture, is tailored to diverse user preferences and offers enhanced customization. We demonstrate the viability of this framework by solving an integrated multi-modal route-optimization problem using the modified Ant Colony Optimization (ACO) algorithm. The primary contribution of this work is to provide a collaborative and transparent framework to tackle the dynamic challenges in the field of e-mobility research using a consolidated approach."
https://arxiv.org/abs/2403.07963,2024-03-12,Copula based dependent censoring in cure models,"['Morine Delhelle', 'Ingrid Van Keilegom']","In this paper we consider a time-to-event variable $T$ that is subject to random right censoring, and we assume that the censoring time $C$ is stochastically dependent on $T$ and that there is a positive probability of not observing the event. There are various situations in practice where this happens, and appropriate models and methods need to be considered to avoid biased estimators of the survival function or incorrect conclusions in clinical trials. We consider a fully parametric model for the bivariate distribution of $(T,C)$, that takes these features into account. The model depends on a parametric copula (with unknown association parameter) and on parametric marginal distributions for $T$ and $C$. Sufficient conditions are developed under which the model is identified, and an estimation procedure is proposed. In particular, our model allows to identify and estimate the association between $T$ and $C$, even though only the smallest of these variables is observable. The finite sample performance of the estimated parameters is illustrated by means of a thorough simulation study and the analysis of breast cancer data."
https://arxiv.org/abs/2403.07962,2024-03-12,Quantum number conservation in meson interactions,['Douglas Newman'],"Quantum number descriptions of spin-zero mesons are obtained from their quark/anti-quark structures, extending the application of the already established seven quantum number conservation in fermion interactions to include mesons. This provides a new tool in the design of experiments and the analysis of results, making it particularly useful for those interactions that involve neutral mesons. One quantum number is parity, conflicting with the evidence for parity non-conservation. The origin of this disagreement is identified as the poor definition of parity in the Standard Model. It is shown that neutral mesons can produce inter-generational interactions, providing an alternative to the see-saw mechanism."
https://arxiv.org/abs/2403.07961,2024-03-12,The $L_p$-discrepancy for finite $p>1$ suffers from the curse of dimensionality,"['Erich Novak', 'Friedrich Pillichshammer']","The $L_p$-discrepancy is a classical quantitative measure for the irregularity of distribution of an $N$-element point set in the $d$-dimensional unit cube. Its inverse for dimension $d$ and error threshold $\varepsilon \in (0,1)$ is the number of points in $[0,1)^d$ that is required such that the minimal normalized $L_p$-discrepancy is less or equal $\varepsilon$. It is well known, that the inverse of $L_2$-discrepancy grows exponentially fast with the dimension $d$, i.e., we have the curse of dimensionality, whereas the inverse of $L_{\infty}$-discrepancy depends exactly linearly on $d$. The behavior of inverse of $L_p$-discrepancy for general $p \not\in \{2,\infty\}$ was an open problem since many years. Recently, the curse of dimensionality for the $L_p$-discrepancy was shown for an infinite sequence of values $p$ in $(1,2]$, but the general result seemed to be out of reach."
https://arxiv.org/abs/2403.07960,2024-03-12,Unsupervised self-organising map of prostate cell Raman spectra shows disease-state subclustering,"['Daniel West', 'Susan Stepney', 'Y. Hancock']","Prostate cancer is a disease which poses an interesting clinical question: should it be treated? A small subset of prostate cancers are aggressive and require removal and treatment to prevent metastatic spread. However, conventional diagnostics remain challenged to risk-stratify such patients, hence, new methods of approach to biomolecularly subclassify the disease are needed. Here we use an unsupervised, self-organising map approach to analyse live-cell Raman spectroscopy data obtained from prostate cell-lines; our aim is to test the feasibility of this method to differentiate, at the single-cell-level, cancer from normal using high-dimensional datasets with minimal preprocessing. The results demonstrate not only successful separation of normal prostate and cancer cells, but also a new subclustering of the prostate cancer cell-line into two groups. Initial analysis of the spectra from each of the cancer subclusters demonstrates a differential expression of lipids, which, against the normal control, may be linked to disease-related changes in cellular signalling."
https://arxiv.org/abs/2403.07959,2024-03-12,An Interpretable Generalization Mechanism for Accurately Detecting Anomaly and Identifying Networking Intrusion Techniques,"['Hao-Ting Pai', 'Yu-Hsuan Kang', 'Wen-Cheng Chung']","Recent advancements in Intrusion Detection Systems (IDS), integrating Explainable AI (XAI) methodologies, have led to notable improvements in system performance via precise feature selection. However, a thorough understanding of cyber-attacks requires inherently explainable decision-making processes within IDS. In this paper, we present the Interpretable Generalization Mechanism (IG), poised to revolutionize IDS capabilities. IG discerns coherent patterns, making it interpretable in distinguishing between normal and anomalous network traffic. Further, the synthesis of coherent patterns sheds light on intricate intrusion pathways, providing essential insights for cybersecurity forensics. By experiments with real-world datasets NSL-KDD, UNSW-NB15, and UKM-IDS20, IG is accurate even at a low ratio of training-to-test. With 10%-to-90%, IG achieves Precision (PRE)=0.93, Recall (REC)=0.94, and Area Under Curve (AUC)=0.94 in NSL-KDD; PRE=0.98, REC=0.99, and AUC=0.99 in UNSW-NB15; and PRE=0.98, REC=0.98, and AUC=0.99 in UKM-IDS20. Notably, in UNSW-NB15, IG achieves REC=1.0 and at least PRE=0.98 since 40%-to-60%; in UKM-IDS20, IG achieves REC=1.0 and at least PRE=0.88 since 20%-to-80%. Importantly, in UKM-IDS20, IG successfully identifies all three anomalous instances without prior exposure, demonstrating its generalization capabilities. These results and inferences are reproducible. In sum, IG showcases superior generalization by consistently performing well across diverse datasets and training-to-test ratios (from 10%-to-90% to 90%-to-10%), and excels in identifying novel anomalies without prior exposure. Its interpretability is enhanced by coherent evidence that accurately distinguishes both normal and anomalous activities, significantly improving detection accuracy and reducing false alarms, thereby strengthening IDS reliability and trustworthiness."
https://arxiv.org/abs/2403.07958,2024-03-12,Temporal Decisions: Leveraging Temporal Correlation for Efficient Decisions in Early Exit Neural Networks,"['Max Sponner', 'Lorenzo Servadei', 'Bernd Waschneck', 'Robert Wille', 'Akash Kumar']","Deep Learning is becoming increasingly relevant in Embedded and Internet-of-things applications. However, deploying models on embedded devices poses a challenge due to their resource limitations. This can impact the model's inference accuracy and latency. One potential solution are Early Exit Neural Networks, which adjust model depth dynamically through additional classifiers attached between their hidden layers. However, the real-time termination decision mechanism is critical for the system's efficiency, latency, and sustained accuracy."
https://arxiv.org/abs/2403.07957,2024-03-12,Efficient Post-Training Augmentation for Adaptive Inference in Heterogeneous and Distributed IoT Environments,"['Max Sponner', 'Lorenzo Servadei', 'Bernd Waschneck', 'Robert Wille', 'Akash Kumar']","Early Exit Neural Networks (EENNs) present a solution to enhance the efficiency of neural network deployments. However, creating EENNs is challenging and requires specialized domain knowledge, due to the large amount of additional design choices. To address this issue, we propose an automated augmentation flow that focuses on converting an existing model into an EENN. It performs all required design decisions for the deployment to heterogeneous or distributed hardware targets: Our framework constructs the EENN architecture, maps its subgraphs to the hardware targets, and configures its decision mechanism. To the best of our knowledge, it is the first framework that is able to perform all of these steps."
https://arxiv.org/abs/2403.07956,2024-03-12,DeepCDCL: An CDCL-based Neural Network Verification Framework,"['Zongxin Liu', 'Pengfei Yang', 'Lijun Zhang', 'Xiaowei Huang']","Neural networks in safety-critical applications face increasing safety and security concerns due to their susceptibility to little disturbance. In this paper, we propose DeepCDCL, a novel neural network verification framework based on the Conflict-Driven Clause Learning (CDCL) algorithm. We introduce an asynchronous clause learning and management structure, reducing redundant time consumption compared to the direct application of the CDCL framework. Furthermore, we also provide a detailed evaluation of the performance of our approach on the ACAS Xu and MNIST datasets, showing that a significant speed-up is achieved in most cases."
https://arxiv.org/abs/2403.07955,2024-03-12,Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery,"['Linan Yue', 'Qi Liu', 'Yichao Du', 'Li Wang', 'Weibo Gao', 'Yanqing An']","The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in data to compose rationales and limited large-scale annotated rationales by human, in this paper, we propose a Shortcuts-fused Selective Rationalization (SSR) method, which boosts the rationalization by discovering and exploiting potential shortcuts. Specifically, SSR first designs a shortcuts discovery approach to detect several potential shortcuts. Then, by introducing the identified shortcuts, we propose two strategies to mitigate the problem of utilizing shortcuts to compose rationales. Finally, we develop two data augmentations methods to close the gap in the number of annotated rationales. Extensive experimental results on real-world datasets clearly validate the effectiveness of our proposed method."
https://arxiv.org/abs/2403.07954,2024-03-12,Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach,"['Keke Huang', 'Wencai Cao', 'Hoang Ta', 'Xiaokui Xiao', 'Pietro Liò']","Graph Neural Networks (GNNs), known as spectral graph filters, find a wide range of applications in web networks. To bypass eigendecomposition, polynomial graph filters are proposed to approximate graph filters by leveraging various polynomial bases for filter training. However, no existing studies have explored the diverse polynomial graph filters from a unified perspective for optimization."
https://arxiv.org/abs/2403.07953,2024-03-12,Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition,"['Geonhwa Jeong', 'Po-An Tsai', 'Abhimanyu R. Bambhaniya', 'Stephen W. Keckler', 'Tushar Krishna']","Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and activation tensors so that they can be accelerated by any systems with structured sparse hardware support. Evaluation results show that, by exploiting prior structured sparse hardware baselines, our method can accelerate off-the-shelf dense and sparse DNNs without fine-tuning and improves energy-delay-product by up to 83% and 74% on average."
https://arxiv.org/abs/2403.07952,2024-03-11,AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production,"['Jiuniu Wang', 'Zehua Du', 'Yuyuan Zhao', 'Bo Yuan', 'Kexiang Wang', 'Jian Liang', 'Yaxi Zhao', 'Yihen Lu', 'Gengliang Li', 'Junlong Gao', 'Xin Tu', 'Zhenyu Guo']","The Agent and AIGC (Artificial Intelligence Generated Content) technologies have recently made significant progress. We propose AesopAgent, an Agent-driven Evolutionary System on Story-to-Video Production. AesopAgent is a practical application of agent technology for multimodal content generation. The system integrates multiple generative capabilities within a unified framework, so that individual users can leverage these modules easily. This innovative system would convert user story proposals into scripts, images, and audio, and then integrate these multimodal contents into videos. Additionally, the animating units (e.g., Gen-2 and Sora) could make the videos more infectious. The AesopAgent system could orchestrate task workflow for video generation, ensuring that the generated video is both rich in content and coherent. This system mainly contains two layers, i.e., the Horizontal Layer and the Utility Layer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary system that optimizes the whole video generation workflow and the steps within the workflow. It continuously evolves and iteratively optimizes workflow by accumulating expert experience and professional knowledge, including optimizing the LLM prompts and utilities usage. The Utility Layer provides multiple utilities, leading to consistent image generation that is visually coherent in terms of composition, characters, and style. Meanwhile, it provides audio and special effects, integrating them into expressive and logically arranged videos. Overall, our AesopAgent achieves state-of-the-art performance compared with many previous works in visual storytelling. Our AesopAgent is designed for convenient service for individual users, which is available on the following page: https://aesopai.github.io/."
https://arxiv.org/abs/2403.07951,2024-03-11,SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation,"['Yiran Wang', 'Li Xiao']","It has been shown that traditional deep learning methods for electronic microscopy segmentation usually suffer from low transferability when samples and annotations are limited, while large-scale vision foundation models are more robust when transferring between different domains but facing sub-optimal improvement under fine-tuning. In this work, we present a new few-shot domain adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with nnUNet in the embedding space to achieve high transferability and accuracy. Specifically, we choose the Unet-based network as the ""expert"" component to learn segmentation features efficiently and design a SAM-based adaptation module as the ""generic"" component for domain transfer. By amalgamating the ""generic"" and ""expert"" components, we mitigate the modality imbalance in the complex pre-training knowledge inherent to large-scale Vision Foundation models and the challenge of transferability inherent to traditional neural networks. The effectiveness of our model is evaluated on two electron microscopic image datasets with different modalities for mitochondria segmentation, which improves the dice coefficient on the target domain by 6.7%. Also, the SAM-based adaptor performs significantly better with only a single annotated image than the 10-shot domain adaptation on nnUNet. We further verify our model on four MRI datasets from different sources to prove its generalization ability."
https://arxiv.org/abs/2403.07950,2024-03-11,Turbulence from First Principles,['Chris Scott'],"We provide a first-principles approach to turbulence by employing the electrodynamics of continuous media at the viscous limit to recover the Navier-Stokes equations. We treat oscillators with two orthogonal angular momenta as a spin network with properties applicable to the Kolmogorov-Arnold-Moser (KAM) theorem. The microscopic viscous limit has an irreducible representation that includes $O(3)$ expansion terms for a radiation-dominated fluid with a Friedmann-Lemaitre-Robertson-Walker (FLRW) metric, equivalent to an oriented toroidal de Sitter space. The turbulence solution in $\mathbb{R}^{3,1}$ lies on 6-choose-3 de Sitter intersections of three orthogonal $n$-tori."
https://arxiv.org/abs/2403.07949,2024-03-11,Algorithmic Bayesian Epistemology,['Eric Neyman'],"One aspect of the algorithmic lens in theoretical computer science is a view on other scientific disciplines that focuses on satisfactory solutions that adhere to real-world constraints, as opposed to solutions that would be optimal ignoring such constraints. The algorithmic lens has provided a unique and important perspective on many academic fields, including molecular biology, ecology, neuroscience, quantum physics, economics, and social science."
https://arxiv.org/abs/2403.07948,2024-03-11,A Study on Actions for Atomic Logics,['Raül Espejo-Boix'],"Nowadays there is a large number of non-classical logics, each one best suited for reasoning about some issues in abstract fields, such as linguistics or epistemology, among others. Proving interesting properties for each one of them supposes a big workload for logicians and computer scientists. We want an approach into this problematic that is modular. To adress this issue, the report shows new insights in the construction of Atomic Logics introduced by Guillaume Aucher. Atomic Logics let us represent very general left and right introduction rules and they come along a new kind of rules based on display logics and residuation. A new approach is taken into the definition of Atomic Logics, which is now built on a class of actions for which we prove cut-elimination. We show that some of them are equivalent to Aucher's Atomic Logics and we prove cut-elimination and Craig Interpolation for a class of them. The introduced theory is applied to the non-associative Lambek Calculus throughout the report. It is accompanied by a computer-checked formalisation of the original syntax in the proof assistant Coq."
https://arxiv.org/abs/2403.07947,2024-03-11,The evaluation of a code-switched Sepedi-English automatic speech recognition system,"['Amanda Phaladi', 'Thipe Modipa']","Speech technology is a field that encompasses various techniques and tools used to enable machines to interact with speech, such as automatic speech recognition (ASR), spoken dialog systems, and others, allowing a device to capture spoken words through a microphone from a human speaker. End-to-end approaches such as Connectionist Temporal Classification (CTC) and attention-based methods are the most used for the development of ASR systems. However, these techniques were commonly used for research and development for many high-resourced languages with large amounts of speech data for training and evaluation, leaving low-resource languages relatively underdeveloped. While the CTC method has been successfully used for other languages, its effectiveness for the Sepedi language remains uncertain. In this study, we present the evaluation of the Sepedi-English code-switched automatic speech recognition system. This end-to-end system was developed using the Sepedi Prompted Code Switching corpus and the CTC approach. The performance of the system was evaluated using both the NCHLT Sepedi test corpus and the Sepedi Prompted Code Switching corpus. The model produced the lowest WER of 41.9%, however, the model faced challenges in recognizing the Sepedi only text."
https://arxiv.org/abs/2403.07946,2024-03-11,Burns turbulent dispersion considers the dispersed phase as a passive scalar,['Corentin Reiss'],"The Burns turbulent dispersion force is the most commonly used turbulent dispersion in the twofluid RANS bubbly-flow literature. However, its derivation is based on a series of hypothesis that are difficult to justify in industrial flows. It is shown that in low-void fraction vertical pipe flow, the Burns turbulent dispersion formulation is equivalent to considering the radial movement of the gas phase like a passive scalar that follows turbulent eddies. This is not apparent in the derivation of the force. As bubbles in pressurized water reactor (PWR) conditions have a low Stokes number ($\sim$ 10--1), considering bubbles are transported by turbulence is a good approximation for bubble dispersion in pipe flow. Therefore, the Burns turbulent dispersion force is appropriate to represent bubble dispersion in low-void fraction PWR flows."
https://arxiv.org/abs/2403.07945,2024-03-10,A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology,['Bryce Allen Bagley'],"The rapid advancement in neurotechnology in recent years has created an emerging critical intersection between neurotechnology and security. Implantable devices, non-invasive monitoring, and non-invasive therapies all carry with them the prospect of violating the privacy and autonomy of individuals' cognition. A growing number of scientists and physicians have made calls to address this issue -- which we term Cognitive Security -- but applied efforts have been limited. A major barrier hampering scientific and engineering efforts to address Cognitive Security is the lack of a clear means of describing and analyzing relevant problems. In this paper we develop Cognitive Security, a mathematical framework which enables such description and analysis by drawing on methods and results from multiple fields. We demonstrate certain statistical properties which have significant implications for Cognitive Security, and then present descriptions of the algorithmic problems faced by attackers attempting to violate privacy and autonomy, and defenders attempting to obstruct such attempts."
https://arxiv.org/abs/2403.07944,2024-03-10,WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs,"['Deshun Yang', 'Luhui Hu', 'Yu Tian', 'Zihao Li', 'Chris Kelly', 'Bang Yang', 'Cindy Yang', 'Yuexian Zou']","Several text-to-video diffusion models have demonstrated commendable capabilities in synthesizing high-quality video content. However, it remains a formidable challenge pertaining to maintaining temporal consistency and ensuring action smoothness throughout the generated sequences. In this paper, we present an innovative video generation AI agent that harnesses the power of Sora-inspired multimodal learning to build skilled world models framework based on textual prompts and accompanying images. The framework includes two parts: prompt enhancer and full video translation. The first part employs the capabilities of ChatGPT to meticulously distill and proactively construct precise prompts for each subsequent step, thereby guaranteeing the utmost accuracy in prompt communication and accurate execution in following model operations. The second part employ compatible with existing advanced diffusion techniques to expansively generate and refine the key frame at the conclusion of a video. Then we can expertly harness the power of leading and trailing key frames to craft videos with enhanced temporal consistency and action smoothness. The experimental results confirm that our method has strong effectiveness and novelty in constructing world models from text and image inputs over the other methods."
https://arxiv.org/abs/2403.07943,2024-03-10,Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation and Attack,"['Xin Liu', 'Yuxiang Zhang', 'Meng Wu', 'Mingyu Yan', 'Kun He', 'Wei Yan', 'Shirui Pan', 'Xiaochun Ye', 'Dongrui Fan']","Edge perturbation is a basic method to modify graph structures. It can be categorized into two veins based on their effects on the performance of graph neural networks (GNNs), i.e., graph data augmentation and attack. Surprisingly, both veins of edge perturbation methods employ the same operations, yet yield opposite effects on GNNs' accuracy. A distinct boundary between these methods in using edge perturbation has never been clearly defined. Consequently, inappropriate perturbations may lead to undesirable outcomes, necessitating precise adjustments to achieve desired effects. Therefore, questions of ``why edge perturbation has a two-faced effect?'' and ``what makes edge perturbation flexible and effective?'' still remain unanswered."
https://arxiv.org/abs/2403.07942,2024-03-09,Attacking Transformers with Feature Diversity Adversarial Perturbation,"['Chenxing Gao', 'Hang Zhou', 'Junqing Yu', 'YuTeng Ye', 'Jiale Cai', 'Junle Wang', 'Wei Yang']","Understanding the mechanisms behind Vision Transformer (ViT), particularly its vulnerability to adversarial perturba tions, is crucial for addressing challenges in its real-world  applications. Existing ViT adversarial attackers rely on la bels to calculate the gradient for perturbation, and exhibit low transferability to other structures and tasks. In this paper, we present a label-free white-box attack approach for ViT-based models that exhibits strong transferability to various black box models, including most ViT variants, CNNs, and MLPs, even for models developed for other modalities. Our inspira tion comes from the feature collapse phenomenon in ViTs, where the critical attention mechanism overly depends on the low-frequency component of features, causing the features in middle-to-end layers to become increasingly similar and eventually collapse. We propose the feature diversity attacker to naturally accelerate this process and achieve remarkable performance and transferability."
https://arxiv.org/abs/2403.07941,2024-03-09,A new theory bridging non-relativistic and QED-based path integrals unveils more than quantum mechanics,['W. Wen'],"The Feynman path integral plays a crucial role in quantum mechanics, offering significant insights into the interaction between classical action and propagators, and linking quantum electrodynamics (QED) with Feynman diagrams. However, the formulations of path integrals in classical quantum mechanics and QED are neither unified nor interconnected, suggesting the potential existence of an important bridging theory that could be key to solving existing puzzles in quantum mechanics. In this work, we delve into the theoretical consistency, completeness, and integration with established path integral theories, revealing this concealed path integral form. This newly uncovered form not only connects various path integral approaches but also demonstrates its potential in explaining quantum phenomena like the origin of spin and quantum nonlocal correlations. It transcends conventional quantum mechanics, proposing a more profound and fundamental physical principle."
https://arxiv.org/abs/2403.07940,2024-03-08,Hair and scalp disease detection using deep learning,"['Kavita Sultanpure', 'Bhairavi Shirsath', 'Bhakti Bhande', 'Harshada Sawai', 'Srushti Gawade', 'Suraj Samgir']","In recent years, there has been a notable advancement in the integration of healthcare and technology, particularly evident in the field of medical image analysis. This paper introduces a pioneering approach in dermatology, presenting a robust method for the detection of hair and scalp diseases using state-of-the-art deep learning techniques. Our methodology relies on Convolutional Neural Networks (CNNs), well-known for their efficacy in image recognition, to meticulously analyze images for various dermatological conditions affecting the hair and scalp. Our proposed system represents a significant advancement in dermatological diagnostics, offering a non-invasive and highly efficient means of early detection and diagnosis. By leveraging the capabilities of CNNs, our model holds the potential to revolutionize dermatology, providing accessible and timely healthcare solutions. Furthermore, the seamless integration of our trained model into a web-based platform developed with the Django framework ensures broad accessibility and usability, democratizing advanced medical diagnostics. The integration of machine learning algorithms into web applications marks a pivotal moment in healthcare delivery, promising empowerment for both healthcare providers and patients. Through the synergy between technology and healthcare, our paper outlines the meticulous methodology, technical intricacies, and promising future prospects of our system. With a steadfast commitment to advancing healthcare frontiers, our goal is to significantly contribute to leveraging technology for improved healthcare outcomes globally. This endeavor underscores the profound impact of technological innovation in shaping the future of healthcare delivery and patient care, highlighting the transformative potential of our approach."
https://arxiv.org/abs/2403.07939,2024-03-08,Dynamic Policy-Driven Adaptive Multi-Instance Learning for Whole Slide Image Classification,"['Tingting Zheng', 'Kui Jiang', 'Hongxun Yao']","Multi-Instance Learning (MIL) has shown impressive performance for histopathology whole slide image (WSI) analysis using bags or pseudo-bags. It involves instance sampling, feature representation, and decision-making. However, existing MIL-based technologies at least suffer from one or more of the following problems: 1) requiring high storage and intensive pre-processing for numerous instances (sampling); 2) potential over-fitting with limited knowledge to predict bag labels (feature representation); 3) pseudo-bag counts and prior biases affect model robustness and generalizability (decision-making). Inspired by clinical diagnostics, using the past sampling instances can facilitate the final WSI analysis, but it is barely explored in prior technologies. To break free these limitations, we integrate the dynamic instance sampling and reinforcement learning into a unified framework to improve the instance selection and feature aggregation, forming a novel Dynamic Policy Instance Selection (DPIS) scheme for better and more credible decision-making. Specifically, the measurement of feature distance and reward function are employed to boost continuous instance sampling. To alleviate the over-fitting, we explore the latent global relations among instances for more robust and discriminative feature representation while establishing reward and punishment mechanisms to correct biases in pseudo-bags using contrastive learning. These strategies form the final Dynamic Policy-Driven Adaptive Multi-Instance Learning (PAMIL) method for WSI tasks. Extensive experiments reveal that our PAMIL method outperforms the state-of-the-art by 3.8\% on CAMELYON16 and 4.4\% on TCGA lung cancer datasets."
https://arxiv.org/abs/2403.07938,2024-03-08,Text-to-Audio Generation Synchronized with Videos,"['Shentong Mo', 'Jing Shi', 'Yapeng Tian']","In recent times, the focus on text-to-audio (TTA) generation has intensified, as researchers strive to synthesize audio from textual descriptions. However, most existing methods, though leveraging latent diffusion models to learn the correlation between audio and text embeddings, fall short when it comes to maintaining a seamless synchronization between the produced audio and its video. This often results in discernible audio-visual mismatches. To bridge this gap, we introduce a groundbreaking benchmark for Text-to-Audio generation that aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself with three novel metrics dedicated to evaluating visual alignment and temporal consistency. To complement this, we also present a simple yet effective video-aligned TTA generation model, namely T2AV. Moving beyond traditional methods, T2AV refines the latent diffusion approach by integrating visual-aligned text embeddings as its conditional foundation. It employs a temporal multi-head attention transformer to extract and understand temporal nuances from video data, a feat amplified by our Audio-Visual ControlNet that adeptly merges temporal visual representations with text embeddings. Further enhancing this integration, we weave in a contrastive learning objective, designed to ensure that the visual-aligned text embeddings resonate closely with the audio features. Extensive evaluations on the AudioCaps and T2AV-Bench demonstrate that our T2AV sets a new standard for video-aligned TTA generation in ensuring visual alignment and temporal consistency."
https://arxiv.org/abs/2403.07937,2024-03-08,Speech Robust Bench: A Robustness Benchmark For Speech Recognition,"['Muhammad A. Shah', 'David Solans Noguero', 'Mikko A. Heikkila', 'Nicolas Kourtellis']","As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model's robustness across subgroups. We believe that SRB will facilitate future research towards robust ASR models, by making it easier to conduct comprehensive and comparable robustness evaluations."
https://arxiv.org/abs/2403.07936,2024-03-07,Accelerating multigrid solver with generative super-resolution,"['Francisco Holguin', 'GS Sidharth', 'Gavin Portwood']","The geometric multigrid algorithm is an efficient numerical method for solving a variety of elliptic partial differential equations (PDEs). The method damps errors at progressively finer grid scales, resulting in faster convergence compared to iterative methods such as Gauss-Seidel. The prolongation or coarse-to-fine interpolation operator within the multigrid algorithm, lends itself to a data-driven treatment with deep learning super-resolution, commonly used to increase the resolution of images. We (i) propose the integration of a super-resolution generative adversarial network (GAN) model with the multigrid algorithm as the prolongation operator and (ii) show that the GAN-interpolation can improve the convergence properties of multigrid in comparison to cubic spline interpolation on a class of multiscale PDEs typically solved in fluid mechanics and engineering simulations. We also highlight the importance of characterizing hybrid (machine learning/traditional) algorithm parameters."
https://arxiv.org/abs/2403.07874,2024-03-12,Beyond Text: Frozen Large Language Models in Visual Signal Comprehension,"['Lei Zhu', 'Fangyun Wei', 'Yanye Lu']","In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multi-modal datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the LLM's vocabulary. To achieve this, we present the Vision-to-Language Tokenizer, abbreviated as V2T Tokenizer, which transforms an image into a ``foreign language'' with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model. With this innovative image encoding, the LLM gains the ability not only for visual comprehension but also for image denoising and restoration in an auto-regressive fashion-crucially, without any fine-tuning. We undertake rigorous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and visual question answering, as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at https://github.com/zh460045050/V2L-Tokenizer."
https://arxiv.org/abs/2403.07873,2024-03-12,Stability of Anomalous Hall Crystals in multilayer rhombohedral graphene,"['Zhihuan Dong', 'Adarsh S. Patri', 'T. Senthil']","Recent experiments showing an integer quantum anomalous Hall effect in pentalayer rhombohedral graphene have been interpreted in terms of a valley-polarized interaction-induced Chern band. The resulting many-body state can be viewed as an Anomalous Hall Crystal (AHC), with a further coupling to a weak moiré potential. We explain the origin of the Chern band and the corresponding AHC in the pentalayer system. We propose a simplified model that focuses on the physics near high symmetry Brillouin zone points to describe the competition between AHC and Wigner Crystal (WC) phases. We discuss the possible role of the moiré potential. We emphasize that even if in the moiré-less limit, the AHC is not favored over a correlated Fermi liquid, the moiré potential will push the system into a `moiré-enabled AHC'. We also suggest that there is a range of alignment angles between R5G and hBN where a $C = 2$ insulator may be found at integer filling."
https://arxiv.org/abs/2403.07872,2024-03-12,Rethinking Generative Large Language Model Evaluation for Semantic Comprehension,"['Fangyun Wei', 'Xi Chen', 'Lin Luo']","Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called ``Real-world questions'' (RWQ), comprising 20,772 authentic user inquiries. Additionally, we thoroughly analyze the characteristics of our system and compare it with prior leaderboards like AlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo system, the feasibility of registering new models, and its potential to reshape LLM leaderboards."
https://arxiv.org/abs/2403.07871,2024-03-12,SIDE-real: Truncated marginal neural ratio estimation for Supernova Ia Dust Extinction with real data,"['Konstantin Karchev', 'Matthew Grayling', 'Benjamin M. Boyd', 'Roberto Trotta', 'Kaisey S. Mandel', 'Christoph Weniger']","We present the first fully simulation-based hierarchical analysis of the light curves of a population of low-redshift type Ia supernovae (SNae Ia). Our hardware-accelerated forward model, released in the Python package slicsim, includes stochastic variations of each SN's spectral flux distribution (based on the pre-trained BayeSN model), extinction from dust in the host and in the Milky Way, redshift, and realistic instrumental noise. By utilising truncated marginal neural ratio estimation (TMNRE), a neural network-enabled simulation-based inference technique, we implicitly marginalise over 4000 latent variables (for a set of $\approx 100$ SNae Ia) to efficiently infer SN Ia absolute magnitudes and host-galaxy dust properties at the population level while also constraining the parameters of individual objects. Amortisation of the inference procedure allows us to obtain coverage guarantees for our results through Bayesian validation and frequentist calibration. Furthermore, we show a detailed comparison to full likelihood-based inference, implemented through Hamiltonian Monte Carlo, on simulated data and then apply TMNRE to the light curves of 86 SNae Ia from the Carnegie Supernova Project, deriving marginal posteriors in excellent agreement with previous work. Given its ability to accommodate arbitrarily complex extensions to the forward model -- e.g. different populations based on host properties, redshift evolution, complicated photometric redshift estimates, selection effects, and non-Ia contamination -- without significant modifications to the inference procedure, TMNRE has the potential to become the tool of choice for cosmological parameter inference from future, large SN Ia samples."
https://arxiv.org/abs/2403.07870,2024-03-12,OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation,"['Aadhithya Iyer', 'Zhuoran Peng', 'Yinlong Dai', 'Irmak Guzey', 'Siddhant Haldar', 'Soumith Chintala', 'Lerrel Pinto']","Open-sourced, user-friendly tools form the bedrock of scientific advancement across disciplines. The widespread adoption of data-driven learning has led to remarkable progress in multi-fingered dexterity, bimanual manipulation, and applications ranging from logistics to home robotics. However, existing data collection platforms are often proprietary, costly, or tailored to specific robotic morphologies. We present OPEN TEACH, a new teleoperation system leveraging VR headsets to immerse users in mixed reality for intuitive robot control. Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH enables real-time control of various robots, including multi-fingered hands and bimanual arms, through an easy-to-use app. Using natural hand gestures and movements, users can manipulate robots at up to 90Hz with smooth visual feedback and interface widgets offering closeup environment views. We demonstrate the versatility of OPEN TEACH across 38 tasks on different robots. A comprehensive user study indicates significant improvement in teleoperation capability over the AnyTeleop framework. Further experiments exhibit that the collected data is compatible with policy learning on 10 dexterous and contact-rich manipulation tasks. Currently supporting Franka, xArm, Jaco, and Allegro platforms, OPEN TEACH is fully open-sourced to promote broader adoption. Videos are available at https://open-teach.github.io/."
https://arxiv.org/abs/2403.07869,2024-03-12,TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation,"['Shivin Dass', 'Wensi Ai', 'Yuqian Jiang', 'Samik Singh', 'Jiaheng Hu', 'Ruohan Zhang', 'Peter Stone', 'Ben Abbatematteo', 'Roberto Martin-Martin']","A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world. We demonstrate the quality of the demonstrations collected with TeleMoMa by training imitation learning policies for mobile manipulation tasks involving synchronized whole-body motion. Finally, we also show that TeleMoMa's teleoperation channel enables teleoperation on site, looking at the robot, or remote, sending commands and observations through a computer network, and perform user studies to evaluate how easy it is for novice users to learn to collect demonstrations with different combinations of human interfaces enabled by our system. We hope TeleMoMa becomes a helpful tool for the community enabling researchers to collect whole-body mobile manipulation demonstrations. For more information and video results, https://robin-lab.cs.utexas.edu/telemoma-web."
https://arxiv.org/abs/2403.07868,2024-03-12,Online Digital Twin-Empowered Content Resale Mechanism in Age of Information-Aware Edge Caching Networks,"['Yuhan Yi', 'Guanglin Zhang', 'Hai Jiang']","For users requesting popular contents from content providers, edge caching can alleviate backhaul pressure and enhance the quality of experience of users. Recently there is also a growing concern about content freshness that is quantified by age of information (AoI). Therefore, AoI-aware online caching algorithms are required, which is challenging because the content popularity is usually unknown in advance and may vary over time. In this paper, we propose an online digital twin (DT) empowered content resale mechanism in AoI-aware edge caching networks. We aim to design an optimal two-timescale caching strategy to maximize the utility of an edge network service provider (ENSP). The formulated optimization problem is non-convex and NP-hard. To tackle this intractable problem, we propose a DT-assisted Online Caching Algorithm (DT-OCA). In specific, we first decompose our formulated problem into a series of subproblems, each handling a cache period. For each cache period, we use a DT-based prediction method to effectively capture future content popularity, and develop online caching strategy. Competitive ratio analysis and extensive experimental results demonstrate that our algorithm has promising performance, and outperforms other benchmark algorithms. Insightful observations are also found and discussed."
https://arxiv.org/abs/2403.07867,2024-03-12,The Virtues of Laziness: Multi-Query Kinodynamic Motion Planning with Lazy Methods,"['Anuj Pasricha', 'Alessandro Roncone']","In this work, we introduce LazyBoE, a multi-query method for kinodynamic motion planning with forward propagation. This algorithm allows for the simultaneous exploration of a robot's state and control spaces, thereby enabling a wider suite of dynamic tasks in real-world applications. Our contributions are three-fold: i) a method for discretizing the state and control spaces to amortize planning times across multiple queries; ii) lazy approaches to collision checking and propagation of control sequences that decrease the cost of physics-based simulation; and iii) LazyBoE, a robust kinodynamic planner that leverages these two contributions to produce dynamically-feasible trajectories. The proposed framework not only reduces planning time but also increases success rate in comparison to previous approaches."
https://arxiv.org/abs/2403.07866,2024-03-12,Robustness of a state with Ising topological order against local projective measurements,"['Sanjeev Kumar', 'Vikram Tripathi']","We investigate the fragility of a topologically ordered state, namely, the ground state of a weakly Zeeman perturbed honeycomb Kitaev model to environment induced decoherence effects mimicked by random local projective measurements. Our findings show the nonabelian Ising topological order, as quantified by a tripartite mutual information (the topological entanglement entropy $γ$,) is resilient to such disturbances. Further, $γ$ is found to evolve smoothly from a topologically ordered state to a distribution of trivial states as a function of rate of measurement (temperature). We assess our model by contrasting it with the Toric Code limit of the Kitaev model, whose ground state has abelian $Z_2$ topological order, and which has garnered greater attention in the literature of fault-tolerant quantum computation. The findings reveal the topological order in the Toric Code limit collapses rapidly as opposed to our model where it can withstand higher measurement rates."
https://arxiv.org/abs/2403.07865,2024-03-12,Exploring Safety Generalization Challenges of Large Language Models via Code,"['Qibing Ren', 'Chang Gao', 'Jing Shao', 'Junchi Yan', 'Xin Tan', 'Wai Lam', 'Lizhuang Ma']","The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages. These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs."
https://arxiv.org/abs/2403.07864,2024-03-12,Unraveling the nature of quasi van der Waals Epitaxy of magnetic topological insulators Cr: (BixSb1-x)2Te3 on a GaAs (111) substrate through coherently strained interface,"['Yuxing Ren', 'Lixuan Tai', 'Kaicheng Pan', 'Yueyun Chen', 'Benjamin Z. Gregory', 'Jin Ho Kang', 'Malcolm Jackson', 'Michael Liao', 'Yifei Sun', 'Noah Bodzin', 'Kin Wong', 'Suchismita Sarker', 'B. C. Regan', 'Chee-Wei Wong', 'Mark Goorsky', 'Andrej Singer', 'Kang L. Wang']","Quasi van der Waals Epitaxy (qvdWE) has been realized for decades at the interfaces between 3D and 2D materials or van der Waals materials. The growth of magnetic topological insulators (MTI) Cr: (BixSb1-x)2Te3 (CBST) on GaAs (111) substrates for Quantum Anomalous Hall Effect (QAH) is actually one of the examples of qvdWE, which is not well noticed despite the fact that its advantages have been used in growth of various MTI materials. This is distinguished from the growth of MTIs on other substrates. Although the qvdWE mode has been used in many 2D growth on III-V substrates, the specific features and mechanisms are not well demonstrated and summarized yet. Here in this work, we have for the first time shown the features of both coherent interfaces and the existence of strain originating from qvdWE at the same time."
https://arxiv.org/abs/2403.07863,2024-03-12,"Spectral invariants for non-compactly supported Hamiltonians on the disc, and an application to the mean action spectrum","['Barney Bramham', 'Abror Pirnapasov']",For a symplectic isotopy on the two-dimensional disc we show that the classical spectral invariants of Viterbo [20] can be extended in a meaningful way to {\it non-compactly} supported Hamiltonians. We establish some basic properties of these extended invariants and as an application we show that Hutchings' inequality in [8] between the Calabi invariant and the mean action spectrum holds without any assumptions on the isotopy; in [8] it is assumed that the Calabi invariant is less than the rotation number (or action) on the boundary.
https://arxiv.org/abs/2403.07862,2024-03-12,Low coordinate degree algorithms I: Universality of computational thresholds for hypothesis testing,['Dmitriy Kunisky'],"We study when low coordinate degree functions (LCDF) -- linear combinations of functions depending on small subsets of entries of a vector -- can hypothesis test between high-dimensional probability measures. These functions are a generalization, proposed in Hopkins' 2018 thesis but seldom studied since, of low degree polynomials (LDP), a class widely used in recent literature as a proxy for all efficient algorithms for tasks in statistics and optimization. Instead of the orthogonal polynomial decompositions used in LDP calculations, our analysis of LCDF is based on the Efron-Stein or ANOVA decomposition, making it much more broadly applicable. By way of illustration, we prove channel universality for the success of LCDF in testing for the presence of sufficiently ""dilute"" random signals through noisy channels: the efficacy of LCDF depends on the channel only through the scalar Fisher information for a class of channels including nearly arbitrary additive i.i.d. noise and nearly arbitrary exponential families. As applications, we extend lower bounds against LDP for spiked matrix and tensor models under additive Gaussian noise to lower bounds against LCDF under general noisy channels. We also give a simple and unified treatment of the effect of censoring models by erasing observations at random and of quantizing models by taking the sign of the observations. These results are the first computational lower bounds against any large class of algorithms for all of these models when the channel is not one of a few special cases, and thereby give the first substantial evidence for the universality of several statistical-to-computational gaps."
https://arxiv.org/abs/2403.07861,2024-03-12,Synthesis of epitaxial magnetic pyrochlore heterojunctions,"['Mikhail Kareev', 'Xiaoran Liu', 'Michael Terilli', 'Fangdi Wen', 'Tsung-Chi Wu', 'Dorothy Doughty', 'Hongze Li', 'Jianshi Zhou', 'Qinghua Zhang', 'Lin Gu', 'Jak Chakhalian']","The synthesis of stoichiometric and epitaxial pyrochlore iridate thin films presents significant challenges yet is critical for unlocking experimental access to novel topological and magnetic states. Towards this goal, we unveil an in-situ two-stage growth mechanism that facilitates the synthesis of high-quality oriented pyrochlore iridate thin films. The growth starts with the deposition of a pyrochlore titanate as an active iso-structural template, followed by the application of an in-situ solid phase epitaxy technique in the second stage to accomplish the formation of single crystalline, large-area films. This novel protocol ensures the preservation of stoichiometry and structural homogeneity, leading to a marked improvement in surface and interface qualities over previously reported methods. The success of this synthesis approach is attributed to the application of directional laser-heat annealing, which effectively reorganizes the continuous random network of ions into a crystalline structure, as evidenced by our comprehensive analysis of the growth kinetics. This new synthesis approach advances our understanding of pyrochlore iridate film fabrication and opens a new perspective for investigating their unique physical properties."
https://arxiv.org/abs/2403.07860,2024-03-12,Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation,"['Shihao Zhao', 'Shaozhe Hao', 'Bojia Zi', 'Huaizhe Xu', 'Kwan-Yee K. Wong']","Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images. As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts. A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation. In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models. Our pipeline is compatible with various language models and generative vision models, accommodating different structures. Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality. Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge. Code is available at https://github.com/ShihaoZhaoZSH/LaVi-Bridge."
https://arxiv.org/abs/2403.07859,2024-03-12,On the stack of 0-dimensional coherent sheaves: motivic aspects,"['Barbara Fantechi', 'Andrea T. Ricolfi']","Let $X$ be a variety. We study (decompositions of) the motivic class, in the Grothendieck ring of stacks, of the stack $\mathscr{C}oh^n(X)$ of $0$-dimensional coherent sheaves of length $n$ on $X$. To do so, we review the construction of the support map $\mathscr{C}oh^n(X) \to \mathrm{Sym}^n(X)$ to the symmetric product and we prove that, for any closed point $p \in X$, the punctual stack $\mathscr{C}oh^n(X)_p$ parametrising sheaves supported at $p$ only depends on a formal neighbourhood of $p$. We perform the same analysis for the Quot-to-Chow morphism $\mathrm{Quot}_X(\mathcal E,n) \to \mathrm{Sym}^n(X)$, for a fixed sheaf $\mathcal E \in \mathrm{Coh}(X)$."
https://arxiv.org/abs/2403.07858,2024-03-12,Accelerating Biclique Counting on GPU,"['Linshan Qiu', 'Zhonggen Li', 'Xiangyu Ke', 'Lu Chen', 'Yunjun Gao']","Counting (p,q)-bicliques in bipartite graphs poses a foundational challenge with broad applications, from densest subgraph discovery in algorithmic research to personalized content recommendation in practical scenarios. Despite its significance, current leading (p,q)-biclique counting algorithms fall short, particularly when faced with larger graph sizes and clique scales. Fortunately, the problem's inherent structure, allowing for the independent counting of each biclique starting from every vertex, combined with a substantial set intersections, makes it highly amenable to parallelization. Recent successes in GPU-accelerated algorithms across various domains motivate our exploration into harnessing the parallelism power of GPUs to efficiently address the (p,q)-biclique counting challenge. We introduce GBC (GPU-based Biclique Counting), a novel approach designed to enable efficient and scalable (p,q)-biclique counting on GPUs. To address major bottleneck arising from redundant comparisons in set intersections (occupying an average of 90% of the runtime), we introduce a novel data structure that hashes adjacency lists into truncated bitmaps to enable efficient set intersection on GPUs via bit-wise AND operations. Our innovative hybrid DFS-BFS exploration strategy further enhances thread utilization and effectively manages memory constraints. A composite load balancing strategy, integrating pre-runtime and runtime workload allocation, ensures equitable distribution among threads. Additionally, we employ vertex reordering and graph partitioning strategies for improved compactness and scalability. Experimental evaluations on eight real-life and two synthetic datasets demonstrate that GBC outperforms state-of-the-art algorithms by a substantial margin. In particular, GBC achieves an average speedup of 497.8x, with the largest instance achieving a remarkable 1217.7x speedup when p = q = 8."
https://arxiv.org/abs/2403.07857,2024-03-12,Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias,"['Sierra Wyllie', 'Ilia Shumailov', 'Nicolas Papernot']","Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative training batches for stochastic gradient descent to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS. Our work takes an important step towards identifying, mitigating, and taking accountability for the unfair feedback loops enabled by the idea that ML systems are inherently neutral and objective."
https://arxiv.org/abs/2403.07856,2024-03-12,Quantum Support Vector Machine for Prostate Cancer Detection: A Performance Analysis,"['Walid El Maouaki', 'Taoufik Said', 'Mohamed Bennai']","This study addresses the urgent need for improved prostate cancer detection methods by harnessing the power of advanced technological solutions. We introduce the application of Quantum Support Vector Machine (QSVM) to this critical healthcare challenge, showcasing an enhancement in diagnostic performance over the classical Support Vector Machine (SVM) approach. Our study not only outlines the remarkable improvements in diagnostic performance made by QSVM over the classic SVM technique, but it delves into the advancements brought about by the quantum feature map architecture, which has been carefully identified and evaluated, ensuring it aligns seamlessly with the unique characteristics of our prostate cancer dataset. This architecture succeded in creating a distinct feature space, enabling the detection of complex, non-linear patterns in the data. The findings reveal not only a comparable accuracy with classical SVM ($92\%$) but also a $7.14\%$ increase in sensitivity and a notably high F1-Score ($93.33\%$). This study's important combination of quantum computing in medical diagnostics marks a pivotal step forward in cancer detection, offering promising implications for the future of healthcare technology."
https://arxiv.org/abs/2403.07855,2024-03-12,On t-structures adjacent and orthogonal to weight structures,['Mikhail V. Bondarko'],We study $t$-structures (on triangulated categories) that are closely related to weight structures.
https://arxiv.org/abs/2403.07854,2024-03-12,Distilling the Knowledge in Data Pruning,"['Emanuel Ben-Baruch', 'Adam Botach', 'Igor Kviatkovsky', 'Manoj Aggarwal', 'Gérard Medioni']","With the increasing size of datasets used for training neural networks, data pruning becomes an attractive field of research. However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data. By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. We first establish a theoretical motivation for employing self-distillation to improve training on pruned data. Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is comparable or superior to sophisticated pruning methods across all pruning regimes. On ImageNet for example, we achieve superior accuracy despite training on a random subset of only 50% of the data. Additionally, we demonstrate a crucial connection between the pruning factor and the optimal knowledge distillation weight. This helps mitigate the impact of samples with noisy labels and low-quality images retained by typical pruning algorithms. Finally, we make an intriguing observation: when using lower pruning fractions, larger teachers lead to accuracy degradation, while surprisingly, employing teachers with a smaller capacity than the student's may improve results. Our code will be made available."
https://arxiv.org/abs/2403.07853,2024-03-12,Improving Fairness in Photovoltaic Curtailments via Daily Topology Reconfiguration for Voltage Control in Power Distribution Networks,"['Rahul K. Gupta', 'Daniel K. Molzahn']","In PV-rich power distribution systems, over-voltage issues are often addressed by curtailing excess generation from PV plants (in addition to reactive power control), raising fairness concerns. Existing fairness-aware control schemes tackle this problem by incorporating fairness objectives into the cost function. However, such schemes result in increased overall curtailments. This paper proposes a solution through daily topology reconfiguration, ensuring that different PV plants face varying grid conditions each day, leading to different curtailment levels and enhancing fairness. We illustrate that implementing this approach enhances overall fairness without significantly increasing overall curtailments. The optimization problem involves two stages. The day-ahead stage optimizes the network topology using day-ahead forecasts of PV generation and demand, minimizing net curtailment and accounting for fairness based on curtailments from prior days. The real-time stage implements the optimized topology and computes active and reactive power setpoints for the PV plants. Day-ahead grid constraints are modeled using LinDistFlow, and real-time control employs a linearized model with a first-order Taylor approximation. The proposed scheme is numerically validated on several benchmark test cases. Results are compared using the Jain Fairness Index, considering fairness and reconfiguration scenarios."
https://arxiv.org/abs/2403.07852,2024-03-12,On the One-dimensional Singular Abreu Equations,['Young Ho Kim'],"Singular fourth-order Abreu equations have been used to approximate minimizers of convex functionals subject to a convexity constraint in dimensions higher than or equal to two. For Abreu type equations, they often exhibit different solvability phenomena in dimension one and dimensions at least two. We prove the analogues of these results for the variational problem and singular Abreu equations in dimension one, and use the approximation scheme to obtain a characterization of limiting minimizers to the one-dimensional variational problem."
https://arxiv.org/abs/2403.07851,2024-03-12,12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning,"['Yoga Esa Wibowo', 'Cristian Cioflan', 'Thorir Mar Ingolfsson', 'Michael Hersche', 'Leo Zhao', 'Abbas Rahimi', 'Luca Benini']","Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems to expand their inference capabilities to new classes using only a few labeled examples, without forgetting the previously learned classes. Classical backpropagation-based learning and its variants are often unsuitable for battery-powered, memory-constrained systems at the extreme edge. In this work, we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a lightweight model consisting of a pretrained and metalearned feature extractor and an expandable explicit memory storing the class prototypes. The architecture is pretrained with a novel feature orthogonality regularization and metalearned with a multi-margin loss. For learning a new class, our approach extends the explicit memory with novel class prototypes, while the remaining architecture is kept frozen. This allows learning previously unseen classes based on only a few examples with one single pass (hence online). O-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark, achieving state-of-the-art results. Tailored for ultra-low-power platforms, we implement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online learning capabilities within just 12 mJ per new class."
https://arxiv.org/abs/2403.07850,2024-03-12,Laser-written waveguide-integrated coherent spins in diamond,"['Yanzhao Guo', 'John P. Hadden', 'Federico Gorrini', 'Giulio Coccia', 'Vibhav Bharadwaj', 'Vinaya Kumar Kavatamane', 'Mohammad Sahnawaz Alam', 'Roberta Ramponi', 'Paul E. Barclay', 'Andrea Chiappini', 'Maurizio Ferrari', 'Alexander Kubanek', 'Angelo Bifone', 'Shane M. Eaton', 'Anthony J. Bennett']","Quantum emitters, such as the negatively charged nitrogen-vacancy center in diamond, are attractive for quantum technologies such as nano-sensing, quantum information processing, and as a non-classical light source. However, it is still challenging to position individual emitters in photonic structures whilst preserving the spin coherence properties of the defect. In this paper, we investigate single and ensemble waveguide-integrated nitrogen-vacancy centers in diamond fabricated by femtosecond laser writing followed by thermal annealing. Their spin coherence properties are systematically investigated and are shown to be comparable to native nitrogen-vacancy centers in diamond. This method paves the way for the fabrication of coherent spins integrated within photonic devices."
https://arxiv.org/abs/2403.07849,2024-03-12,Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations,"['Harish G. Naik', 'Jan Polster', 'Raj Shekhar', 'Tamás Horváth', 'György Turán']","We formulate an XAI-based model improvement approach for Graph Neural Networks (GNNs) for node classification, called Explanation Enhanced Graph Learning (EEGL). The goal is to improve predictive performance of GNN using explanations. EEGL is an iterative self-improving algorithm, which starts with a learned ""vanilla"" GNN, and repeatedly uses frequent subgraph mining to find relevant patterns in explanation subgraphs. These patterns are then filtered further to obtain application-dependent features corresponding to the presence of certain subgraphs in the node neighborhoods. Giving an application-dependent algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL) algorithm has previously been posed as an open problem. We present experimental evidence, with synthetic and real-world data, which show that EEGL outperforms related approaches in predictive performance and that it has a node-distinguishing power beyond that of vanilla GNNs. We also analyze EEGL's training dynamics."
https://arxiv.org/abs/2403.07848,2024-03-12,Infinite tower of higher-curvature corrections: Quasinormal modes and late-time behavior of D-dimensional regular black holes,"['R. A. Konoplya', 'A. Zhidenko']","Recently, Bueno, Cano, and Hennigar [arXiv: 2403.04827] proposed a generic approach for incorporating an infinite tower of higher curvature corrections into the Einstein theory. In this study, we compute quasinormal modes for certain regular D-dimensional black holes resulting from this infinite series of higher curvature corrections, specifically focusing on the D-dimensional extensions of the Bardeen and Hayward black holes. We demonstrate that while the fundamental mode is minimally affected by moderate coupling constants, the higher overtones exhibit significant sensitivity even to small coupling values, yielding unconventional modes characterized by vanishing real oscillation frequencies."
https://arxiv.org/abs/2403.07847,2024-03-12,On measuring the topological charge of anyons,['Andrey Morozov'],In this paper we discuss the principles of measuring topological charge or representation traveling in the set of anyons. We describe the procedure and analyze how it works for the different values of parameters of the theory. We also show how it can be modified to be more effective for different levels of Chern-Simons theory.
https://arxiv.org/abs/2403.07846,2024-03-12,Antiferromagnetic magnons on a Möbius strip: topology-induced symmetry breaking,"['Kuangyin Deng', 'Ran Cheng']","We study a Möbius strip comprising of two antiferromagnetically coupled spin chains. To satisfy the boundary condition, magnon excitations feature linear polarization of the Néel vector devoid of chirality, forming two non-degenerate branches of modes that can neither be smoothly connected to nor be decomposed by the circularly-polarized magnons of opposite chirality commonly found in antiferromagnets. Only one branch supports standing-wave formation on the Möbius strip while the other does not, owing to its spectral shift incurred by the boundary condition. Our findings unravel the profound impact of topology-induced symmetry breaking on magnons."
https://arxiv.org/abs/2403.07845,2024-03-12,Hyper-density functional theory of soft matter,"['Florian Sammüller', 'Silas Robitschko', 'Sophie Hermann', 'Matthias Schmidt']","We present a scheme for investigating arbitrary thermal observables in spatially inhomogeneous many-body systems. Extending the equilibrium ensemble yields any given observable as an explicit hyper-density functional. Associated local fluctuation profiles follow from an exact hyper-Ornstein-Zernike equation. Simulation-based supervised machine learning trains neural networks that act as hyper-direct correlation functionals which facilitate efficient and accurate predictions. We exemplify the approach for the cluster statistics of hard rods and square well particles. The theory provides access to complex order parameters, as is impossible in standard density functional theory."
https://arxiv.org/abs/2403.07844,2024-03-12,A twist-grain boundary (TGB) phase in aqueous solutions of the DNA tetramer GTAC,"['Gregory P. Smith', 'Chenhui Zhu', 'Mikhail Zernenkov', 'Guillaume Freychet', 'Noel A. Clark']","We report the observation of a Twist Grain Boundary (TGB) phase of DNA, a striking motif of three dimensional (3D) equilibrium self-assembly of the DNA tetramer 5'-GTAC-3', a base sequence that is self-complimentary, pairing to form 4-base long, blunt-end Watson/Crick (WC) duplexes. Hydrophobic blunt ends and liquid crystal ordering enable these short duplexes to aggregate into long-DNA-like columns, even though the double helix has a break every 4 bases. A further step assembles these columns into monolayer sheets in which the columns are mutually parallel, and, finally, these sheets stack into lamellar arrays in which the column axis of each layer rotates in helical fashion, through a 60 degree angle with each passage to the next layer. This reorientation in a left-handed TGB helix enables each 2nm diameter WC column to be parallel to, and to partially enter, the major grooves of its neighbors."
https://arxiv.org/abs/2403.07843,2024-03-12,A Machine learning and Empirical Bayesian Approach for Predictive Buying in B2B E-commerce,"['Tuhin Subhra De', 'Pranjal Singh', 'Alok Patel']","In the context of developing nations like India, traditional business to business (B2B) commerce heavily relies on the establishment of robust relationships, trust, and credit arrangements between buyers and sellers. Consequently, ecommerce enterprises frequently. Established in 2016 with a vision to revolutionize trade in India through technology, Udaan is the countrys largest business to business ecommerce platform. Udaan operates across diverse product categories, including lifestyle, electronics, home and employ telecallers to cultivate buyer relationships, streamline order placement procedures, and promote special promotions. The accurate anticipation of buyer order placement behavior emerges as a pivotal factor for attaining sustainable growth, heightening competitiveness, and optimizing the efficiency of these telecallers. To address this challenge, we have employed an ensemble approach comprising XGBoost and a modified version of Poisson Gamma model to predict customer order patterns with precision. This paper provides an in-depth exploration of the strategic fusion of machine learning and an empirical Bayesian approach, bolstered by the judicious selection of pertinent features. This innovative approach has yielded a remarkable 3 times increase in customer order rates, show casing its potential for transformative impact in the ecommerce industry."
https://arxiv.org/abs/2403.07842,2024-03-12,Quantifying and Mitigating Privacy Risks for Tabular Generative Models,"['Chaoyi Zhu', 'Jiayi Tang', 'Hans Brouwer', 'Juan F. Pérez', 'Marten van Dijk', 'Lydia Y. Chen']","Synthetic data from generative models emerges as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models. Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder network to encode the tabular data and a latent diffusion model to synthesize the latent tables. Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms. Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data. Specifically, compared to other DP-protected tabular generative models, DP-TLDM improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk."
https://arxiv.org/abs/2403.07841,2024-03-12,Critical metrics of eigenvalue functionals via Clarke subdifferential,"['Romain Petrides', 'David Tewodrose']","We set up a new framework to study critical points of functionals defined as combinations of eigenvalues of operators with respect to a given set of parameters: Riemannian metrics, potentials, etc. Our setting builds upon Clarke's differentiation theory to provide a novel understanding of critical metrics. In particular, we unify and refine previous research carried out on Laplace and Steklov eigenvalues. We also use our theory to tackle original examples such as the conformal GJMS operators, the conformal Laplacian, and the Laplacian with mixed boundary conditions."
https://arxiv.org/abs/2403.07840,2024-03-12,Programming droplet motion using metamaterials,"['Mohammad Charara', 'Zak Kujala', 'Sungyon Lee', 'Stefano Gonella']","Motion control of droplets has generated much attention for its applications to microfluidics, where precise control of small fluid volumes is an imperative requirement. Mechanical vibrations have been shown to be effective at inducing controllable depinning, and activation of different drop motion regimes. However, existing vibration-based strategies involve establishing homogeneous rigid-body dynamics on the substrate, and therefore lack any form of spatial heterogeneity and tuning. Addressing this limitation, metamaterials provide an ideal platform to achieve spectrally and spatially selective drop motion control, which leverages their ability to attenuate vibrations in selected frequency bands and in selected regions of a substrate. In this work, we illustrate the potential of metamaterials-based drop control by experimentally demonstrating a variety of drop motion capabilities on the surface of metaplates endowed with locally resonant stubs. The experiments leverage the design versatility of a LEGO component-enabled reconfigurable design platform and laser vibrometry measurements with high spatial resolution."
https://arxiv.org/abs/2403.07839,2024-03-12,MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric,"['Haokun Lin', 'Haoli Bai', 'Zhili Liu', 'Lu Hou', 'Muyi Sun', 'Linqi Song', 'Ying Wei', 'Zhenan Sun']","Vision-language pre-trained models have achieved impressive performance on various downstream tasks. However, their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance. Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks. In this paper, we first propose the Module-wise Pruning Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks. Using the MoPE metric, we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly reducing pre-training costs while maintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning from width to depth yields highly competitive task-specific models. Extensive experiments in two stages demonstrate the effectiveness of the MoPE metric, and MoPE-CLIP outperforms previous state-of-the-art VLP compression methods."
https://arxiv.org/abs/2403.07838,2024-03-12,MPCPA: Multi-Center Privacy Computing with Predictions Aggregation based on Denoising Diffusion Probabilistic Model,"['Guibo Luo', 'Hanwen Zhang', 'Xiuling Wang', 'Mingzhi Chen', 'Yuesheng Zhu']","Privacy-preserving computing is crucial for multi-center machine learning in many applications such as healthcare and finance. In this paper a Multi-center Privacy Computing framework with Predictions Aggregation (MPCPA) based on denoising diffusion probabilistic model (DDPM) is proposed, in which conditional diffusion model training, DDPM data generation, a classifier, and strategy of prediction aggregation are included. Compared to federated learning, this framework necessitates fewer communications and leverages high-quality generated data to support robust privacy computing. Experimental validation across multiple datasets demonstrates that the proposed framework outperforms classic federated learning and approaches the performance of centralized learning with original data. Moreover, our approach demonstrates robust security, effectively addressing challenges such as image memorization and membership inference attacks. Our experiments underscore the efficacy of the proposed framework in the realm of privacy computing, with the code set to be released soon."
https://arxiv.org/abs/2403.07837,2024-03-12,Topological Protection of Optical Skyrmions through Complex Media,"['An Aloysius Wang', 'Zimo Zhao', 'Yifei Ma', 'Yuxi Cai', 'Tade Marozsak', 'Binguo Chen', 'Honghui He', 'Lin Luo', 'Martin J Booth', 'Steve J Elston', 'Stephen M Morris', 'Chao He']","Recent experimental realizations of optical Skyrmions through the techniques of structured light have opened the doors to a completely new way of representing data in electromagnetic fields, namely its topology. Apart from potentially enhancing the bandwidth of optical systems, the intrinsically discrete nature of the topological number allows Skyrmions to naturally interface with the digital world. However, investigations into the topological protection of optical Skyrmions through various media remain limited to date. Here, we rigorously define the optical Skyrmion and establish a framework that can be used to analyze the effects of complex media on the topology of Skyrmion fields. Using this framework, we provide simple criteria for spatially varying retarders, diattenuators, depolarizers, and combinations of the former under which topological protection is guaranteed. We then present experimental results validating the robustness of the Skyrmion number against corrupting media and discuss ways of extending the optical Skyrmion to more general settings. We believe that the work presented in this paper provides a theoretical underpinning for the use of Skyrmions in practical applications ranging from optical communications to photonic computing."
https://arxiv.org/abs/2403.07836,2024-03-12,Syncopated Dynamical Decoupling for Suppressing Crosstalk in Quantum Circuits,"['Bram Evert', 'Zoe Gonzalez Izquierdo', 'James Sud', 'Hong-Ye Hu', 'Shon Grabbe', 'Eleanor G. Rieffel', 'Matthew J. Reagor', 'Zhihui Wang']","Theoretically understanding and experimentally characterizing and modifying the underlying Hamiltonian of a quantum system is of utmost importance in achieving high-fidelity quantum gates for quantum computing. In this work, we explore the use of dynamical decoupling (DD) in characterizing undesired two-qubit couplings as well as the underlying single-qubit decoherence, and in suppressing them. We develop a syncopated dynamical decoupling technique which protects against decoherence and selectively targets unwanted two-qubit interactions, overcoming both significant hurdles to achieving precise quantum control and realizing quantum computing on many hardware prototypes. On a transmon-qubit-based superconducting quantum device, we identify separate white and $1/f$ noise components underlying the single-qubit decoherence and a static ZZ coupling between pairs of qubits. We suppress these errors using syncopated dynamical decoupling in two-qubit benchmarking experiments and significantly boost performance in a realistic algorithmic quantum circuit."
https://arxiv.org/abs/2403.07835,2024-03-12,On Modeling Adequacy and Stability Analysis of IBR-related Subsynchronous Oscillations in Multimachine Systems,"['Lilan Karunaratne', 'Nilanjan Ray Chaudhuri', 'Amirthagunaraj Yogarathnam', 'Meng Yue']","Time-varying phasor-based analysis of subsynchronous oscillations (SSOs) involving grid-following converters (GFLCs) and its benchmarking with electromagnetic transient (EMT) models have so far been restricted to highly simplified grid models with constant voltage sources behind series R-L circuits. In this paper, modeling adequacy of bulk power systems with synchronous generators (SGs), transmission systems, loads, and GFLCs are considered. To this end, we revisit the notions of time-varying phasor calculus, highlighting the distinction between space-phasor-calculus (SPC) and two often interchangeably used frameworks namely baseband-abc and generalized averaging. We present the models of grids in SPC framework that include transmission line dynamics, load dynamics, and SG stator transients. Next, we propose a generic approach to study modeling adequacy in small-signal sense by (a) identifying critical modes through eigenvalue and singular value analysis followed by (b) using weighted maximum singular value error magnitudes as metrics, and (c) further cross-validation. Using a modified 4-machine IEEE benchmark model with up to 3 GFLCs we show that SPC framework can be used for analysis of SSOs. Further, we consider the quasistationary phasor calculus (QPC) framework that neglects transmission line, load, and SG stator dynamics to show its adequacy in SSO modeling and analysis. Time-domain and frequency-domain results with EMT models are also presented."
https://arxiv.org/abs/2403.07834,2024-03-12,When Eye-Tracking Meets Machine Learning: A Systematic Review on Applications in Medical Image Analysis,"['Sahar Moradizeyveh', 'Mehnaz Tabassum', 'Sidong Liu', 'Robert Ahadizad Newport', 'Amin Beheshti', 'Antonio Di Ieva']","Eye-gaze tracking research offers significant promise in enhancing various healthcare-related tasks, above all in medical image analysis and interpretation. Eye tracking, a technology that monitors and records the movement of the eyes, provides valuable insights into human visual attention patterns. This technology can transform how healthcare professionals and medical specialists engage with and analyze diagnostic images, offering a more insightful and efficient approach to medical diagnostics. Hence, extracting meaningful features and insights from medical images by leveraging eye-gaze data improves our understanding of how radiologists and other medical experts monitor, interpret, and understand images for diagnostic purposes. Eye-tracking data, with intricate human visual attention patterns embedded, provides a bridge to integrating artificial intelligence (AI) development and human cognition. This integration allows novel methods to incorporate domain knowledge into machine learning (ML) and deep learning (DL) approaches to enhance their alignment with human-like perception and decision-making. Moreover, extensive collections of eye-tracking data have also enabled novel ML/DL methods to analyze human visual patterns, paving the way to a better understanding of human vision, attention, and cognition. This systematic review investigates eye-gaze tracking applications and methodologies for enhancing ML/DL algorithms for medical image analysis in depth."
https://arxiv.org/abs/2403.07833,2024-03-12,A Science4Peace initiative: Alleviating the consequences of sanctions in international scientific cooperation,"['A. Ali', 'M. Barone', 'S. Brentjes', 'D. Britzger', 'M. Dittmar', 'T. Ekelöf', 'J. Ellis', 'S. Fonseca de Souza', 'A. Glazov', 'A. V. Gritsan', 'R. Hoffmann', 'H. Jung', 'M. Klein', 'V. Klyukhin', 'V. Korbel', 'P. Kokkas', 'P. Kostka', 'U. Langenegger', 'J. List', 'N. Raicevic', 'A. Rostovtsev', 'A. Sabio Vera', 'M. Spiro', 'G. Tonelli', 'P. van Mechelen']","The armed invasion of Ukraine by the Russian Federation has adversely affected the relations between Russia and Western countries. Among other aspects, it has put scientific cooperation and collaboration into question and changed the scientific landscape significantly. Cooperation between some Western institutions and their Russian and Belarusian partners were put on hold after February 24, 2022. The CERN Council decided at its meeting in December 2023 to terminate cooperation agreements with Russia and Belarus that date back a decade. CERN is an international institution with UN observer status, and has so far played a role in international cooperation which was independent of national political strategies. We argue that the Science4Peace idea still has a great value and scientific collaboration between scientists must continue, since fundamental science is by its nature an international discipline. A ban of scientists participating in international cooperation and collaboration is against the traditions, requirements and understanding of science. We call for measures to reactivate the peaceful cooperation of individual scientists on fundamental research in order to stimulate international cooperation for a more peaceful world in the future. Specifically, we plead for finding ways to continue this cooperation through international organizations, such as CERN and JINR."
https://arxiv.org/abs/2403.07832,2024-03-12,"DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for Adaptive and Minimally Deforming Grasp Policies","['William Xie', 'Jensen Lavering', 'Nikolaus Correll']","Large language models (LLMs) can provide rich physical descriptions of most worldly objects, allowing robots to achieve more informed and capable grasping. We leverage LLMs' common sense physical reasoning and code-writing abilities to infer an object's physical characteristics--mass $m$, friction coefficient $μ$, and spring constant $k$--from a semantic description, and then translate those characteristics into an executable adaptive grasp policy. Using a current-controllable, two-finger gripper with a built-in depth camera, we demonstrate that LLM-generated, physically-grounded grasp policies outperform traditional grasp policies on a custom benchmark of 12 delicate and deformable items including food, produce, toys, and other everyday items, spanning two orders of magnitude in mass and required pick-up force. We also demonstrate how compliance feedback from DeliGrasp policies can aid in downstream tasks such as measuring produce ripeness. Our code and videos are available at: https://deligrasp.github.io"
https://arxiv.org/abs/2403.07831,2024-03-12,Utilizing Load Shifting for Optimal Compressor Sequencing in Industrial Refrigeration,"['Rohit Konda', 'Vikas Chandan', 'Jesse Crossno', 'Blake Pollard', 'Dan Walsh', 'Rick Bohonek', 'Jason R. Marden']","The ubiquity and energy needs of industrial refrigeration has prompted several research studies investigating various control opportunities for reducing energy demand. This work focuses on one such opportunity, termed compressor sequencing, which entails intelligently selecting the operational state of the compressors to service the required refrigeration load with the least possible work. We first study the static compressor sequencing problem and observe that deriving the optimal compressor operational state is computationally challenging and can vary dramatically based on the refrigeration load. Thus we introduce load shifting in conjunction with compressor sequencing, which entails strategically precooling the facility to allow for more efficient compressor operation. Interestingly, we show that load shifting not only provides benefits in computing the optimal compressor operational state, but also can lead to significant energy savings. Our results are based on and compared to real-world sensor data from an operating industrial refrigeration site of Butterball LLC located in Huntsville, AR, which demonstrated that without load shifting, even optimal compressor operation results in compressors often running at intermediate capacity levels, which can lead to inefficiencies. Through collected data, we demonstrate that a load shifting approach for compressor sequencing has the potential to reduce energy use of the compressors up to 20% compared to optimal sequencing without load shifting."
https://arxiv.org/abs/2403.07830,2024-03-12,"Parity questions in critical planar Brownian loop-soups (or ""where did the free planar bosons go?"")","['Matthis Lehmkuehler', 'Wei Qian', 'Wendelin Werner']","The critical two-dimensional Brownian loop-soup is an infinite collection of non-interacting Brownian loops in a planar domain that possesses some combinatorial features related to the notion of indistinguishability of bosons. The properly renormalized occupation time field of this collection of loops is known to be distributed like the properly defined square of a Gaussian free field. In the present paper, we investigate aspects of the question about how much information these fields provide about the loop-soup. Among other things, we show that the exact set of points that are actually visited by some loops in the loop-soup is not determined by these fields. We further prove that given the fields, a dense family of special points will each have a conditional probability 1/2 of being part of the loop-soup. We also exhibit another instance where the possible decompositions (given the field) into individual loops and excursions can be grouped into two clearly different groups, each having a conditional probability 1/2 of occurring."
https://arxiv.org/abs/2403.07829,2024-03-12,"Convex cones, assessment functions, balanced attributes",['Ignacy Kaliszewski'],"We investigate a class of polyhedral convex cones, with $R^k_+$ (the nonegative orthant in $\mathbb{R}^k$) as a special case. We start with the observation that for convex cones contained in $\mathbb{R}^k$, the respective cone efficiency is inconsistent with the Pareto efficiency, the latter being deeply rooted in economics, the decision theory, and the multiobjective optimization theory. Despite that, we argue that convex cones contained in $\mathbb{R}^k$ and the respective cone efficiency are also relevant to these domains."
https://arxiv.org/abs/2403.07828,2024-03-12,From Files to Streams: Revisiting Web History and Exploring Potentials for Future Prospects,"['Lucas Vogel', 'Thomas Springer', 'Matthias Wählisch']","Over the last 30 years, the World Wide Web has changed significantly. In this paper, we argue that common practices to prepare web pages for delivery conflict with many efforts to present content with minimal latency, one fundamental goal that pushed changes in the WWW. To bolster our arguments, we revisit reasons that led to changes of HTTP and compare them systematically with techniques to prepare web pages. We found that the structure of many web pages leverages features of HTTP/1.1 but hinders the use of recent HTTP features to present content quickly. To improve the situation in the future, we propose fine-grained content segmentation. This would allow to exploit streaming capabilities of recent HTTP versions and to render content as quickly as possible without changing underlying protocols or web browsers."
https://arxiv.org/abs/2403.07827,2024-03-12,Affine Gateaux Differentials and the von Mises Statistical Calculus,"['Simone Cerreia-Vioglio', 'Fabio Maccheroni', 'Massimo Marinacci', 'Luigi Montrucchio', 'Lorenzo Stanca']","This paper presents a general study of one-dimensional differentiability for functionals on convex domains that are not necessarily open. The local approximation is carried out by affine functionals, rather than linear ones as in standard Gateaux differentiability. This affine notion of differentiability naturally arises in many applications and, here and there, it appeared in the literature. Our systematic analysis aims to give a general perspective on it."
https://arxiv.org/abs/2403.07826,2024-03-12,Computational modelling of complex multiphase behavior of environmentally-friendly materials for sustainable technological solutions,"['Akshayveer Akshayveer', 'Federico C Buroni', 'Roderick Melnik', 'Luis Rodriguez-Tembleque', 'Andres Saez', 'Sundeep Singh']","This research introduces a detailed computational framework designed to analyze and forecast the complex multiphase characteristics of eco-friendly lead-free piezoelectric materials, which are essential for developing sustainable technological advancements. Lead-free piezoelectric materials have a significant thermo-electromechanical response, although their electromechanical characteristics vary throughout different phases of the material. Lead-free piezoelectric materials undergo phase changes, including rhombohedral (R3c), orthorhombic (Pnma), tetragonal (P4bm), and cubic (Cc) phases, when the temperature changes. These phases are determined by the symmetry and alignment of the ferroelectric domains. Furthermore, multiple phases exist simultaneously under certain temperature, electrical, and mechanical conditions, resulting in the material displaying intricate multiphase behavior. Studying such behaviour is crucial for evaluating the performance of these materials. The computational approach in this research relies on Landau-Ginzburg-Devonshire theory to model micro-domain phase changes in the material. This research will enhance our comprehension of the significance of complex multiphase behaviour in creating environment-friendly and sustainable technological solutions."
https://arxiv.org/abs/2403.07825,2024-03-12,The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing,"['Jianchen Wang', 'Zhouhong Gu', 'Zhuozhi Xiong', 'Hongwei Feng', 'Yanghua Xiao']","Large Language Models have revolutionized numerous tasks with their remarkable efficacy.However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space. This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing. Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect. Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods. However, our proposed methods, GORA and SORA, effectively identify and alleviate this issue, respectively, contributing to the advancement of LLM editing techniques."
https://arxiv.org/abs/2403.07824,2024-03-12,Preconditioners based on Voronoi quantizers of random variable coefficients for stochastic elliptic partial differential equations,"['Nicolas Venkovic', 'Paul Mycek', 'Olivier Le Maître', 'Luc Giraud']","A preconditioning strategy is proposed for the iterative solve of large numbers of linear systems with variable matrix and right-hand side which arise during the computation of solution statistics of stochastic elliptic partial differential equations with random variable coefficients sampled by Monte Carlo. Building on the assumption that a truncated Karhunen-Loève expansion of a known transform of the random variable coefficient is known, we introduce a compact representation of the random coefficient in the form of a Voronoi quantizer. The number of Voronoi cells, each of which is represented by a centroidal variable coefficient, is set to the prescribed number $P$ of preconditioners. Upon sampling the random variable coefficient, the linear system assembled with a given realization of the coefficient is solved with the preconditioner whose centroidal variable coefficient is the closest to the realization. We consider different ways to define and obtain the centroidal variable coefficients, and we investigate the properties of the induced preconditioning strategies in terms of average number of solver iterations for sequential simulations, and of load balancing for parallel simulations. Another approach, which is based on deterministic grids on the system of stochastic coordinates of the truncated representation of the random variable coefficient, is proposed with a stochastic dimension which increases with the number $P$ of preconditioners. This approach allows to bypass the need for preliminary computations in order to determine the optimal stochastic dimension of the truncated approximation of the random variable coefficient for a given number of preconditioners."
https://arxiv.org/abs/2403.07823,2024-03-12,Time-discretization method for a multi-term time fractional differential equation with delay,"['Areefa Khatoon', 'Abdur Raheem', 'Asma Afreen']","This paper discusses a multi-term time-fractional delay differential equation in a real Hilbert space. An iterative scheme for a multi-term time-fractional differential equation is established using Rothe's method. The method of semi-discretization is extended to this kind of time fractional problem with delay in the case that the time delay parameter $ν>0$ satisfies $ν\leq T$, where $T$ denotes the final time. We apply the accretivity of the operator $A$ in an iterative scheme to establish the existence and regularity of strong solutions to the considered problem. Finally, an example is provided to demonstrate the abstract result."
https://arxiv.org/abs/2403.07822,2024-03-12,Fusing Climate Data Products using a Spatially Varying Autoencoder,"['Jacob A. Johnson', 'Matthew J. Heaton', 'William F. Christensen', 'Lynsie R. Warr', 'Summer B. Rupper']","Autoencoders are powerful machine learning models used to compress information from multiple data sources. However, autoencoders, like all artificial neural networks, are often unidentifiable and uninterpretable. This research focuses on creating an identifiable and interpretable autoencoder that can be used to meld and combine climate data products. The proposed autoencoder utilizes a Bayesian statistical framework, allowing for probabilistic interpretations while also varying spatially to capture useful spatial patterns across the various data products. Constraints are placed on the autoencoder as it learns patterns in the data, creating an interpretable consensus that includes the important features from each input. We demonstrate the utility of the autoencoder by combining information from multiple precipitation products in High Mountain Asia."
https://arxiv.org/abs/2403.07821,2024-03-12,Augmenting Interpolation-Based Model Checking with Auxiliary Invariants (Extended Version),"['Dirk Beyer', 'Po-Chun Chien', 'Nian-Ze Lee']","Software model checking is a challenging problem, and generating relevant invariants is a key factor in proving the safety properties of a program. Program invariants can be obtained by various approaches, including lightweight procedures based on data-flow analysis and intensive techniques using Craig interpolation. Although data-flow analysis runs efficiently, it often produces invariants that are too weak to prove the properties. By contrast, interpolation-based approaches build strong invariants from interpolants, but they might not scale well due to expensive interpolation procedures. Invariants can also be injected into model-checking algorithms to assist the analysis. Invariant injection has been studied for many well-known approaches, including k-induction, predicate abstraction, and symbolic execution. We propose an augmented interpolation-based verification algorithm that injects external invariants into interpolation-based model checking (McMillan, 2003), a hardware model-checking algorithm recently adopted for software verification. The auxiliary invariants help prune unreachable states in Craig interpolants and confine the analysis to the reachable parts of a program. We implemented the proposed technique in the verification framework CPAchecker and evaluated it against mature SMT-based methods in CPAchecker as well as other state-of-the-art software verifiers. We found that injecting invariants reduces the number of interpolation queries needed to prove safety properties and improves the run-time efficiency. Consequently, the proposed invariant-injection approach verified difficult tasks that none of its plain version (i.e., without invariants), the invariant generator, or any compared tools could solve."
https://arxiv.org/abs/2403.07820,2024-03-12,The Variant of Designated Verifier Signature Scheme with Message Recovery,"['Hong-Sheng Huang', 'Yu-Lei Fu', 'Han-Yu Lin']","In this work, we introduce a strong Designated Verifier Signature (DVS) scheme that incorporates a message recovery mechanism inspired by the concept of the Universal Designated Verifier Signature (UDVS) scheme. It is worth noting that Saeednia's strong designated verifier signature scheme fails to guarantee the privacy of the signature, making it unsuitable for certain applications such as medical record certificates or voting systems. To overcome this limitation, we extend Lee's strong designated verifier signature with a message recovery scheme to develop a universal designated verifier signature scheme. This universal designated verifier scheme is crafted to safeguard the privacy of signature holders, ensuring that only designated verifiers can authenticate the true signer and recover the messages."
https://arxiv.org/abs/2403.07819,2024-03-12,NPCoronaPredict: A computational pipeline for the prediction of the nanoparticle-biomolecule corona,"['Ian Rouse', 'David Power', 'Julia Subbotina', 'Vladimir Lobaskin']","The corona of a nanoparticle immersed in a biological fluid is of key importance to its eventual fate and bioactivity in the environment or inside live tissues. It is critical to have insight into both the underlying bionano interactions and the corona composition to ensure biocompatibility of novel engineered nanomaterials. A prediction of these properties in silico requires the successful spanning of multiple orders of magnitude of both time and physical dimensions to produce results in a reasonable amount of time, necessitating the development of a multiscale modelling approach. Here, we present the NPCoronaPredict open-source software package: a suite of software tools to enable this prediction for complex multi-component nanomaterials in essentially arbitrary biological fluids, or more generally any medium containing organic molecules. The package integrates several recent physics-based computational models and a library of both physics-based and data-driven parameterisations for nanomaterials and organic molecules. We describe the underlying theoretical background and the package functionality from the design of multi-component NPs through to the evaluation of the corona."
https://arxiv.org/abs/2403.07818,2024-03-12,Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling,"['Iman Islam', 'Esther Puyol-Antón', 'Bram Ruijsink', 'Andrew J. Reader', 'Andrew P. King']","Echocardiography (echo) is the first imaging modality used when assessing cardiac function. The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process. However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.). To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets. A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled. Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data. In this paper we show that training naively with such a loss function and multiple diverse datasets can lead to a form of shortcut learning, where the model associates label presence with domain characteristics, leading to a drop in performance. To address this problem, we propose a novel label dropout scheme to break the link between domain characteristics and the presence or absence of labels. We demonstrate that label dropout improves echo segmentation Dice score by 62% and 25% on two cardiac structures when training using multiple diverse partially labelled datasets."
https://arxiv.org/abs/2403.07817,2024-03-12,UniHand: Privacy-preserving Universal Handover for Small-Cell Networks in 5G-enabled Mobile Communication with KCI Resilience,"['Rabiah Alnashwan', 'Prosanta Gope', 'Benjamin Dowling']","Introducing Small Cell Networks (SCN) has significantly improved wireless link quality, spectrum efficiency and network capacity, which has been viewed as one of the key technologies in the fifth-generation (5G) mobile network. However, this technology increases the frequency of handover (HO) procedures caused by the dense deployment of cells in the network with reduced cell coverage, bringing new security and privacy issues. The current 5G-AKA and HO protocols are vulnerable to security weaknesses, such as the lack of forward secrecy and identity confusion attacks. The high HO frequency of HOs might magnify these security and privacy concerns in the 5G mobile network. This work addresses these issues by proposing a secure privacy-preserving universal HO scheme ($\UniHand$) for SCNs in 5G mobile communication. $\UniHand$ can achieve mutual authentication, strong anonymity, perfect forward secrecy, key-escrow-free and key compromise impersonation (KCI) resilience. To the best of our knowledge, this is the \textit{first} scheme to achieve secure, privacy-preserving universal HO with \textit{KCI} resilience for roaming users in 5G environment. We demonstrate that our proposed scheme is resilient against all the essential security threats by performing a comprehensive formal security analysis and conducting relevant experiments to show the cost-effectiveness of the proposed scheme."
https://arxiv.org/abs/2403.07816,2024-03-12,Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM,"['Sainbayar Sukhbaatar', 'Olga Golovneva', 'Vasu Sharma', 'Hu Xu', 'Xi Victoria Lin', 'Baptiste Rozière', 'Jacob Kahn', 'Daniel Li', 'Wen-tau Yih', 'Jason Weston', 'Xian Li']","We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff."
https://arxiv.org/abs/2403.07815,2024-03-12,Chronos: Learning the Language of Time Series,"['Abdul Fatir Ansari', 'Lorenzo Stella', 'Caner Turkmen', 'Xiyuan Zhang', 'Pedro Mercado', 'Huibin Shen', 'Oleksandr Shchur', 'Syama Sundar Rangapuram', 'Sebastian Pineda Arango', 'Shubham Kapoor', 'Jasper Zschiegner', 'Danielle C. Maddix', 'Michael W. Mahoney', 'Kari Torkkola', 'Andrew Gordon Wilson', 'Michael Bohlke-Schneider', 'Yuyang Wang']","We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained specifically on them. Our results demonstrate that Chronos models can leverage time series data from diverse domains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines."
https://arxiv.org/abs/2403.07814,2024-03-12,Implications of tristability on localization phenomena: a necking bifurcation's tale,"['Edem Kossi Akakpo', 'Marc Haelterman', 'Francois Leo', 'Pedro Parra-Rivas']","We analyze the implication of tristability on localization phenomena in one-dimensional extended dissipative systems. In this context, localized states appear due to the interaction and locking of front waves connecting different extended states. In the tristable regime investigated here two extended uniform states coexist with one periodic Turing pattern. This scenario leads to the transition from the standard-homoclinic-snaking-related localized states associated with uniform-pattern bistability to the collapsed-homoclinic-snaking-related states which arise in a uniform-bistable configuration. We find that this transition is mediated by the emergence of hybrid states through codimension-two necking bifurcations. To perform this study we use bifurcation analysis on a non-variational mean-field model describing the spatiotemporal dynamics of light pulses in passive Kerr cavities."
https://arxiv.org/abs/2403.07813,2024-03-12,Higher condensation theory,"['Liang Kong', 'Zhi-Hao Zhang', 'Jiaheng Zhao', 'Hao Zheng']","We develop a unified theory of defect condensations for topological orders in all dimensions based on higher categories, higher algebras and higher representations. We show that condensing a $k$-codimensional topological defect $A$ in an $n$+1D (potentially anomalous) topological order $\mathsf C^{n+1}$ amounts to a $k$-step process. In the first step, we condense $A$ along one of the transversal directions, thus obtaining a $(k-1)$-codimensional defect $ΣA$, which can be further condensed as the second step, so on and so forth. In the $k$-th step, condensing $Σ^{k-1}A$ along the only transversal direction defines a phase transition to a new phase $\mathsf D^{n+1}$. Mathematically, a $k$-codimensional defect $A$ is condensable if it is equipped with the structure of a condensable $E_k$-algebra. In this case, $ΣA$ is naturally a condensable $E_{k-1}$-algebra, thus it can be further condensed. The condensed phase $\mathsf D^{n+1}$ consists of all deconfined topological defects in $\mathsf C^{n+1}$. A $k$-codimensional topological defect is deconfined if and only if it is equipped with a $k$-dimensional $A$-action, which defines an $E_k$-module over $A$. When $\mathsf C^{n+1}$ is anomaly-free, the same condensation can be alternatively defined by replacing the last two steps by a single step of condensing the $E_2$-algebra $Σ^{k-2}A$ directly. The condensed phase $\mathsf D^{n+1}$ is determined by the category of $E_2$-modules over $Σ^{k-2}A$. When $n=2$, this modified last step is precisely a usual anyon condensation in a 2+1D topological order. The proofs of the most mathematical results will appear in a mathematical companion of this paper. We also briefly discuss some generalizations and applications that naturally arise from our condensation theory such as higher Morita theory, factorization homology and the condensation theory of non-topological defects."
https://arxiv.org/abs/2403.07812,2024-03-12,Probing quantum criticality in ferromagnetic CeRh6Ge4,"['S. M. Thomas', 'S. Seo', 'T. Asaba', 'F. Ronning', 'P. F. S. Rosa', 'E. D. Bauer', 'J. D. Thompson']","CeRh$_6$Ge$_4$ is unusual in that its ferromagnetic transition can be suppressed continuously to zero temperature, i.e., to a ferromagnetic quantum-critical point (QCP), through the application of modest hydrostatic pressure. This discovery has raised the possibility that the ferromagnetic QCP may be of the Kondo-breakdown type characterized by a jump in Fermi volume, to which thermopower S measurements should be sensitive. Though $S/T$ changes both sign and magnitude around the critical pressure P$_{c}\approx{}0.8$ GPa, these changes are not abrupt but extend over a pressure interval from within the ferromagnetic state up to P$_c$. Together with temperature and pressure variations in electrical resistivity and previously reported heat capacity, thermopower results point to the near coincidence of two sequential effects near P$_c$, delocalization of 4f degrees-of-freedom through orbital-selective hybridization followed by quantum criticality of itinerant ferromagnetism."
https://arxiv.org/abs/2403.07811,2024-03-12,Mesh Refinement with Early Termination for Dynamic Feasibility Problems,"['Eduardo M. G. Vila', 'Eric C. Kerrigan', 'Paul Bruce']","We propose a novel early-terminating mesh refinement strategy using an integrated residual method to solve dynamic feasibility problems. As a generalization of direct collocation, the integrated residual method is used to approximate an infinite-dimensional problem into a sequence of finite-dimensional optimization subproblems. Each subproblem in the sequence is a finer approximation of the previous. It is shown that these subproblems need not be solved to a high precision; instead, an early termination procedure can determine when mesh refinement should be performed. The new refinement strategy, applied to an inverted pendulum swing-up problem, outperforms a conventional refinement method by up to a factor of three in function evaluations."
https://arxiv.org/abs/2403.07810,2024-03-12,A geometric model for the module category of a string algebra,"['Karin Baur', 'Raquel Coelho Simoes']","In this paper, we give a geometric construction of string algebras and of their module categories. Our approach uses dissections of punctured Riemann surfaces with extra data at marked points, called labels. As an application, we give a classification of support tau-tilting modules in terms of arcs in such a tiled surface. In the case when the string algebra is gentle, we recover the classification given arXiv:2004.11136."
https://arxiv.org/abs/2403.07809,2024-03-12,pyvene: A Library for Understanding and Improving PyTorch Models via Interventions,"['Zhengxuan Wu', 'Atticus Geiger', 'Aryaman Arora', 'Jing Huang', 'Zheng Wang', 'Noah D. Goodman', 'Christopher D. Manning', 'Christopher Potts']","Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce $\textbf{pyvene}$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. $\textbf{pyvene}$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how $\textbf{pyvene}$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene."
https://arxiv.org/abs/2403.07808,2024-03-12,Supporting Error Chains in Static Analysis for Precise Evaluation Results and Enhanced Usability,"['Anna-Katharina Wickert', 'Michael Schlichtig', 'Marvin Vogel', 'Lukas Winter', 'Mira Mezini', 'Eric Bodden']","Context: Static analyses are well-established to aid in understanding bugs or vulnerabilities during the development process or in large-scale studies. A low false-positive rate is essential for the adaption in practice and for precise results of empirical studies. Unfortunately, static analyses tend to report where a vulnerability manifests rather than the fix location. This can cause presumed false positives or imprecise results. Method: To address this problem, we designed an adaption of an existing static analysis algorithm that can distinguish between a manifestation and fix location, and reports error chains. An error chain represents at least two interconnected errors that occur successively, thus building the connection between the fix and manifestation location. We used our tool CogniCryptSUBS for a case study on 471 GitHub repositories, a performance benchmark to compare different analysis configurations, and conducted an expert interview. Result: We found that 50 % of the projects with a report had at least one error chain. Our runtime benchmark demonstrated that our improvement caused only a minimal runtime overhead of less than 4 %. The results of our expert interview indicate that with our adapted version participants require fewer executions of the analysis. Conclusion: Our results indicate that error chains occur frequently in real-world projects, and ignoring them can lead to imprecise evaluation results. The runtime benchmark indicates that our tool is a feasible and efficient solution for detecting error chains in real-world projects. Further, our results gave a hint that the usability of static analyses may benefit from supporting error chains."
https://arxiv.org/abs/2403.07807,2024-03-12,StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting,"['Kunhao Liu', 'Fangneng Zhan', 'Muyu Xu', 'Christian Theobalt', 'Ling Shao', 'Shijian Lu']","We introduce StyleGaussian, a novel 3D style transfer technique that allows instant transfer of any image's style to a 3D scene at 10 frames per second (fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style transfer without compromising its real-time rendering ability and multi-view consistency. It achieves instant style transfer with three steps: embedding, transfer, and decoding. Initially, 2D VGG scene features are embedded into reconstructed 3D Gaussians. Next, the embedded features are transformed according to a reference style image. Finally, the transformed features are decoded into the stylized RGB. StyleGaussian has two novel designs. The first is an efficient feature rendering strategy that first renders low-dimensional features and then maps them into high-dimensional features while embedding VGG features. It cuts the memory consumption significantly and enables 3DGS to render the high-dimensional memory-intensive features. The second is a K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized features, it eliminates the 2D CNN operations that compromise strict multi-view consistency. Extensive experiments show that StyleGaussian achieves instant 3D stylization with superior stylization quality while preserving real-time rendering and strict multi-view consistency. Project page: https://kunhao-liu.github.io/StyleGaussian/"
https://arxiv.org/abs/2403.07806,2024-03-12,A Stochastic GDA Method With Backtracking For Solving Nonconvex (Strongly) Concave Minimax Problems,"['Qiushui Xu', 'Xuan Zhang', 'Necdet Serhat Aybat', 'Mert Gürbüzbalaban']","We propose a stochastic GDA (gradient descent ascent) method with backtracking (SGDA-B) to solve nonconvex-(strongly) concave (NCC) minimax problems $\min_x \max_y \sum_{i=1}^N g_i(x_i)+f(x,y)-h(y)$, where $h$ and $g_i$ for $i = 1, \ldots, N$ are closed, convex functions, $f$ is $L$-smooth and $μ$-strongly concave in $y$ for some $μ\geq 0$. We consider two scenarios: (i) the deterministic setting where we assume one can compute $\nabla f$ exactly, and (ii) the stochastic setting where we have only access to $\nabla f$ through an unbiased stochastic oracle with a finite variance. While most of the existing methods assume knowledge of the Lipschitz constant $L$, SGDA-B is agnostic to $L$. Moreover, SGDA-B can support random block-coordinate updates. In the deterministic setting, SGDA-B can compute an $ε$-stationary point within $\mathcal{O}(Lκ^2/ε^2)$ and $\mathcal{O}(L^3/ε^4)$ gradient calls when $μ>0$ and $μ=0$, respectively, where $κ=L/μ$. In the stochastic setting, for any $p \in (0, 1)$ and $ε>0$, it can compute an $ε$-stationary point with high probability, which requires $\mathcal{O}(Lκ^3ε^{-4}\log(1/p))$ and $\tilde{\mathcal{O}}(L^4ε^{-7}\log(1/p))$ stochastic oracle calls, with probability at least $1-p$, when $μ>0$ and $μ=0$, respectively. To our knowledge, SGDA-B is the first GDA-type method with backtracking to solve NCC minimax problems and achieves the best complexity among the methods that are agnostic to $L$. We also provide numerical results for SGDA-B on a distributionally robust learning problem illustrating the potential performance gains that can be achieved by SGDA-B."
https://arxiv.org/abs/2403.07805,2024-03-13,Beyond Memorization: The Challenge of Random Memory Access in Language Models,"['Tongyao Zhu', 'Qian Liu', 'Liang Pang', 'Zhengbao Jiang', 'Min-Yen Kan', 'Min Lin']","Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering. The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access."
https://arxiv.org/abs/2403.07804,2024-03-12,Type IV-like Solar Radio Burst Consisting of a Series of Spikes Observed by PSP,"['Bing Ma', 'Ling Chen', 'De-Jin Wu', 'Marc Pulupa', 'Stuart D. Bale']","Solar and interplanetary radio bursts can reflect the existence and motion of energetic electrons and are therefore a kind of vital phenomenon in solar activities. The present study reported a solar radio burst (SRB) event observed by Parker Solar Probe (PSP) in its 8th orbital encounter phase, and it lasted about 20 hours in a frequency range of 0.5-15 MHz, called the type IV-like SRB. This type IV-like SRB consists of a series of numerous spikes with the center-frequency drifting slowly from ~5 MHz to ~1 MHz, and each individual spike appears a much faster frequency drifting and has a narrow frequency range of a few MHz and short duration of a few minutes. Based on the empirical models of the solar atmosphere adopted commonly, combining the in-situ measurement by PSP, we propose that these small-scale spikes were generated by a group of solitary kinetic Alfvén waves (SKAWs) in a magnetic loop accompanying coronal mass ejection (CME) and moving outwards, in which the frequency drifting of individual spike is caused by the SKAW's propagation and the center-frequency drifting may be attributed to the motion of the magnetic loop."
https://arxiv.org/abs/2403.07803,2024-03-12,Variational structures for the Fokker--Planck equation with general Dirichlet boundary conditions,['Filippo Quattrocchi'],"We prove convergence of a modified Jordan--Kinderlehrer--Otto scheme to a solution to the Fokker--Planck equation in $Ω\Subset \mathbb{R}^d$ with spatially nonconstant Dirichlet boundary conditions. We work under mild assumptions on the domain, on the drift, and on the initial datum. In the special case where $Ω$ is an interval in $\mathbb{R}^1$, we prove that such a solution is a gradient flow -- curve of maximal slope -- within a suitable space of measures, endowed with a modified Wasserstein distance. Our discrete scheme and modified distance draw inspiration from contributions by A. Figalli and N. Gigli [J. Math. Pures Appl. 94, (2010), pp. 107--130], and J. Morales [J. Math. Pures Appl. 112, (2018), pp. 41--88] on an optimal-transport approach to evolution equations with Dirichlet boundary conditions."
https://arxiv.org/abs/2403.07802,2024-03-12,Boosting keyword spotting through on-device learnable user speech characteristics,"['Cristian Cioflan', 'Lukas Cavigelli', 'Luca Benini']","Keyword spotting systems for always-on TinyML-constrained applications require on-site tuning to boost the accuracy of offline trained classifiers when deployed in unseen inference conditions. Adapting to the speech peculiarities of target users requires many in-domain samples, often unavailable in real-world scenarios. Furthermore, current on-device learning techniques rely on computationally intensive and memory-hungry backbone update schemes, unfit for always-on, battery-powered devices. In this work, we propose a novel on-device learning architecture, composed of a pretrained backbone and a user-aware embedding learning the user's speech characteristics. The so-generated features are fused and used to classify the input utterance. For domain shifts generated by unseen speakers, we measure error rate reductions of up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google Speech Commands dataset, through the inexpensive update of the user projections. We moreover demonstrate the few-shot learning capabilities of our proposed architecture in sample- and class-scarce learning conditions. With 23.7 kparameters and 1 MFLOP per epoch required for on-device training, our system is feasible for TinyML applications aimed at battery-powered microcontrollers."
https://arxiv.org/abs/2403.07801,2024-03-12,The Importance of Optical Wavelength Data on Atmospheric Retrievals of Exoplanet Transmission Spectra,"['Charlotte Fairman', 'Hannah R. Wakeford', 'Ryan J. MacDonald']","Exoplanet transmission spectra provide rich information about the chemical composition, clouds and temperature structure of exoplanet atmospheres. Most exoplanet transmission spectra only span infrared wavelengths ($\gtrsim$ 1 $\rm{μm}$), which can preclude crucial atmospheric information from shorter wavelengths. Here, we explore how retrieved atmospheric parameters from exoplanet transmission spectra change with the addition of optical data. From a sample of 14 giant planets with transit spectra from 0.3-4.5 $\rm{μm}$, primarily from the Hubble and Spitzer space telescopes, we apply a free chemistry retrieval to planetary spectra for wavelength ranges of 0.3-4.5 $\rm{μm}$, 0.6-4.5 $\rm{μm}$, and 1.1-4.5 $\rm{μm}$. We analyse the posterior distributions of these retrievals and perform an information content analysis, finding wavelengths below 0.6 $\rm{μm}$ are necessary to constrain cloud scattering slope parameters ($\log{a}$ and $γ$) and alkali species Na and K. There is limited improvement in the constraints on the remaining atmospheric parameters. Across the population, we find limb temperatures are retrieved colder than planetary equilibrium temperatures but have an overall good agreement with Global Circulation Models. As JWST extends to a minimum wavelength of 0.6 $\rm{μm}$, we demonstrate that exploration into complementing JWST observations with optical HST data is important to further our understanding of aerosol properties and alkali abundances in exoplanet atmospheres."
https://arxiv.org/abs/2403.07800,2024-03-12,BraSyn 2023 challenge: Missing MRI synthesis and the effect of different learning objectives,"['Ivo M. Baltruschat', 'Parvaneh Janbakhshi', 'Matthias Lenga']","This work is addressing the Brain Magnetic Resonance Image Synthesis for Tumor Segmentation (BraSyn) challenge which was hosted as part of the Brain Tumor Segmentation challenge (BraTS) 2023. In this challenge researchers are invited to work on synthesizing a missing magnetic resonance image sequence given other available sequences to facilitate tumor segmentation pipelines trained on complete sets of image sequences. This problem can be addressed using deep learning in the framework of paired images-to-image translation. In this work, we proposed to investigate the effectiveness of a commonly-used deep learning framework such as Pix2Pix trained under supervision of different image-quality loss functions. Our results indicate that using different loss functions significantly affects the synthesis quality. We systematically study the impact of different loss functions in the multi-sequence MR image synthesis setting of the BraSyn challenge. Furthermore, we show how image synthesis performance can be optimized by beneficially combining different learning objectives."
https://arxiv.org/abs/2403.07799,2024-03-12,Equitable Pricing in Auctions,"['Simon Finster', 'Patrick Loiseau', 'Simon Mauras', 'Mathieu Molina', 'Bary Pradelski']","We study how pricing affects the division of surplus among buyers in auctions for multiple units. Our equity objective may be important, e.g., for competition concerns in downstream markets, complementing the long-standing debate on revenue and efficiency. We study a canonical model of auctions for multiple indivisible units with unit demand buyers and valuations with a private and a common component and consider all pricing rules that are a mixture (i.e., a convex combination) of pay-as-bid and uniform pricing. We propose the winners' empirical variance (WEV), the expected empirical variance of surplus among the winners, as a metric for surplus equity. We show that, for a range of private-common value proportions, a strictly interior mix of pay-as-bid and uniform pricing minimizes WEV. From an equity perspective, auctions with a higher private value component benefit from more price discrimination, whereas only auctions with a sufficiently high common value justify a more uniform pricing rule. We provide a criterion under which strictly mixed pricing dominates uniform pricing, a partial ranking of different mixed pricing formats, and bounds on the WEV-minimizing pricing under the assumption of log-concave signal distributions. In numerical experiments, we further illustrate the WEV-minimal pricing as a function of the private-common-value mix."
https://arxiv.org/abs/2403.07798,2024-03-12,A Fourier Transform Framework for Domain Adaptation,"['Le Luo', 'Bingrong Xu', 'Qingyong Zhang', 'Cheng Lian', 'Jie Luo']","By using unsupervised domain adaptation (UDA), knowledge can be transferred from a label-rich source domain to a target domain that contains relevant information but lacks labels. Many existing UDA algorithms suffer from directly using raw images as input, resulting in models that overly focus on redundant information and exhibit poor generalization capability. To address this issue, we attempt to improve the performance of unsupervised domain adaptation by employing the Fourier method (FTF).Specifically, FTF is inspired by the amplitude of Fourier spectra, which primarily preserves low-level statistical information. In FTF, we effectively incorporate low-level information from the target domain into the source domain by fusing the amplitudes of both domains in the Fourier domain. Additionally, we observe that extracting features from batches of images can eliminate redundant information while retaining class-specific features relevant to the task. Building upon this observation, we apply the Fourier Transform at the data stream level for the first time. To further align multiple sources of data, we introduce the concept of correlation alignment. To evaluate the effectiveness of our FTF method, we conducted evaluations on four benchmark datasets for domain adaptation, including Office-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our results demonstrate superior performance."
https://arxiv.org/abs/2403.07797,2024-03-12,Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data,"['Miguel Fuentes', 'Brett Mullins', 'Ryan McKenna', 'Gerome Miklau', 'Daniel Sheldon']","Mechanisms for generating differentially private synthetic data based on marginals and graphical models have been successful in a wide range of settings. However, one limitation of these methods is their inability to incorporate public data. Initializing a data generating model by pre-training on public data has shown to improve the quality of synthetic data, but this technique is not applicable when model structure is not determined a priori. We develop the mechanism jam-pgm, which expands the adaptive measurements framework to jointly select between measuring public data and private data. This technique allows for public data to be included in a graphical-model-based mechanism. We show that jam-pgm is able to outperform both publicly assisted and non publicly assisted synthetic data generation mechanisms even when the public data distribution is biased."
https://arxiv.org/abs/2403.07796,2024-03-12,Second gadolinium loading to Super-Kamiokande,"['K. Abe', 'C. Bronner', 'Y. Hayato', 'K. Hiraide', 'K. Hosokawa', 'K. Ieki', 'M. Ikeda', 'J. Kameda', 'Y. Kanemura', 'R. Kaneshima', 'Y. Kashiwagi', 'Y. Kataoka', 'S. Miki', 'S. Mine', 'M. Miura', 'S. Moriyama', 'Y. Nakano', 'M. Nakahata', 'S. Nakayama', 'Y. Noguchi', 'K. Sato', 'H. Sekiya', 'H. Shiba', 'K. Shimizu', 'M. Shiozawa']","The first loading of gadolinium (Gd) into Super-Kamiokande in 2020 was successful, and the neutron capture efficiency on Gd reached 50\%. To further increase the Gd neutron capture efficiency to 75\%, 26.1 tons of $\rm Gd_2(\rm SO_4)_3\cdot \rm 8H_2O$ was additionally loaded into Super-Kamiokande (SK) from May 31 to July 4, 2022. As the amount of loaded $\rm Gd_2(\rm SO_4)_3\cdot \rm 8H_2O$ was doubled compared to the first loading, the capacity of the powder dissolving system was doubled. We also developed new batches of gadolinium sulfate with even further reduced radioactive impurities. In addition, a more efficient screening method was devised and implemented to evaluate these new batches of $\rm Gd_2(\rm SO_4)_3\cdot \rm 8H_2O$. Following the second loading, the Gd concentration in SK was measured to be $333.5\pm2.5$ ppm via an Atomic Absorption Spectrometer (AAS). From the mean neutron capture time constant of neutrons from an Am/Be calibration source, the Gd concentration was independently measured to be 332.7 $\pm$ 6.8(sys.) $\pm$ 1.1(stat.) ppm, consistent with the AAS result. Furthermore, during the loading the Gd concentration was monitored continually using the capture time constant of each spallation neutron produced by cosmic-ray muons,and the final neutron capture efficiency was shown to become 1.5 times higher than that of the first loaded phase, as expected."
https://arxiv.org/abs/2403.07795,2024-03-12,Fine-tuning Neural Network Quantum States,"['Riccardo Rende', 'Sebastian Goldt', 'Federico Becca', 'Luciano Loris Viteritti']","Recent progress in the design and optimization of Neural Network Quantum States (NNQS) have made them an effective method to investigate ground-state properties of quantum many-body systems. In contrast to the standard approach of training a separate NNQS from scratch at every point of the phase diagram, we demonstrate that the optimization at a highly expressive point of the phase diagram (i.e., close to a phase transition) yields interpretable features that can be reused to accurately describe a wide region across the transition. We demonstrate the feasibility of our approach on different systems in one and two dimensions by initially pretraining a NNQS at a given point of the phase diagram, followed by fine-tuning only the output layer for all other points. Notably, the computational cost of the fine-tuning step is very low compared to the pretraining stage. We argue that the reduced cost of this paradigm has significant potential to advance the exploration of condensed matter systems using NNQS, mirroring the success of fine-tuning in machine learning and natural language processing."
https://arxiv.org/abs/2403.07794,2024-03-12,Fine-tuning Large Language Models with Sequential Instructions,"['Hanxu Hu', 'Pinzhen Chen', 'Edoardo M. Ponti']","Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks. We empirically verify this with open-source LLMs as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential instructions in present-day data, we propose sequential instruction tuning, a simple yet effective strategy to automatically augment instruction tuning data and equip LLMs with the ability to execute multiple sequential instructions. After exploring interleaving instructions in existing datasets, such as Alpaca, with a wide range of intermediate tasks, we find that sequential instruction-tuned models consistently outperform the conventional instruction-tuned baselines in downstream tasks involving reasoning, multilingual, and multimodal abilities. To shed further light on our technique, we analyse how adversarial intermediate texts, unseen tasks, prompt verbalization, number of tasks, and prompt length affect SIT. We hope that this method will open new research avenues on instruction tuning for complex tasks."
https://arxiv.org/abs/2403.07793,2024-03-12,Optimal regularity for nonlocal elliptic equations and free boundary problems,"['Xavier Ros-Oton', 'Marvin Weidner']","In this article we establish for the first time the $C^s$ boundary regularity of solutions to nonlocal elliptic equations with kernels $K(y)\asymp |y|^{-n-2s}$. This was known to hold only when $K$ is homogeneous, and it is quite surprising that it holds for general inhomogeneous kernels, too. As an application of our results, we also establish the optimal $C^{1+s}$ regularity of solutions to obstacle problems for general nonlocal operators with kernels $K(y)\asymp |y|^{-n-2s}$. Again, this was only known when $K$ is homogeneous, and it solves a long-standing open question in the field. A new key idea is to construct a 1D solution as a minimizer of an appropriate nonlocal one-phase free boundary problem, for which we establish optimal $C^s$ regularity and non-degeneracy estimates."
https://arxiv.org/abs/2403.07792,2024-03-12,Search for new bosons with ytterbium isotope shifts,"['Menno Door', 'Chih-Han Yeh', 'Matthias Heinz', 'Fiona Kirk', 'Chunhai Lyu', 'Takayuki Miyagi', 'Julian C. Berengut', 'Jacek Bieroń', 'Klaus Blaum', 'Laura S. Dreissen', 'Sergey Eliseev', 'Pavel Filianin', 'Melina Filzinger', 'Elina Fuchs', 'Henning A. Fürst', 'Gediminas Gaigalas', 'Zoltán Harman', 'Jost Herkenhoff', 'Nils Huntemann', 'Christoph H. Keitel', 'Kathrin Kromer', 'Daniel Lange', 'Alexander Rischka', 'Christoph Schweiger', 'Achim Schwenk']","The Standard Model of particle physics describes the properties of elementary particles and their interactions remarkably well, but in particular does not account for dark matter. Isotope-shift spectroscopy is a sensitive probe of fifth forces and new particles that illuminate the dark matter sector. This method sets bounds on new bosons that couple neutrons and electrons with masses in the keV/c2 to MeV/c2 range. With increasing spectroscopic precision, such searches are limited by uncertainties of isotope masses and the understanding of nuclear structure. Here, we report on high-precision mass-ratio and isotope-shift measurements of the ytterbium isotopes $^{168,170,172,174,176}$Yb that exceed previous measurements by up to two orders of magnitude. From these measurements, we extract higher-order changes in the nuclear charge distribution along the Yb isotope chain and use these to benchmark novel ab initio calculations. Our measurements set new bounds on the existence of the proposed boson."
https://arxiv.org/abs/2403.07791,2024-03-12,Stability of the Favorable Falkner-Skan Profiles for the Stationary Prandtl Equations,['Sameer Iyer'],"The (favorable) Falkner-Skan boundary layer profiles are a one parameter ($β\in [0,2]$) family of self-similar solutions to the stationary Prandtl system which describes the flow over a wedge with angle $β\fracπ{2}$. The most famous member of this family is the endpoint Blasius profile, $β= 0$, which exhibits pressureless flow over a flat plate. In contrast, the $β> 0$ profiles are physically expected to exhibit a \textit{favorable pressure gradient}, a common adage in the physics literature. In this work, we prove quantitative scattering estimates as $x \rightarrow \infty$ which precisely captures the effect of this favorable gradient through the presence of new ``CK"" (Cauchy-Kovalevskaya) terms that appear in a quasilinear energy cascade."
https://arxiv.org/abs/2403.07790,2024-03-12,RADES axion search results with a High-Temperature Superconducting cavity in an 11.7 T magnet,"['S. Ahyoune', 'A. Álvarez Melcón', 'S. Arguedas Cuendis', 'S. Calatroni', 'C. Cogollos', 'A. Díaz-Morcillo', 'B. Döbrich', 'J. D. Gallego', 'J. M. García-Barceló', 'B. Gimeno', 'J. Golm', 'X. Granados', 'J. Gutierrez', 'L. Herwig', 'I. G. Irastorza', 'N. Lamas', 'A. Lozano-Guerrero', 'W. L. Millar', 'C. Malbrunot', 'J. Miralda-Escudé', 'P. Navarro', 'J. R. Navarro-Madrid', 'T. Puig', 'M. Siodlaczek', 'G. T. Telles']","We describe the results of a haloscope axion search performed with an 11.7 T dipole magnet at CERN. The search used a custom-made radio-frequency cavity coated with high-temperature superconducting tape. A set of 27 h of data at a resonant frequency of around 8.84 GHz was analysed. In the range of axion mass 36.5676 $μ$eV to 36.5699 $μ$eV, corresponding to a width of 554 kHz, no signal excess hinting at an axion-like particle was found. Correspondingly, in this mass range, a limit on the axion to photon coupling-strength was set in the range between g$_{aγ}\gtrsim$ 6.2e-13 GeV$^{-1}$ and g$_{aγ}\gtrsim$ 1.54e-13 GeV$^{-1}$ with a 95% confidence level."
https://arxiv.org/abs/2403.07789,2024-03-12,RobotCycle: Assessing Cycling Safety in Urban Environments,"['Efimia Panagiotaki', 'Tyler Reinmund', 'Brian Liu', 'Stephan Mouton', 'Luke Pitt', 'Arundathi Shaji Shanthini', 'Matthew Towlson', 'Wayne Tubby', 'Chris Prahacs', 'Daniele De Martini', 'Lars Kunze']","This paper introduces RobotCycle, a novel ongoing project that leverages Autonomous Vehicle (AV) research to investigate how cycling infrastructure influences cyclist behaviour and safety during real-world journeys. The project's requirements were defined in collaboration with key stakeholders (i.e. city planners, cyclists, and policymakers), informing the design of risk and safety metrics and the data collection criteria. We propose a data-driven approach relying on a novel, rich dataset of diverse traffic scenes captured through a custom-designed wearable sensing unit. We extract road-user trajectories and analyse deviations suggesting risk or potentially hazardous interactions in correlation with infrastructural elements in the environment. Driving profiles and trajectory patterns are associated with local road segments, driving conditions, and road-user interactions to predict traffic behaviour and identify critical scenarios. Moreover, leveraging advancements in AV research, the project extracts detailed 3D maps, traffic flow patterns, and trajectory models to provide an in-depth assessment and analysis of the behaviour of all traffic agents. This data can then inform the design of cyclist-friendly road infrastructure, improving road safety and cyclability, as it provides valuable insights for enhancing cyclist protection and promoting sustainable urban mobility."
https://arxiv.org/abs/2403.07788,2024-03-12,DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation,"['Chen Wang', 'Haochen Shi', 'Weizhuo Wang', 'Ruohan Zhang', 'Li Fei-Fei', 'C. Karen Liu']","Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism to refine and further improve robot performance. Through extensive evaluation across six dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods for dexterous manipulation. More details can be found at https://dex-cap.github.io"
https://arxiv.org/abs/2403.07787,2024-03-12,Transparent boundary condition and its effectively local approximation for the Schrödinger equation on a rectangular computational domain,"['Samardhi Yadav', 'Vishal Vaibhav']","The transparent boundary condition for the free Schrödinger equation on a rectangular computational domain requires implementation of an operator of the form $\sqrt{\partial_t-i\triangle_Γ}$ where $\triangle_Γ$ is the Laplace-Beltrami operator. It is known that this operator is nonlocal in time as well as space which poses a significant challenge in developing an efficient numerical method of solution. The computational complexity of the existing methods scale with the number of time-steps which can be attributed to the nonlocal nature of the boundary operator. In this work, we report an effectively local approximation for the boundary operator such that the resulting complexity remains independent of number of time-steps. At the heart of this algorithm is a Padé approximant based rational approximation of certain fractional operators that handles corners of the domain adequately. For the spatial discretization, we use a Legendre-Galerkin spectral method with a new boundary adapted basis which ensures that the resulting linear system is banded. A compatible boundary-lifting procedure is also presented which accommodates the segments as well as the corners on the boundary. The proposed novel scheme can be implemented within the framework of any one-step time marching schemes. In particular, we demonstrate these ideas for two one-step methods, namely, the backward-differentiation formula of order 1 (BDF1) and the trapezoidal rule (TR). For the sake of comparison, we also present a convolution quadrature based scheme conforming to the one-step methods which is computationally expensive but serves as a golden standard. Finally, several numerical tests are presented to demonstrate the effectiveness of our novel method as well as to verify the order of convergence empirically."
https://arxiv.org/abs/2403.07786,2024-03-12,Generative deep learning-enabled ultra-large field-of-view lens-free imaging,"['Ronald B. Liu', 'Zhe Liu', 'Max G. A. Wolf', 'Krishna P. Purohit', 'Gregor Fritz', 'Yi Feng', 'Carsten G. Hansen', 'Pierre O. Bagnaninchi', 'Xavier Casadevall i Solvas', 'Yunjie Yang']","Advancements in high-throughput biomedical applications necessitate real-time, large field-of-view (FOV) imaging capabilities. Conventional lens-free imaging (LFI) systems, while addressing the limitations of physical lenses, have been constrained by dynamic, hard-to-model optical fields, resulting in a limited one-shot FOV of approximately 20 $mm^2$. This restriction has been a major bottleneck in applications like live-cell imaging and automation of microfluidic systems for biomedical research. Here, we present a deep-learning(DL)-based imaging framework -- GenLFI -- leveraging generative artificial intelligence (AI) for holographic image reconstruction. We demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$, surpassing the current LFI system by more than 20-fold, and even larger than the world's largest confocal microscope by 1.76 times. The resolution is at the sub-pixel level of 5.52 $μm$, without the need for a shifting light source. The unsupervised learning-based reconstruction does not require optical field modeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics and 3D cell models) in complex optical fields possible. This GenLFI framework unlocks the potential of LFI systems, offering a robust tool to tackle new frontiers in high-throughput biomedical applications such as drug discovery."
https://arxiv.org/abs/2403.07785,2024-03-12,Multi-period stochastic covering location problems: Modeling framework and solution approach,"['Alfredo Marín', 'Luisa I. Martínez-Merio', 'Antonio M. Rodríguez-Chía', 'Francisco Saldanha-da-Gama']","This paper introduces a very general discrete covering location model that accounts for uncertainty and time-dependent aspects. A MILP formulation is proposed for the problem. Afterwards, it is observed that most of the models existing in the literature related with covering location can be considered as particular cases of this formulation. In order to tackle large instances of this problem a Lagrangian relaxation based heuristic is developed. A computational study is addressed to check the potentials and limits of the formulation and some variants proposed for the problem, as well as to evaluate the heuristic. Finally, different measures to report the relevance of considering a multi-period stochastic setting are studied."
https://arxiv.org/abs/2403.07784,2024-03-12,Finite time BV blowup for Liu-admissible solutions to $p$-system via computer-assisted proof,['Sam G. Krupa'],"In this paper, we consider finite time blowup of the $BV$-norm for exact solutions to genuinely nonlinear hyperbolic systems in one space dimension, in particular the $p$-system. We consider solutions verifying shock admissibility criteria such as the Lax E-condition and the Liu E-condition. In particular, we present Riemann initial data which admits infinitely many bounded solutions, each of which experience, not just finite time, but in fact instantaneous blowup of the $BV$ norm. The Riemann initial data is allowed to come from an open set in state space. Our method provably does not admit a strictly convex entropy."
https://arxiv.org/abs/2403.07783,2024-03-12,QCD Equation of State at nonzero baryon density in external magnetic field,"['N. Astrakhantsev', 'V. V. Braguta', 'A. Yu. Kotov', 'A. A. Roenko']","This paper is devoted to the study of QCD equation of state in external magnetic field and nonzero baryon density. Our study is carried out by means of lattice simulation with 2+1 dynamical staggered quarks at the physical masses. The simulation is conducted at imaginary baryon chemical potential what allowed us to overcome the sign problem. We expand the pressure in the baryon imaginary chemical potential and study three leading nonzero coefficients in this expansion. These coefficients were calculated for the following values of magnetic field: $eB=0.3$, $0.6$, $1.2$ GeV$^2$ with the lattice sizes $8\times32^3$, $10\times40^3$, $12\times48^3$. Using these data we take continuum limit for the coefficients. Our results indicate considerable enhancement of the expansion coefficients by the magnetic field."
https://arxiv.org/abs/2403.07782,2024-03-12,Minimal Elements of the Causal Boundary with Applications to Spacetime Splitting,['Leonardo García-Heveling'],"In 1972, Geroch, Kronheimer, and Penrose introduced what is now called the causal boundary of a spacetime. This boundary is constructed out of Terminal Indecomposable Past sets (TIPs) and their future analogues (TIFs), which are the pasts and futures of inextendible causal curves. The causal boundary is a key tool to understand the global structure of a spacetime. In this paper, we show that in a spacetime with compact Cauchy surfaces, there is always at least one minimal TIP and one minimal TIF, minimal meaning that it does not contain another TIP (resp. TIF) as a proper subset. We then study the implications of the minimal TIP and TIF meeting each other. This condition generalizes some of the ``no observer horizon'' conditions that have been used in the literature to obtain partial solutions of the Bartnik splitting conjecture."
https://arxiv.org/abs/2403.07781,2024-03-12,Conservative Black Hole Scattering at Fifth Post-Minkowskian and First Self-Force Order,"['Mathias Driesse', 'Gustav Uhre Jakobsen', 'Gustav Mogull', 'Jan Plefka', 'Benjamin Sauer', 'Johann Usovitsch']","We compute the 5PM order contributions to the scattering angle and impulse of classical black hole scattering in the conservative sector at first self-force order (1SF) using the worldline quantum field theory formalism. This challenging four-loop computation required the use of advanced integration-by-parts and differential equation technology implemented on high-perfomance computing systems. Use of partial fraction identities allowed us to render the complete integrand in a fully planar form. The resulting function space is simpler than expected: in the scattering angle we see only multiple polylogarithms up to weight three, and a total absence of the elliptic integrals that appeared at 4PM order. All checks on our result, both internal - cancellation of dimensional regularization poles, preservation of the on-shell condition - and external - matching the slow-velocity limit with the post-Newtonian (PN) literature up to 5PN order and matching the tail terms to the 4PM loss of energy - are passed."
https://arxiv.org/abs/2403.07780,2024-03-12,FairRR: Pre-Processing for Group Fairness through Randomized Response,"['Xianli Zeng', 'Joshua Ward', 'Guang Cheng']","The increasing usage of machine learning models in consequential decision-making processes has spurred research into the fairness of these systems. While significant work has been done to study group fairness in the in-processing and post-processing setting, there has been little that theoretically connects these results to the pre-processing domain. This paper proposes that achieving group fairness in downstream models can be formulated as finding the optimal design matrix in which to modify a response variable in a Randomized Response framework. We show that measures of group fairness can be directly controlled for with optimal model utility, proposing a pre-processing algorithm called FairRR that yields excellent downstream model utility and fairness."
https://arxiv.org/abs/2403.07779,2024-03-12,A boundary integral based particle initialization algorithm for Smooth Particle Hydrodynamics,"['Parikshit Boregowda', 'G R Liu']","Algorithms for initializing particle distribution in SPH simulations of complex geometries have been proven essential for improving the accuracy of SPH simulations. However, no such algorithms exist for boundary integral SPH models, which can model complex geometries without needing virtual particle layers. This study introduces a Boundary Integral based Particle Initialization (BIPI) algorithm. It consists of a particle-shifting technique carefully designed to redistribute particles to fit the boundary by using the boundary integral formulation for particles adjacent to the boundary. The proposed BIPI algorithm gives special consideration to particles adjacent to the boundary to prevent artificial volume compression. It can automatically produce a ""uniform"" particle distribution with reduced and stabilized concentration gradient for domains with complex geometrical shapes. Finally, a number of examples are presented to demonstrate the effectiveness of the proposed algorithm."
https://arxiv.org/abs/2403.07778,2024-03-12,Joint Modeling of Longitudinal Measurements and Time-to-event Outcomes Using BUGS,"['Taban Baghfalaki', 'Mojtaba Ganjali', 'Antoine Barbieri', 'Reza Hashemi', 'Hélène Jacqmin-Gadda']","The objective of this paper is to provide an introduction to the principles of Bayesian joint modeling of longitudinal measurements and time-to-event outcomes, as well as model implementation using the BUGS language syntax. This syntax can be executed directly using OpenBUGS or by utilizing convenient functions to invoke OpenBUGS and JAGS from R software. In this paper, all details of joint models are provided, ranging from simple to more advanced models. The presentation started with the joint modeling of a Gaussian longitudinal marker and time-to-event outcome. The implementation of the Bayesian paradigm of the model is reviewed. The strategies for simulating data from the JM are also discussed. A proportional hazard model with various forms of baseline hazards, along with the discussion of all possible association structures between the two sub-models are taken into consideration. The paper covers joint models with multivariate longitudinal measurements, zero-inflated longitudinal measurements, competing risks, and time-to-event with cure fraction. The models are illustrated by the analyses of several real data sets. All simulated and real data and code are available at \url{https://github.com/tbaghfalaki/JM-with-BUGS-and-JAGS}."
https://arxiv.org/abs/2403.07777,2024-03-12,Fragmentation of Dense Rotation-Dominated Structures Fed by Collapsing Gravomagneto-Sheetlets and Origin of Misaligned 100 au-Scale Binaries and Multiple Systems,"['Yisheng Tu', 'Zhi-Yun Li', 'Zhaohuan Zhu', 'Chun-Yen Hsu']","The majority of stars are in binary/multiple systems. How such systems form in turbulent, magnetized cores of molecular clouds in the presence of non-ideal MHD effects remains relatively under-explored. Through ATHENA++-based non-ideal MHD AMR simulations with ambipolar diffusion, we show that the collapsing protostellar envelope is dominated by dense gravomagneto-sheetlets, a turbulence-warped version of the classic pseuodisk produced by anisotropic magnetic resistance to the gravitational collapse, in agreement with previous simulations of turbulent, magnetized single-star formation. The sheetlets feed mass, magnetic fields, and angular momentum to a Dense ROtation-Dominated (DROD) structure, which fragments into binary/multiple systems. This DROD fragmentation scenario is a more dynamic variant of the traditional disk fragmentation scenario for binary/multiple formation, with dense spiral filaments created by inhomogeneous feeding from the highly structured larger-scale sheetlets rather than the need for angular momentum transport, which is dominated by magnetic braking. Collisions between the dense spiraling filaments play a key role in pushing the local magnetic Toomre parameter $Q_\mathrm{m}$ below unity, leading to gravitational collapse and stellar companion formation provided that the local material is sufficiently demagnetized, with a plasma-$β$ of order unity or more. This mechanism can naturally produce {\it in situ} misaligned systems on the 100-au scale, often detected with high-resolution ALMA observations. Our simulations also highlight the importance of non-ideal MHD effects, which affect whether fragmentation occurs and, if so, the masses and orbital parameters of the stellar companions formed."
https://arxiv.org/abs/2403.07776,2024-03-12,Towards a Stallings-type theorem for finite groups,"['Johannes Carmesin', 'George Kontogeorgiou', 'Jan Kurkofka', 'Will J. Turner']","A recent development in graph theory is to study local separators, vertex sets that need not separate graphs globally but just locally. We use this idea to conjecture an extension of Stallings' theorem to finite nilpotent groups. We provide an example demonstrating that the assumption of nilpotency is necessary, and prove a stronger form of this conjecture for low connectivities, as follows."
https://arxiv.org/abs/2403.07775,2024-03-12,The probabilistic p-center problem: Planning service for potential customers,"['Luisa I. Martínez-Merino', 'Maria Albareda-Sambola', 'Antonio M. Rodríguez-Chía']","This work deals with the probabilistic p-center problem, which aims at minimizing the expected maximum distance between any site with demand and its center, considering that each site has demand with a specific probability. The problem is of interest when emergencies may occur at predefined sites with known probabilities. For this problem we propose and analyze different formulations as well as a Variable Neighborhood Search heuristic. Computational tests are reported, showing the potentials and limits of each formulation, the impact of their enhancements, and the effectiveness of the heuristic."
https://arxiv.org/abs/2403.07774,2024-03-12,Superexchange Mechanism in Coupled Triangulenes Forming Spin-1 Chains,"['Yasser Saleem', 'Torben Steenbock', 'Emha Riyadhul Jinan Alhadi', 'Weronika Pasek', 'Gabriel Bester', 'Pawel Potasz']","We show that the origin of the antiferromagnetic coupling in spin-1 triangulene chains, which were recently synthesized and measured by Mishra et al. Nature 598, 287-292 (2021) originates from a superexchange mechanism. This process, mediated by inter-triangulene states, opens the possibility to control parameters in the effective bilinear-biquadratic spin model. We start from the derivation of an effective tight-binding model for triangulene chains using a combination of tight-binding and Hartree-Fock methods fitted to hybrid density functional theory results. Next, correlation effects are investigated within the configuration interaction method. Our low-energy many-body spectrum for $N_{\rm Tr}=2$ and $N_{\rm Tr}=4$ triangulene chains agree well with the bilinear-biquadratic spin-1 chain antiferromagnetic model when indirect coupling processes, and superexchange coupling between triangulene spins are taken into account."
https://arxiv.org/abs/2403.07773,2024-03-12,SemCity: Semantic Scene Generation with Triplane Diffusion,"['Jumin Lee', 'Sebin Lee', 'Changho Jo', 'Woobin Im', 'Juhyeong Seon', 'Sung-Eui Yoon']","We present ""SemCity,"" a 3D diffusion model for semantic scene generation in real-world outdoor environments. Most 3D diffusion models focus on generating a single object, synthetic indoor scenes, or synthetic outdoor scenes, while the generation of real-world outdoor scenes is rarely addressed. In this paper, we concentrate on generating a real-outdoor scene through learning a diffusion model on a real-world outdoor dataset. In contrast to synthetic data, real-outdoor datasets often contain more empty spaces due to sensor limitations, causing challenges in learning real-outdoor distributions. To address this issue, we exploit a triplane representation as a proxy form of scene distributions to be learned by our diffusion model. Furthermore, we propose a triplane manipulation that integrates seamlessly with our triplane diffusion model. The manipulation improves our diffusion model's applicability in a variety of downstream tasks related to outdoor scene generation such as scene inpainting, scene outpainting, and semantic scene completion refinements. In experimental results, we demonstrate that our triplane diffusion model shows meaningful generation results compared with existing work in a real-outdoor dataset, SemanticKITTI. We also show our triplane manipulation facilitates seamlessly adding, removing, or modifying objects within a scene. Further, it also enables the expansion of scenes toward a city-level scale. Finally, we evaluate our method on semantic scene completion refinements where our diffusion model enhances predictions of semantic scene completion networks by learning scene distribution. Our code is available at https://github.com/zoomin-lee/SemCity."
https://arxiv.org/abs/2403.07772,2024-03-12,Privacy Guarantees in Posterior Sampling under Contamination,"['Shenggang Hu', 'Louis Aslett', 'Hongsheng Dai', 'Murray Pollock', 'Gareth O. Roberts']","In recent years, differential privacy has been adopted by tech-companies and governmental agencies as the standard for measuring privacy in algorithms. We study the level of differential privacy in Bayesian posterior sampling setups. As opposed to the common privatization approach of injecting Laplace/Gaussian noise into the output, Huber's contamination model is considered, where we replace at random the data points with samples from a heavy-tailed distribution. We derived bounds for the differential privacy level $(ε,δ)$ for our approach while lifting the common restriction on assuming bounded observation and parameter space seen in the existing literature. We further consider the effect of sample size on privacy level and the convergence rate of $(ε,δ)$ to zero. Asymptotically, the contamination approach is fully private at no cost of information loss. We also provide some examples depicting inference models that our setup is applicable to with a theoretical estimation of convergence rate."
https://arxiv.org/abs/2403.07771,2024-03-12,A first principles study of the Stark shift effect on the zero-phonon line of the NV center in diamond,"['Louis Alaerts', 'Yihuang Xiong', 'Sinéad Griffin', 'Geoffroy Hautier']","Point defects in semiconductors are attractive candidates for quantum information science applications owing to their ability to act as spin-photon interface or single-photon emitters. However, the coupling between the change of dipole moment upon electronic excitation and stray electric fields in the vicinity of the defect, an effect known as Stark shift, can cause significant spectral diffusion in the emitted photons. In this work, using first principles computations, we revisit the methodology to compute the Stark shift of point defects up to the second order. The approach consists of applying an electric field on a defect in a slab and monitoring the changes in the computed zero-phonon line (i.e., difference in energy between the ground and excited state) obtained from constraining the orbital occupations (constrained-DFT). We study the Stark shift of the negatively charged nitrogen-vacancy (NV) center in diamond using this slab approach. We discuss and compare two approaches to ensure a negatively charged defect in a slab and we show that converged values of the Stark shift measured by the change in dipole moment between the ground and excited states ($Δμ$) can be obtained. We obtain a Stark shift of $Δμ$=2.68D using the semi-local GGA-PBE functional and of $Δμ$=2.23D using the HSE hybrid-functional. The results of the slab computations are significantly different than those obtained with Modern Theory of Polarization ($Δμ$=4.34D for GGA-PBE) indicating a potential issue with the combination of constrained-DFT and Modern Theory of Polarization, at least in certain codes."
https://arxiv.org/abs/2403.07770,2024-03-12,PROSKILL: A formal skill language for acting in robotics,['Félix Ingrand'],"Acting is an important decisional function for autonomous robots. Acting relies on skills to implement and to model the activities it oversees: refinement, local recovery, temporal dispatching, external asynchronous events, and commands execution, all done online. While sitting between planning and the robotic platform, acting often relies on programming primitives and an interpreter which executes these skills. Following our experience in providing a formal framework to program the functional components of our robots, we propose a new language, to program the acting skills. This language maps unequivocally into a formal model which can then be used to check properties offline or execute the skills, or more precisely their formal equivalent, and perform runtime verification. We illustrate with a real example how we can program a survey mission for a drone in this new language, prove some formal properties on the program and directly execute the formal model on the drone to perform the mission."
https://arxiv.org/abs/2403.07769,2024-03-12,Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations,['Carlos Jose Xavier Cruz'],"This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct prototyping that considers behavioral elements, driven by strategies that stimulate the generation of knowledge based on the use case proposed in the scenario (role-play) business, using a discussion approach between agents (guided conversation). We demonstrate the potential of developing agents useful for organizational strategies, based on multi-agent system theories (SMA) and innovative uses based on large language models (LLM based), offering a differentiated and adaptable experiment to different applications, complexities, domains, and capabilities from LLM."
https://arxiv.org/abs/2403.07768,2024-03-12,Higher Witt Groups for 2-Categories I: Centralizers,['Hao Xu'],"In this article, we investigate monoidal, braided, sylleptic centralizers of monoidal, braided, sylleptic 2-functors. We specifically focus on multifusion 2-categories and show that monoidal, braided, sylleptic centralizers are multifusion again, via studying the corresponding enveloping algebras. We provide a characterization of the non-degeneracy condition for monoidal, braided, and sylleptic fusion 2-categories, via vanishing of their centers. Applying Double Centralizer Theorems, we establish the relationship between monoidal, braided, symmetric local modules and free modules. In particular, we obtain factorization properties of non-degenerate monoidal, braided, and sylleptic fusion 2-categories. Main results in this article will be used to study higher Witt equivalences of non-degenerate monoidal, braided, sylleptic 2-categories in the sequential articles."
https://arxiv.org/abs/2403.07767,2024-03-12,Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets,"['Jan Pešán', 'Santosh Kesiraju', 'Lukáš Burget', ""Jan ''Honza'' Černocký""]","Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in speech recognition research, often examined through specialized datasets like CLSE and IEMOCAP. However, the integrity of these datasets is seldom scrutinized for text-dependency. This paper critically evaluates the prevalent assumption that machine learning models trained on such datasets genuinely learn to identify paralinguistic traits, rather than merely capturing lexical features. By examining the lexical overlap in these datasets and testing the performance of machine learning models, we expose significant text-dependency in trait-labeling. Our results suggest that some machine learning models, especially large pre-trained models like HuBERT, might inadvertently focus on lexical characteristics rather than the intended paralinguistic features. The study serves as a call to action for the research community to reevaluate the reliability of existing datasets and methodologies, ensuring that machine learning models genuinely learn what they are designed to recognize."
https://arxiv.org/abs/2403.07766,2024-03-12,Gapless superfluidity in neutron stars: Thermal properties,"['Valentin Allard', 'Nicolas Chamel']","The interior of mature neutron stars is expected to contain superfluid neutrons and superconducting protons. The influence of temperature and currents on superfluid properties is studied within the self-consistent time-dependent nuclear energy-density functional theory. We find that this theory predicts the existence of a regime in which nucleons are superfluid (the order parameter remains finite) even though the energy spectrum of quasiparticle excitations exhibits no gap. We show that the disappearance of the gap leads to a specific heat that is not exponentially suppressed at low temperatures as in the BCS regime but can be comparable to that in the normal phase. Introducing some dimensionless effective superfluid velocity, we show that the behavior of the specific heat is essentially universal and we derive general approximate analytical formulas for applications to neutron-star cooling simulations."
https://arxiv.org/abs/2403.07765,2024-03-12,Configuration spaces of orbits and their $S_n$-equivariant $E$-polynomials,['Alejandro Calleja'],"In this paper, we introduce the configuration space of orbits, a generalization of the configuration space of points but for algebraic varieties that are acted by an algebraic reductive group. We develop a novel method for computing the $S_n$-equivariant $E$-polynomial of an algebraic variety, and we apply it to this new kind of varieties."
https://arxiv.org/abs/2403.07764,2024-03-12,Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model,"['Yuxuan Zhang', 'Lifu Wei', 'Qing Zhang', 'Yiren Song', 'Jiaming Liu', 'Huaxia Li', 'Xu Tang', 'Yao Hu', 'Haibo Zhao']","Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields."
https://arxiv.org/abs/2403.07763,2024-03-12,Emerging Technologies for 6G Non-Terrestrial-Networks: From Academia to Industrial Applications,"['Cong T. Nguyen', 'Yuris Mulya Saputra', 'Nguyen Van Huynh', 'Tan N. Nguyen', 'Dinh Thai Hoang', 'Diep N Nguyen', 'Van-Quan Pham', 'Miroslav Voznak', 'Symeon Chatzinotas', 'Dinh-Hieu Tran']","Terrestrial networks form the fundamental infrastructure of modern communication systems, serving more than 4 billion users globally. However, terrestrial networks are facing a wide range of challenges, from coverage and reliability to interference and congestion. As the demands of the 6G era are expected to be much higher, it is crucial to address these challenges to ensure a robust and efficient communication infrastructure for the future. To address these problems, Non-terrestrial Network (NTN) has emerged to be a promising solution. NTNs are communication networks that leverage airborne (e.g., unmanned aerial vehicles) and spaceborne vehicles (e.g., satellites) to facilitate ultra-reliable communications and connectivity with high data rates and low latency over expansive regions. This article aims to provide a comprehensive survey on the utilization of network slicing, Artificial Intelligence/Machine Learning (AI/ML), and Open Radio Access Network (ORAN) to address diverse challenges of NTNs from the perspectives of both academia and industry. Particularly, we first provide an in-depth tutorial on NTN and the key enabling technologies including network slicing, AI/ML, and ORAN. Then, we provide a comprehensive survey on how network slicing and AI/ML have been leveraged to overcome the challenges that NTNs are facing. Moreover, we present how ORAN can be utilized for NTNs. Finally, we highlight important challenges, open issues, and future research directions of NTN in the 6G era."
https://arxiv.org/abs/2403.07762,2024-03-12,Supporting Annotators with Affordances for Efficiently Labeling Conversational Data,"['Austin Z. Henley', 'David Piorkowski']","Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time consuming and expensive. To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling. We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels. We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet. Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet."
https://arxiv.org/abs/2403.07761,2024-03-12,Elliptical Halbach magnet and gradient modules for low-field portable MRI,"['Fernando Galve', 'Eduardo Pallás', 'Teresa Guallart-Naval', 'Pablo García-Cristóbal', 'Pablo Martínez', 'José M. Algarín', 'José Borreguero', 'Rubén Bosch', 'Francisco Juan-Lloris', 'José M. Benlloch', 'Joseba Alonso']","Objective. To develop methods to design the complete magnetic system for a truly portable MRI scanner for neurological and musculoskeletal (MSK) applications, optimized for field homogeneity, field of view (FoV) and gradient performance compared to existing low-weight configurations. Approach. We explore optimal elliptic-bore Halbach configurations based on discrete arrays of permanent magnets. In this way, we seek to improve the field homogeneity and remove constraints to the extent of the gradient coils typical of Halbach magnets. Specifically, we have optimized a tightly-packed distribution of magnetic Nd$_2$Fe$_14$B cubes with differential evolution algorithms, and a second array of shimming magnets with interior point and differential evolution methods. We have also designed and constructed an elliptical set of gradient coils that extend over the whole magnet length, maximizing the distance between the lobe centers. These are optimized with a target field method minimizing a cost function that considers also heat dissipation. Main result. We have employed the new toolbox to build the main magnet and gradient modules for a portable MRI scanner designed for point-of-care and residential use. The elliptical Halbach bore has semi-axes of 10 & 14 cm and the magnet generates a field of 87 mT homogeneous down to 5,700 ppm (parts per million) in a 20 cm diameter FoV, it weighs 216 kg and has a width of 65 cm and a height of 72 cm. Gradient efficiencies go up to around 0.8 mT/m/A, for a maximum of 12 mT/m with in 0.5 ms with 15 A & 15 V amplifier. The distance between lobes is 28 cm, significantly increased with respect to other Halbach-based scanners. Heat dissipation is around 25 W at maximum power, and gradient deviations from linearity are below 20% in a 20 cm sphere."
https://arxiv.org/abs/2403.07760,2024-03-12,Simplified Tight Bounds for Monotone Minimal Perfect Hashing,['Dmitry Kosolobov'],"Given an increasing sequence of integers $x_1,\ldots,x_n$ from a universe $\{0,\ldots,u-1\}$, the monotone minimal perfect hash function (MMPHF) for this sequence is a data structure that answers the following rank queries: $rank(x) = i$ if $x = x_i$, for $i\in \{1,\ldots,n\}$, and $rank(x)$ is arbitrary otherwise. Assadi, Farach-Colton, and Kuszmaul recently presented at SODA'23 a proof of the lower bound $Ω(n \min\{\log\log\log u, \log n\})$ for the bits of space required by MMPHF, provided $u \ge n 2^{2^{\sqrt{\log\log n}}}$, which is tight since there is a data structure for MMPHF that attains this space bound (and answers the queries in $O(\log u)$ time). In this paper, we close the remaining gap by proving that, for $u \ge (1+ε)n$, where $ε> 0$ is any constant, the tight lower bound is $Ω(n \min\{\log\log\log \frac{u}{n}, \log n\})$, which is also attainable; we observe that, for all reasonable cases when $n < u < (1+ε)n$, known facts imply tight bounds, which virtually settles the problem. Along the way we substantially simplify the proof of Assadi et al. replacing a part of their heavy combinatorial machinery by trivial observations. However, an important part of the proof still remains complicated. This part of our paper repeats arguments of Assadi et al. and is not novel. Nevertheless, we include it, for completeness, offering a somewhat different perspective on these arguments."
https://arxiv.org/abs/2403.07759,2024-03-12,Models of Accidental Dark Matter with a Fundamental Scalar,"['Stefano Palmisano', 'Francesco Rescigno', 'Federica Troni']","We consider models of accidental dark matter, namely models in which the dark matter is a composite state that is stable thanks to an accidental symmetry of the theory. The fundamental constituents are vectorlike fermions, taken to be fragments of representations of the grand unifying gauge group $SU(5)$, as well as a scalar singlet. All the new fields are charged under a new confining gauge group, which we take to be $SU(N)$, leading to models with complex dark matter. We analyse the models in the context of $SU(5)$ grand unification with a non-standard approach recently proposed in the literature. The advantage of including the scalar mainly resides in the fact that it allows to break several undesired accidental symmetries leading to a larger set of viable models with respect to previous literature, in which only fermions (or only scalars) were considered. Moreover these models present distinct novelties, namely dark states with non-zero baryon and lepton number and the existence of composite hybrid states of fermions and scalars. We identify phenomena that are specific to the inclusion of the scalar and discuss possibilities to test this setup."
https://arxiv.org/abs/2403.07758,2024-03-12,HermEIS: A Parallel Multichannel Approach to Rapid Spectral Characterization of Neural MEAs,"['Akwasi Akwaboah', 'Ralph Etienne-Cummings']","The promise of increasing channel counts in high density ($> 10^4$) neural Microelectrode Arrays (MEAs) for high resolution recording comes with the curse of developing faster characterization strategies for concurrent acquisition of multichannel electrode integrities over a wide frequency spectrum. To circumvent the latency associated with the current multiplexed technique for impedance acquisition, it is common practice to resort to the single frequency impedance measurement (i.e. $Z_{1 \text{kHz}}$). This, however, does not offer sufficient spectral impedance information crucial for determining the capacity of electrodes at withstanding slow and fast-changing stimulus and recordings. In this work, we present \textit{HermEIS}, a novel approach that leverages single cycle in-phase and quadrature signal integrations for reducing the massive data throughput characteristic of such high density acquisition systems. As an initial proof-of-concept, we demonstrate over $6$ decades of impedance bandwidth ($5\times10^{-2} - 5\times10^{4}\text{ Hz}$) in a parallel $4$-channel potentiostatic setup composed of a custom PCB with off-the-shelf electronics working in tandem with an FPGA."
https://arxiv.org/abs/2403.07757,2024-03-12,FAUST XI: Enhancement of the complex organic material in the shocked matter surrounding the [BHB2007] 11 protobinary system,"['C. Vastel', 'T. Sakai', 'C. Ceccarelli', 'I. Jiménez-Serra', 'F. Alves', 'N. Balucani', 'E. Bianchi', 'M. Bouvier', 'P. Caselli', 'C. J. Chandler', 'S. Charnley', 'C. Codella', 'M. De Simone', 'F. Dulieu', 'L. Evans', 'F. Fontani', 'B. Lefloch', 'L. Loinard', 'F. Menard', 'L. Podio', 'G. Sabatini', 'N. Sakai', 'S. Yamamoto']","iCOMs are species commonly found in the interstellar medium. They are believed to be crucial seed species for the build-up of chemical complexity in star forming regions as well as our own Solar System. Thus, understanding how their abundances evolve during the star formation process and whether it enriches the emerging planetary system is of paramount importance. We use data from the ALMA Large Program FAUST to study the compact line emission towards the [BHB2007] 11 proto-binary system (sources A and B), where a complex structure of filaments connecting the two sources with a larger circumbinary disk has previously been detected. More than 45 CH3OCHO lines are clearly detected, as well as 8 CH3OCH3 transitions , 1 H2CCO transition and 4 t-HCOOH transitions. We compute the abundance ratios with respect to CH3OH for CH3OCHO, CH3OCH3, H2CCO, t-HCOOH (as well as an upper limit for CH3CHO) through a radiative transfer analysis. We also report the upper limits on the column densities of nitrogen bearing iCOMs, N(C2H5CN) and N(C2H3CN). The emission from the detected iCOMs and their precursors is compact and encompasses both protostars, which are separated by only 0.2"" (~ 28 au). The integrated intensities tend to align with the Southern filament, revealed by the high spatial resolution observations of the dust emission at 1.3 mm. A PV and 2D analysis are performed on the strongest and uncontaminated CH3OCH3 transition and show three different spatial and velocity regions, two of them being close to 11B (Southern filament) and the third one near 11A. All our observations suggest that the detected methanol, as well as the other iCOMs, are generated by the shocked gas from the incoming filaments streaming towards [BHB2007] 11A and 11B, respectively, making this source one of the few where chemical enrichment of the gas caused by the streaming material is observed."
https://arxiv.org/abs/2403.07756,2024-03-12,Complete Minimal Left-Right Symmetric Model File,"['Jonathan Kriewald', 'Miha Nemevšek', 'Fabrizio Nesti']","We develop a comprehensive implementation of the minimal Left-Right symmetric model within a FeynRules model file. We derive the complete set of mass spectra and mixings for the charged and neutral gauge bosons, would-be-Goldstones and gauge fixing, together with the ghost Lagrangian. In the scalar sector, we analytically re-derive all the massive states with mixings and devise a physical input scheme, which expresses the model couplings in terms of masses and mixing angles of all states. Fermion couplings are determined in closed form, including the Dirac mixing in the neutrino sector, evaluated explicitly with the Cayley-Hamilton method. We calculate the one loop next-to-leading QCD corrections and provide a complete UFO file for NLO studies, demonstrated on relevant benchmarks. We provide various restricted variants of the model file with different gauges, massless states, neutrino hierarchies and parity violating $g_L \neq g_R$ gauge couplings."
https://arxiv.org/abs/2403.07755,2024-03-12,Pediatric vaccine tender scheduling in low- and middle-income countries,"['Nicholas Uhorchak', 'Ruben A. Proano', 'Sandra Eksioglu', 'Fatih Cengil', 'Burak Eksioglu']","Effective and efficient scheduling of vaccine distribution can significantly impact vaccine uptake, which is critical to controlling the spread of infectious diseases. Ineffective scheduling can lead to waste, delays, and low vaccine coverage, potentially weakening the efforts to protect the public. Organizations such as UNICEF (United Nations Children's Fund), PAHO (Pan American Health Organization), and GAVI (Gavi, the Vaccine Alliance) coordinate vaccine tenders to ensure that enough supply is available on the international market at the lowest possible prices. Scheduling vaccine tenders over a planning horizon in a way that is equitable, efficient, and accessible is a complex problem that involves trade-offs between multiple objectives while ensuring that vaccine availability, demand, and logistical constraints are met. The current method for scheduling tenders is generally reactive and over short planning horizons. Vaccine tenders are scheduled when supply is insufficient to cover demand. We propose an optimization model to dynamically and proactively generate vaccine tender schedules over long planning horizons. This model helps us address the following research questions: What should the optimal sequencing and scheduling of vaccine tenders be to enhance affordability and profit over long time horizons? What is the optimal tender procurement schedule for single or multiple antigen scenarios? We use several real-life data sources to validate the model and address our research questions. Results from our analysis show when to schedule vaccine tenders, what volumes manufacturers should commit to, and the optimal tender lengths to satisfy demand. We show that vaccine tenders tend towards maximum lengths, generally converge over long time horizons, and are robust to changes in varying conditions."
https://arxiv.org/abs/2403.07754,2024-03-12,An Optimal Sequence Reconstruction Algorithm for Reed-Solomon Codes,"['Shubhransh Singhvi', 'Roni Con', 'Han Mao Kiah', 'Eitan Yaakobi']","The sequence reconstruction problem, introduced by Levenshtein in 2001, considers a scenario where the sender transmits a codeword from some codebook, and the receiver obtains $N$ noisy outputs of the codeword. We study the problem of efficient reconstruction using $N$ outputs that are each corrupted by at most $t$ substitutions. Specifically, for the ubiquitous Reed-Solomon codes, we adapt the Koetter-Vardy soft-decoding algorithm, presenting a reconstruction algorithm capable of correcting beyond Johnson radius. Furthermore, the algorithm uses $\mathcal{O}(nN)$ field operations, where $n$ is the codeword length."
https://arxiv.org/abs/2403.07753,2024-03-12,A robust SVM-based approach with feature selection and outliers detection for classification problems,"['Marta Baldomero-Naranjo', 'Luisa I. Martínez-Merino', 'Antonio M. Rodríguez-Chía']","This paper proposes a robust classification model, based on support vector machine (SVM), which simultaneously deals with outliers detection and feature selection. The classifier is built considering the ramp loss margin error and it includes a budget constraint to limit the number of selected features. The search of this classifier is modeled using a mixed-integer formulation with big M parameters. Two different approaches (exact and heuristic) are proposed to solve the model. The heuristic approach is validated by comparing the quality of the solutions provided by this approach with the exact approach. In addition, the classifiers obtained with the heuristic method are tested and compared with existing SVM-based models to demonstrate their efficiency."
https://arxiv.org/abs/2403.07752,2024-03-12,Vision-based Vehicle Re-identification in Bridge Scenario using Flock Similarity,"['Chunfeng Zhang', 'Ping Wang']","Due to the needs of road traffic flow monitoring and public safety management, video surveillance cameras are widely distributed in urban roads. However, the information captured directly by each camera is siloed, making it difficult to use it effectively. Vehicle re-identification refers to finding a vehicle that appears under one camera in another camera, which can correlate the information captured by multiple cameras. While license plate recognition plays an important role in some applications, there are some scenarios where re-identification method based on vehicle appearance are more suitable. The main challenge is that the data of vehicle appearance has the characteristics of high inter-class similarity and large intra-class differences. Therefore, it is difficult to accurately distinguish between different vehicles by relying only on vehicle appearance information. At this time, it is often necessary to introduce some extra information, such as spatio-temporal information. Nevertheless, the relative position of the vehicles rarely changes when passing through two adjacent cameras in the bridge scenario. In this paper, we present a vehicle re-identification method based on flock similarity, which improves the accuracy of vehicle re-identification by utilizing vehicle information adjacent to the target vehicle. When the relative position of the vehicles remains unchanged and flock size is appropriate, we obtain an average relative improvement of 204% on VeRi dataset in our experiments. Then, the effect of the magnitude of the relative position change of the vehicles as they pass through two cameras is discussed. We present two metrics that can be used to quantify the difference and establish a connection between them. Although this assumption is based on the bridge scenario, it is often true in other scenarios due to driving safety and camera location."
https://arxiv.org/abs/2403.07751,2024-03-12,Quotients of M-convex sets and M-convex functions,"['Marie-Charlotte Brandenburg', 'Georg Loho', 'Ben Smith']","We unify the study of quotients of matroids, polymatroids, valuated matroids and strong maps of submodular functions in the framework of Murota's discrete convex analysis. As a main result, we compile a list of ten equivalent characterizations of quotients for M-convex sets, generalizing existing formulations for (poly)matroids and submodular functions. We also initiate the study of quotients of M-convex functions, constructing a hierarchy of four separate characterizations. Our investigations yield new insights into the fundamental operation of induction, as well as the structure of linking sets and linking functions, which are generalizations of linking systems and bimatroids."
https://arxiv.org/abs/2403.07750,2024-03-12,Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings,"['Sahand Sharifzadeh', 'Christos Kaplanis', 'Shreya Pathak', 'Dharshan Kumaran', 'Anastasija Ilic', 'Jovana Mitrovic', 'Charles Blundell', 'Andrea Banino']","The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel space. This research introduces a promising technique for generating large-scale, customizable image datasets, leading to enhanced VLM performance and wider applicability across various domains, all with improved data efficiency and resource utilization."
https://arxiv.org/abs/2403.07749,2024-03-12,Distributed Estimation by Two Agents with Different Feature Spaces,"['Aneesh Raghavan', 'Karl Henrik Johansson']","We consider the problem of estimation of a function by two agents (and a fusion center) given local data. Data comprises of samples of an independent variable and the corresponding value of a dependent variable. The agents are given a set of features using which they construct suitable function spaces to formulate and solve the estimation problem. The estimated functions are to be uploaded to a fusion space where they are fused to obtain the system estimate of the mapping and then downloaded by the agents to gather knowledge about the other agents estimate of the function. To this end, we present the following: a systematic construction of fusion space given the features of the agents; the derivation of an uploading operator for the agents to upload their estimated functions to a fusion space; an optimization problem in the fusion space to fuse the functions uploaded; the derivation of a downloading operator for the fused function to be downloaded. Through an example on least squares regression, we demonstrate the distributed estimation architecture that has been developed."
https://arxiv.org/abs/2403.07748,2024-03-12,Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph,['Romain Cosson'],"We investigate two fundamental problems in mobile computing: exploration and rendezvous, with two distinct mobile agents in an unknown graph. The agents can read and write information on whiteboards that are located at all nodes. They both move along one adjacent edge at every time-step. In the exploration problem, both agents start from the same node of the graph and must traverse all of its edges. We show that a simple variant of depth-first search achieves collective exploration in $m$ synchronous time-steps, where $m$ is the number of edges of the graph. This improves the competitive ratio of collective graph exploration. In the rendezvous problem, the agents start from different nodes of the graph and must meet as fast as possible. We introduce an algorithm guaranteeing rendezvous in at most $\frac{3}{2}m$ time-steps. This improves over the so-called `wait for Mommy' algorithm which requires $2m$ time-steps. All our guarantees are derived from a more general asynchronous setting in which the speeds of the agents are controlled by an adversary at all times. Our guarantees also generalize to weighted graphs, if the number of edges $m$ is replaced by the sum of all edge lengths."
https://arxiv.org/abs/2403.07747,2024-03-12,FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models,"['Yan Liu', 'Renren Jin', 'Lin Shi', 'Zheng Yao', 'Deyi Xiong']","To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathematical reasoning capability of Chinese LLMs. We also carry out an in-depth analysis on the evaluation process and methods that have been overlooked previously. These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities. The dataset will be publicly available soon."
https://arxiv.org/abs/2403.07746,2024-03-12,"Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception","['Philipp Wolters', 'Johannes Gilg', 'Torben Teepe', 'Fabian Herzog', 'Anouar Laouichi', 'Martin Hofmann', 'Gerhard Rigoll']","Low-cost, vision-centric 3D perception systems for autonomous driving have made significant progress in recent years, narrowing the gap to expensive LiDAR-based methods. The primary challenge in becoming a fully reliable alternative lies in robust depth prediction capabilities, as camera-based systems struggle with long detection ranges and adverse lighting and weather conditions. In this work, we introduce HyDRa, a novel camera-radar fusion architecture for diverse 3D perception tasks. Building upon the principles of dense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid fusion approach to combine the strengths of complementary camera and radar features in two distinct representation spaces. Our Height Association Transformer module leverages radar features already in the perspective view to produce more robust and accurate depth predictions. In the BEV, we refine the initial sparse representation by a Radar-weighted Depth Consistency. HyDRa achieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and 58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new semantically rich and spatially accurate BEV features can be directly converted into a powerful occupancy representation, beating all previous camera-based methods on the Occ3D benchmark by an impressive 3.7 mIoU."
https://arxiv.org/abs/2403.07745,2024-03-12,Probabilistic Easy Variational Causal Effect,"['Usef Faghihi', 'Amir Saki']","Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the total variation and the flux of $g$, we develop a point of view in causal inference capable of dealing with a broad domain of causal problems. Indeed, we focus on a function, called Probabilistic Easy Variational Causal Effect (PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect to continuously and interventionally changing the values of $X$ while keeping the value of $Z$ constant. PEACE is a function of $d\ge 0$, which is a degree managing the strengths of probability density values $f(x|z)$. On the other hand, we generalize the above idea for the discrete case and show its compatibility with the continuous case. Further, we investigate some properties of PEACE using measure theoretical concepts. Furthermore, we provide some identifiability criteria and several examples showing the generic capability of PEACE. We note that PEACE can deal with the causal problems for which micro-level or just macro-level changes in the value of the input variables are important. Finally, PEACE is stable under small changes in $\partial g_{in}/\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is obtained from $g$ by removing all functional relationships defining $X$ and $Z$."
https://arxiv.org/abs/2403.07744,2024-03-12,Harnessing two-photon dissipation for enhanced quantum measurement and control,"['Antoine Marquet', 'Simon Dupouy', 'Ulysse Réglade', 'Antoine Essig', 'Joachim Cohen', 'Emanuele Abertinale', 'Audrey Bienfait', 'Théau Peronnin', 'Sébastien Jezouin', 'Raphaël Lescanne', 'Benjamin Huard']","Dissipation engineering offers a powerful tool for quantum technologies. Recently, new superconducting devices demonstrated an engineered two-photon dissipation rate exceeding all other relevant timescales. In particular, they have proven most useful to prevent transitions between the logical states $|\pmα\rangle$ of a cat qubit. Here, we present three key applications of strong two-photon dissipation for quantum measurement and control, beyond cat qubit stabilization. Firstly, we demonstrate its efficacy in overcoming limitations encountered in Wigner tomography at high photon numbers. Secondly, we showcase its potential for realizing universal gates on cat qubits, exploiting the coherent mapping between cat qubit states and superpositions of 0 and 1 photons. Finally, we harness the transient dynamics of a cat state under two-photon dissipation to prepare squeezed cat states with a squeezing factor exceeding 3.8 dB."
https://arxiv.org/abs/2403.07743,2024-03-13,Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs,"['Neel Kanwal', 'Farbod Khoraminia', 'Umay Kiraz', 'Andres Mosquera-Zamudio', 'Carlos Monteagudo', 'Emiel A. M. Janssen', 'Tahlita C. M. Zuiverloon', 'Chunmig Rong', 'Kjersti Engan']","Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probability distribution to improve the sensitivity of the MoE. We developed DL pipelines using two MoEs and two multiclass models of state-of-the-art deep convolutional neural networks (DCNNs) and vision transformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed simpler multiclass models and were tested on datasets from different hospitals and cancer types, where MoE using DCNNs yielded the best results. The proposed MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining less computational cost for inference than MoE using ViTs. This best performance of MoEs comes with relatively higher computational trade-offs than multiclass models. The proposed artifact detection pipeline will not only ensure reliable CPATH predictions but may also provide quality control."
https://arxiv.org/abs/2403.07742,2024-03-12,Satellite quenching and morphological transformation of galaxies in groups and clusters,"['M. Oxland', 'L. C. Parker', 'R. R. de Carvalho', 'V. M. Sampaio']","We investigate the role that dense environments have on the quenching of star formation and the transformation of morphology for a sample of galaxies selected from the Sloan Digital Sky Survey. We make a distinction between galaxies falling into groups $(13 \leq \log{(M_{\text{halo}}/M_{\odot})} < 14)$ and clusters $(\log{(M_{\text{halo}}/M_{\odot})} \geq 14)$, and compare to a large sample of field galaxies. Using galaxy position in projected phase space as a proxy for time since infall, we study how galaxy specific star formation rate (sSFR) and morphology, parameterized by the bulge-to-total light ratio (B/T), change over time. After controlling for stellar mass, we find clear trends of increasing quenched and elliptical fractions as functions of infall time for galaxies falling into both groups and clusters. The trends are strongest for low mass galaxies falling into clusters. By computing quenching and morphological transformation timescales, we find evidence that star formation quenching occurs faster than morphological transformation in both environments. Comparing field galaxies to recently infalling galaxies, we determine there is pre-processing of both star formation and morphology, with pre-processing affecting star formation rates more strongly. Our analysis favours quenching mechanisms that act quickly to suppress star formation, while other mechanisms that act on longer timescales transform morphology through bulge growth and disc fading."
https://arxiv.org/abs/2403.07741,2024-03-12,Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation,"['Kira Wursthorn', 'Markus Hillemann', 'Markus Ulrich']","The estimation of 6D object poses is a fundamental task in many computer vision applications. Particularly, in high risk scenarios such as human-robot interaction, industrial inspection, and automation, reliable pose estimates are crucial. In the last years, increasingly accurate and robust deep-learning-based approaches for 6D object pose estimation have been proposed. Many top-performing methods are not end-to-end trainable but consist of multiple stages. In the context of deep uncertainty quantification, deep ensembles are considered as state of the art since they have been proven to produce well-calibrated and robust uncertainty estimates. However, deep ensembles can only be applied to methods that can be trained end-to-end. In this work, we propose a method to quantify the uncertainty of multi-stage 6D object pose estimation approaches with deep ensembles. For the implementation, we choose SurfEmb as representative, since it is one of the top-performing 6D object pose estimation approaches in the BOP Challenge 2022. We apply established metrics and concepts for deep uncertainty quantification to evaluate the results. Furthermore, we propose a novel uncertainty calibration score for regression tasks to quantify the quality of the estimated uncertainty."
https://arxiv.org/abs/2403.07740,2024-03-12,Gapless neutron superfluidity can explain the late time cooling of transiently accreting neutron stars,"['Valentin Allard', 'Nicolas Chamel']","The current interpretation of the observed late time cooling of transiently accreting neutron stars in low-mass X-ray binaries during quiescence requires the suppression of neutron superfluidity in their crust at variance with recent ab initio many-body calculations of dense matter. Focusing on the two emblematic sources KS~1731$-$260 and MXB~1659$-$29, we show that their thermal evolution can be naturally explained by considering the existence of a neutron superflow driven by the pinning of quantized vortices. Under such circumstances, we find that the neutron superfluid can be in a gapless state in which the specific heat is dramatically increased compared to that in the classical BCS state assumed so far, thus delaying the thermal relaxation of the crust. We have performed neutron-star cooling simulations taking into account gapless superfluidity and we have obtained excellent fits to the data thus reconciling astrophysical observations with microscopic theories. The imprint of gapless superfluidity on other observable phenomena is briefly discussed."
https://arxiv.org/abs/2403.07739,2024-03-12,Topological Data Analysis of Monopoles in $U(1)$ Lattice Gauge Theory,"['Xavier Crean', 'Jeffrey Giansiracusa', 'Biagio Lucini']","In $4$-dimensional pure compact $U(1)$ lattice gauge theory, we analyse topological aspects of the dynamics of monopoles across the deconfinement phase transition. We do this using tools from Topological Data Analysis (TDA). We demonstrate that observables constructed from the zeroth and first homology groups of monopole current networks may be used to quantitatively and robustly locate the critical inverse coupling $β_{c}$ through finite-size scaling. Our method provides a mathematically robust framework for the characterisation of topological invariants related to monopole currents, putting on firmer ground earlier investigations. Moreover, our approach can be generalised to the study of Abelian monopoles in non-Abelian gauge theories."
https://arxiv.org/abs/2403.07738,2024-03-12,Three statistical descriptions of classical systems and their extensions to hybrid quantum-classical systems,"['Andrés Darío Bermúdez Manjarres', 'Marcel Reginatto', 'Sebastian Ulbricht']","We present three statistical descriptions for systems of classical particles and consider their extension to hybrid quantum-classical systems. The classical descriptions are ensembles on configuration space, ensembles on phase space, and a Hilbert space approach using van Hove operators which provides an alternative to the Koopman-von Neumann formulation. In all cases, there is a natural way to define classical observables and a corresponding Lie algebra that is isomorphic to the usual Poisson algebra in phase space. We show that in the case of classical particles, the three descriptions are equivalent and indicate how they are related. We then modify and extend these descriptions to introduce hybrid models where a classical particle interacts with a quantum particle. The approach of ensembles on phase space and the Hilbert space approach, which are novel, lead to equivalent hybrid models, while they are not equivalent to the hybrid model of the approach of ensembles on configuration space. Thus, we end up identifying two inequivalent types of hybrid systems, making different predictions, especially when it comes to entanglement. These results are of interest regarding ``no-go'' theorems about quantum systems interacting via a classical mediator which address the issue of whether gravity must be quantized. Such theorems typically require assumptions that make them model dependent. The hybrid systems that we discuss provide concrete examples of inequivalent models that can be used to compute simple examples to test the assumptions of the ``no-go'' theorems and their applicability."
https://arxiv.org/abs/2403.07737,2024-03-12,Photo-induced Ferromagnetic and Superconducting Orders in Multi-orbital Hubbard Models,"['Sujay Ray', 'Philipp Werner']","The search for hidden orders in photoexcited lattice systems is an active research field driven by experimental reports of light-induced or light-stabilized phases. In this study, we investigate hidden electronic orders in strongly correlated two-orbital Hubbard models with orbital-dependent bandwidths. In equilibrium, the half-filled systems are antiferromagnetically ordered. Using non-equilibrium dynamical mean field theory we demonstrate the appearance of nonthermal ferromagnetic order in the photo-doped state, if the two bandwidths are sufficiently different, and its coexistence with spin-singlet $η$-superconductivity in the high photo-doping region. Spin-triplet $η$-superconducting order appears instead if the two bandwidths are comparable. The rich nonequilibrium phasediagram uncovered in this work shows that Mott insulating multi-orbital systems provide an interesting platform for the realization of nonthermal electronic orders."
https://arxiv.org/abs/2403.07736,2024-03-12,Tightening big Ms in integer programming formulations for support vector machines with ramp loss,"['Marta Baldomero-Naranjo', 'Luisa I. Martínez-Merino', 'Antonio M. Rodríguez-Chía']","This paper considers various models of support vector machines with ramp loss, these being an efficient and robust tool in supervised classification for the detection of outliers. The exact solution approaches for the resulting optimization problem are of high demand for large datasets. Hence, the goal of this paper is to develop algorithms that provide efficient methodologies to exactly solve these optimization problems. These approaches are based on three strategies for obtaining tightened values of the big M parameters included in the formulation of the problem. Two of them require solving a sequence of continuous problems, while the third uses the Lagrangian relaxation to tighten the bounds. The proposed resolution methods are valid for the l1-norm and l2-norm ramp loss formulations. They were tested and compared with existing solution methods in simulated and real-life datasets, showing the efficiency of the developed methodology."
https://arxiv.org/abs/2403.07735,2024-03-12,The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels,"['Florian Kalinke', 'Zoltan Szabo']","Kernel techniques are among the most influential approaches in data science and statistics. Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of $M\ge 2$ random variables. Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature). Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open. In this work, we prove that the minimax optimal rate of HSIC estimation on $\mathbb R^d$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $\mathcal O\!\left(n^{-1/2}\right)$. Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nyström-based one) on $\mathbb R^d$."
https://arxiv.org/abs/2403.07734,2024-03-12,On disintegration of lean hydrogen flames in narrow gaps,"['Jorge Yanez', 'Leonid Kagan', 'Mike Kuznetsov', 'Gregory Sivashinsky']","The disintegration of near limit flames propagating through the gap of Hele-Shaw cells has recently become a subject of active research. In this paper, the flamelets resulting from the disintegration of the continuous front are interpreted in terms of the Zeldovich flame-balls stabilized by volumetric heat losses. A complicated free-boundary problem for 2D self-drifting near circular flamelets is reduced to a 1D model. The 1D formulation is then utilized to obtain the locus of the flamelet velocity, size, heat losses and Lewis numbers at which the self-drifting flamelets may exist."
https://arxiv.org/abs/2403.07733,2024-03-12,DSEG-LIME - Improving Image Explanation by Hierarchical Data-Driven Segmentation,"['Patrick Knab', 'Sascha Marton', 'Christian Bartelt']","Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition. We benchmark DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge. The analysis includes a quantitative evaluation using established XAI metrics, complemented by a qualitative assessment through a user study. Our findings demonstrate that DSEG outperforms in most of the XAI metrics and enhances the alignment of explanations with human-recognized concepts, significantly improving interpretability. The code is available under: https://github. com/patrick-knab/DSEG-LIME"
https://arxiv.org/abs/2403.07732,2024-03-12,DESERE: The 1st Workshop on Decentralised Search and Recommendation,"['Mohamed Ragab', 'Yury Savateev', 'Wenjie Wang', 'Reza Moosaei', 'Thanassis Tiropanis', 'Alexandra Poulovassilis', 'Adriane Chapman', 'Helen Oliver', 'George Roussos']","The DESERE Workshop, our First Workshop on Decentralised Search and Recommendation, offers a platform for researchers to explore and share innovative ideas on decentralised web services, mainly focusing on three major topics: (i) societal impact of decentralised systems: their effect on privacy, policy, and regulation; (ii) decentralising applications: algorithmic and performance challenges that arise from decentralisation; and (iii) infrastructure to support decentralised systems and services: peer-to-peer networks, routing, and performance evaluation tools"
https://arxiv.org/abs/2403.07731,2024-03-12,Performance Analysis of Matrix Multiplication for Deep Learning on the Edge,"['Cristian Ramírez', 'Adrián Castelló', 'Héctor Martínez', 'Enrique S. Quintana-Ortí']","The devices designed for the Internet-of-Things encompass a large variety of distinct processor architectures, forming a highly heterogeneous zoo. In order to tackle this, we employ a simulator to estimate the performance of the matrix-matrix multiplication (GEMM) kernel on processors designed to operate at the edge. Our simulator adheres to the modern implementations of GEMM, advocated by GotoBLAS2, BLIS, OpenBLAS, etc., to carefully account for the amount of data transfers across the memory hierarchy of different algorithmic variants of the kernel. %Armed with this tool, A small collection of experiments provide the necessary data to calibrate the simulator and deliver highly accurate estimations of the execution time for a given processor architecture."
https://arxiv.org/abs/2403.07730,2024-03-12,Mechanisms of Elevated Temperature Galling in Hardfacings,"['Samuel R. Rogers', 'David Stewart', 'Paul Taplin', 'David Dye']","The galling mechanism of Tristelle 5183, an Fe-based hardfacing alloy, was investigated at elevated temperature. The test was performed using a bespoke galling rig. Adhesive transfer and galling were found to occur, as a result of shear at the adhesion boundary and the activation of an internal shear plane within one of the tribosurfaces. During deformation, carbides were observed to have fractured, as a result of the shear train they were exposed to and their lack of ductility. In the case of niobium carbides, their fracture resulted in the formation of voids, which were found to coalesce and led to cracking and adhesive transfer. A tribologically affected zone (TAZ) was found to form, which contained nanocrystalline austenite, as a result of the shear exerted within 30μm of the adhesion boundaries. The galling of Tristelle 5183 initiated from the formation of an adhesive boundary, followed by sub-surface shear in only one tribosurface, Following further sub-surface shear, an internal shear plane is activated. internal shear and shear at the adhesion boundary continues until fracture occur, resulting in adhesive transfer."
https://arxiv.org/abs/2403.07729,2024-03-12,Phonon pseudoangular momentum in $α$-MoO$_3$,"['Meiqi Li', 'Zhibing Li', 'Huanjun Chen', 'Weiliang Wang']","In recent studies, it has been discovered that phonons can carry angular momentum, leading to a series of investigations into systems with 3-fold rotation symmetry. However, for systems with 2-fold screw rotational symmetry, such as $α$-MoO$_3$, there has been no relevant discussion. In this paper, we investigated the pseudoangular momentum of phonons in crystals with 2-fold screw rotational symmetry. Taking $α$-MoO$_3$ as an example, we explain the selection rules in circularly polarized Raman experiments resulting from pseudoangular momentum conservation, providing important guidance for experiments. This study of pseudoangular momentum in $α$-MoO$_3$ opens up a new degree of freedom for its potential applications, expanding into new application domains."
https://arxiv.org/abs/2403.07728,2024-03-12,CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control,"['Yajie Bao', 'Yuyang Huo', 'Haojie Ren', 'Changliang Zou']","We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for popular online selection rules. We proved that CAS can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes. For the decision-driven selection rule, including most online multiple-testing procedures, CAS can exactly control the real-time FCR below the target level without any distributional assumptions. For the online selection with symmetric thresholds, we establish the error bound for the control gap of FCR under mild distributional assumptions. To account for the distribution shift in online data, we also embed CAS into some recent dynamic conformal prediction methods and examine the long-run FCR control. Numerical results on both synthetic and real data corroborate that CAS can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings."
https://arxiv.org/abs/2403.07727,2024-03-12,Tidal dissipation of binaries in asteroid pairs,"['Laurent Pou', 'Francis Nimmo']","Tidal dissipation in a celestial body can be used to probe its internal structure. Tides govern the orbital evolution of binary systems and therefore constraints on the interior of binary system members can be derived by knowing the age and tidal state of the binary system. For asteroids, age estimates are challenging due to a lack of direct observation of their surface. However, the age of asteroid pairs formed by rotational fission of a parent body can be derived from dynamical modeling, and as such can be used to constrain the age of binary systems existing within asteroid pairs.We study 13 binary asteroid systems existing in asteroid pairs by modeling their tidal locking and eccentricity damping timescales from tidal dissipation in the primaries and secondaries. We consider the impact of thermal torques on these timescales from the YORP and BYORP effects. The resulting constraints on the tidal dissipation ratio Q/k2 are compared to monolithic and rubble pile asteroid theories, showing that all secondaries are consistent with rubble piles with regolith layers greater than 3m and suggest that Q/k2 for rubble piles increases with radius. A particular case is the first bound secondary of asteroid (3749) Balam, whose Q/k2 is constrained to be between 2.7x10^4 and 1.4x10^6, consistent with a rubble-pile with a regolith thickness between 30m and 100m."
https://arxiv.org/abs/2403.07726,2024-03-12,"SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes","['Timothee Mickus', 'Elaine Zosa', 'Raúl Vázquez', 'Teemu Vahtola', 'Jörg Tiedemann', 'Vincent Segonne', 'Alessandro Raganato', 'Marianna Apidianaki']","This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate. Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical. The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling."
https://arxiv.org/abs/2403.07725,2024-03-12,Algebraicity in monochromatic homotopy theory,['Torgeir Aambø'],"Using Patchkoria--Pstrągowski's version of Franke's algebraicity theorem, we prove that the category of $K_p(n)$-local spectra is exotically equivalent to the category of derived $I_n$-complete periodic comodules over the Adams Hopf algebroid $(E_*, E_*E)$ for large primes. This gives a finite prime result analogous to the asymptotic algebraicity for $\mathrm{Sp}_{K_p(n)}$ of Barthel--Schlank--Stapleton."
https://arxiv.org/abs/2403.07724,2024-03-12,Balancing Fairness and Accuracy in Data-Restricted Binary Classification,"['Zachary McBride Lazri', 'Danial Dervovic', 'Antigoni Polychroniadou', 'Ivan Brugere', 'Dana Dachman-Soled', 'Min Wu']","Applications that deal with sensitive information may have restrictions placed on the data available to a machine learning (ML) classifier. For example, in some applications, a classifier may not have direct access to sensitive attributes, affecting its ability to produce accurate and fair decisions. This paper proposes a framework that models the trade-off between accuracy and fairness under four practical scenarios that dictate the type of data available for analysis. Prior works examine this trade-off by analyzing the outputs of a scoring function that has been trained to implicitly learn the underlying distribution of the feature vector, class label, and sensitive attribute of a dataset. In contrast, our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself. This approach enables us to formulate multiple convex optimization problems, which allow us to answer the question: How is the accuracy of a Bayesian classifier affected in different data restricting scenarios when constrained to be fair? Analysis is performed on a set of fairness definitions that include group and individual fairness. Experiments on three datasets demonstrate the utility of the proposed framework as a tool for quantifying the trade-offs among different fairness notions and their distributional dependencies."
https://arxiv.org/abs/2403.07723,2024-03-12,On the Last-Iterate Convergence of Shuffling Gradient Methods,"['Zijian Liu', 'Zhengyuan Zhou']","Shuffling gradient methods, which are also known as stochastic gradient descent (SGD) without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objective value even without strong convexity. Our new results either (nearly) match the existing last-iterate lower bounds or are as fast as the previous best upper bounds for the average iterate."
https://arxiv.org/abs/2403.07722,2024-03-12,Higgs photon associated production in a Two Higgs Doublet Type-II Seesaw Model at future electron-positron colliders,"['B. Ait Ouazghour', 'M. Chabab', 'K. Goure']","We studied the one-loop prediction for the single production of a Higgs-like boson in association with a photon in electron-positron collisions in the context of two Higgs doublet type-II seesaw model (2HDMcT). We explored to what extent the new scalars in the (2HDMcT) spectrum affect its production cross-section, the ratio $R_{γh_1}$ as well as the signal strengths $R_{γZ}$ and $R_{γγ}$ when $h_1$ is identified with the observed $SM $ Higgs boson within the $2HDMcT$ delimited parameter space. More specifically, we focused on $e^+e^- \to h_1γ$ process at one-loop, and analyzed how it evolves under a full set of theoretical constraints and the available experimental data, including $B\to X_sγ$ constraint at 95$\%$ C.L. Our analysis showed that these observables are particularly sensitive to the parameters $α_1$, $λ_{7}$, $λ_{9}$, the trilinear Higgs couplings and to the charged Higgs masses. We found that $σ(e^+e^- \to h_1γ)$ can significantly be enhanced up to $8.1\,\, 10^{-2}$ fb, so exceeding the Standard Model prediction. Additionally, as a byproduct, we also found that $R_{γh_1}$ is entirely correlated with both the $h_1\toγγ$ and $h_1\toγZ$ signal strengths."
https://arxiv.org/abs/2403.07721,2024-03-13,Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion,"['Dongyang Li', 'Chen Wei', 'Shiying Li', 'Jiachen Zou', 'Quanying Liu']","How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning. Modern contrastive learning and generative models improved the performance of fMRI-based visual decoding and reconstruction. However, the high cost and low temporal resolution of fMRI limit their applications in brain-computer interfaces (BCIs), prompting a high need for EEG-based visual reconstruction. In this study, we present an EEG-based visual reconstruction framework. It consists of a plug-and-play EEG encoder called the Adaptive Thinking Mapper (ATM), which is aligned with image embeddings, and a two-stage EEG guidance image generator that first transforms EEG features into image priors and then reconstructs the visual stimuli with a pre-trained image generator. Our approach allows EEG embeddings to achieve superior performance in image classification and retrieval tasks. Our two-stage image generation strategy vividly reconstructs images seen by humans. Furthermore, we analyzed the impact of signals from different time windows and brain regions on decoding and reconstruction. The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality. We report that EEG-based visual decoding achieves SOTA performance, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications. The code of ATM is available at https://github.com/dongyangli-del/EEG_Image_decode."
https://arxiv.org/abs/2403.07720,2024-03-12,Multi-modal Auto-regressive Modeling via Visual Words,"['Tianshuo Peng', 'Zuchao Li', 'Lefei Zhang', 'Hai Zhao', 'Ping Wang', 'Bo Du']","Large Language Models (LLMs), benefiting from the auto-regressive modelling approach performed on massive unannotated texts corpora, demonstrates powerful perceptual and reasoning capabilities. However, as for extending auto-regressive modelling to multi-modal scenarios to build Large Multi-modal Models (LMMs), there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification. In this paper, we successfully perform multi-modal auto-regressive modeling with a unified objective for the first time. Specifically, we propose the concept of visual words, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling. We further explore the distribution of visual features in the semantic space within LMM and the possibility of using text embeddings to represent visual information. Experimental results and ablation studies on 5 VQA tasks and 4 benchmark toolkits validate the powerful performance of our proposed approach."
https://arxiv.org/abs/2403.07719,2024-03-12,Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis,"['Jiawen Li', 'Yuxuan Chen', 'Hongbo Chu', 'Qiehe Sun', 'Tian Guan', 'Anjia Han', 'Yonghong He']","Histopathological whole slide images (WSIs) classification has become a foundation task in medical microscopic imaging processing. Prevailing approaches involve learning WSIs as instance-bag representations, emphasizing significant instances but struggling to capture the interactions between instances. Additionally, conventional graph representation methods utilize explicit spatial positions to construct topological structures but restrict the flexible interaction capabilities between instances at arbitrary locations, particularly when spatially distant. In response, we propose a novel dynamic graph representation algorithm that conceptualizes WSIs as a form of the knowledge graph structure. Specifically, we dynamically construct neighbors and directed edge embeddings based on the head and tail relationships between instances. Then, we devise a knowledge-aware attention mechanism that can update the head node features by learning the joint attention score of each neighbor and edge. Finally, we obtain a graph-level embedding through the global pooling process of the updated head, serving as an implicit representation for the WSI classification. Our end-to-end graph representation learning approach has outperformed the state-of-the-art WSI analysis methods on three TCGA benchmark datasets and in-house test sets. Our code is available at https://github.com/WonderLandxD/WiKG."
https://arxiv.org/abs/2403.07718,2024-03-12,WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?,"['Alexandre Drouin', 'Maxime Gasse', 'Massimo Caccia', 'Issam H. Laradji', 'Manuel Del Verme', 'Tom Marty', 'Léo Boisvert', 'Megh Thakkar', 'Quentin Cappart', 'David Vazquez', 'Nicolas Chapados', 'Alexandre Lacoste']","We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field."
https://arxiv.org/abs/2403.07717,2024-03-12,Resolving Dual Active Galactic Nuclei with ~100 pc separation in MCG-03-34-64,"['Anna Trindade Falcao', 'T. J. Turner', 'S. B. Kraemer', 'V. Braito', 'J. Reeves', 'H. R. Schmitt']","Galaxy mergers are expected to play a key role in the evolution of galaxies and their central supermassive black holes (SMBHs). An observational signature of this phenomenon is the detection of dual active galactic nuclei (AGNs) amongst merging systems, as predicted by cosmological models of structure formation. Dual AGNs at sub-kiloparsec-scale separation are the precursors of merging black hole binaries, an important source of gravitational waves, but a paucity of such systems have been confirmed to date by direct imaging, while other similar claims have been strongly disputed. Here we report the serendipitous multiwavelength discovery of a dual black hole system with a separation of ~100 pc, in the gas-rich luminous infrared galaxy MCG-03-34-64 (z=0.016). Chandra/ACIS imaging shows two spatially-resolved peaks of equal intensity in the neutral Fe Ka emission line, consistent with a dual SMBH, which is supported by Hubble Space Telescope (HST), and Very Large Array (VLA) observations. The separation of ~100 pc is the closest dual AGN separation reported to date with spatially-resolved, multiwavelength observations."
https://arxiv.org/abs/2403.07716,2024-03-12,Main-sequence exoplanet systems: tidal evolution,['Kaloyan Penev'],"The easiest exoplanets to detect are those that orbit very close to their hoststars. As a result, even though these planets are quite rare, they represent amajor fraction of the current exoplanet population. A side-effect of theproximity between the planet and the star is that the two have strong mutualinteractions through a number of physical processes. One of the most importantof these processes is tides. Tides are thought to shape the orbits of close-inexoplanets, heat the planet making its radius expand, and even drive someplanets to spiral into their host stars. This chapter briefly introduces thebasics of tidal physics and describes the various fingerprints tides leavewithin the observed exoplanet population."
https://arxiv.org/abs/2403.07715,2024-03-12,Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound,"['Blake VanBerlo', 'Alexander Wong', 'Jesse Hoey', 'Robert Arntfield']","Self-supervised learning (SSL) is one strategy for addressing the paucity of labelled data in medical imaging by learning representations from unlabelled images. Contrastive and non-contrastive SSL methods produce learned representations that are similar for pairs of related images. Such pairs are commonly constructed by randomly distorting the same image twice. The videographic nature of ultrasound offers flexibility for defining the similarity relationship between pairs of images. In this study, we investigated the effect of utilizing proximal, distinct images from the same B-mode ultrasound video as pairs for SSL. Additionally, we introduced a sample weighting scheme that increases the weight of closer image pairs and demonstrated how it can be integrated into SSL objectives. Named Intra-Video Positive Pairs (IVPP), the method surpassed previous ultrasound-specific contrastive learning methods' average test accuracy on COVID-19 classification with the POCUS dataset by $\ge 1.3\%$. Detailed investigations of IVPP's hyperparameters revealed that some combinations of IVPP hyperparameters can lead to improved or worsened performance, depending on the downstream task. Guidelines for practitioners were synthesized based on the results, such as the merit of IVPP with task-specific hyperparameters, and the improved performance of contrastive methods for ultrasound compared to non-contrastive counterparts."
https://arxiv.org/abs/2403.07714,2024-03-13,StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models,"['Zhicheng Guo', 'Sijie Cheng', 'Hao Wang', 'Shihao Liang', 'Yujia Qin', 'Peng Li', 'Zhiyuan Liu', 'Maosong Sun', 'Yang Liu']","Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system."
https://arxiv.org/abs/2403.07713,2024-03-12,Coupling of radiation and magnetospheric accretion flow in ULX pulsars: radiation pressure and photon escape time,"['Caitlyn Flexer', 'Alexander A. Mushtukov']","The accretion flow within the magnetospheric radius of bright X-ray pulsars can form an optically thick envelope, concealing the central neutron star from the distant observer. Most photons are emitted at the surface of a neutron star and leave the system after multiple reflections by the accretion material covering the magnetosphere. Reflections cause momentum to be transferred between photons and the accretion flow, which contributes to the radiative force and should thus influence the dynamics of accretion. We employ Monte Carlo simulations and estimate the acceleration along magnetic field lines due to the radiative force as well as the radiation pressure across magnetic field lines. We demonstrate that the radiative acceleration can exceed gravitational acceleration along the field lines, and similarly, radiation pressure can exceed magnetic field pressure. Multiple reflections of X-ray photons back into the envelope tend to amplify both radiative force along the field lines and radiative pressure. We analyze the average photon escape time from the magnetosphere of a star and show that its absolute value is weakly dependent on the magnetic field strength of a star and roughly linearly dependent on the mass accretion rate being $\sim 0.1\,{\rm s}$ at $\dot{M}\sim 10^{20}\,{\rm g\,s^{-1}}$. At high mass accretion rates, the escape time can be longer than free-fall time from the inner disc radius."
https://arxiv.org/abs/2403.07712,2024-03-12,Nonlocal Stokes equation with relaxation on the divergence free equation,"['Yajie Zhang', 'Qiang Du', 'Zuoqiang Shi']","In this paper, we consider a new nonlocal approximation to the linear Stokes system with periodic boundary conditions in two and three dimensional spaces . A relaxation term is added to the equation of nonlocal divergence free equation, which is reminiscent to the relaxation of local Stokes equation with small artificial compressibility. Our analysis shows that the well-posedness of the nonlocal system can be established under some mild assumptions on the kernel of nonlocal interactions. Furthermore, the new nonlocal system converges to the conventional, local Stokes system in second order as the horizon parameter of the nonlocal interaction goes to zero. The study provides more theoretical understanding to some numerical methods, such as smoothed particle hydrodynamics, for simulating incompressible viscous flows."
https://arxiv.org/abs/2403.07711,2024-03-12,SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces,"['Yuta Oshima', 'Shohei Taniguchi', 'Masahiro Suzuki', 'Yutaka Matsuo']","Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models."
https://arxiv.org/abs/2403.07710,2024-03-12,Focusing Surface Acoustic Waves with a Plasmonic Hypersonic Lens,"['Hilario D. Boggiano', 'Lin Nan', 'Gustavo Grinblat', 'Stefan A. Maier', 'Emiliano Cortés', 'Andrea V. Bragas']","Plasmonic nanoantennas have proven to be efficient transducers of electromagnetic to mechanical energy and vice versa. The sudden thermal expansion of these structures after an ultrafast optical pulsed excitation leads to the emission of hypersonic acoustic waves to the supporting substrate, which can be detected by another antenna that acts as a high-sensitive mechanical probe due to the strong modulation of its optical response. Sophisticated fabrication techniques, together with the implementation of numerical simulations, have allowed the engineering of nanostructures for the controlled directional generation and detection of high-frequency acoustic phonons at the nanoscale, with many potential applications in telecommunications, sensing, mechanical switching, and energy transport. Here, we propose and experimentally demonstrate a nanoscale acoustic lens comprised of 11 gold nanodisks whose collective oscillation gives rise to an interference pattern that results in a diffraction-limited surface acoustic beam of about 340 nm width, with an amplitude contrast of 60%. Via spatially decoupled pump-probe experiments, we were able to map the radiated acoustic energy in the proximity of the focal area, obtaining a very good agreement with the continuum elastic theory."
https://arxiv.org/abs/2403.07709,2024-03-12,Tomography of nonlinear materials via the Monotonicity Principle,"['Vincenzo Mottola', 'Antonio Corbo Esposito', 'Gianpaolo Piscitelli', 'Antonello Tamburrino']","In this paper we present a first non-iterative imaging method for nonlinear materials, based on Monotonicity Principle. Specifically, we deal with the inverse obstacle problem, where the aim is to retrieve a nonlinear anomaly embedded in linear known background."
https://arxiv.org/abs/2403.07708,2024-03-12,Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards,"['Wei Shen', 'Xiaoying Zhang', 'Yuanshun Yao', 'Rui Zheng', 'Hongyi Guo', 'Yang Liu']","Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and reduce variance in PPO. We show empirically contrastive rewards can improve RLHF substantially, evaluated by both GPTs and humans, and our method consistently outperforms strong baselines."
https://arxiv.org/abs/2403.07707,2024-03-13,Tightly Bounded Polynomials via Flexible Discretizations for Dynamic Optimization Problems,"['Eduardo M. G. Vila', 'Eric C. Kerrigan']","Polynomials are widely used to represent the trajectories of states and/or inputs. It has been shown that a polynomial can be bounded by its coefficients, when expressed in the Bernstein basis. However, in general, the bounds provided by the Bernstein coefficients are not tight. We propose a method for obtaining numerical solutions to dynamic optimization problems, where a flexible discretization is used to achieve tight polynomial bounds. The proposed method is used to solve a constrained cart-pole swing-up optimal control problem. The flexible discretization eliminates the conservatism of the Bernstein bounds and enables a lower cost, in comparison with non-flexible discretizations. A theoretical result on obtaining tight polynomial bounds with a finite discretization is presented. In some applications with linear dynamics, the non-convexity introduced by the flexible discretization may be a drawback."
https://arxiv.org/abs/2403.07706,2024-03-12,Fast and Simple Explainability for Point Cloud Networks,"['Meir Yossef Levi', 'Guy Gilboa']","We propose a fast and simple explainable AI (XAI) method for point cloud data. It computes pointwise importance with respect to a trained network downstream task. This allows better understanding of the network properties, which is imperative for safety-critical applications. In addition to debugging and visualization, our low computational complexity facilitates online feedback to the network at inference. This can be used to reduce uncertainty and to increase robustness. In this work, we introduce \emph{Feature Based Interpretability} (FBI), where we compute the features' norm, per point, before the bottleneck. We analyze the use of gradients and post- and pre-bottleneck strategies, showing pre-bottleneck is preferred, in terms of smoothness and ranking. We obtain at least three orders of magnitude speedup, compared to current XAI methods, thus, scalable for big point clouds or large-scale architectures. Our approach achieves SOTA results, in terms of classification explainability. We demonstrate how the proposed measure is helpful in analyzing and characterizing various aspects of 3D learning, such as rotation invariance, robustness to out-of-distribution (OOD) outliers or domain shift and dataset bias."
https://arxiv.org/abs/2403.07705,2024-03-12,Robust Synthetic-to-Real Transfer for Stereo Matching,"['Jiawei Zhang', 'Jiahe Li', 'Lei Huang', 'Xiaohan Yu', 'Lin Gu', 'Jin Zheng', 'Xiao Bai']","With advancements in domain generalized stereo matching networks, models pre-trained on synthetic data demonstrate strong robustness to unseen domains. However, few studies have investigated the robustness after fine-tuning them in real-world scenarios, during which the domain generalization ability can be seriously degraded. In this paper, we explore fine-tuning stereo matching networks without compromising their robustness to unseen domains. Our motivation stems from comparing Ground Truth (GT) versus Pseudo Label (PL) for fine-tuning: GT degrades, but PL preserves the domain generalization ability. Empirically, we find the difference between GT and PL implies valuable information that can regularize networks during fine-tuning. We also propose a framework to utilize this difference for fine-tuning, consisting of a frozen Teacher, an exponential moving average (EMA) Teacher, and a Student network. The core idea is to utilize the EMA Teacher to measure what the Student has learned and dynamically improve GT and PL for fine-tuning. We integrate our framework with state-of-the-art networks and evaluate its effectiveness on several real-world datasets. Extensive experiments show that our method effectively preserves the domain generalization ability during fine-tuning."
https://arxiv.org/abs/2403.07704,2024-03-12,Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning,"['Motoki Omura', 'Takayuki Osa', 'Yusuke Mukuta', 'Tatsuya Harada']","In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution."
https://arxiv.org/abs/2403.07703,2024-03-12,Zitterbewegung CPT violation in neutral kaons system,['Jean-Marcel Rax'],"The observed CP violation in neutral kaons experiments is explained as an interplay between two oscillations in the earth's Schwarzschild geometry: (i) mixing associated with second order weak coupling and (ii) quark's zitterbewegung. This violation is in fact a CPT violation with T conservation rather than a T violation with CPT conservation. The Hermitian evolution of a stable kaons system leads to the identification of this CPT violation. Then, the finite lifetime of the short-lived kaon rotates the violation parameter such that it appears as a T violation."
https://arxiv.org/abs/2403.07702,2024-03-12,Lipschitz maps with prescribed local Lipschitz constants,"['Aidan Backus', 'Ng Ze-An']","Let $Γ$ be a closed subset of a complete Riemannian manifold $M$ of dimension $\geq 2$, let $f: M \to N$ be a Lipschitz map to a complete Riemannian manifold $N$, and let $ψ$ be a continuous function which dominates the local Lipschitz constant of $f$. We construct a Lipschitz map which agress with $f$ on $Γ$ and whose local Lipschitz constant is $ψ$."
https://arxiv.org/abs/2403.07701,2024-03-12,Categorizing $SU(3)_f$ representations of scalar mesons by $J/ψ$ decays,"['Chao-Qiang Geng', 'Chia-Wei Liu', 'Xiao Yu', 'Ao-Wen Zhou']","We explore the possibilities of categorizing $SU(3)_f$ representations of scalar mesons through $J/ψ\to SV$ and $γS$, with $S$ ($V$) being the scalar(vector) mesons. We find that $f_0(500)$ and $f_0(980)$ are singlet and octet states, respectively; which both belong to a nonet of the $SU(3)_f$ flavor symmetry. In addition, we determine the singlet-octet mixing angle of $θ= (84.2\pm13.9)^{\circ}$ between $f_0(500)$ and $f_0(980)$, which supports the quark-antiquark ($q\bar{q}$) hypothesis. For the scalar mesons in the range of 1-2 GeV, containing two of $f_0(1370,\ 1500,\ 1700)$, we discuss the mixings between $q\bar{q}$ and glueballs. Our numerical results suggest that $f_0(1370 (1500))$ has the a significant component of $n\bar{n}$ ($s\bar{s}$), while $f_0(1710)$ is likely composed of the scalar glueball."
https://arxiv.org/abs/2403.07700,2024-03-12,CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers,"['Shahaf Arica', 'Or Rubin', 'Sapir Gershov', 'Shlomi Laufer']","In this paper, we introduce VoteCut, an innovative method for unsupervised object discovery that leverages feature representations from multiple self-supervised models. VoteCut employs normalized-cut based graph partitioning, clustering and a pixel voting approach. Additionally, We present CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels, generated by VoteCut, and a novel soft target loss to refine segmentation accuracy. Through rigorous evaluations across multiple datasets and several unsupervised setups, our methods demonstrate significant improvements in comparison to previous state-of-the-art models. Our ablation studies further highlight the contributions of each component, revealing the robustness and efficacy of our approach. Collectively, VoteCut and CuVLER pave the way for future advancements in image segmentation."
https://arxiv.org/abs/2403.07699,2024-03-12,Ion Kinetics and Neutron Generation Associated with Electromagnetic Turbulence in Laboratory-scale Counter-streaming Plasmas,"['P. Liu', 'D. Wu', 'T. X. Hu', 'D. W. Yuan', 'G. Zhao', 'Z. M. Sheng', 'X. T. He', 'J. Zhang']","Electromagnetic turbulence and ion kinetics in counter-streaming plasmas hold great significance in laboratory astrophysics, such as turbulence field amplification and particle energization. Here, we quantitatively demonstrate for the first time how electromagnetic turbulence affects ion kinetics under achievable laboratory conditions (millimeter-scale interpenetrating plasmas with initial velocity of $2000\ \mathrm{km/s}$, density of $4 \times 10^{19}\ \mathrm{cm}^{-3}$, and temperature of $100\ \mathrm{eV}$) utilizing a recently developed high-order implicit particle-in-cell code without scaling transformation. It is found that the electromagnetic turbulence is driven by ion two-stream and filamentation instabilities. For the magnetized scenarios where an applied magnetic field of tens of Tesla is perpendicular to plasma flows, the growth rates of instabilities increase with the strengthening of applied magnetic field, which therefore leads to a significant enhancement of turbulence fields. Under the competition between the stochastic acceleration due to electromagnetic turbulence and collisional thermalization, ion distribution function shows a distinct super-Gaussian shape, and the ion kinetics are manifested in neutron yields and spectra. Our results have well explained the recent unmagnetized experimental observations, and the findings of magnetized scenario can be verified by current astrophysical experiments."
https://arxiv.org/abs/2403.07698,2024-03-12,The Kazdan-Warner problem on compact Kähler surfaces,['Weike Yu'],"In this paper, we investigate a Kazdan-Warner problem on compact Kähler surfaces with negative Gauduchon degree, which corresponds to prescribing sign-changing Chern scalar curvatures. By the method of our recent paper [J. Funt. Anal. 285 (2023): 109948], we establish a Chen-Li type existence theorem on compact Kähler surfaces when the candidate curvature function is of negative average. Moreover, we give an alternative proof of Ding-Liu's theorem [Trans. Amer. Math. Soc. 347(1995) 1059-1066] on prescribing sign-changing Gaussian curvatures by using the $\sup+\inf$ inequality due to H. Brezis, Y. Y. Li and I. Shafrir."
https://arxiv.org/abs/2403.07697,2024-03-12,Using Equation of State Constraints to Classify Low-Mass Compact Binary Mergers,"['Jacob Golomb', 'Isaac Legred', 'Katerina Chatziioannou', 'Adrian Abac', 'Tim Dietrich']","Compact objects observed via gravitational waves are classified as black holes or neutron stars primarily based on their inferred mass with respect to stellar evolution expectations. However, astrophysical expectations for the lowest mass range, $\lesssim 1.2 \,M_\odot$, are uncertain. If such low-mass compact objects exist, ground-based gravitational wave detectors may observe them in binary mergers. Lacking astrophysical expectations for classifying such observations, we go beyond the mass and explore the role of tidal effects. We evaluate how combined mass and tidal inference can inform whether each binary component is a black hole or a neutron star based on consistency with the supranuclear-density equation of state. Low-mass neutron stars experience a large tidal deformation; its observational identification (or lack thereof) can therefore aid in determining the nature of the binary components. Using simulated data, we find that the presence of a sub-solar mass neutron star (black hole) can be established with odds $\sim 100:1$ when two neutron stars (black holes) merge and emit gravitational waves at signal-to-noise ratio $\sim 20$. For the same systems, the absence of a black hole (neutron star) can be established with odds $\sim 10:1$. For mixed neutron star-black hole binaries, we can establish that the system contains a neutron star with odds $\gtrsim 5:1$. Establishing the presence of a black hole in mixed neutron star-black hole binaries is more challenging, except for the case of a $\lesssim 1\,M_{\odot}$ black hole with a $\gtrsim 1\,M_{\odot}$ neutron star companion. On the other hand, classifying each individual binary component suffers from an inherent labeling ambiguity."
https://arxiv.org/abs/2403.07696,2024-03-12,Peak-Brightness Localization of the CNEOS 2014-01-08 (IM1) Fireball,['Abraham Loeb'],"In a recent preprint, Fernando et al. (2024) used public data from infrasound stations to constrain the localization of the fireball of the CNEOS 2014-01-08 (IM1) bolide. The analysis inferred a 90-percent-confidence ellipse with semi-minor and semi-major axes of 186 and 388 km, respectively. This large error ellipse includes the much better localization box derived by sensors aboard U.S. Government satellites which detected the fireball light. At the fireball's peak brightness, the CNEOS localization box documented by NASA/JPL measures 11.112km on a side and is centered on a latitude of 1.3S and a longitude of 147.6E. Here, we point out that the recent expedition to retrieve materials from IM1's site (Loeb et al. 2024a,b,c) surveyed a region of tens of km around the CNEOS box center, and was not dictated by the data studied by Fernando et al. (2024) because of its larger uncertainties."
https://arxiv.org/abs/2403.07695,2024-03-12,Harmonically m-Concave Set-Valued Function,"['Gabriel Santana', 'Maira Valera-López', 'Nelson Merentes']","This research aimed to introduce the concept of harmonically m-concave set-valued functions, which is obtained from the combination of two definitions: harmonically m-concave functions and set-valued functions. In this work some properties and characteristics are developed, as well as a Kuhn type theorem and Bernstein-Doetcsh type result for such functions."
https://arxiv.org/abs/2403.07694,2024-03-12,Precision timing of eclipsing binaries from TESS full frame images. Method and performance,"['Frédéric Marcadon', 'Andrej Prša']","Several hundreds of thousands of eclipsing binaries (EBs) are expected to be detected in the Transiting Exoplanet Survey Satellite (TESS) full frame images (FFIs). This represents a significant increase in the number of EBs available for eclipse timing variation studies. In this paper, we investigate the feasibility of performing precise eclipse timing of TESS EBs using the FFIs. To this end, we developed a fast, automated method and applied it to a sample of $\sim$100 EBs selected from the Villanova TESS EB catalog. Our timing analysis resulted in the detection of ten new triple candidates with outer periods shorter than $\sim$1300$\,$d. For five of them, we were able to constrain the outer orbit by analyzing independently the short-cadence (SC) and FFI data and to derive the minimum mass of the third body with a precision better than 4 per cent for SC and 12 per cent for FFI data. We then compared the results obtained from the two datasets and found that using the FFI data leads to (1) a degradation of both the accuracy and precision of the tertiary mass determination for the tightest EBs and (2) an overall underestimation of the third component's mass. However, we stress that our main conclusions on the nature of the detected signals do not depend on which dataset is used. This confirms the great potential of TESS FFIs, which will allow us to search for rare objects such as substellar circumbinary companions and compact triple stellar systems."
https://arxiv.org/abs/2403.07693,2024-03-12,"Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization","['Yanyue Zhang', 'Pengfei Li', 'Yilong Lai', 'Deyu Zhou']","As more than 70$\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated data. After training, a large amount of synthetic data can be obtained by decoding the new representation obtained from the combination of different sample representations and filtering based on confusion degree and sentiment classification. Experiments have proved that our framework can effectively alleviate emotional bias same as using only large models, but more economically."
https://arxiv.org/abs/2403.07692,2024-03-12,Masked AutoDecoder is Effective Multi-Task Vision Generalist,"['Han Qiu', 'Jiaxing Huang', 'Peng Gao', 'Lewei Lu', 'Xiaoqin Zhang', 'Shijian Lu']","Inspired by the success of general-purpose models in NLP, recent studies attempt to unify different vision tasks in the same sequence format and employ autoregressive Transformers for sequence prediction. They apply uni-directional attention to capture sequential dependencies and generate task sequences recursively. However, such autoregressive Transformers may not fit vision tasks well, as vision task sequences usually lack the sequential dependencies typically observed in natural languages. In this work, we design Masked AutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of two core designs. First, we develop a parallel decoding framework that introduces bi-directional attention to capture contextual dependencies comprehensively and decode vision task sequences in parallel. Second, we design a masked sequence modeling approach that learns rich task contexts by masking and reconstructing task sequences. In this way, MAD handles all the tasks by a single network branch and a simple cross-entropy loss with minimal task-specific designs. Extensive experiments demonstrate the great potential of MAD as a new paradigm for unifying various vision tasks. MAD achieves superior performance and inference efficiency compared to autoregressive counterparts while obtaining competitive accuracy with task-specific models. Code will be released."
https://arxiv.org/abs/2403.07691,2024-03-12,Reference-free Monolithic Preference Optimization with Odds Ratio,"['Jiwoo Hong', 'Noah Lee', 'James Thorne']","While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20% on $\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12. We release code and model checkpoints for Mistral-ORPO-$α$ (7B) and Mistral-ORPO-$β$ (7B)."
https://arxiv.org/abs/2403.07690,2024-03-12,SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted Technical Debt,"['Edi Sutoyo', 'Andrea Capiluppi']","Self-admitted technical debt (SATD) refers to a form of technical debt in which developers explicitly acknowledge and document the existence of technical shortcuts, workarounds, or temporary solutions within the codebase. Over recent years, researchers have manually labeled datasets derived from various software development artifacts: source code comments, messages from the issue tracker and pull request sections, and commit messages. These datasets are designed for training, evaluation, performance validation, and improvement of machine learning and deep learning models to accurately identify SATD instances. However, class imbalance poses a serious challenge across all the existing datasets, particularly when researchers are interested in categorizing the specific types of SATD. In order to address the scarcity of labeled data for SATD \textit{identification} (i.e., whether an instance is SATD or not) and \textit{categorization} (i.e., which type of SATD is being classified) in existing datasets, we share the \textit{SATDAUG} dataset, an augmented version of existing SATD datasets, including source code comments, issue tracker, pull requests, and commit messages. These augmented datasets have been balanced in relation to the available artifacts and provide a much richer source of labeled data for training machine learning or deep learning models."
https://arxiv.org/abs/2403.07689,2024-03-12,Probing anomalous $Zγγγ$ couplings at a future muon collider,"['H. Amarkhail', 'S. C. İnan', 'A. V. Kisselev']","The sensitivity to anomalous quartic gauge couplings (AQGCs) of the $γγγZ$ interaction is studied in the $μ^+μ^- \rightarrow μ^+γγμ^-$ scattering at a future muon collider with unpolarized beams. The anomalous $γγγZ$ vertex is described by two couplings, $ζ_1$ and $ζ_2$. The differential and total cross sections are calculated for the center-of-mass energies of 3 TeV, 14 TeV, and 100 TeV. For these values of the collision energy the $95\%$ C.L. exclusion regions for AQGCs are obtained depending on the systematic error. In particular, for the 14 TeV muon collider with the integrated luminosity $L = 20$ ab$^{-1}$ the best sensitivities are derived to be $ζ_1 = 3.1 \times 10^{-5}$ TeV$^{-4}$ and $ζ_2 = 6.5 \times 10^{-5}$ TeV$^{-4}$. These constraints are three orders of magnitude stronger than the bounds obtained for the 27 TeV HE-LHC with $L = 15$ ab$^{-1}$. At the 100 TeV muon collider with $L = 1000$ ab$^{-1}$ AQGCs can be probed up to $(1.64 ÷3.4) \times 10^{-8}$ TeV$^{-4}$. The partial-wave unitarity constraints on couplings $ζ_1$, $ζ_2$ are evaluated. It is shown that the unitarity is not violated in the region of the AQGCs examined in the present paper."
https://arxiv.org/abs/2403.07688,2024-03-12,Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons,"['Simon Dufort-Labbé', ""Pierluca D'Oro"", 'Evgenii Nikishin', 'Razvan Pascanu', 'Pierre-Luc Bacon', 'Aristide Baratin']","When training deep neural networks, the phenomenon of $\textit{dying neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero during training$\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 and ImageNet datasets demonstrate that DemP surpasses existing structured pruning techniques, showcasing superior accuracy-sparsity tradeoffs and training speedups. These findings suggest a novel perspective on dying neurons as a valuable resource for efficient model compression and optimization."
https://arxiv.org/abs/2403.07687,2024-03-12,Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost,"['Oana Ignat', 'Longju Bai', 'Joan Nwatu', 'Rada Mihalcea']","Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs. The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual_diversity_budget."
https://arxiv.org/abs/2403.07686,2024-03-12,Presenting the topological stratified homotopy hypothesis,['Lukas Waas'],"This article is concerned with three different homotopy theories of stratified spaces: The one defined by Douteau and Henriques, the one defined by Haine, and the one defined by Nand-Lal. One of the central questions concerning these theories has been how precisely they connect with geometric and topological examples of stratified spaces, such as piecewise linear pseudomanifolds, Whitney stratified spaces, or more recently Ayala, Francis and Tanaka's conically smooth stratified spaces. More precisely, so far, it has been an open question whether there exist (semi)model structures on stratified topological spaces that present these theories, in which such relevant examples of stratified spaces are bifibrant. Here, we prove an affirmative answer to this question. As a consequence, we obtain a model categorical interpretation of a stratified homotopy hypothesis. Specifically, we show that Lurie's stratified singular simplicial set functor induces a Quillen equivalence between the semimodel category of stratified topological spaces presenting Nand-Lal's homotopy theory of stratified spaces and the left Bousfield localization of the Joyal model structure that corresponds to such $\infty$-categories in which every endomorphism is an isomorphism. We then perform a detailed investigation of bifibrant objects in these model structures of stratified spaces, proving a series of detection criteria and illuminating the relationship to Quinn's homotopically stratified spaces."
https://arxiv.org/abs/2403.07685,2024-03-12,On fluctuations of complexity measures for the FIND algorithm,"['Jasper Ischebeck', 'Ralph Neininger']","The FIND algorithm (also called Quickselect) is a fundamental algorithm to select ranks or quantiles within a set of data. It was shown by Grübel and Rösler that the number of key comparisons required by Find as a process of the quantiles $α\in[0,1]$ in a natural probabilistic model converges after normalization in distribution within the càdlàg space $D[0,1]$ endowed with the Skorokhod metric. We show that the process of the residuals in the latter convergence after normalization converges in distribution to a mixture of Gaussian processes in $D[0,1]$ and identify the limit's conditional covariance functions. A similar result holds for the related algorithm QuickVal. Our method extends to other cost measures such as the number of swaps (key exchanges) required by Find or cost measures which are based on key comparisons but take into account that the cost of a comparison between two keys may depend on their values, an example being the number of bit comparisons needed to compare keys given by their bit expansions."
https://arxiv.org/abs/2403.07684,2024-03-12,Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal,"['Yijun Yang', 'Hongtao Wu', 'Angelica I. Aviles-Rivero', 'Yulun Zhang', 'Jing Qin', 'Lei Zhu']","Real-world vision tasks frequently suffer from the appearance of unexpected adverse weather conditions, including rain, haze, snow, and raindrops. In the last decade, convolutional neural networks and vision transformers have yielded outstanding results in single-weather video removal. However, due to the absence of appropriate adaptation, most of them fail to generalize to other weather conditions. Although ViWS-Net is proposed to remove adverse weather conditions in videos with a single set of pre-trained weights, it is seriously blinded by seen weather at train-time and degenerates when coming to unseen weather during test-time. In this work, we introduce test-time adaptation into adverse weather removal in videos, and propose the first framework that integrates test-time adaptation into the iterative diffusion reverse process. Specifically, we devise a diffusion-based network with a novel temporal noise model to efficiently explore frame-correlated information in degraded video clips at training stage. During inference stage, we introduce a proxy task named Diffusion Tubelet Self-Calibration to learn the primer distribution of test video stream and optimize the model by approximating the temporal noise model for online adaptation. Experimental results, on benchmark datasets, demonstrate that our Test-Time Adaptation method with Diffusion-based network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring videos degraded by seen weather conditions. Its generalizable capability is also validated with unseen weather conditions in both synthesized and real-world videos."
https://arxiv.org/abs/2403.07683,2024-03-12,Towards a Unified Formalism of Multivariate Coefficients of Variation -- Application to the Analysis of Polarimetric Speckle Time Series,"['Elise Colin', 'Razvigor Ossikovski']","This article primarily aims to unify the various formalisms of multivariate coefficients of variation, leveraging advanced concepts of generalized means, whether weighted or not, applied to the eigenvalues of covariance matrices. We highlight the existence of an infinite number of these coefficients and demonstrate that they are bounded. Moreover, we link the various coefficients of variation identified in the literature to specific instances within our unified formalism. We illustrate the utility of our method by applying it to a time series of polarimetric radar imagery. In this context, the coefficient of variation emerges as a key tool for detecting changes or identifying permanent scatterers, which are characterized by their remarkable temporal stability. The multidimensionality arises from the diversity of polarizations. The introduction of the various possible coefficients demonstrates how their selection impacts the detection of samples exhibiting specific temporal behaviors and underscores the contribution of polarimetry to dynamic speckle analysis."
https://arxiv.org/abs/2403.07682,2024-03-12,Testing Gravity with Binary Black Hole Gravitational Waves,"['Marta Colleoni', 'N. V. Krishnendu', 'Pierre Mourier', 'Sayantani Bera', 'Xisco Jiménez-Forteza']","General Relativity (GR) remains the most accurate theory of gravity to date. It has passed many experimental tests in the Solar System as well as binary pulsar, cosmological and gravitational-wave (GW) observations. Some of these tests probe regimes where gravitational fields are weak, the spacetime curvature is small, and the characteristic velocities are not comparable to the speed of light. Observations of compact binary coalescences enable us to test GR in extreme environments of strong and dynamical gravitational fields, large spacetime curvature, and velocities comparable to the speed of light. Since the breakthrough observation of the first GW signal produced by the merger of two black holes, GW150914, in September 2015, the number of confirmed detections of binary mergers has rapidly increased to nearly 100. The analysis of these events has already placed significant constraints on possible deviations from GR and on the nature of the coalescing compact objects. In this chapter, we discuss a selection of tests of GR applicable to observations of GWs from compact binaries. In particular, we will cover consistency tests, which check for consistency between the different phases of the binary's evolution, tests of GW generation, polarization and propagation, and tests of the remnant's nature. We conclude with a brief overview of the challenges and prospects for present and future observatories."
https://arxiv.org/abs/2403.07681,2024-03-12,Feasible climate policies in a democracy with a climate-denying party,"['Andrea Di Benedetto', 'Claudia E. Wieners', 'Anna S. von der Heydt']","Climate policy has become increasingly politicized in many countries including the US, with some political parties unwilling to pursue strong measures. Therefore, to be successful in mitigation, climate policies must be politically feasible. Currently, climate mitigation pathways are explored in so-called Integrated Assessment Models (IAMs) which evaluate climate policies from an economic perspective, typically focusing on cost-effectiveness and overlooking transition costs. However, the economy is intertwined with the political system, in which policymakers impose economic policies, but are (in democracies) dependent on public opinion, which in turn can be influenced by economic performance. In cases where some parties are much less ambitious in climate mitigation than others, climate policy can be abruptly disrupted, influencing voting behaviour. In this study, we analyze the political feasibility of a set of green policies in case some parties are strongly unwilling to protect the climate. We show that this simple additional social layer of complexity largely affects the outcome of the abatement measures. In particular, we conclude that a (high) pure carbon tax is particularly vulnerable to abrupt interruptions and its economic side effects discourage votes for green parties. Nevertheless, a strategically selected combination of policies can reduce political uncertainty, resulting in a more feasible and effective mitigation measure."
https://arxiv.org/abs/2403.07680,2024-03-12,Adapting LoRaWAN to the Open-RAN Architecture,"['Sobhi Alfayoumi', 'Joan Melia-Segui', 'Xavier Vilajosana']","This article proposes O-LoRaWAN, an adaptation of the LoRaWAN architecture into a modular network architecture based on the Open RAN (O-RAN) principles. In our vision, standardization of the network components and interfaces will enable the reuse of network functions, and thus, foster an accelerated tailoring of the network functions to the changing application demands. LoRaWAN shares similarities to cellular networks and becomes an interesting candidate for a transformation to the O-RAN standard. In the article we draw several transition strategies, these include the reorganization of the LoRa gateway functions into Radio and Distributed Units; enhancing network performance with RAN Intelligent Controllers exploiting the network data; and the standardization of the management and orchestration of network components. Key for that adaptation are the O-RAN interfaces. Along the article, we analyze them and suggest protocol extensions or adjustments for compatibility and interoperability between network components, advocating for the design of extensible protocols"
https://arxiv.org/abs/2403.07679,2024-03-12,Directional testing for one-way MANOVA in divergent dimensions,"['Caizhu Huang', 'Claudia Di Caterina', 'Nicola Sartori']","Testing the equality of mean vectors across $g$ different groups plays an important role in many scientific fields. In regular frameworks, likelihood-based statistics under the normality assumption offer a general solution to this task. However, the accuracy of standard asymptotic results is not reliable when the dimension $p$ of the data is large relative to the sample size $n_i$ of each group. We propose here an exact directional test for the equality of $g$ normal mean vectors with identical unknown covariance matrix, provided that $\sum_{i=1}^g n_i \ge p+g+1$. In the case of two groups ($g=2$), the directional test is equivalent to the Hotelling's $T^2$ test. In the more general situation where the $g$ independent groups may have different unknown covariance matrices, although exactness does not hold, simulation studies show that the directional test is more accurate than most commonly used likelihood based solutions. Robustness of the directional approach and its competitors under deviation from multivariate normality is also numerically investigated."
https://arxiv.org/abs/2403.07678,2024-03-12,MoralBERT: Detecting Moral Values in Social Discourse,"['Vjosa Preniqi', 'Iacopo Ghinassi', 'Kyriaki Kalimeri', 'Charalampos Saitis']","Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements. Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews. Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content. Here, we design a range of language representation models fine-tuned to capture exactly the moral nuances in text, called MoralBERT. We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics. This approach broadens linguistic diversity and potentially enhances the models' ability to comprehend morality in various contexts. We also explore a domain adaptation technique and compare it to the standard fine-tuned BERT model, using two different frameworks for moral prediction: single-label and multi-label. We compare in-domain approaches with conventional models relying on lexicon-based techniques, as well as a Machine Learning classifier with Word2Vec representation. Our results showed that in-domain prediction models significantly outperformed traditional models. While the single-label setting reaches a higher accuracy than previously achieved for the task when using BERT pretrained models. Experiments in an out-of-domain setting, instead, suggest that further work is needed for existing domain adaptation techniques to generalise between different social media platforms, especially for the multi-label task. The investigations and outcomes from this study pave the way for further exploration, enabling a more profound comprehension of moral narratives about controversial social issues."
https://arxiv.org/abs/2403.07677,2024-03-12,Many-sided Poisson-Voronoi cells with only Gabriel neighbors,['H. J. Hilhorst'],"Let $p_n^G$ be the probability for a planar Poisson-Voronoi cell to be $n$-sided {\it and\,} have only Gabriel neighbors. Using an exact coordinate transformation followed by scaling arguments and a mean-field type calculation, we obtain the asymptotic expansion of $\log p_n^G$ in the limit of large $n$. We determine several statistical properties of a many-sided cell obeying this `Gabriel condition.' In particular, the cell perimeter, when parametrized as a function $τ(θ)$ of the polar angle $θ$, behaves as a Brownian bridge on the interval $0\leθ\le 2π$. We point out similarities and differences with related problems in random geometry."
https://arxiv.org/abs/2403.07676,2024-03-12,Parametrized higher semiadditivity and the universality of spans,"['Bastiaan Cnossen', 'Tobias Lenz', 'Sil Linskens']","Using the framework of ambidexterity developed by Hopkins and Lurie, we introduce a parametrized analogue of higher semiadditivity called $\mathcal Q$-semiadditivity, depending on a chosen class of morphisms $\mathcal Q$. Our first main result identifies the free $\mathcal Q$-semiadditive parametrized category on a single generator with a certain parametrized span category $\underline{\mathrm{Span}}(\mathcal Q)$, simultaneously generalizing a result of Harpaz in the non-parametrized setting and a result of Nardin in the equivariant setting. As a consequence, we deduce that the $\mathcal Q$-semiadditive completion of a parametrized category $\mathcal C$ consists of the $\mathcal Q$-commutative monoids in $\mathcal C$, defined as $\mathcal Q$-limit preserving parametrized functors from $\underline{\mathrm{Span}}(\mathcal Q)$ to $\mathcal C$."
https://arxiv.org/abs/2403.07675,2024-03-12,Multichannel Long-Term Streaming Neural Speech Enhancement for Static and Moving Speakers,"['Changsheng Quan', 'Xiaofei Li']","In this work, we extend our previously proposed offline SpatialNet for long-term streaming multichannel speech enhancement in both static and moving speaker scenarios. SpatialNet exploits spatial information, such as the spatial/steering direction of speech, for discriminating between target speech and interferences, and achieved outstanding performance. The core of SpatialNet is a narrow-band self-attention module used for learning the temporal dynamic of spatial vectors. Towards long-term streaming speech enhancement, we propose to replace the offline self-attention network with online networks that have linear inference complexity w.r.t signal length and meanwhile maintain the capability of learning long-term information. Three variants are developed based on (i) masked self-attention, (ii) Retention, a self-attention variant with linear inference complexity, and (iii) Mamba, a structured-state-space-based RNN-like network. Moreover, we investigate the length extrapolation ability of different networks, namely test on signals that are much longer than training signals, and propose a short-signal training plus long-signal fine-tuning strategy, which largely improves the length extrapolation ability of the networks within limited training time. Overall, the proposed online SpatialNet achieves outstanding speech enhancement performance for long audio streams, and for both static and moving speakers. The proposed method will be open-sourced in https://github.com/Audio-WestlakeU/NBSS."
https://arxiv.org/abs/2403.07674,2024-03-12,The frequency problem of the three gap theorem,['Huixing Zhang'],"The three gap theorem was originally a conjecture by Steinhaus, who asserted that there are at most three distinct gap lengths in the fractional parts of the sequence α,{2}α,{\cdots},{N}α for any integer {N} and real number α. This conjecture has been proved by many different methods, and extended to higher dimensional cases. For the three gap theorem, what is the frequence of two gaps or three gaps? On the basis of Ravenstein's work, this paper studies the frequency of the three gap theorem through cake model. We show that for almost all {α\in (0,1)\backslash \mathbb {Q}} in the sence of the Lebesgue measure, the frequency of the two gaps is {0}, in other words, the frequency of the three gaps is {1}. The proof makes full use of the continued fraction expansion of α, as well as partial results in the Diophantine approximation. Finally, we illustrate that two gaps occur with a frequency of {0}."
https://arxiv.org/abs/2403.07673,2024-03-13,Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation,"['Di Mi', 'Yanjun Zhang', 'Leo Yu Zhang', 'Shengshan Hu', 'Qi Zhong', 'Haizhuan Yuan', 'Shirui Pan']","Model extraction attacks (MEAs) enable an attacker to replicate the functionality of a victim deep neural network (DNN) model by only querying its API service remotely, posing a severe threat to the security and integrity of pay-per-query DNN-based services. Although the majority of current research on MEAs has primarily concentrated on neural classifiers, there is a growing prevalence of image-to-image translation (I2IT) tasks in our everyday activities. However, techniques developed for MEA of DNN classifiers cannot be directly transferred to the case of I2IT, rendering the vulnerability of I2IT models to MEA attacks often underestimated. This paper unveils the threat of MEA in I2IT tasks from a new perspective. Diverging from the traditional approach of bridging the distribution gap between attacker queries and victim training samples, we opt to mitigate the effect caused by the different distributions, known as the domain shift. This is achieved by introducing a new regularization term that penalizes high-frequency noise, and seeking a flatter minimum to avoid overfitting to the shifted distribution. Extensive experiments on different image translation tasks, including image super-resolution and style transfer, are performed on different backbone victim models, and the new design consistently outperforms the baseline by a large margin across all metrics. A few real-life I2IT APIs are also verified to be extremely vulnerable to our attack, emphasizing the need for enhanced defenses and potentially revised API publishing policies."
https://arxiv.org/abs/2403.07672,2024-03-12,Quantitative estimates in almost periodic homogenization of parabolic systems,"['Jun Geng', 'Bojing Shi']","We consider a family of second-order parabolic operators $\partial_t+\mathcal{L}_\varepsilon$ in divergence form with rapidly oscillating, time-dependent and almost-periodic coefficients. We establish uniform interior and boundary Hölder and Lipschitz estimates as well as convergence rate. The estimates of fundamental solution and Green's function are also established. In contrast to periodic case, the main difficulty is that the corrector equation $"
https://arxiv.org/abs/2403.07671,2024-03-12,Characterizing the diffuse continuum excitations in the classical spin liquid $h$-YMnO$_3$,"['Jakob Lass', 'Emma Y. Lenander', 'Kristine M. L. Krighaar', 'Tara N. Tošić', 'Dharmalingam Prabhakaran', 'Pascale P. Deen', 'Sofie Holm-Janas', 'Kim Lefmann']","We extend previous inelastic neutron scattering results on the geometrically frustrated antiferromagnet hexagonal-YMnO$_3$, which has been suggested to belong to the class of classical spin liquids. We extend the energy transfer coverage of the diffuse signal up to 6.9 meV within a wide temperature range around the ordering temperature, $T_\mathrm{N}$. The two distinct diffuse signals in the a-b plane, the signal localized at $Γ$' and the scattering intensity connecting $Γ$' points over the M', are shown to be only weakly energy dependent. In addition, an external magnetic field of up to 10.5 T applied along c is shown to have no effect on the diffuse signal. In the orthogonal scattering plane, the signals are shown to be dependent on l only through the magnetic form factor, showing that the correlations are purely two-dimensional, and supporting its origin to be the frustrated Mn$^{3+}$ triangles. This result is corroborated by atomistic spin dynamics simulations showing similar scattering vector and temperature behaviours. Lastly, data for the spin wave scattering in the (h, 0, l) plane allow for a discussion of the magnetic ground state where better agreement is found between the data and an ordered structure of the $Γ_1$ or $Γ_3$ symmetry, albeit crystal electric field arguments dismisses the $Γ_1$ as possibility."
https://arxiv.org/abs/2403.07670,2024-03-12,Linear and non-linear integrate-and-fire neurons driven by synaptic shot noise with reversal potentials,['Magnus J E Richardson'],"The steady-state firing rate and firing-rate response of the leaky and exponential integrate-and-fire models receiving synaptic shot noise with excitatory and inhibitory reversal potentials is examined. For the particular case where the underlying synaptic conductances are exponentially distributed, it is shown that the master equation for a population of such model neurons can be reduced from an integro-differential form to a more tractable set of three differential equations. The system is nevertheless more challenging analytically than for current-based synapses: where possible analytical results are provided with an efficient numerical scheme and code provided for other quantities. The increased tractability of the framework developed supports an ongoing critical comparison between models in which synapses are treated with and without reversal potentials, such as recently in the context of networks with balanced excitatory and inhibitory conductances."
https://arxiv.org/abs/2403.07669,2024-03-12,Machine Learning for Soccer Match Result Prediction,"['Rory Bunker', 'Calvin Yeung', 'Keisuke Fujii']","Machine learning has become a common approach to predicting the outcomes of soccer matches, and the body of literature in this domain has grown substantially in the past decade and a half. This chapter discusses available datasets, the types of models and features, and ways of evaluating model performance in this application domain. The aim of this chapter is to give a broad overview of the current state and potential future developments in machine learning for soccer match results prediction, as a resource for those interested in conducting future studies in the area. Our main findings are that while gradient-boosted tree models such as CatBoost, applied to soccer-specific ratings such as pi-ratings, are currently the best-performing models on datasets containing only goals as the match features, there needs to be a more thorough comparison of the performance of deep learning models and Random Forest on a range of datasets with different types of features. Furthermore, new rating systems using both player- and team-level information and incorporating additional information from, e.g., spatiotemporal tracking and event data, could be investigated further. Finally, the interpretability of match result prediction models needs to be enhanced for them to be more useful for team management."
https://arxiv.org/abs/2403.07668,2024-03-12,Conditions of positivity on a shadow Markoff Tree,['Nathan Bonin'],An analogue of the Markoff equation has recently been introduced by the author and Valentin Ovsienko. A conjecture about the necessary and sufficient conditions for positivity of solutions to this equation is formulated and discussed.
https://arxiv.org/abs/2403.07667,2024-03-12,Optical computing with supercontinuum generation in photonic crystal fibers,"['Azka Maula Iskandar Muda', 'Uğur Teğin']","We introduce a novel photonic neural network using photonic crystal fibers, leveraging femtosecond pulse supercontinuum generation for optical computing. Investigating its efficacy across machine learning tasks, we uncover the crucial impact of nonlinear pulse propagation dynamics on network performance. Our findings show that octave-spanning supercontinuum generation results in loss of dataset variety due to many-to-one mapping, and optimal performance requires balancing optical nonlinearity. This study offers guidance for designing energy-efficient and high-performance photonic neural network architectures by explaining the interplay between nonlinear dynamics and optical computing."
https://arxiv.org/abs/2403.07666,2024-03-12,Holistic numerical simulation of a quenching process on a real-size multifilamentary superconducting coil,"['Cun Xue', 'Han-Xi Ren', 'Peng Jia', 'Qing-Yu Wang', 'Wei Liu', 'Xian-Jin Ou', 'Liang-Ting Sun', 'Alejandro V Silhanek']","Superconductors play a crucial role in the advancement of high-field electromagnets. Unfortunately, their performance can be compromised by thermomagnetic instabilities, wherein the interplay of rapid magnetic and slow heat diffusion can result in catastrophic flux jumps eventually leading to irreversible damage. This issue has long plagued high-$J_c$ Nb$_3$Sn wires at the core of high-field magnets. In this study, we introduce a groundbreaking large-scale GPU-optimized algorithm aimed at tackling the complex intertwined effects of electromagnetism, heating, and strain acting concomitantly during the quenching process of superconducting coils. We validate our model by conducting comparisons with magnetization measurements obtained from short multifilamentary Nb$_3$Sn wires and further experimental tests conducted on solenoid coils while subject to ramping transport currents. Furthermore, leveraging our developed numerical algorithm, we unveil the dynamic propagation mechanisms underlying thermomagnetic instabilities (including flux jumps and quenches) within the coils. Remarkably, our findings reveal that the velocity field of flux jumps and quenches within the coil is correlated with the amount of Joule heating experienced by each wire over a specific time interval, rather than solely being dependent on instantaneous Joule heating or maximum temperature. These insights have the potential to pave the way for optimizing the design of next-generation superconducting magnets, thereby directly influencing a wide array of technologically relevant and multidisciplinary applications."
https://arxiv.org/abs/2403.07665,2024-03-12,Superconformal indices and localization in $N=2B$ quantum mechanics,"['Joris Raeymaekers', 'Canberk Sanli', 'Dieter Van den Bleeken']","Superconformal `type B' quantum mechanical sigma models arise in a variety of interesting contexts, such as the description of D-brane bound states in an AdS$_2$ decoupling limit. Focusing on $N=2B$ models, we study superconformal indices which count short multiplets and provide an alternative to the standard Witten index, as the latter suffers from infrared issues. We show that the basic index receives contributions from lowest Landau level states in an effective magnetic field and that, due to the noncompactness of the target space, it is typically divergent. Fortunately, the models of interest possess an additional target space isometry which allows for the definition of a well-behaved refined index. We compute this index using localization of the functional integral and find that the result agrees with a naive application of the Atiyah-Bott fixed point formula outside of it's starting assumptions. In the simplest examples, this formula can also be directly verified by explicitly computing the short multiplet spectrum."
https://arxiv.org/abs/2403.07664,2024-03-12,Enabling self-identification in intelligent agent: insights from computational psychoanalysis,"['Lingyu Li', 'Chunbo Li']","Building upon prior framework of computational Lacanian psychoanalysis with the theory of active inference, this paper aims to further explore the concept of self-identification and its potential applications. Beginning with two classic paradigms in psychology, mirror self-recognition and rubber hand illusion, we suggest that imaginary identification is characterized by an integrated body schema with minimal free energy. Next, we briefly survey three dimensions of symbolic identification (sociological, psychoanalytic, and linguistical) and corresponding active inference accounts. To provide intuition, we respectively employ a convolutional neural network (CNN) and a multi-layer perceptron (MLP) supervised by ChatGPT to showcase optimization of free energy during motor skill and language mastery underlying identification formation. We then introduce Lacan's Graph II of desire, unifying imaginary and symbolic identification, and propose an illustrative model called FreeAgent. In concluding remarks, we discuss some key issues in the potential of computational Lacanian psychoanalysis to advance mental health and artificial intelligence, including digital twin mind, large language models as avatars of the Lacanian Other, and the feasibility of human-level artificial general intelligence with self-awareness in the context of post-structuralism."
https://arxiv.org/abs/2403.07663,2024-03-12,Tilings of Benzels via Generalized Compression,"['Colin Defant', 'Leigh Foster', 'Rupert Li', 'James Propp', 'Benjamin Young']","Defant, Li, Propp, and Young recently resolved two enumerative conjectures of Propp concerning the tilings of regions in the hexagonal grid called benzels using two types of prototiles called stones and bones (with varying constraints on allowed orientations of the tiles). Their primary tool, a bijection called compression that converts certain $k$-ribbon tilings to $(k-1)$-ribbon tilings, allowed them to reduce their problems to the enumeration of dimers (i.e., perfect matchings) of certain graphs. We present a generalized version of compression that no longer relies on the perspective of partitions and skew shapes. Using this strengthened tool, we resolve three more of Propp's conjectures and recast several others as problems about perfect matchings."
https://arxiv.org/abs/2403.07662,2024-03-12,Signatures of correlated defects in an ultra-clean Wigner crystal in the extreme quantum limit,"['P. T. Madathil', 'C. Wang', 'S. K. Singh', 'A. Gupta', 'K. A. Villegas Rosales', 'Y. J. Chung', 'K. W. West', 'K. W. Baldwin', 'L. N. Pfeiffer', 'L. W. Engel', 'M. Shayegan']","Low-disorder two-dimensional electron systems in the presence of a strong, perpendicular magnetic field terminate at very small Landau level filling factors in a Wigner crystal (WC), where the electrons form an ordered array to minimize the Coulomb repulsion. The nature of this exotic, many-body, quantum phase is yet to be fully understood and experimentally revealed. Here we probe one of WC's most fundamental parameters, namely the energy gap that determines its low-temperature conductivity, in record-mobility, ultra-high-purity, two-dimensional electrons confined to GaAs quantum wells. The WC domains in these samples contain $\simeq$ 1000 electrons. The measured gaps are a factor of three larger than previously reported for lower quality samples, and agree remarkably well with values predicted for the lowest-energy, intrinsic, hyper-corelated bubble defects in a WC made of flux-electron composite fermions, rather than bare electrons. The agreement is particularly noteworthy, given that the calculations are done for disorder-free composite fermion WCs, and there are no adjustable parameters. The results reflect the exceptionally high quality of the samples, and suggest that composite fermion WCs are indeed more stable compared to their electron counterparts."
https://arxiv.org/abs/2403.07661,2024-03-12,Gender-ambiguous voice generation through feminine speaking style transfer in male voices,"['Maria Koutsogiannaki', 'Shafel Mc Dowall', 'Ioannis Agiomyrgiannakis']","Recently, and under the umbrella of Responsible AI, efforts have been made to develop gender-ambiguous synthetic speech to represent with a single voice all individuals in the gender spectrum. However, research efforts have completely overlooked the speaking style despite differences found among binary and non-binary populations. In this work, we synthesise gender-ambiguous speech by combining the timbre of a male speaker with the manner of speech of a female speaker using voice morphing and pitch shifting towards the male-female boundary. Subjective evaluations indicate that the ambiguity of the morphed samples that convey the female speech style is higher than those that undergo pure pitch transformations suggesting that the speaking style can be a contributing factor in creating gender-ambiguous speech. To our knowledge, this is the first study that explicitly uses the transfer of the speaking style to create gender-ambiguous voices."
https://arxiv.org/abs/2403.07660,2024-03-12,Broadcasting Quantum Information using Finite Resources,"['Tiago Debarba', 'Marcus Huber', 'Nicolai Friis']","Measurements can be viewed as interactions between systems and specifically prepared pointers. Ideally, these interactions create accurate copies of the information corresponding to the diagonal of the system's density operator with respect to the measurement basis. However, establishing measurement outcomes as objective facts requires redundancy. We therefore consider the problem of unitarily distributing this information to several quantum memories. We show that the accuracy of this broadcasting process is limited by thermodynamic restrictions on preparing the memories in pure states: ideal broadcasting is impossible using finite resources. For finite-temperature memories we put forward a lower bound on the entropy production of the broadcasting process. This Holevo-Landauer bound demonstrates that the mixedness of the initial memory limits the ability to accurately broadcast information to more than one memory component, thus fundamentally restricting the creation of redundancies while maintaining the integrity of the original information. Finally, we show how the full information can be recovered in the classical limit -- via coarse-graining or asymptotically as the number of subsystems of each memory component increases -- thus elucidating how objective properties can emerge despite inherent imperfections."
https://arxiv.org/abs/2403.07659,2024-03-12,The power operation in the Galois cohomology of a reductive group over a number field,"['Mikhail Borovoi', 'Zinovy Reichstein']","For a connected reductive group $G$ over a local or global field $K$, we define a *diamond* (or *power*) operation $$(ξ, n)\mapsto ξ^{\Diamond n}\, \colon\ H^1(K,G)\times {\mathbb Z}\to H^1(K,G)$$ of raising to power $n$ in the Galois cohomology pointed set (this operation is new when $K$ is a number field). We show that this operation has many functorial properties. When $G$ is a torus, the ponted set $H^1(K,G)$ has a natural group structure, and our new operation coincides with the usual power operation $(ξ,n)\mapsto ξ^n$."
https://arxiv.org/abs/2403.07658,2024-03-13,An overdetermined problem in 2D linearised hydrostatics,['Giovanni Franzina'],"In two spatial dimensions, we discuss the relation between the solvability of Schiffer's overdetermined problem and the optimality, among sets of prescribed area, of the first eigenvalue in the buckling problem for a clamped plate and that of the first eigenvalue of the Stokes operator. For the latter, we deduce that the minimisers under area constraint that are smooth and simply connected must be discs from the fact that a pressureless velocity is a necessary condition of optimality."
https://arxiv.org/abs/2403.07657,2024-03-12,Scalable Spatiotemporal Prediction with Bayesian Neural Fields,"['Feras Saad', 'Jacob Burnim', 'Colin Carroll', 'Brian Patton', 'Urs Köster', 'Rif A. Saurous', 'Matthew Hoffman']","Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent. We evaluate BayesNF against prominent statistical and machine-learning baselines, showing considerable improvements on diverse prediction problems from climate and public health datasets that contain tens to hundreds of thousands of measurements. The paper is accompanied with an open-source software package (https://github.com/google/bayesnf) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform."
https://arxiv.org/abs/2403.07656,2024-03-12,Optimal control of stochastic cylinder flow using data-driven compressive sensing method,"['Liuhong Chen', 'Ju Ming', 'Max D. Gunzburger']","A stochastic optimal control problem for incompressible Newtonian channel flow past a circular cylinder is used as a prototype optimal control problem for the stochastic Navier-Stokes equations. The inlet flow and the rotation speed of the cylinder are allowed to have stochastic perturbations. The control acts on the cylinder via adjustment of the rotation speed. Possible objectives of the control include, among others, tracking a desired (given) velocity field or minimizing the kinetic energy, enstrophy, or the drag of the flow over a given body. Owing to the high computational requirements, the direct application of the classical Monte Carlo methods for our problem is limited. To overcome the difficulty, we use a multi-fidelity data-driven compressive sensing based polynomial chaos expansions (MDCS-PCE). An effective gradient-based optimization for the discrete optimality systems resulted from the MDCS-PCE discretization is developed. The strategy can be applied broadly to many stochastic flow control problems. Numerical tests are performed to validate our methodology."
https://arxiv.org/abs/2403.07655,2024-03-12,Enhancing Physical Layer Security in Dual-Function Radar-Communication Systems with Hybrid Beamforming Architecture,"['Lingyun Xu', 'Bowen Wang', 'Jieming Shi', 'Huiyong Li', 'Ziyang Cheng']","In this letter, we investigate enhancing the physical layer security (PLS) for the dual-function radar-communication (DFRC) system with hybrid beamforming (HBF) architecture, where the base station (BS) achieves downlink communication and radar target detection simultaneously. We consider an eavesdropper intercepting the information transmitted from the BS to the downlink communication users with imperfectly known channel state information. Additionally, the location of the radar target is also imperfectly known by the BS. To enhance PLS in the considered DFRC system, we propose a novel HBF architecture, which introduces a new integrated sensing and security (I2S) symbol. The secure HBF design problem for DFRC is formulated by maximizing the minimum legitimate user communication rate subject to radar interference-plus-noise ratio, eavesdropping rate, hardware and power constraints. To solve this non-convex problem, we propose an alternating optimization based method to jointly optimize transmit and receive beamformers. Numerical simulation results validate the effectiveness of the proposed algorithm and show the superiority of the proposed I2S-aided HBF architecture for achieving DFRC and enhancing PLS."
https://arxiv.org/abs/2403.07654,2024-03-12,Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models,"['Andrew Parry', 'Maik Fröbe', 'Sean MacAvaney', 'Martin Potthast', 'Matthias Hagen']","Modern sequence-to-sequence relevance models like monoT5 can effectively capture complex textual interactions between queries and documents through cross-encoding. However, the use of natural language tokens in prompts, such as Query, Document, and Relevant for monoT5, opens an attack vector for malicious documents to manipulate their relevance score through prompt injection, e.g., by adding target words such as true. Since such possibilities have not yet been considered in retrieval evaluation, we analyze the impact of query-independent prompt injection via manually constructed templates and LLM-based rewriting of documents on several existing relevance models. Our experiments on the TREC Deep Learning track show that adversarial documents can easily manipulate different sequence-to-sequence relevance models, while BM25 (as a typical lexical model) is not affected. Remarkably, the attacks also affect encoder-only relevance models (which do not rely on natural language prompt tokens), albeit to a lesser extent."
https://arxiv.org/abs/2403.07653,2024-03-12,OmniMatch: Effective Self-Supervised Any-Join Discovery in Tabular Data Repositories,"['Christos Koutras', 'Jiani Zhang', 'Xiao Qin', 'Chuan Lei', 'Vasileios Ioannidis', 'Christos Faloutsos', 'George Karypis', 'Asterios Katsifodimos']","How can we discover join relationships among columns of tabular data in a data repository? Can this be done effectively when metadata is missing? Traditional column matching works mainly rely on similarity measures based on exact value overlaps, hence missing important semantics or failing to handle noise in the data. At the same time, recent dataset discovery methods focusing on deep table representation learning techniques, do not take into consideration the rich set of column similarity signals found in prior matching and discovery methods. Finally, existing methods heavily depend on user-provided similarity thresholds, hindering their deployability in real-world settings. In this paper, we propose OmniMatch, a novel join discovery technique that detects equi-joins and fuzzy-joins betwen columns by combining column-pair similarity measures with Graph Neural Networks (GNNs). OmniMatch's GNN can capture column relatedness leveraging graph transitivity, significantly improving the recall of join discovery tasks. At the same time, OmniMatch also increases the precision by augmenting its training data with negative column join examples through an automated negative example generation process. Most importantly, compared to the state-of-the-art matching and discovery methods, OmniMatch exhibits up to 14% higher effectiveness in F1 score and AUC without relying on metadata or user-provided thresholds for each similarity metric."
https://arxiv.org/abs/2403.07652,2024-03-12,Harder Tasks Need More Experts: Dynamic Routing in MoE Models,"['Quzhe Huang', 'Zhenwei An', 'Nan Zhuang', 'Mingxu Tao', 'Chen Zhang', 'Yang Jin', 'Kun Xu', 'Kun Xu', 'Liwei Chen', 'Songfang Huang', 'Yansong Feng']","In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model dispatches more experts to tasks requiring complex reasoning skills, like BBH, confirming its ability to dynamically allocate computational resources in alignment with the input's complexity. Our findings also highlight a variation in the number of experts needed across different layers of the transformer model, offering insights into the potential for designing heterogeneous MoE frameworks. The code and models are available at https://github.com/ZhenweiAn/Dynamic_MoE."
https://arxiv.org/abs/2403.07651,2024-03-12,Directionally Tunable Co- and Counter-Propagating Photon Pairs from a Nonlinear Metasurface,"['Maximilian A. Weissflog', 'Jinyong Ma', 'Jihua Zhang', 'Tongmiao Fan', 'Thomas Pertsch', 'Dragomir N. Neshev', 'Sina Saravi', 'Frank Setzpfandt', 'Andrey A. Sukhorukov']","Nonlinear metasurfaces have recently been established as a new platform for generating photon pairs via spontaneous parametric down-conversion. While for classical harmonic generation in metasurfaces a high level of control over all degrees of freedom of light has been reached, this capability is yet to be developed for photon pair generation. In this work, we theoretically and experimentally demonstrate for the first time precise control of the emission angle of photon pairs generated from a nonlinear metasurface. Our measurements show angularly tunable pair-generation with high coincidence-to-accidental ratio for both co- and counter-propagating emission. The underlying principle is the transverse phase-matching of guided-mode resonances with strong angular dispersion in a nonlinear lithium niobate metagrating. We provide a straightforward design strategy for photon pair generation in such a device and find very good agreement between the calculations and experimental results. Here we use all-optical emission angle tuning by means of the pump wavelength, however the principle could be extended to modulation via the electro-optic effect in lithium niobate. In sum, this work provides an important addition to the toolset of sub-wavelength thickness photon pair sources."
https://arxiv.org/abs/2403.07650,2024-03-12,A Class of Semiparametric Yang and Prentice Frailty Models,"['Cassius Henrique Xavier Oliveira', 'Fabio Nogueira Demarqui', 'Vinicius Diniz Mayrink']","The Yang and Prentice (YP) regression models have garnered interest from the scientific community due to their ability to analyze data whose survival curves exhibit intersection. These models include proportional hazards (PH) and proportional odds (PO) models as specific cases. However, they encounter limitations when dealing with multivariate survival data due to potential dependencies between the times-to-event. A solution is introducing a frailty term into the hazard functions, making it possible for the times-to-event to be considered independent, given the frailty term. In this study, we propose a new class of YP models that incorporate frailty. We use the exponential distribution, the piecewise exponential distribution (PE), and Bernstein polynomials (BP) as baseline functions. Our approach adopts a Bayesian methodology. The proposed models are evaluated through a simulation study, which shows that the YP frailty models with BP and PE baselines perform similarly to the generator parametric model of the data. We apply the models in two real data sets."
https://arxiv.org/abs/2403.07649,2024-03-12,A phase-resolved Fermi-LAT analysis of the mode-changing pulsar PSR J2021+4026 shows hints of a multipolar magnetosphere,"['A. Fiori', 'M. Razzano', 'A. K. Harding', 'M. Kerr', 'R. P. Mignani', 'P. M. Saz Parkinson']","The goal of our work is to study the mode changes of the radio-quiet gamma-ray pulsar PSR J2021+4026 with improved detail. By accurately characterizing variations in the gamma-ray spectrum and pulse profile, we aim to relate the Fermi-LAT observations to theoretical models and interpret the mode changes in terms of variations in the structure of a multipolar dissipative magnetosphere. We continually monitored the rotational evolution and the gamma-ray flux of PSR J2021+4026 using more than 13 years of Fermi-LAT data with a binned likelihood approach. We clearly detect the previous mode changes and confirm a more recent mode change that occurred around June 2020. We investigated the features of the phase-resolved spectrum and pulse profile, and we inferred the macroscopic conductivity, the electric field parallel to the magnetic field, and the curvature radiation cutoff energy. These physical quantities are related to the spin-down rate and the gamma-ray flux and therefore are relevant to the theoretical interpretation of the mode changes. We computed the relative variations in the best-fit parameters, finding typical flux changes between 13% and 20%. Correlations appear between the gamma-ray flux and the spectral parameters, as the peak of the spectrum shifts by about 10% toward lower energies when the flux decreases. The analysis of the pulse profile reveals that the pulsed fraction of the light curve is larger when the flux is low. We introduced a simple magnetosphere model that combines a dipole field with a strong quadrupole component. We simulated magnetic field configurations to determine the positions of the polar caps for different sets of parameters, and we conclude that some configurations could explain the observed multiwavelength variability."
https://arxiv.org/abs/2403.07648,2024-03-12,Characterization of Large Language Model Development in the Datacenter,"['Qinghao Hu', 'Zhisheng Ye', 'Zerui Wang', 'Guoteng Wang', 'Meng Zhang', 'Qiaoling Chen', 'Peng Sun', 'Dahua Lin', 'Xiaolin Wang', 'Yingwei Luo', 'Yonggang Wen', 'Tianwei Zhang']","Large Language Models (LLMs) have presented impressive performance across several transformative tasks. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. In this paper, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures. Our analysis summarizes hurdles we encountered and uncovers potential opportunities to optimize systems tailored for LLMs. Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through LLM-involved failure diagnosis and automatic recovery. (2) decoupled scheduling for evaluation, which achieves timely performance feedback via trial decomposition and scheduling optimization."
https://arxiv.org/abs/2403.07647,2024-03-12,Expiring opacity problems in parametric timed automata,"['Étienne André', 'Engel Lefaucheux', 'Dylan Marinho']","Information leakage can have dramatic consequences on the security of real-time systems. Timing leaks occur when an attacker is able to infer private behavior depending on timing information. In this work, we propose a definition of expiring timed opacity w.r.t. execution time, where a system is opaque whenever the attacker is unable to deduce the reachability of some private state solely based on the execution time; in addition, the secrecy is violated only when the private state was entered ""recently"", i.e., within a given time bound (or expiration date) prior to system completion. This has an interesting parallel with concrete applications, notably cache deducibility: it may be useless for the attacker to know the cache content too late after its observance. We study here expiring timed opacity problems in timed automata. We consider the set of time bounds (or expiration dates) for which a system is opaque and show when they can be effectively computed for timed automata. We then study the decidability of several parameterized problems, when not only the bounds, but also some internal timing constants become timing parameters of unknown constant values."
https://arxiv.org/abs/2403.07646,2024-03-12,Diameter of 2-distance graphs,"['S. H. Jafari', 'S. R. Musawi']","For a simple graph $G$, the $2$-distance graph, $D_2(G)$, is a graph with the vertex set $V(G)$ and two vertices are adjacent if and only if their distance is $2$ in the graph $G$. In this paper, for graphs $G$ with diameter 2, we show that $diam(D_2(G))$ can be any integer $t\geqslant2$. For graphs $G$ with $diam(G)\geqslant3$, we prove that $\frac{1}{2}diam(G)\leqslant diam(D_2(G))$ and this inequality is sharp. Also, for $diam(G)=3$, we prove that $diam(D_2(G))\leqslant5$ and this inequality is sharp."
https://arxiv.org/abs/2403.07645,2024-03-12,Motivic six-functor formalism for log schemes,['Doosung Park'],"We establish the motivic six-functor formalism for fs log schemes. In particular, we prove the exact base change property, projection formula, and Poincaré duality. We also define Borel-Moore motivic homology, G-theory, and Chow homology of fs log schemes and the category of Chow motives over fs log schemes."
https://arxiv.org/abs/2403.07644,2024-03-12,Discrete Laplacian thermostat for flocks and swarms: the fully conserved Inertial Spin Model,"['Andrea Cavagna', 'Javier Cristín', 'Irene Giardina', 'Tomas S. Grigera', 'Mario Veca']","Experiments on bird flocks and midge swarms reveal that these natural systems are well described by an active theory in which conservation laws play a crucial role. By building a symplectic structure that couples the particles' velocities to the generator of their internal rotations (spin), the Inertial Spin Model (ISM) reinstates a second-order temporal dynamics that captures many phenomenological traits of flocks and swarms. The reversible structure of the ISM predicts that the total spin is a constant of motion, the central conservation law responsible for all the novel dynamical features of the model. However, fluctuations and dissipation introduced in the original model to make it relax, violate the spin conservation law, so that the ISM aligns with the biophysical phenomenology only within finite-size regimes, beyond which the overdamped dynamics characteristic of the Vicsek model takes over. Here, we introduce a novel version of the ISM, in which the irreversible terms needed to relax the dynamics strictly respect the conservation of the spin. We perform a numerical investigation of the fully conservative model, exploring both the fixed-network case, which belongs to the equilibrium class of Model G, and the active case, characterized by self-propulsion of the agents and an out-of-equilibrium reshuffling of the underlying interaction network. Our simulations not only capture the correct spin wave phenomenology of the ordered phase, but they also yield dynamical critical exponents in the near-ordering phase that agree very well with the theoretical predictions."
https://arxiv.org/abs/2403.07643,2024-03-12,Quantitative 2D propagation of smallness and control for 1D heat equations with power growth potentials,['Yunlei Wang'],"We study the relation between propagation of smallness in the plane and control for heat equations. The former has been proved by Zhu who showed how the value of solutions in some small set propagates to a larger domain. By reviewing his proof, we establish a quantitative version with the explicit dependence of parameters. Using this explicit version, we establish new exact null-controllability results of 1D heat equations with any nonnegative power growth potentials $V\in\mathcal{C}(\mathbb{R})$. As a key ingredient, new spectral inequalities are established. The control set $Ω$ that we consider satisfy \begin{equation*}"
https://arxiv.org/abs/2403.07642,2024-03-11,Absence of ground states for anions,['Yukimi Goto'],"We show that the $N$-electron Hamiltonian $H(N, Z)$ with the total nuclear charge $Z$ has no normalizable ground state if the ground state energy $E(N, Z)$ satisfies $E(N, Z)= E(N-1, Z)$ for $Z=N-1$. For anions $\mathrm{He}^-, \mathrm{Be}^-, \mathrm{N}^-, \mathrm{Ne}^-$, etc., many numerical results give strong evidence of the condition $E(N, Z)= E(N-1, Z)$."
https://arxiv.org/abs/2403.07641,2024-03-12,Sign-changing bubbling solutions for an exponential nonlinearity in $\mathbb{R}^2$,['Yibin Zhang'],"Quite differently from those perturbation techniques in \cite{DM}, we use the assumption of a $C^1$-stable critical point to construct positive or sign-changing bubbling solutions to the boundary value problem $-Δu=λu|u|^{p-2}e^{|u|^p}$ with homogeneous Dirichlet boundary condition in a bounded, smooth planar domain $Ω$, when $0<p<2$ and $λ>0$ is a small parameter. A sufficient condition on the intersection between the nodal line of these sign-changing solutions and the boundary of the domain is founded. Moreover, for $λ$ small enough, we prove that when $Ω$ is an arbitrary bounded domain, this problem has not only at least two pairs of bubbling solutions which change sign exactly once and whose nodal lines intersect the boundary, but also a bubbling solution which changes sign exactly twice or three times; when $Ω$ has an axial symmetry, this problem has a bubbling solution which alternately changes sign arbitrarily many times along the axis of symmetry through the domain."
https://arxiv.org/abs/2403.07640,2024-03-12,Asynchronous Approximate Byzantine Consensus: A Multi-hop Relay Method and Tight Graph Conditions,"['Liwei Yuan', 'Hideaki Ishii']","We study a multi-agent resilient consensus problem, where some agents are of the Byzantine type and try to prevent the normal ones from reaching consensus. In our setting, normal agents communicate with each other asynchronously over multi-hop relay channels with delays. To solve this asynchronous Byzantine consensus problem, we develop the multi-hop weighted mean subsequence reduced (MW-MSR) algorithm. The main contribution is that we characterize a tight graph condition for our algorithm to achieve Byzantine consensus, which is expressed in the novel notion of strictly robust graphs. We show that the multi-hop communication is effective for enhancing the network's resilience against Byzantine agents. As a result, we also obtain novel conditions for resilient consensus under the malicious attack model, which are tighter than those known in the literature. Furthermore, the proposed algorithm can be viewed as a generalization of the conventional flooding-based algorithms, with less computational complexity. Lastly, we provide numerical examples to show the effectiveness of the proposed algorithm."
https://arxiv.org/abs/2403.07639,2024-03-12,A Framework for Controlling Multiple Industrial Robots using Mobile Applications,"['Daniela Alvarado', 'Dr. Seemal Asif']","Purpose: Over the last few decades, the development of the hardware and software has enabled the application of advanced systems. In the robotics field, the UI design is an intriguing area to be explored due to the creation of devices with a wide range of functionalities in a reduced size. Moreover, the idea of using the same UI to control several systems arouses a great interest considering that this involves less learning effort and time for the users. Therefore, this paper will present a mobile application to control two industrial robots with four modes of operation. Design/methodology/approach: The smartphone was selected to be the interface due to its wide range of capabilities and the MIT Inventor App was used to create the application, whose environment is supported by Android smartphones. For the validation, ROS was used since it is a fundamental framework utilised in industrial robotics and the Arduino Uno was used to establish the data transmission between the smartphone and the board NVIDIA Jetson TX2. In MIT Inventor App, the graphical interface was created to visualize the options available in the app whereas two scripts in python were programmed to perform the simulations in ROS and carry out the tests. Findings: The results indicated that the use of the sliders to control the robots is more favourable than the Orientation Sensor due to the sensibility of the sensor and human limitations to hold the smartphone perfectly still. Another important finding was the limitations of the autonomous mode, in which the robot grabs an object. In this case, the configuration of the Kinect camera and the controllers has a significant impact on the success of the simulation. Finally, it was observed that the delay was appropriate despite the use of the Arduino UNO to transfer the data between the Smartphone and the Nvidia Jetson TX2."
https://arxiv.org/abs/2403.07638,2024-03-12,Online Adaptation of Sampling-Based Motion Planning with Inaccurate Models,"['Marco Faroni', 'Dmitry Berenson']","Robotic manipulation relies on analytical or learned models to simulate the system dynamics. These models are often inaccurate and based on offline information, so that the robot planner is unable to cope with mismatches between the expected and the actual behavior of the system (e.g., the presence of an unexpected obstacle). In these situations, the robot should use information gathered online to correct its planning strategy and adapt to the actual system response. We propose a sampling-based motion planning approach that uses an estimate of the model error and online observations to correct the planning strategy at each new replanning. Our approach adapts the cost function and the sampling bias of a kinodynamic motion planner when the outcome of the executed transitions is different from the expected one (e.g., when the robot unexpectedly collides with an obstacle) so that future trajectories will avoid unreliable motions. To infer the properties of a new transition, we introduce the notion of context-awareness, i.e., we store local environment information for each executed transition and avoid new transitions with context similar to previous unreliable ones. This is helpful for leveraging online information even if the simulated transitions are far (in the state-and-action space) from the executed ones. Simulation and experimental results show that the proposed approach increases the success rate in execution and reduces the number of replannings needed to reach the goal."
https://arxiv.org/abs/2403.07637,2024-03-12,Discovery of a Magnetic Topological Semimetal Eu3In2As4 with a Single Pair of Weyl Points,"['Ke Jia', 'Jingyu Yao', 'Xiaobo He', 'Yupeng Li', 'Junze Deng', 'Ming Yang', 'Junfeng Wang', 'Zengwei Zhu', 'Cuixiang Wang', 'Dayu Yan', 'Hai L. Feng', 'Jie Shen', 'Yongkang Luo', 'Zhijun Wang', 'Youguo Shi']","Magnetic Weyl semimetal (MWS) is a unique topological state with open surface Fermi arc states and other exotic transport phenomena. However, most reported MWSs show multiple pairs of Weyl points and complicated Fermi surfaces, which increases the difficulty of the investigation into the intrinsic chiral transport property. In this wor, we successfully synthesized a soft magnetic Weyl semimetal Eu3In2As4 with a single pair of Weyl points under magnetic fields. The Shubnikov de Haas (SdH) oscillation with a single frequency, as well as a linear hall resistance with the same carrier density, is observed up to 50 Tesla, indicating a single pair of Weyl points around the Fermi level with a massless fermion (m* = 0.121 m0, Berry phase). Such a single pair of Weyl points is further confirmed by the density functional theory calculations. The magnetic ordering and band topology can be easily tuned by the external magnetic field. The field-induced MWS Eu3In2As4 with a single pair of Weyl points is a good platform to detect chiral transport properties, including possible quantum anomalous Hall effect."
https://arxiv.org/abs/2403.07636,2024-03-12,Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Matching Framework,"['Minh Hieu Phan', 'Yutong Xie', 'Yuankai Qi', 'Lingqiao Liu', 'Liyang Liu', 'Bowen Zhang', 'Zhibin Liao', 'Qi Wu', 'Minh-Son To', 'Johan W. Verjans']","Medical vision language pre-training (VLP) has emerged as a frontier of research, enabling zero-shot pathological recognition by comparing the query image with the textual descriptions for each disease. Due to the complex semantics of biomedical texts, current methods struggle to align medical images with key pathological findings in unstructured reports. This leads to the misalignment with the target disease's textual representation. In this paper, we introduce a novel VLP framework designed to dissect disease descriptions into their fundamental aspects, leveraging prior knowledge about the visual manifestations of pathologies. This is achieved by consulting a large language model and medical experts. Integrating a Transformer module, our approach aligns an input image with the diverse elements of a disease, generating aspect-centric image representations. By consolidating the matches from each aspect, we improve the compatibility between an image and its associated disease. Additionally, capitalizing on the aspect-oriented representations, we present a dual-head Transformer tailored to process known and unknown diseases, optimizing the comprehensive detection efficacy. Conducting experiments on seven downstream datasets, ours outperforms recent methods by up to 8.07% and 11.23% in AUC scores for seen and novel categories, respectively. Our code is released at \href{https://github.com/HieuPhan33/MAVL}{https://github.com/HieuPhan33/MAVL}."
https://arxiv.org/abs/2403.07635,2024-03-12,A Study on Centralised and Decentralised Swarm Robotics Architecture for Part Delivery System,"['Angelos Dimakos', 'Daniel Woodhall', 'Seemal Asif']","Drones are also known as UAVs are originally designed for military purposes. With the technological advances, they can be seen in most of the aspects of life from filming to logistics. The increased use of drones made it sometimes essential to form a collaboration between them to perform the task efficiently in a defined process. This paper investigates the use of a combined centralised and decentralised architecture for the collaborative operation of drones in a parts delivery scenario to enable and expedite the operation of the factories of the future. The centralised and decentralised approaches were extensively researched, with experimentation being undertaken to determine the appropriateness of each approach for this use-case. Decentralised control was utilised to remove the need for excessive communication during the operation of the drones, resulting in smoother operations. Initial results suggested that the decentralised approach is more appropriate for this use-case. The individual functionalities necessary for the implementation of a decentralised architecture were proven and assessed, determining that a combination of multiple individual functionalities, namely VSLAM, dynamic collision avoidance and object tracking, would give an appropriate solution for use in an industrial setting. A final architecture for the parts delivery system was proposed for future work, using a combined centralised and decentralised approach to combat the limitations inherent in each architecture."
https://arxiv.org/abs/2403.07634,2024-03-12,AstroSat View of Transient Low-mass X-ray Binary XTE J1701-462: Spectral and Temporal Evolution along the Z-track,['Vivek K. Agrawal'],AstroSat observed transient neutron star low-mass X-ray binary XTE J1701-462 for a total duration of 135 ks during its 2022 outburst. The source traced a complete 'Z' shaped structure in the hardness intensity diagram (HID) during the observation. The source exhibited an extended horizontal branch and a short-dipping flaring branch in the HID. We find that most suitable spectral model comprises emission from a standard multi-color accretion disk and Comptonized radiation from a hot central corona. The observed disk component is cool having temperature in the range of 0.3-0.4 keV and truncated far ( ~100 - 520 km) from the compact object. The Compton corona has optical depth in the range of 3.3-5 and temperature in the range of 3.3-4.9 keV. The disk and corona flux as well as truncation radius varies significantly along the HID. We discuss the possible scenarios to explain the relationship between spectral evolution and motion of the source along the HID. The timing analysis revealed horizontal branch oscillations (HBOs) in the frequency range 34-60 Hz. The frequency and rms strength of HBO vary systematically as the source moves along the horizontal branch (HB). The observed correlation of HBO properties with the position on the HB is similar to that reported previously in this source using \textit{RXTE} data during the 2006 outburst of the source. The source also showed NBOs with frequency $\sim$ 6.7 Hz in the middle and lower normal branch. The energy dependent study of the HBO properties suggests that the HBO is stronger in the higher energy band. We also observed very-low frequency noise (VLFN) and band limited noise (BLN) components in the power-density spectra. The break frequency of BLN component was found to be tightly correlated with the HBO frequency. We discuss possible models to explain the origin and nature of the observed features in the PDS.
https://arxiv.org/abs/2403.07633,2024-03-12,Asymptotic dynamics of generalized Kantorovich operators,"['Krzysztof Bartoszek', 'Wojciech Bartoszek']","We characterize the family of continuous functions $f\in C([0,1])$ such that the iterates $\widehat{T}^{k}_{i} f$ converge uniformly on $[0,1]$, where $\widehat{T}_i$ is a generalized Kantorovich operator. This gives an affirmative answer to the problem raised in 2021 by Acu and Rasa."
https://arxiv.org/abs/2403.07632,2024-03-12,CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability,"['Gregory W. Kyro', 'Matthew T. Martin', 'Eric D. Watt', 'Victor S. Batista']","Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel. It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity. In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity. The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade. These models can also serve independently as effective components of a virtual screening pipeline. We applied the complete framework to pimozide, an FDA-approved antipsychotic agent that demonstrates high affinity to the hERG channel, and generated 100 refined candidates. Remarkably, among the candidates is fluspirilene, a compound which is of the same class of drugs (diphenylmethanes) as pimozide and therefore has similar pharmacological activity, yet exhibits over 700-fold weaker binding to hERG. We have made all of our software open-source to facilitate integration of the CardioGenAI framework for molecular hypothesis generation into drug discovery workflows."
https://arxiv.org/abs/2403.07631,2024-03-12,Efficient Global Navigational Planning in 3D Structures based on Point Cloud Tomography,"['Bowen Yang', 'Jie Cheng', 'Bohuan Xue', 'Jianhao Jiao', 'Ming Liu']","Navigation in complex 3D scenarios requires appropriate environment representation for efficient scene understanding and trajectory generation. We propose a highly efficient and extensible global navigation framework based on a tomographic understanding of the environment to navigate ground robots in multi-layer structures. Our approach generates tomogram slices using the point cloud map to encode the geometric structure as ground and ceiling elevations. Then it evaluates the scene traversability considering the robot's motion capabilities. Both the tomogram construction and the scene evaluation are accelerated through parallel computation. Our approach further alleviates the trajectory generation complexity compared with planning in 3D spaces directly. It generates 3D trajectories by searching through multiple tomogram slices and separately adjusts the robot height to avoid overhangs. We evaluate our framework in various simulation scenarios and further test it in the real world on a quadrupedal robot. Our approach reduces the scene evaluation time by 3 orders of magnitude and improves the path planning speed by 3 times compared with existing approaches, demonstrating highly efficient global navigation in various complex 3D environments. The code is available at: https://github.com/byangw/PCT_planner."
https://arxiv.org/abs/2403.07630,2024-03-12,Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation,"['Feilong Tang', 'Zhongxing Xu', 'Zhaojun Qu', 'Wei Feng', 'Xingjian Jiang', 'Zongyuan Ge']","Recent weakly supervised semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM). In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics. Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances. The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias. Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions. With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension. The core of this method is to accurately capture intra-class variations in object features through context-aware prototypes, facilitating the adaptation to the semantic attributes of various instances. We design feature distribution alignment to optimize prototype awareness, aligning instance feature distributions with dense features. In addition, a unified training framework is proposed to combine label-guided classification supervision and prototypes-guided self-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show that CPAL significantly improves off-the-shelf methods and achieves state-of-the-art performance. The project is available at https://github.com/Barrett-python/CPAL."
https://arxiv.org/abs/2403.07629,2024-03-12,On the number of 8-cycles for two particular regular tournaments of order N with diametrically opposite local properties,['Sergey Savchenko'],"For a regular tournament $T$ of order $n,$ denote by $c_{8}(T)$ the number of cycles of length $8$ in $T.$ Let $DR_{n}$ be a doubly-regular tournament of order $n\equiv 3\mod4$ (so, the out-sets and in-sets of its vertices are also regular and hence, contain the maximum possible number of cyclic triples) and $RLT_{n}$ be the unique regular locally transitive tournament of (odd) order $n$ (so, the out-sets and in-sets of its vertices are transitive and hence, contain no cyclic triples, at all). Some arguments based on the spectral properties of tournaments allow us to suggest that $c_{8}(T) \le c_{8}(RLT_{n}),$ where $n$ is sufficiently large. This restriction on $n$ is essential because our computer processing of B. McKay's file of tournaments implies that for $n=9,11,13,$ the maximum of $c_{8}(T)$ is attained at tournaments with regular structure of the out and in-sets of their vertices. In the present paper, we show that $c_{8}(DR_{n})$ does not depend on a particular choice of $DR_{n}$ and determine expressions for $c_{8}(DR_{n})$ and $c_{8}(RLT_{n}).$ They are both polynomials of degree $8$ in $n.$ Comparing $c_{8}(DR_{n})$ with $c_{8}(RLT_{n})$ yields the inequality $c_{8}(DR_{n})>c_{8}(RLT_{n})$ for $11\le n\le 35,$ while $c_{8}(RLT_{n}) > c_{8}(DR_{n})$ for $n\ge 39.$ This allows us to treat the value $n=39$ as the point of phase transition in the local properties of maximizers and minimizers of $c_{8}(T)$ in the class of regular tournaments of order $n.$"
https://arxiv.org/abs/2403.07628,2024-03-12,Asymptotic Expansions of the Limit Laws of Gaussian and Laguerre (Wishart) Ensembles at the Soft Edge,['Folkmar Bornemann'],"The large-matrix limit laws of the rescaled largest eigenvalue of the orthogonal, unitary and symplectic $n$-dimensional Gaussian ensembles -- and of the corresponding Laguerre ensembles (Wishart distributions) for various regimes of the parameter $α$ (degrees of freedom $p$) -- are known to be the Tracy-Widom distributions $F_β$ ($β=1,2,4$). We will establish (paying particular attention to large, or small, ratios $p/n$) that, with careful choices of the rescaling constants and the expansion parameter $h$, the limit laws embed into asymptotic expansions in powers of $h$, where $h \asymp n^{-2/3}$ resp. $h \asymp (n\,\wedge\,p)^{-2/3}$. We find explicit analytic expressions of the first few expansions terms as linear combinations, with rational polynomial coefficients, of higher order derivatives of the limit law $F_β$. With a proper parametrization, the expansions in the Gaussian cases can be understood, for given $n$, as the limit $p\to\infty$ of the Laguerre cases. Whereas the results for $β=2$ are presented with proof, the discussion of the cases $β=1,4$ is based on some hypotheses, focussing on the algebraic aspects of actually computing the polynomial coefficients. For the purposes of illustration and validation, the various results are checked against simulation data with a sample size of a thousand million."
https://arxiv.org/abs/2403.07627,2024-03-12,generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation,"['Thilo Spinner', 'Rebecca Kehlbeck', 'Rita Sevastjanova', 'Tobias Stähle', 'Daniel A. Keim', 'Oliver Deussen', 'Mennatallah El-Assady']","Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data. Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods. Additionally, we demonstrate the applicability of our approach in a qualitative user study. Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases."
https://arxiv.org/abs/2403.07626,2024-03-12,Trade-offs and thermodynamics of energy-relay proofreading,"['Jonas Berx', 'Karel Proesmans']","Biological processes that are able to discriminate between different molecules consume energy and dissipate heat. They operate at different levels of fidelity and speed, and as a consequence there exist fundamental trade-offs between these quantities and the entropy production rate. Usually, the energy source required to operate in a high-fidelity regime comes from the consumption of external energetic molecules, e.g., GTP hydrolysis in protein translation . In this work, we study trade-offs between several kinetic and thermodynamic observables for Hopfield's energy-relay mechanism, which does not consume external molecules and is able to operate in depleted regions, at the cost of a higher error rate. The trade-offs are obtained both analytically and numerically via Pareto optimal fronts. We find that the scheme is able to operate in three distinct regimes: an energy relay regime, a mixed relay-Michaelis-Menten regime, and a Michaelis-Menten regime, depending on the kinetic and energetic parameters that tune transitions between states. The mixed regime features a dynamical phase transition in the error-entropy production Pareto trade-off, while the pure energy relay regime contains a region where this type of proofreading energetically outperforms standard kinetic proofreading."
https://arxiv.org/abs/2403.07625,2024-03-12,Identifying a point-symmetric morphology in supernova remnant Cassiopeia A: explosion by jittering jets,"['Ealeal Bear', 'Noam Soker']","We identify a point-symmetric morphology of the supernova remnant (SNR) Cassiopeia A compatible with shaping by at least two, and more likely more than four, pairs of opposite jets, as expected in the jittering jets explosion mechanism (JJEM) of core-collapse supernovae. Using an old X-ray map of argon, we identify seven pairs of opposite morphological features that we connect with lines that cross each other at the same point on the plane of the sky. The opposite morphological features include protrusions, clumps, filaments, and funnels in the main SNR shell. In addition to these seven symmetry axes, we find two tentative symmetry axes (lines). These lines form a point-symmetric wind-rose. We place this point-symmetric wind-rose on a new JWST and X-ray images of Cassiopeia A. We find other morphological features and one more symmetry axis that strengthen the identified point-symmetric morphology. Not all symmetry axes correspond to jets; e.g., some clumps are formed by the compression of ejecta between two jet-inflated lobes (bubbles). The robust point-symmetric morphology in the iconic Cassiopeia A SNR strongly supports the JJEM and poses a severe challenge to the neutrino-driven explosion mechanism."
https://arxiv.org/abs/2403.07624,2024-03-12,Monocentric or polycentric city? An empirical perspective,['Rémi Lemoy'],"Do cities have just one or several centers? Studies performing radial or monocentric analyses of cities are usually criticised by researchers stating that cities are actually polycentric, and this has been well known for a long time. Reversely, when cities are studied independently of any center, other researchers will wonder how the variables of interest evolve with the distance to the center, because this distance is known to be a major determinant at the intra-urban scale. Both monocentric and polycentric formalisms have been introduced centuries (respectively, decades) ago for the study of urban areas, and used both on the empirical and the theoretical side in different disciplines (economics, geography, complex systems, physics...). The present work performs a synthesis of both viewpoints on cities, regarding their use in the literature, and explores with data on European urban areas how some cities considered to be the most polycentric in Europe compare to more standard cities when studied through a combination of radial analysis and scaling laws."
https://arxiv.org/abs/2403.07623,2024-03-12,Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness,"['Mingyue Cheng', 'Hao Zhang', 'Qi Liu', 'Fajie Yuan', 'Zhi Li', 'Zhenya Huang', 'Enhong Chen', 'Jun Zhou', 'Longfei Li']","Sequential recommender systems (SRS) could capture dynamic user preferences by modeling historical behaviors ordered in time. Despite effectiveness, focusing only on the \textit{collaborative signals} from behaviors does not fully grasp user interests. It is also significant to model the \textit{semantic relatedness} reflected in content features, e.g., images and text. Towards that end, in this paper, we aim to enhance the SRS tasks by effectively unifying collaborative signals and semantic relatedness together. Notably, we empirically point out that it is nontrivial to achieve such a goal due to semantic gap issues. Thus, we propose an end-to-end two-stream architecture for sequential recommendation, named TSSR, to learn user preferences from ID-based and content-based sequence. Specifically, we first present novel hierarchical contrasting module, including coarse user-grained and fine item-grained terms, to align the representations of inter-modality. Furthermore, we also design a two-stream architecture to learn the dependence of intra-modality sequence and the complex interactions of inter-modality sequence, which can yield more expressive capacity in understanding user interests. We conduct extensive experiments on five public datasets. The experimental results show that the TSSR could yield superior performance than competitive baselines. We also make our experimental codes publicly available at https://anonymous.4open.science/r/TSSR-2A27/."
https://arxiv.org/abs/2403.07622,2024-03-12,Multiple Latent Space Mapping for Compressed Dark Image Enhancement,"['Yi Zeng', 'Zhengning Wang', 'Yuxuan Liu', 'Tianjiao Zeng', 'Xuhang Liu', 'Xinglong Luo', 'Shuaicheng Liu', 'Shuyuan Zhu', 'Bing Zeng']","Dark image enhancement aims at converting dark images to normal-light images. Existing dark image enhancement methods take uncompressed dark images as inputs and achieve great performance. However, in practice, dark images are often compressed before storage or transmission over the Internet. Current methods get poor performance when processing compressed dark images. Artifacts hidden in the dark regions are amplified by current methods, which results in uncomfortable visual effects for observers. Based on this observation, this study aims at enhancing compressed dark images while avoiding compression artifacts amplification. Since texture details intertwine with compression artifacts in compressed dark images, detail enhancement and blocking artifacts suppression contradict each other in image space. Therefore, we handle the task in latent space. To this end, we propose a novel latent mapping network based on variational auto-encoder (VAE). Firstly, different from previous VAE-based methods with single-resolution features only, we exploit multiple latent spaces with multi-resolution features, to reduce the detail blur and improve image fidelity. Specifically, we train two multi-level VAEs to project compressed dark images and normal-light images into their latent spaces respectively. Secondly, we leverage a latent mapping network to transform features from compressed dark space to normal-light space. Specifically, since the degradation models of darkness and compression are different from each other, the latent mapping process is divided mapping into enlightening branch and deblocking branch. Comprehensive experiments demonstrate that the proposed method achieves state-of-the-art performance in compressed dark image enhancement."
https://arxiv.org/abs/2403.07621,2024-03-12,Smartphone region-wise image indoor localization using deep learning for indoor tourist attraction,"['Gabriel Toshio Hirokawa Higa', 'Rodrigo Stuqui Monzani', 'Jorge Fernando da Silva Cecatto', 'Maria Fernanda Balestieri Mariano de Souza', 'Vanessa Aparecida de Moraes Weber', 'Hemerson Pistori', 'Edson Takashi Matsubara']","Smart indoor tourist attractions, such as smart museums and aquariums, usually require a significant investment in indoor localization devices. The smartphone Global Positional Systems use is unsuitable for scenarios where dense materials such as concrete and metal block weaken the GPS signals, which is the most common scenario in an indoor tourist attraction. Deep learning makes it possible to perform region-wise indoor localization using smartphone images. This approach does not require any investment in infrastructure, reducing the cost and time to turn museums and aquariums into smart museums or smart aquariums. This paper proposes using deep learning algorithms to classify locations using smartphone camera images for indoor tourism attractions. We evaluate our proposal in a real-world scenario in Brazil. We extensively collect images from ten different smartphones to classify biome-themed fish tanks inside the Pantanal Biopark, creating a new dataset of 3654 images. We tested seven state-of-the-art neural networks, three being transformer-based, achieving precision around 90% on average and recall and f-score around 89% on average. The results indicate good feasibility of the proposal in a most indoor tourist attractions."
https://arxiv.org/abs/2403.07620,2024-03-12,Strong Local Bosonic Fluctuation: The Key to Understanding Strongly Correlated Metals,"['S. R. Hassan', 'Gopal Prakash', 'N. S. Vidhyadhiraja', 'T. V. Ramakrishnan']","In this paper, we present a theoretical framework for understanding the Extremely Correlated Fermi Liquid (ECFL) phenomenon within the $U=\infty$ Hubbard model. Our approach involves deriving equations of motion for the single-particle Green's function $G$ and its associated self-energy $Σ$, which involves the product of the bosonic correlation function comprising both density ($D_N$) and spin ($D_S$) correlations with $G$. By solving these equations self-consistently, we explore the behavior of $G$, $D_N$, and $D_S$ as functions of frequency, temperature, and hole concentration. Our results reveal distinct coherent and incoherent Fermi liquid regimes characterized by the presence or absence of quasiparticle excitations. Additionally, we analyze the intrinsic dc resistivity $ρ(T)$, observing a crossover from $T^2$ to linear behavior with increasing temperature. Our findings delineate Fermi liquid, quantum incoherent, and `classical' regimes in strongly correlated systems, emphasizing the importance of quantum diffusive local charge and spin fluctuations."
https://arxiv.org/abs/2403.07619,2024-03-12,Simple-minded systems and coherent rings,['Zhen Zhang'],Let $A$ be a finite dimensional algebra over an algebraically closed field. We present a relationship between simple-minded systems and coherent rings.
https://arxiv.org/abs/2403.07618,2024-03-12,Markov Chain Aggregation with Error Bounds on Transient Distributions,"['Fabian Michel', 'Markus Siegle']","We extend the existing theory of formal error bounds for the transient distribution of an aggregated (or lumped) Markov chain when compared to the transient distribution of the original chain, for both discrete- and continuous-time Markov chains. In the discrete-time setting, we bound the stepwise increment of the error, and in the continuous-time setting, we bound the rate at which the error grows. We then compare these error bounds with relevant concepts in the literature such as exact and ordinary lumpability as well as deflatability and aggregatability. These concepts define stricter than necessary conditions to identify settings in which the aggregation error is zero. We also consider possible algorithms for finding suitable aggregations for which the formal error bounds are low, and we analyse first experiments with these algorithms on different models."
https://arxiv.org/abs/2403.07617,2024-03-12,Price Gouging or Market Forces? Fairness Perceptions of Price Hikes in the Pandemic,"['Avichai Snir', 'Daniel Levy', 'Dudi Levy', 'Haipeng Allan Chen']","We report the results of surveys we conducted in the US and Israel in 2020, a time when many prices increased following the spread of the pandemic. To assess respondents perceptions of price increases, we focus on goods whose prices have increased during the pandemic, including some essential goods. Consistent with the principle of dual entitlement, we find that respondents perceive price increases as more acceptable if they are due to cost shocks than if they are due to demand shocks. However, we also find large differences across the two populations, as well as across goods."
https://arxiv.org/abs/2403.07616,2024-03-13,On some Fraisse limits with free amalgamation,['Yvon Bossut'],"In the first section of this work we discuss some curiosities around stable Kim-forking. In the second section we give a general way of building some known and some new examples of NSOP1 theories as the limit of some Fraisse class satisfying some strong conditions. These limits will satisfy existence, that Kim independence and algebraic independence coincide over arbitrary sets, that forcing base monotonicity on Kim-independance gives forking independence, and they come with a stationary independence relation. This study is based on the results of Baudish, Ramsey, Chernikov and Kruckman."
https://arxiv.org/abs/2403.07615,2024-03-12,Radiative corrections and threshold resummed predictions to pseudoscalar Higgs boson production in QCD,['Arunima Bhattacharya'],"This thesis studies the pseudoscalar Higgs boson production via gluon fusion in the EFT framework in a CP-conserving model. First, it presents the di-pseudoscalar Higgs boson production cross-section via gluon fusion till NNLO with the results valid for the pseudoscalar Higgs boson of MSSM and 2HDM with small tan $β$ by adjusting the top Yukawa coupling. We have used dimensional regularization to regulate the UV and IR divergences while carefully treating the Levi-Civita tensor and $γ_5$. Unlike the amplitudes involving a pair of scalar Higgs bosons, we do not need UV contact counter terms. We used Catani's predictions to factorize the IR singularities, and our IR poles agreed with Catani's predictions. Our results are essential for studies on producing a pair of pseudoscalar Higgs bosons at the LHC up to NNLO accuracy. This thesis also includes the next-to-soft-virtual (NSV) resummed corrections to $\overline{\text{NNLL}}$ accuracy, which are also matched to the NNLO cross-sections for a pseudoscalar Higgs boson production via gluon fusion. These NSV corrections are potentially significant compared to the conventional soft-virtual (SV) logarithms. We have estimated the uncertainties due to the choice of various PDFs and those due to the renormalization and factorization scales. The scale uncertainties show improvement for renormalization scale variation only, which suggests that NSV contributions from other parton channels and beyond NSV contributions in the gluon fusion channel are necessary for improved stability. We also studied the production cross-sections for mixed scalar-pseudoscalar states and their impact on QCD cross-sections for different values of the mixing angle $α$. The study indicates the necessity of improving the precision results for the pseudoscalar Higgs boson up to an order comparable to that of the scalar Higgs boson."
https://arxiv.org/abs/2403.07614,2024-03-12,Memory of a Random Walk: Astrometric deflections from gravitational wave memory accumulation over cosmological scales,"['Tore Boybeyi', 'Vuk Mandic', 'Alexandros Papageorgiou']","We study the impact of gravitational wave memory on the distribution of far away light sources in the sky. For the first time we compute the built up of small, but permanent tensor distortions of the metric over cosmological time-scales using realistic models of compact binary coalescences (CBCs) whose rate of occurrence is extrapolated at $z\sim {\cal O}(1)$. This allows for a consistent computation of the random-walk like evolution of gravitational wave memory which, in turn, is used to estimate the overall shape and magnitude of astrometric deflections of far away sources of light. We find that for pulsar or quasar proper motions, the near-Earth contribution to the astrometric deflections dominates the result and the deflection is analogous to a stochastic gravitational wave memory background that is generally subdominant to the primary stochastic gravitational wave background. We find that this contribution can be within the reach of future surveys such as Theia. Finally, we also study the deviation of the presently observed angular distribution of quasars from perfect isotropy, which arises from the slow build-up of gravitational wave memory over the entire history of the universe. In this case, we find that astrometric deflections depend on the entire light trajectory from the source to the Earth, yielding a quadruple pattern whose magnitude is unlikely to be within reach of the next generation of astrometric surveys due to shot noise and cosmic variance limitations."
https://arxiv.org/abs/2403.07613,2024-03-12,Imagine a dragon made of seaweed: How images enhance learning in Wikipedia,"['Anita Silva', 'Maria Tracy', 'Katharina Reinecke', 'Eytan Adar', 'Miriam Redi']","Though images are ubiquitous across Wikipedia, it is not obvious that the image choices optimally support learning. When well selected, images can enhance learning by dual coding, complementing, or supporting articles. When chosen poorly, images can mislead, distract, and confuse. We developed a large dataset containing 470 questions & answers to 94 Wikipedia articles with images on a wide range of topics. Through an online experiment (n=704), we determined whether the images displayed alongside the text of the article are effective in helping readers understand and learn. For certain tasks, such as learning to identify targets visually (e.g., ""which of these pictures is a gujia?""), article images significantly improve accuracy. Images did not significantly improve general knowledge questions (e.g., ""where are gujia from?""). Most interestingly, only some images helped with visual knowledge questions (e.g., ""what shape is a gujia?""). Using our findings, we reflect on the implications for editors and tools to support image selection."
https://arxiv.org/abs/2403.07612,2024-03-12,Minimal cellular automaton model with heterogeneous cell sizes predicts epithelial colony growth,"['Steffen Lange', 'Jannik Schmied', 'Paul Willam', 'Anja Voss-Böhme']","Regulation of cell proliferation is a crucial aspect of tissue development and homeostasis and plays a major role in morphogenesis, wound healing, and tumor invasion. A phenomenon of such regulation is contact inhibition, which describes the dramatic slowing of proliferation, cell migration and individual cell growth when multiple cells are in contact with each other. While many physiological, molecular and genetic factors are known, the mechanism of contact inhibition is still not fully understood. In particular, the relevance of cellular signaling due to interfacial contact for contact inhibition is still debated. Cellular automata (CA) have been employed in the past as numerically efficient mathematical models to study the dynamics of cell ensembles, but they are not suitable to explore the origins of contact inhibition as such agent-based models assume fixed cell sizes. We develop a minimal, data-driven model to simulate the dynamics of planar cell cultures by extending a probabilistic CA to incorporate size changes of individual cells during growth and cell division. We successfully apply this model to previous in-vitro experiments on contact inhibition in epithelial tissue: After a systematic calibration of the model parameters to measurements of single-cell dynamics, our CA model quantitatively reproduces independent measurements of emergent, culture-wide features, like colony size, cell density and collective cell migration. In particular, the dynamics of the CA model also exhibit the transition from a low-density confluent regime to a stationary postconfluent regime with a rapid decrease in cell size and motion. This implies that the volume exclusion principle, a mechanical constraint which is the only inter-cellular interaction incorporated in the model, paired with a size-dependent proliferation rate is sufficient to generate the observed contact inhibition."
https://arxiv.org/abs/2403.07611,2024-03-12,Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning,"['Vinay Chakravarthi Gogineni', 'Esmaeil S. Nadimi']","Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model. This capability enables data holders to adhere strictly to data protection regulations. However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage. In response, this paper introduces a novel class of machine unlearning algorithms. First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning. In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model. The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of data deletion on model efficacy. Through a detailed experimental evaluation, we showcase the effectiveness of proposed unlearning methods. Experimental results highlight that the partial amnesiac unlearning not only preserves model efficacy but also eliminates the necessity for brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover, employing layer-wise partial updates in label-flipping and optimization-based unlearning techniques demonstrates superiority in preserving model efficacy compared to their naive counterparts."
https://arxiv.org/abs/2403.07610,2024-03-12,FOXSI-2: Upgrades of the Focusing Optics X-ray Solar Imager for its Second Flight,"['Steven Christe', 'Lindsay Glesener', 'Camilo Buitrago-Casas', 'Shin-Nosuke Ishikawa', 'Brian Ramsey', 'Mikhail Gubarev', 'Kiranmayee Kilaru', 'Jeffery J. Kolodziejczak', 'Shin Watanabe', 'Tadayuki Takahashi', 'Hiroyasu Tajima', 'Paul Turin', 'Van Shourt', 'Natalie Foster', 'Sam Krucker']","The Focusing Optics X-ray Solar Imager (FOXSI) sounding rocket payload flew for the second time on 2014 December 11. To enable direct Hard X-Ray (HXR) imaging spectroscopy, FOXSI makes use of grazing-incidence replicated focusing optics combined with fine-pitch solid-state detectors. FOXSI's first flight provided the first HXR focused images of the Sun. For FOXSI's second flight several updates were made to the instrument including updating the optics and detectors as well as adding a new Solar Aspect and Alignment System (SAAS). This paper provides an overview of these updates as well as a discussion of their measured performance."
https://arxiv.org/abs/2403.07609,2024-03-12,Lattice dynamics of altermagnetic ruthenium oxide RuO$_{2}$,"['Surajit Basak', 'Andrzej Ptok']",Altermagnetic ruthenium oxide RuO$_{2}$ crystallizes with P4$_{2}$/mnm symmetry. Here we discuss the lattice dynamics of this structure. We show and discuss the phonon dispersion and density of states. The phonon dispersion curves contain several Dirac nodal lines and highly degenerate Dirac points. We present the characteristic frequencies and their irreducible representations at the $Γ$ point. Theoretically obtained frequencies of the Raman active modes nicely reproduce the ones reported experimentally.
https://arxiv.org/abs/2403.07608,2024-03-12,Couler: Unified Machine Learning Workflow Optimization in Cloud,"['Xiaoda Wang', 'Yuan Tang', 'Tengda Guo', 'Bo Sang', 'Jingji Wu', 'Jian Sha', 'Ke Zhang', 'Jiang Qian', 'Mingjie Tang']","Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines."
https://arxiv.org/abs/2403.07607,2024-03-12,On Graph Grammars and Games,"['Jayakrishna Vijayakumar', 'Lisa Mathew']","Graph grammars form an interesting area of research because of their versatility in modelling diverse situations with graphs as the structures which are to be manipulated. A new class of graph grammars, nc-eNCE Graph Grammars has been introduced recently with an aim of restricting the order of application of graph production rules, thereby generating different graph classes using the same set of rules. On the other hand 2D game design using an algorithmic approach known as procedural content generation has been of interest recently. In this paper we modify the structure of nc-eNCE graph grammars with the aim of generating directed graphs. We show that employing these graph grammars simplifies the design of 2D games. We have also developed an algorithm which makes use of these graph grammars for generating random game level layouts ensuring that the players will get a different gaming experience each time they play."
https://arxiv.org/abs/2403.07606,2024-03-12,Bilocal holography and locality in the bulk,"['Robert de Mello Koch', 'Garreth Kemp', 'Hendrik J. R. Van Zyl']",Bilocal holography provides a constructive approach to the vector model/higher spin gravity duality. It has two ingredients: a change of field variables and a change of space time coordinates. The change of field variables ensures that the loop expansion parameter becomes ${1\over N}$. The change of coordinates solves the Clebsch-Gordan problem of moving from the tensor product basis (in which the collective bilocal field is written) to the direct sum basis (appropriate for the description of the gravity fields). We argue that the change of space time coordinates can be deduced by requiring that operators constructed in the bilocal collective field theory are dual to local operators in the AdS bulk.
https://arxiv.org/abs/2403.07605,2024-03-12,Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation,"['Michael Ogezi', 'Ning Shi']","In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts."
https://arxiv.org/abs/2403.07604,2024-03-12,Approximating many-body quantum states with quantum circuits and measurements,"['Lorenzo Piroli', 'Georgios Styliaris', 'J. Ignacio Cirac']","We introduce protocols to prepare many-body quantum states with quantum circuits assisted by local operations and classical communication. First, we show that by lifting the requirement of exact preparation, one can substantially save resources. In particular, the so-called $W$ and, more generally, Dicke states require a circuit depth and number of ancillas that are independent of the system size. We also show how one can save resources in the preparation of eigenstates of well-known spin models, both free and interacting. As a biproduct of our work, we introduce an efficient scheme to implement certain non-local, non-Clifford unitary operators."
https://arxiv.org/abs/2403.07603,2024-03-12,ProPML: Probability Partial Multi-label Learning,"['Łukasz Struski', 'Adam Pardyl', 'Jacek Tabor', 'Bartosz Zieliński']","Partial Multi-label Learning (PML) is a type of weakly supervised learning where each training instance corresponds to a set of candidate labels, among which only some are true. In this paper, we introduce \our{}, a novel probabilistic approach to this problem that extends the binary cross entropy to the PML setup. In contrast to existing methods, it does not require suboptimal disambiguation and, as such, can be applied to any deep architecture. Furthermore, experiments conducted on artificial and real-world datasets indicate that \our{} outperforms existing approaches, especially for high noise in a candidate set."
https://arxiv.org/abs/2403.07602,2024-03-12,Capturing the Variability of the Nocturnal Boundary Layer through Localized Perturbation Modeling,"['Amandine Kaiser', 'Nikki Vercauteren', 'Sebastian Krumscheid']","A single-column model is used to investigate regime transitions within the stable atmospheric boundary layer, focusing on the role of small-scale fluctuations in wind and temperature dynamics and of turbulence intermittency as triggers for these transitions. Previous studies revealed abrupt near-surface temperature inversion transitions within a limited wind speed range. However, representing these transitions in numerical weather prediction (NWP) and climate models is a known difficulty. To shed light on boundary layer processes that explain these abrupt transitions, the Ekman layer height and its correlation with regime shifts are analyzed. A sensitivity study is performed with several types of perturbations of the wind and temperature tendencies, as well as with the inclusion of intermittent turbulent mixing through a stochastic stability function, to quantify the effect of small fluctuations of the dynamics on regime transitions. The combined results for all tested perturbation types indicate that small-scale phenomena can drive persistent regime transitions from very to weakly stable regimes, but for the opposite direction, no evidence of persistent regime transitions was found. The inclusion of intermittency prevents the model from getting trapped in the very stable regime, thus preventing the so-called ""runaway cooling"", an issue for commonly used short-tail stability functions. The findings suggest that using stochastic parameterizations of boundary layer processes, either through stochastically perturbed tendencies or parameters, is an effective approach to represent sharp transitions in the boundary layer regimes and is, therefore, a promising avenue to improve the representation of stable boundary layers in NWP and climate models."
https://arxiv.org/abs/2403.07601,2024-03-12,Unified Source-Free Domain Adaptation,"['Song Tang', 'Wenxin Su', 'Mao Ye', 'Jianwei Zhang', 'Xiatian Zhu']","In the pursuit of transferring a source model to a target domain without access to the source training data, Source-Free Domain Adaptation (SFDA) has been extensively explored across various scenarios, including closed-set, open-set, partial-set, and generalized settings. Existing methods, focusing on specific scenarios, not only address only a subset of challenges but also necessitate prior knowledge of the target domain, significantly limiting their practical utility and deployability. In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner. To tackle this unified SFDA problem, we propose a novel approach called Latent Causal Factors Discovery (LCFD). In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate LCFD from a causality perspective. The objective is to uncover the causal relationships between latent variables and model decisions, enhancing the reliability and robustness of the learned model against domain shifts. To integrate extensive world knowledge, we leverage a pre-trained vision-language model such as CLIP. This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees. Extensive experiments demonstrate that LCFD can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free out-of-distribution generalization.Our code and data are available at https://github.com/tntek/source-free-domain-adaptation."
https://arxiv.org/abs/2403.07600,2024-03-12,Asymptotic $ψ$-densities of subsets of natural numbers,"['Janne Heittokangas', 'Zinelaabidine Latreuch']","The sizes of subsets of the natural numbers are typically quantified in terms of asymptotic (linear) and logarithmic densities. These concepts have been generalized to weighted $w$-densities, where a specific weight function $w$ plays a key role. In this paper, a parallel theory of asymptotic $ψ$-densities is introduced, where the weight is expressed slightly differently in terms of differentiable functions $ψ$, which are either concave or convex and satisfy certain asymptotic properties. Alternative new proofs for known results on analytic and Abel densities are also given."
https://arxiv.org/abs/2403.07599,2024-03-12,Gas pressure manipulation of exciton states in monolayer WS2,"['Shuangping Han', 'Pengyu Zan', 'Yu Yan', 'Yaoxing Bian', 'Chengbing Qin', 'Liantuan Xiao']","Over the past few decades, thin film optoelectronic devices based on transition metal dichalcogenides (TMDs) have made significant progress. However, the sensitivity of the exciton states to environmental change presents challenges for device applications. This work reports on the evolution of photo-induced exciton states in monolayer WS2 in a chamber with low gas pressure. It elucidates the physical mechanism of the transition between neutral and charged excitons. At 222 mTorr, the transition rate between excitons includes two components, 0.09 s-1 and 1.68 s-1, respectively. Based on this phenomenon, we have developed a pressure-tuning method for exciton manipulation, allowing a tuning range of approximately 40% in exciton weight. We also demonstrate that the intensity of neutral exciton emission from monolayer WS2 follows a power-law distribution concerning gas pressure, indicating a highly sensitive pressure dependence. This work presents a non-destructive and highly sensitive method for exciton conversion through in-situ manipulation. It highlights the potential development of monolayer WS2 in pressure sensing and explains the impact of environmental factors on product quality in photovoltaic devices."
https://arxiv.org/abs/2403.07598,2024-03-12,Mondrian: On-Device High-Performance Video Analytics with Compressive Packed Inference,"['Changmin Jeon', 'Seonjun Kim', 'Juheon Yi', 'Youngki Lee']","In this paper, we present Mondrian, an edge system that enables high-performance object detection on high-resolution video streams. Many lightweight models and system optimization techniques have been proposed for resource-constrained devices, but they do not fully utilize the potential of the accelerators over dynamic, high-resolution videos. To enable such capability, we devise a novel Compressive Packed Inference to minimize per-pixel processing costs by selectively determining the necessary pixels to process and combining them to maximize processing parallelism. In particular, our system quickly extracts ROIs and dynamically shrinks them, reflecting the effect of the fast-changing characteristics of objects and scenes. It then intelligently combines such scaled ROIs into large canvases to maximize the utilization of inference accelerators such as GPU. Evaluation across various datasets, models, and devices shows Mondrian outperforms state-of-the-art baselines (e.g., input rescaling, ROI extractions, ROI extractions+batching) by 15.0-19.7% higher accuracy, leading to $\times$6.65 higher throughput than frame-wise inference for processing various 1080p video streams. We will release the code after the paper review."
https://arxiv.org/abs/2403.07597,2024-03-12,Generalized paths and cycles in semicomplete multipartite digraphs,"['Jørgen Bang-Jensen', 'Yun Wang', 'Anders Yeo']","It is well-known and easy to show that even the following version of the directed travelling salesman problem is NP-complete: Given a strongly connected complete digraph $D=(V,A)$, a cost function $w: A\rightarrow \{0,1\}$ and a natural number $K$; decide whether $D$ has a directed Hamiltonian cycle of cost at most $K$. We study the following variant of this problem for $\{0,1\}$-weighted semicomplete digraphs where the set of arcs which have cost 1 form a collection of vertex-disjoint complete digraphs. A digraph is \textbf{semicomplete multipartite} if it can be obtained from a semicomplete digraph $D$ by choosing a collection of vertex-disjoint subsets $X_1,\ldots{},X_c$ of $V(D)$ and then deleting all arcs both of whose end-vertices lie inside some $X_i$. Let $D$ be a semicomplete digraph with a cost function $w$ as above, where $w(a)=1$ precisely when $a$ is an arc inside one of the subsets $X_1,\ldots{},X_c$ and let $D^*$ be the corresponding \smd{} that we obtain by deleting all arcs inside the $X_i$'s. Then every cycle $C$ of $D$ corresponds to a {\bf generalized cycle} $C^g$ of $D^*$ which is either the cycle $C$ itself if $w(C)=0$ or a collection of two or more paths that we obtain by deleting all arcs of cost 1 on $C$. Similarly we can define a {\bf generalized path} $P^g$ in a semicomplete multipartite digraph. The purpose of this paper is to study structural and algorithmic properties of generalized paths and cycles in semicomplete multipartite digraphs. This allows us to identify classes of directed $\{0,1\}$-weighted TSP instances that can be solved in polynomial time as well as others for which we can get very close to the optimum in polynomial time. Along with these results we also show that two natural questions about properties of cycles meeting all partite sets in semicomplete multipartite digraphs are NP-complete."
https://arxiv.org/abs/2403.07596,2024-03-12,An Architecture for Noise-Aware Distributed Quantum Computation,"['Sanidhya Gupta', 'Ankur Raina']","Suppose Alice has access to $n$ remote quantum computing nodes capable of universal quantum computation, connected to her by a quantum channel. She wants to use these remote nodes jointly to make computations and store her quantum states such that the actual computation is hidden from these remote nodes. We describe a protocol to help Alice carry out her computation using these remote nodes and store her computation results. We also make sure these nodes can handle noise themselves in case of any error on these nodes. More precisely, we develop an architecture for distributed quantum computation and storage, addressing key challenges in quantum processing across remote nodes. Additionally, we enhance the robustness of each node against noise by developing quantum error-correcting methods suitable for each node."
https://arxiv.org/abs/2403.07595,2024-03-12,Scale-free identity: The emergence of social network science,['Haiko Lietz'],"Social Network Analysis is a way of studying agents embedded in contexts. In about 1998, physicists discovered social networks as representations of complex systems. Small-world and scale-free networks are the paradigmatic models of this Network Science. Relying on various models and mechanisms of socio-cultural processes, an identity model is developed and calibrated in a case study of Social Network Science. This research domain results from the union of Social Network Analysis and Network Science. A unique dataset of 25,760 scholarly articles from one century of research (1916-2012) is created. Clustering this set of publications, five subdomains are detected and analyzed in terms of authorship, citation, and word usage structures and dynamics. The scaling hypothesis of percolation theory is formulated for socio-cultural systems, namely that power-law size distributions like Lotka's, Bradford's, and Zipf's Law mean that the described identity resides at the phase transition between the stability and change of meaning. In this case, it can be diagnosed using bivariate scaling laws and Abbott's heuristic of fractal distinctions. Identities are not dichotomies but dualities of social network and cultural domain, micro and macro phenomena, as well as stability and change. Story sets that give direction to research fluctuate less, are less distinctive, and more inert than the individuals doing the research. Identities are scale-free. Six senses are diagnostic of different aspects of identity, and when they come together as process, a complex socio-cultural system comes into existence. A mutual benefit that results from mating Relational Sociology and Network Science is identified. The latter can learn from the former that social systems are dualities of transactions and meaning. For the social sciences, the importance of Paretian thinking (scale invariance) is pointed out."
https://arxiv.org/abs/2403.07594,2024-03-12,Stability of Stationary Solutions to the Nonisentropic Euler-Poisson System in a Perturbed Half Space,"['Mingjie Li', 'Masahiro Suzuki']","The main concern of this paper is to mathematically investigate the formation of a plasma sheath near the surface of nonplanar walls. We study the existence and asymptotic stability of stationary solutions for the nonisentropic Euler-Poisson equations in a domain of which boundary is drawn by a graph, by employing a space weighted energy method. Moreover, the convergence rate of the solution toward the stationary solution is obtained, provided that the initial perturbation belongs to the weighted Sobolev space. Because the domain is the perturbed half space, we first show the time-global solvability of the nonisentropic Euler-Poisson equations, then construct stationary solutions by using the time-global solutions."
https://arxiv.org/abs/2403.07593,2024-03-13,MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D Sparse Convolutions,"['J. J. Cabrera', 'A. Santo', 'A. Gil', 'C. Viegas', 'L. Payá']","This paper presents MinkUNeXt, an effective and efficient architecture for place-recognition from point clouds entirely based on the new 3D MinkNeXt Block, a residual block composed of 3D sparse convolutions that follows the philosophy established by recent Transformers but purely using simple 3D convolutions. Feature extraction is performed at different scales by a U-Net encoder-decoder network and the feature aggregation of those features into a single descriptor is carried out by a Generalized Mean Pooling (GeM). The proposed architecture demonstrates that it is possible to surpass the current state-of-the-art by only relying on conventional 3D sparse convolutions without making use of more complex and sophisticated proposals such as Transformers, Attention-Layers or Deformable Convolutions. A thorough assessment of the proposal has been carried out using the Oxford RobotCar and the In-house datasets. As a result, MinkUNeXt proves to outperform other methods in the state-of-the-art."
https://arxiv.org/abs/2403.07592,2024-03-12,Accurate Spatial Gene Expression Prediction by integrating Multi-resolution features,"['Youngmin Chung', 'Ji Hun Ha', 'Kyeong Chan Im', 'Joo Sang Lee']","Recent advancements in Spatial Transcriptomics (ST) technology have facilitated detailed gene expression analysis within tissue contexts. However, the high costs and methodological limitations of ST necessitate a more robust predictive model. In response, this paper introduces TRIPLEX, a novel deep learning framework designed to predict spatial gene expression from Whole Slide Images (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing cellular morphology at individual spots, the local context around these spots, and the global tissue organization. By integrating these features through an effective fusion strategy, TRIPLEX achieves accurate gene expression prediction. Our comprehensive benchmark study, conducted on three public ST datasets and supplemented with Visium data from 10X Genomics, demonstrates that TRIPLEX outperforms current state-of-the-art models in Mean Squared Error (MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC). The model's predictions align closely with ground truth gene expression profiles and tumor annotations, underscoring TRIPLEX's potential in advancing cancer diagnosis and treatment."
https://arxiv.org/abs/2403.07591,2024-03-12,Robustifying and Boosting Training-Free Neural Architecture Search,"['Zhenfeng He', 'Yao Shu', 'Zhongxiang Dai', 'Bryan Kian Hsiang Low']","Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics. Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric. Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance. To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimization to develop a robust and consistently better-performing metric on diverse tasks, and (b) applies greedy search, i.e., the exploitation, on the newly developed metric to bridge the aforementioned gap and consequently to boost the search performance of standard training-free NAS further. Remarkably, the expected performance of our RoBoT can be theoretically guaranteed, which improves over the existing training-free NAS under mild conditions with additional interesting insights. Our extensive experiments on various NAS benchmark tasks yield substantial empirical evidence to support our theoretical results."
https://arxiv.org/abs/2403.07590,2024-03-12,Topological Quantum Mechanics on Orbifolds and Orbifold Index,"['Si Li', 'Peng Yang']","In this paper, we study topological quantum mechanical models on symplectic orbifolds. The correlation map gives an explicit orbifold version of quantum HKR map. The exact semi-classical approximation in this model leads to a geometric and quantum field theoretic interpretation of the orbifold algebraic index."
https://arxiv.org/abs/2403.07589,2024-03-12,PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution,"['Honghao Chen', 'Xiangxiang Chu', 'Yongjian Ren', 'Xin Zhao', 'Kaiqi Huang']","Recently, some large kernel convnets strike back with appealing performance and efficiency. However, given the square complexity of convolution, scaling up kernels can bring about an enormous amount of parameters and the proliferated parameters can induce severe optimization problem. Due to these issues, current CNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e., 51x5 + 5x51) and start to saturate as the kernel size continues growing. In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains. Inspired by human vision, we propose a human-like peripheral convolution that efficiently reduces over 90% parameter count of dense grid convolution through parameter sharing, and manage to scale up kernel size to extremely large. Our peripheral convolution behaves highly similar to human, reducing the complexity of convolution from O(K^2) to O(logK) without backfiring performance. Built on this, we propose Parameter-efficient Large Kernel Network (PeLK). Our PeLK outperforms modern vision Transformers and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet classification, semantic segmentation on ADE20K and object detection on MS COCO. For the first time, we successfully scale up the kernel size of CNNs to an unprecedented 101x101 and demonstrate consistent improvements."
https://arxiv.org/abs/2403.07588,2024-03-12,Visual Privacy Auditing with Diffusion Models,"['Kristian Schwethelm', 'Johannes Kaiser', 'Moritz Knolle', 'Daniel Rueckert', 'Georgios Kaissis', 'Alexander Ziller']","Image reconstruction attacks on machine learning models pose a significant risk to privacy by potentially leaking sensitive information. Although defending against such attacks using differential privacy (DP) has proven effective, determining appropriate DP parameters remains challenging. Current formal guarantees on data reconstruction success suffer from overly theoretical assumptions regarding adversary knowledge about the target data, particularly in the image domain. In this work, we empirically investigate this discrepancy and find that the practicality of these assumptions strongly depends on the domain shift between the data prior and the reconstruction target. We propose a reconstruction attack based on diffusion models (DMs) that assumes adversary access to real-world image priors and assess its implications on privacy leakage under DP-SGD. We show that (1) real-world data priors significantly influence reconstruction success, (2) current reconstruction bounds do not model the risk posed by data priors well, and (3) DMs can serve as effective auditing tools for visualizing privacy leakage."
https://arxiv.org/abs/2403.07587,2024-03-12,Perennial Semantic Data Terms of Use for Decentralized Web,"['Rui Zhao', 'Jun Zhao']","In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives policies for output data. This constitutes a ``perennial'' DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles. Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine. It ensures seamless integration with other semantic tools for enhanced interoperability. We have successfully integrated this language into the Solid framework, and conducted performance benchmark. We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability."
https://arxiv.org/abs/2403.07586,2024-03-12,Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments,"['Saksham Checker', 'Nikhil Churamani', 'Hatice Gunes']","As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial. For their widespread open-world application, it is important to explore Federated Learning (FL) settings where individual robots can learn about their unique environments while also learning from each others' experiences. In this paper, we present a novel FL benchmark that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others. Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel Federated Continual Learning (FCL) benchmark that adapts FL-based methods to use state-of-the-art Continual Learning (CL) methods to continually learn socially appropriate agent behaviours under different contextual settings. Federated Averaging (FedAvg) of weights emerges as a robust FL strategy while rehearsal-based FCL enables incrementally learning the social appropriateness of robot actions, across contextual splits."
https://arxiv.org/abs/2403.07585,2024-03-12,"Communication Optimization for Distributed Training: Architecture, Advances, and Opportunities","['Yunze Wei', 'Tianshuo Hu', 'Cong Liang', 'Yong Cui']","The past few years have witnessed the flourishing of large-scale deep neural network models with ever-growing parameter numbers. Training such large-scale models typically requires massive memory and computing resources that exceed those of a single GPU, necessitating distributed training. As GPU performance has rapidly evolved in recent years, computation time has shrunk, thereby increasing the proportion of communication in the overall training time. Therefore, optimizing communication for distributed training has become an urgent issue. In this article, we briefly introduce the general architecture of distributed deep neural network training and analyze relationships among Parallelization Strategy, Collective Communication Library, and Network from the perspective of communication optimization, which forms a three-layer paradigm. We then review current representative research advances with this three-layer paradigm. We find that layers in the current three-layer paradigm are relatively independent, but there is a rich design space for cross-layer collaborative optimization in distributed training scenarios. Therefore, we further advocate a communication-efficient five-layer paradigm underlining opportunities for collaboration designs and look forward to the perspectives of ""Vertical"", ""Horizontal"", ""Intra-Inter"" and ""Host-Net"" collaboration designs. We hope this article can shed some light on future research on communication optimization for distributed training."
https://arxiv.org/abs/2403.07584,2024-03-12,Molecularity: a fast and efficient criterion for probing superconductivity,"['Matías E. di Mauro', 'Benoît Braïda', 'Ion Errea', 'Trinidad Novoa', 'Julia Contreras-García']","We present an efficient criterion for probing the critical temperature of hydrogen based superconductors. We start by expanding the applicability of 3D descriptors of electron localization to superconducting states within the framework of superconducting DFT. We first apply this descriptor to a model system, the hydrogen chain, which allows to prove two main concepts: i) that the electron localization changes very little when the transition from the normal to the superconducting state takes place, i.e. that it can be described at the DFT level from the normal state; and ii) that the formation of molecules can be characterized within this theoretical framework, enabling to filter out systems with marked molecular character and hence with low potential to be good superconductors. These two ideas, are then exploited in real binary and ternary systems, showing i) that the bonding type can be characterized automatically; and ii) that this provides a new index which enables to feed machine learning algorithms for a better prediction of critical temperatures. Overall, this sets a grounded theoretical scenario for an automatic and efficient high-throughput of potential hydrogen based superconductors."
https://arxiv.org/abs/2403.07583,2024-03-12,"Semi-brittle flow of rocks: Cracks, dislocations and strain hardening",['Nicolas Brantut'],"Strain hardening is a key feature observed in many rocks deformed in the so-called ``semi-brittle'' regime, where both crystal plastic and brittle deformation mechanisms operate. Dislocation storage has long been recognised as a major process leading to strain hardening. Here, we suggest that tensile microcracks may be viewed as dislocation sinks, by offering internal free surfaces where dislocations can escape individual crystals within an aggregate. Strain hardening is modelled with a conventional approach, combining Taylor's equation relating stress to dislocation density, and a dislocation density evolution law based on dislocation mean-free path and dynamic recovery. The initiation of microcracks is modelled as a function dislocation density, assuming dislocation pile-ups at grain boundaries. Microcrack growth is modelled using linear elastic fracture mechanics. The model captures important qualitative features observed in calcite marble deformation experiments: pressure-dependency of strength in the ductile regime, and a reduction in hardening linked to an increase in crack growth with decreasing confining pressure. Grain-size dependency of strength and hardening is also captured but requires significant toughening (or limitation to crack growth) at small grain sizes. The model can be improved significantly once detailed, systematic microstructural observations become available."
https://arxiv.org/abs/2403.07582,2024-03-12,A Continuum Theory of Elastic Semiconductors with Consideration of Mobile Charge Inertia,['Jiashi Yang'],A set of nonlinear equations for the macroscopic theory of elastic semiconductors is derived which generalizes the seminal work of Tiersten by including the inertia of the mobile charge carriers. The equations obtained can describe carrier plasma waves and their interactions with elastic waves. Another generalization in this paper is that the internal energy densities of the mobile charges are allowed to depend on temperature in addition to charge densities. The equations are in SI units.
https://arxiv.org/abs/2403.07581,2024-03-12,LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model,"['Linmei Hu', 'Hongyu He', 'Duokang Wang', 'Ziwang Zhao', 'Yingxia Shao', 'Liqiang Nie']","Personality detection aims to detect one's personality traits underlying in social media posts. One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires. Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance. In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them. In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even when the LLM fails in this task. Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, and linguistic, which are critical for personality detection. By using contrastive learning to pull them together in the embedding space, the post encoder can better capture the psycho-linguistic information within the post representations, thus improving personality detection. Furthermore, we utilize the LLM to enrich the information of personality labels for enhancing the detection performance. Experimental results on the benchmark datasets demonstrate that our model outperforms the state-of-the-art methods on personality detection."
https://arxiv.org/abs/2403.07580,2024-03-12,Electronic and dynamical properties of cobalt monogermanide CoGe phases under pressure,"['Surajit Basak', 'Aksel Kobiałka', 'Małgorzata Sternik', 'Jan Łażewski', 'Paweł T. Jochym', 'Andrzej M. Oleś', 'Przemysław Piekarz', 'Andrzej Ptok']","We present the pressure dependence of the electronic and dynamical properties of six different CoGe phases: orthorhombic Cmmm, hexagonal P6/mmm and P$\bar{6}$2m, monoclinic C2/m, cubic P2$_{1}$3, and orthorhombic Pnma. Using first-principles DFT calculations and the direct force-constants method, we study the dynamical stability of individual phases under external pressure. We show that the orthorombic Cmmm and hexagonal P6/mmm structures are unstable over a broad pressure range and most pronounced imaginary phonon soft mode in both cases leads to a stable hexagonal P$\bar{6}$2m structure of the lowest ground-state energy of all studied phases at ambient and low (below $\sim 3$ GPa) external pressure. Under these conditions, the cubic P2$_{1}$3 phase has the highest energy, however, together with monoclinic C2/m and orthorombic Pnma it is dynamically stable and all these three structures can potentially coexist as meta-stable phases. Above $\sim 3$ GPa, the cubic P2$_{1}$3 phase becomes the most energetically favorable. Fitting the Birch--Murnaghan equation of state we derive bulk modulus for all mentioned phases, which indicate relatively high resistance of CoGe to compression. Such conclusions are confirmed by band structure calculations. Additionally, we show that electronic bands of the hexagonal P$\bar{6}$2m phase reveal characteristic features of the kagome-like structure, while in the cubic P2$_{1}$3 phase spectrum, one can locate spin-1 and double Weyl fermions. In both cases, the external pressure induces the Lifshitz transition, related to the modification of the Fermi surface topology."
https://arxiv.org/abs/2403.07579,2024-03-12,On HRTF Notch Frequency Prediction Using Anthropometric Features and Neural Networks,"['Lior Arbel', 'Ishwarya Ananthabhotla', 'Zamir Ben-Hur', 'David Lou Alon', 'Boaz Rafaely']","High fidelity spatial audio often performs better when produced using a personalized head-related transfer function (HRTF). However, the direct acquisition of HRTFs is cumbersome and requires specialized equipment. Thus, many personalization methods estimate HRTF features from easily obtained anthropometric features of the pinna, head, and torso. The first HRTF notch frequency (N1) is known to be a dominant feature in elevation localization, and thus a useful feature for HRTF personalization. This paper describes the prediction of N1 frequency from pinna anthropometry using a neural model. Prediction is performed separately on three databases, both simulated and measured, and then by domain mixing in-between the databases. The model successfully predicts N1 frequency for individual databases and by domain mixing between some databases. Prediction errors are better or comparable to those previously reported, showing significant improvement when acquired over a large database and with a larger output range."
https://arxiv.org/abs/2403.07578,2024-03-12,AACP: Aesthetics assessment of children's paintings based on self-supervised learning,"['Shiqi Jiang', 'Ning Li', 'Chen Shi', 'Liping Guo', 'Changbo Wang', 'Chenhui Li']","The Aesthetics Assessment of Children's Paintings (AACP) is an important branch of the image aesthetics assessment (IAA), playing a significant role in children's education. This task presents unique challenges, such as limited available data and the requirement for evaluation metrics from multiple perspectives. However, previous approaches have relied on training large datasets and subsequently providing an aesthetics score to the image, which is not applicable to AACP. To solve this problem, we construct an aesthetics assessment dataset of children's paintings and a model based on self-supervised learning. 1) We build a novel dataset composed of two parts: the first part contains more than 20k unlabeled images of children's paintings; the second part contains 1.2k images of children's paintings, and each image contains eight attributes labeled by multiple design experts. 2) We design a pipeline that includes a feature extraction module, perception modules and a disentangled evaluation module. 3) We conduct both qualitative and quantitative experiments to compare our model's performance with five other methods using the AACP dataset. Our experiments reveal that our method can accurately capture aesthetic features and achieve state-of-the-art performance."
https://arxiv.org/abs/2403.07577,2024-03-12,Functional renormalization group for p=2 like glassy matrices in the planar approximation: I. Vertex expansion at equilibrium,"['Vincent Lahoche', 'Dine Ousmane Samary']","In this paper, we study the equilibrium states of a $N\times N$ stochastic complex random matrix $M$, whose entries evolve in time accordingly with a Langevin equation including both Gaussian white noises and a linear disorder, materialized by the Wigner random matrices. In large $N$-limit, the disorders behave as effective kinetics, and we examine a coarse-graining over the Wigner spectrum accordingly with two different schemes that we call respectively active and passive. We then investigate explicit solutions of the nonperturbative renormalization group using vertex and derivative expansion, a simple way to deal with the nonlocal nature of the effective field theory at large $N$. Our main statement is the existence of well-behaved fixed point solutions and at least some evidence about a discontinuous (first order) phase transition between a condensed and a dilute phase. We finally interpret the resulting phase space regarding the out-of-equilibrium process related to the dynamical phase transitions."
https://arxiv.org/abs/2403.07576,2024-03-12,FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine Tuning in High-resolution Medical Image Classification,"['Yijin Huang', 'Pujin Cheng', 'Roger Tam', 'Xiaoying Tang']","Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to transfer pre-trained models to downstream tasks, avoiding the high cost of updating entire large-scale pre-trained models (LPMs). In this work, we present Fine-grained Prompt Tuning (FPT), a novel PEFT method for medical image classification. FPT significantly reduces memory consumption compared to other PEFT methods, especially in high-resolution contexts. To achieve this, we first freeze the weights of the LPM and construct a learnable lightweight side network. The frozen LPM takes high-resolution images as input to extract fine-grained features, while the side network is fed low-resolution images to reduce memory usage. To allow the side network to access pre-trained knowledge, we introduce fine-grained prompts that summarize information from the LPM through a fusion module. Important tokens selection and preloading techniques are employed to further reduce training cost and memory requirements. We evaluate FPT on four medical datasets with varying sizes, modalities, and complexities. Experimental results demonstrate that FPT achieves comparable performance to fine-tuning the entire LPM while using only 1.8% of the learnable parameters and 13% of the memory costs of an encoder ViT-B model with a 512 x 512 input resolution."
https://arxiv.org/abs/2403.07575,2024-03-12,Emergent (2+1)D topological orders from iterative (1+1)D gauging,['Jose Garre Rubio'],"Gauging involves introducing new degrees of freedom, known as gauge fields, to localize an existing global symmetry. It is known that, following this process, the gauge fields exhibit a dual global symmetry. Subsequently, one can gauge this emergent global symmetry by creating new gauge fields that once again exhibit a global symmetry. We investigate this iterative process, wherein new degrees of freedom are created and entangled with the previous ones through local symmetries. We focus on gauging spin chains with Abelian group symmetries and arranging the new spins on a 2D lattice. The local symmetries of the emergent 2D state, which are modified by the concatenation of the following gauging maps surprisingly correspond to the stabilizer terms of the $XZZX$-code generalized to any Abelian group. We encode our construction in the family of tensor network states that we dub ``projected entangled pair emergent states'' (PEPES). By utilizing this representation and by considering the local symmetries as stabilizer Hamiltonian terms, we establish a connection between the condensable anyons at the boundary and the quantum phase of the initial symmetric state before the gauging process."
https://arxiv.org/abs/2403.07574,2024-03-12,Direct observation of strong t-e orbital hybridization and the effects of f orbitals,"['Wang Mian', 'Qian Zhang', 'Shuai Jing', 'Xiang-Guo Li', 'Yanglong Hou']","Recent research has revealed that the Cr family perovskite ReCrO$_3$ exhibits intriguing magnetic coupling interactions within Cr pairs, which may not follow the Goodenough-Kanamori (GK) rules due to the t-e hybridization between Cr$^\mathrm{III}$ ions. We investigate the complex magnetism involving both t-e hybridization and Re-$f$ orbitals in the molecular analogue of perovskite [$\mathrm{Ce_2^{III}Ce^{IV}Cr_8^{III}O_8(O_2CPh)_{18}(HO_2CPh)}$] ($\mathrm{Ce_3Cr_8}$) using first-principles method. Our results have shown that distinct from the bulk ReCrO$_3$, the superexchange via Cr-$d$ and O-$p$ orbitals can exhibit a unexpected dominate ferromagnetic (FM) Cr-O-Cr superexchange interaction in $\mathrm{Ce_3Cr_8}$ due to the strong t-e hybridization originated from the distorted molecular structure. The great sensitivity of the t-e hybridization with respect to the molecular structure, e.g., the angle of Cr-O-Cr, can lead to a ground state transition from ferromagnetic to antiferromagnetic state with the changes in the angle of Cr-O-Cr. The Ce-$f$ orbitals near the Fermi level can reduce this sensitivity through interacting with the Cr-$d$ orbitals via the virtual charge transfer process. Our results are strongly supported by a modified superexchange model based on the t-e hybridization theory. These findings complete the theory of superexchange magnetism involving the t-e hybridization and $f$ orbitals, and in the meanwhile introduce a new avenue for fine-tuning the magnetic characteristics via Tm-d/Re-f interactions at nanoscale."
https://arxiv.org/abs/2403.07573,2024-03-12,Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC),"['Masoud Shokrnezhad', 'Hao Yu', 'Tarik Taleb', 'Richard Li', 'Kyunghan Lee', 'Jaeseung Song', 'Cedric Westphal']","In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent requirements. ACNC encompasses two primary functionalities: state recognition and context detection. Given the intricate nature of the user-service-computing-network space, the paper employs dimension reduction to generate live, holistic, abstract system states in a hierarchical structure. To address the challenges posed by dynamic changes, Continual Learning (CL) is employed, classifying the system state into contexts controlled by dedicated ML agents, enabling them to operate efficiently. These two functionalities are intricately linked within a closed loop overseen by the End-to-End (E2E) orchestrator to allocate resources. The paper introduces the components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow, details a numerical analysis for efficiency assessment, and concludes with discussions on relevant challenges and potential avenues for future research."
https://arxiv.org/abs/2403.07572,2024-03-12,On Weakly Contracting Dynamics for Convex Optimization,"['Veronica Centorrino', 'Alexander Davydov', 'Anand Gokhale', 'Giovanni Russo', 'Francesco Bullo']","We investigate the convergence characteristics of dynamics that are \emph{globally weakly} and \emph{locally strongly contracting}. Such dynamics naturally arise in the context of convex optimization problems with a unique minimizer. We show that convergence to the equilibrium is \emph{linear-exponential}, in the sense that the distance between each solution and the equilibrium is upper bounded by a function that first decreases linearly and then exponentially. As we show, the linear-exponential dependency arises naturally in certain dynamics with saturations. Additionally, we provide a sufficient condition for local input-to-state stability. Finally, we illustrate our results on, and propose a conjecture for, continuous-time dynamical systems solving linear programs."
https://arxiv.org/abs/2403.07571,2024-03-12,Proactive Recommendation with Iterative Preference Guidance,"['Shuxian Bi', 'Wenjie Wang', 'Hang Pan', 'Fuli Feng', 'Xiangnan He']","Recommender systems mainly tailor personalized recommendations according to user interests learned from user feedback. However, such recommender systems passively cater to user interests and even reinforce existing interests in the feedback loop, leading to problems like filter bubbles and opinion polarization. To counteract this, proactive recommendation actively steers users towards developing new interests in a target item or topic by strategically modulating recommendation sequences. Existing work for proactive recommendation faces significant hurdles: 1) overlooking the user feedback in the guidance process; 2) lacking explicit modeling of the guiding objective; and 3) insufficient flexibility for integration into existing industrial recommender systems. To address these issues, we introduce an Iterative Preference Guidance (IPG) framework. IPG performs proactive recommendation in a flexible post-processing manner by ranking items according to their IPG scores that consider both interaction probability and guiding value. These scores are explicitly estimated with iteratively updated user representation that considers the most recent user interactions. Extensive experiments validate that IPG can effectively guide user interests toward target interests with a reasonable trade-off in recommender accuracy. The code is available at https://github.com/GabyUSTC/IPG-Rec."
https://arxiv.org/abs/2403.07570,2024-03-12,An Active Contour Model Driven By the Hybrid Signed Pressure Function,['Jing Zhao'],"Due to the influence of imaging equipment and complex imaging environments, most images in daily life have features of intensity inhomogeneity and noise. Therefore, many scholars have designed many image segmentation algorithms to address these issues. Among them, the active contour model is one of the most effective image segmentation algorithms.This paper proposes an active contour model driven by the hybrid signed pressure function that combines global and local information construction. Firstly, a new global region-based signed pressure function is introduced by combining the average intensity of the inner and outer regions of the curve with the median intensity of the inner region of the evolution curve. Then, the paper uses the energy differences between the inner and outer regions of the curve in the local region to design the signed pressure function of the local term. Combine the two SPF function to obtain a new signed pressure function and get the evolution equation of the new model. Finally, experiments and numerical analysis show that the model has excellent segmentation performance for both intensity inhomogeneous images and noisy images."
https://arxiv.org/abs/2403.07569,2024-03-12,Exploring Challenges in Deep Learning of Single-Station Ground Motion Records,"['Ümit Mert Çağlar', 'Baris Yilmaz', 'Melek Türkmen', 'Erdem Akagündüz', 'Salih Tileylioglu']","Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. However, the extent to which these models effectively learn from these complex time-series signals has not been thoroughly analyzed. In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. We perform a hyperparameter search on two deep learning models to assess their effectiveness in deep learning from ground motion records while also examining the impact of auxiliary information on model performance. Experimental results reveal a strong reliance on the highly correlated P and S phase arrival information. Our observations highlight a potential gap in the field, indicating an absence of robust methodologies for deep learning of single-station ground motion recordings independent of any auxiliary information."
https://arxiv.org/abs/2403.07568,2024-03-12,Theoretical demonstration of mode transmission in ZGP-based micrometer waveguide platforms,"['Siyi Lu', 'Bo Hu', 'Xuemei Yang', 'Yang Li', 'Han Wu', 'Houkun Liang']","Birefringence phase-matching based \c{hi}(2) ZnGeP2 (ZGP) waveguide platform has been recently reported for excellent mid-infrared laser generation. Here, a detailed theoretical characterization of mode transmission taking waveguide anisotropy and substrate material absorption into account in a micrometer ZGP waveguide platform (ZGP-on-SiO2) is conducted. Benefited from high-index contrast between ZGP and substrate (SiO2/Air), Transverse electric and magnetic (TM and TE) mode transmission loss at interested wavelengths range of 2 - 12 μm is calculated to be less than 4 dB/cm and 1.5 dB/cm, respectively, in the designed ZGP waveguide. Notably, non-obvious oscillation of mode transmission loss versus phase-matching angles is observed, which is different from that in the previously reported weakly guided anisotropic waveguide. A vital phenomenon named mode crossing at some wavelengths in TM polarization is also exhibited in our waveguide platforms, which jeopardizes waveguide performances and could be avoided by changing the phase-matching angle in practice. This work provides a significant indication of ZGP waveguide design optimization in future and also exhibits extendibility to other birefringent crystal waveguide platforms."
https://arxiv.org/abs/2403.07567,2024-03-12,Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation,"['Francois Meyer', 'Jan Buys']","Most data-to-text datasets are for English, so the difficulties of modelling data-to-text for low-resource languages are largely unexplored. In this paper we tackle data-to-text for isiXhosa, which is low-resource and agglutinative. We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of WebNLG, which presents a new linguistic context that shifts modelling demands to subword-driven techniques. We also develop an evaluation framework for T2X that measures how accurately generated text describes the data. This enables future users of T2X to go beyond surface-level metrics in evaluation. On the modelling side we explore two classes of methods - dedicated data-to-text models trained from scratch and pretrained language models (PLMs). We propose a new dedicated architecture aimed at agglutinative data-to-text, the Subword Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy entities, and outperforms existing dedicated models for 2 agglutinative languages (isiXhosa and Finnish). We investigate pretrained solutions for T2X, which reveals that standard PLMs come up short. Fine-tuning machine translation models emerges as the best method overall. These findings underscore the distinct challenge presented by T2X: neither well-established data-to-text architectures nor customary pretrained methodologies prove optimal. We conclude with a qualitative analysis of generation errors and an ablation study."
https://arxiv.org/abs/2403.07566,2024-03-12,An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning,"['Weiwei Gu', 'Senquan Wang']","Blood Glucose (BG) control involves keeping an individual's BG within a healthy range through extracorporeal insulin injections is an important task for people with type 1 diabetes. However,traditional patient self-management is cumbersome and risky. Recent research has been devoted to exploring individualized and automated BG control approaches, among which Deep Reinforcement Learning (DRL) shows potential as an emerging approach. In this paper, we use an exponential decay model of drug concentration to convert the formalization of the BG control problem, which takes into account the delay and prolongedness of drug effects, from a PAE-POMDP (Prolonged Action Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a novel multi-step DRL-based algorithm to solve the problem. The Prioritized Experience Replay (PER) sampling method is also used in it. Compared to single-step bootstrapped updates, multi-step learning is more efficient and reduces the influence from biasing targets. Our proposed method converges faster and achieves higher cumulative rewards compared to the benchmark in the same training environment, and improves the time-in-range (TIR), the percentage of time the patient's BG is within the target range, in the evaluation phase. Our work validates the effectiveness of multi-step reinforcement learning in BG control, which may help to explore the optimal glycemic control measure and improve the survival of diabetic patients."
https://arxiv.org/abs/2403.07565,2024-03-12,Logarithmic critical slowing down in complex systems: from statics to dynamics,"['Luca Leuzzi', 'Tommaso Rizzo']","We consider second-order phase transitions in which the order parameter is a replicated overlap matrix. We focus on a tricritical point that occurs in a variety of mean-field models and that, more generically, describes higher order liquid-liquid or liquid-glass transitions. We show that the static replicated theory implies slowing down with a logarithmic decay in time. The dynamical equations turn out to be those predicted by schematic Mode Coupling Theory for supercooled viscous liquids at a $A_3$ singularity, where the parameter exponent is $λ=1$. We obtain a quantitative expression for the parameter $μ$ of the logarithmic decay in terms of cumulants of the overlap, which are physically observable in experiments or numerical simulations."
https://arxiv.org/abs/2403.07564,2024-03-12,RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model,"['Mingze Wang', 'Keyan Chen', 'Lili Su', 'Cilin Yan', 'Sheng Xu', 'Haotian Zhang', 'Pengcheng Yuan', 'Xiaolong Jiang', 'Baochang Zhang']","The intelligent interpretation of buildings plays a significant role in urban planning and management, macroeconomic analysis, population dynamics, etc. Remote sensing image building interpretation primarily encompasses building extraction and change detection. However, current methodologies often treat these two tasks as separate entities, thereby failing to leverage shared knowledge. Moreover, the complexity and diversity of remote sensing image scenes pose additional challenges, as most algorithms are designed to model individual small datasets, thus lacking cross-scene generalization. In this paper, we propose a comprehensive remote sensing image building understanding model, termed RSBuilding, developed from the perspective of the foundation model. RSBuilding is designed to enhance cross-scene generalization and task universality. Specifically, we extract image features based on the prior knowledge of the foundation model and devise a multi-level feature sampler to augment scale information. To unify task representation and integrate image spatiotemporal clues, we introduce a cross-attention decoder with task prompts. Addressing the current shortage of datasets that incorporate annotations for both tasks, we have developed a federated training strategy to facilitate smooth model convergence even when supervision for some tasks is missing, thereby bolstering the complementarity of different tasks. Our model was trained on a dataset comprising up to 245,000 images and validated on multiple building extraction and change detection datasets. The experimental results substantiate that RSBuilding can concurrently handle two structurally distinct tasks and exhibits robust zero-shot generalization capabilities."
https://arxiv.org/abs/2403.07563,2024-03-12,Learning Generalizable Feature Fields for Mobile Manipulation,"['Ri-Zhao Qiu', 'Yafei Hu', 'Ge Yang', 'Yuchen Song', 'Yang Fu', 'Jianglong Ye', 'Jiteng Mu', 'Ruihan Yang', 'Nikolay Atanasov', 'Sebastian Scherer', 'Xiaolong Wang']","An open problem in mobile manipulation is how to represent objects and scenes in a unified manner, so that robots can use it both for navigating in the environment and manipulating objects. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherit to an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's ability to generalize to open-set objects as well as running time, when performing open-vocabulary mobile manipulation in dynamic scenes."
https://arxiv.org/abs/2403.07562,2024-03-12,A Flexible Cell Classification for ML Projects in Jupyter Notebooks,"['Miguel Perez', 'Selin Aydin', 'Horst Lichter']","Jupyter Notebook is an interactive development environment commonly used for rapid experimentation of machine learning (ML) solutions. Describing the ML activities performed along code cells improves the readability and understanding of Notebooks. Manual annotation of code cells is time-consuming and error-prone. Therefore, tools have been developed that classify the cells of a notebook concerning the ML activity performed in them. However, the current tools are not flexible, as they work based on look-up tables that have been created, which map function calls of commonly used ML libraries to ML activities. These tables must be manually adjusted to account for new or changed libraries."
https://arxiv.org/abs/2403.07561,2024-03-12,Maximum Defective Clique Computation: Improved Time Complexities and Practical Performance,['Lijun Chang'],"The concept of $k$-defective clique, a relaxation of clique by allowing up-to $k$ missing edges, has been receiving increasing interests recently. Although the problem of finding the maximum $k$-defective clique is NP-hard, several practical algorithms have been recently proposed in the literature, with kDC being the state of the art. kDC not only runs the fastest in practice, but also achieves the best time complexity. Specifically, it runs in $O^*(γ_k^n)$ time when ignoring polynomial factors; here, $γ_k$ is a constant that is smaller than two and only depends on $k$, and $n$ is the number of vertices in the input graph $G$. In this paper, we propose the kDC-Two algorithm to improve the time complexity as well as practical performance. kDC-Two runs in $O^*( (αΔ)^{k+2} γ_{k-1}^α)$ time when the maximum $k$-defective clique size $ω_k(G)$ is at least $k+2$, and in $O^*(γ_{k-1}^n)$ time otherwise, where $α$ and $Δ$ are the degeneracy and maximum degree of $G$, respectively. In addition, with slight modification, kDC-Two also runs in $O^*( (αΔ)^{k+2} (k+1)^{α+k+1-ω_k(G)})$ time by using the degeneracy gap $α+k+1-ω_k(G)$ parameterization; this is better than $O^*( (αΔ)^{k+2}γ_{k-1}^α)$ when $ω_k(G)$ is close to the degeneracy-based upper bound $α+k+1$. Finally, to further improve the practical performance, we propose a new degree-sequence-based reduction rule that can be efficiently applied, and theoretically demonstrate its effectiveness compared with those proposed in the literature. Extensive empirical studies on three benchmark graph collections show that our algorithm outperforms the existing fastest algorithm by several orders of magnitude."
https://arxiv.org/abs/2403.07560,2024-03-12,Unleashing Network Potentials for Semantic Scene Completion,"['Fengyun Wang', 'Qianru Sun', 'Dong Zhang', 'Jinhui Tang']","Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy and semantics from a single-view RGB-D image, and recent SSC methods commonly adopt multi-modal inputs. However, our investigation reveals two limitations: ineffective feature learning from single modalities and overfitting to limited datasets. To address these issues, this paper proposes a novel SSC framework - Adversarial Modality Modulation Network (AMMNet) - with a fresh perspective of optimizing gradient updates. The proposed AMMNet introduces two core modules: a cross-modal modulation enabling the interdependence of gradient flows between modalities, and a customized adversarial training scheme leveraging dynamic gradient competition. Specifically, the cross-modal modulation adaptively re-calibrates the features to better excite representation potentials from each single modality. The adversarial training employs a minimax game of evolving gradients, with customized guidance to strengthen the generator's perception of visual fidelity from both geometric completeness and semantic correctness. Extensive experimental results demonstrate that AMMNet outperforms state-of-the-art SSC methods by a large margin, providing a promising direction for improving the effectiveness and generalization of SSC methods."
https://arxiv.org/abs/2403.07559,2024-03-12,Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding,"['Huijie Tang', 'Federico Berto', 'Jinkyoo Park']","Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Secondly, we propose Q value-based methods for prioritized resolution of conflicts as well as deadlock situations. Finally, we introduce a robust ensemble method that can efficiently collect the best out of multiple possible solutions. We empirically evaluate EPH in complex multi-agent environments and demonstrate competitive performance against state-of-the-art neural methods for MAPF."
https://arxiv.org/abs/2403.07558,2024-03-12,Controlling Delegations in Liquid Democracy,"['Shiri Alouf-Heffetz', 'Tanmay Inamdar', 'Pallavi Jain', 'Yash More', 'Nimrod Talmon']","In liquid democracy, agents can either vote directly or delegate their vote to a different agent of their choice. This results in a power structure in which certain agents possess more voting weight than others. As a result, it opens up certain possibilities of vote manipulation, including control and bribery, that do not exist in standard voting scenarios of direct democracy. Here we formalize a certain kind of election control -- in which an external agent may change certain delegation arcs -- and study the computational complexity of the corresponding combinatorial problem."
https://arxiv.org/abs/2403.07557,2024-03-12,SIFiD: Reassess Summary Factual Inconsistency Detection with LLM,"['Jiuding Yang', 'Hui Liu', 'Weidong Guo', 'Zhuwei Rao', 'Yu Xu', 'Di Niu']","Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documents."
https://arxiv.org/abs/2403.07556,2024-03-12,Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts,"['Tian Yu', 'Shaolei Zhang', 'Yang Feng']","Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations. To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental results show that TACS can effectively filter information in context and significantly improve the overall quality of LLMs' responses when presented with misleading information."
https://arxiv.org/abs/2403.07555,2024-03-12,"Steady Granular Flow in a Rotating Drum: Universal description of stress, velocity and packing fraction profiles covering grain shape effects from convex to very concave","['Weiyi Wang', 'Jonathan Barés', 'Mathieu Renouf', 'Emilien Azéma']","The flow behavior of granular matter is significantly influenced by the shape of constituent particles. This effect is particularly pronounced for very concave particles, which exhibit unique flow characteristics such as higher porosity and sharper phase transitions between jamming and unjamming states. Despite the richness and ubiquitousness of these systems, our understanding of their intricate flow behavior and the local mechanisms driving these behaviors remains incomplete. In this work, we investigate the effect of particle shape, ranging from spherical to highly concave, on steady flows in a rotating drum - a system that facilitates a continuous phase transition from a jamming state at greater depths to an unjamming state at shallower regions. We develop an analytical model to elucidate granular behavior within the rotating drum: (i) Firstly, by decomposing the shear stress, we reconcile the discrepancy between simulation data and theoretical predictions, establishing a relationship with the angle of repose. (ii)Secondly, we extend the generalized Bagnold scaling , coupled with a non-local fluidity relation based on packing fraction, providing a framework for a correlation between shear stress, shear rate, and packing fraction. Additionally, we introduce a characteristic length to quantify the influence of particle shape and drum speed. This analytical model offers explicit functional forms for physical quantity profiles, which are validated experimentally in a thin rotating drum and numerically in a two-dimensional rotating drum. Our results demonstrate that this model accurately describes the change of velocity due to the phase transition of granular flow within a rotating drum. Moreover, for different shapes of particle and drum speeds, the characteristic length captures the interplay between shear stress, shear rate, and the variation of packing fraction."
https://arxiv.org/abs/2403.07554,2024-03-12,An Adaptive Learning Approach to Multivariate Time Forecasting in Industrial Processes,"['Fernando Miguelez', 'Josu Doncel', 'Maria Dolores Ugarte']","Industrial processes generate a massive amount of monitoring data that can be exploited to uncover hidden time losses in the system, leading to enhanced accuracy of maintenance policies and, consequently, increasing the effectiveness of the equipment. In this work, we propose a method for one-step probabilistic multivariate forecasting of time variables based on a Hidden Markov Model with covariates (IO-HMM). These covariates account for the correlation of the predicted variables with their past values and additional process measurements by means of a discrete model and a continuous model. The probabilities of the former are updated using Bayesian principles, while the parameter estimates for the latter are recursively computed through an adaptive algorithm that also admits a Bayesian interpretation. This approach permits the integration of new samples into the estimation of unknown parameters, computationally improving the efficiency of the process. We evaluate the performance of the method using a real data set obtained from a company of a particular sector; however, it is a versatile technique applicable to any other data set. The results show a consistent improvement over a persistence model, which assumes that future values are the same as current values, and more importantly, over univariate versions of our model."
https://arxiv.org/abs/2403.07553,2024-03-12,The future of document indexing: GPT and Donut revolutionize table of content processing,"['Degaga Wolde Feyisa', 'Haylemicheal Berihun', 'Amanuel Zewdu', 'Mahsa Najimoghadam', 'Marzieh Zare']","Industrial projects rely heavily on lengthy, complex specification documents, making tedious manual extraction of structured information a major bottleneck. This paper introduces an innovative approach to automate this process, leveraging the capabilities of two cutting-edge AI models: Donut, a model that extracts information directly from scanned documents without OCR, and OpenAI GPT-3.5 Turbo, a robust large language model. The proposed methodology is initiated by acquiring the table of contents (ToCs) from construction specification documents and subsequently structuring the ToCs text into JSON data. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5 Turbo reaching 89% in effectively organizing the ToCs. This landmark achievement represents a significant leap forward in document indexing, demonstrating the immense potential of AI to automate information extraction tasks across diverse document types, boosting efficiency and liberating critical resources in various industries."
https://arxiv.org/abs/2403.07552,2024-03-12,Springer correspondence and mirror symmetries for parabolic Hitchin systems,"['Bin Wang', 'Xueqing Wen', 'Yaoxiong Wen']","We prove the Strominger--Yau--Zaslow and topological mirror symmetries for parabolic Hitchin systems of types B and C. In contrast to type A, a geometric reinterpretation of Springer duality is necessary. Furthermore, unlike Hitchin's construction in the non-parabolic case, the map between generic fibers in type B and C needs more analysis due to the change of partitions of Springer dual nilpotent orbits, which is the main difficulty in this article. To tackle this challenge, we first construct and study the geometry of the generic Hitchin fibers of moduli spaces of Higgs bundles associated to the nilpotent orbit closures. Then we study their relation with the generic Hitchin fibers of parabolic Hitchin systems. Along this way, we establish intriguing connections between Springer duality, Kazhdan--Lusztig maps, and singularities of spectral curves, and uncover a new geometric interpretation of Lusztig's canonical quotient."
https://arxiv.org/abs/2403.07551,2024-03-12,Isolated nearly flat higher Chern band in monolayer transition metal trihalides,"['Kejie Bao', 'Huan Wang', 'Yadong Jiang', 'Haosheng Xu', 'Jing Wang']","The interplay between non-trivial topology and strong electron interaction can generate a variety of exotic quantum matter. Here we theoretically propose that monolayer transition metal trihalides MoF$_3$ and WI$_3$ have isolated nearly flat band near the Fermi level with higher Chern number $\mathcal{C}=+3$ and $\mathcal{C}=-2$, respectively. The nontrivial topology of these flat Chern bands originates from the effective $sd^2$ hybridization of transition metal atom, which transform the apparent atomic $d$ orbitals on a hexagonal lattice into $(s, p_+, p_-)$ orbitals on a triangular lattice. Interestingly, the quantum geometry of flat Chern bands in both materials are comparable with those in moiré systems exhibiting fractional Chern insulator state. The Hofstadter butterfly of such flat Chern bands are further studied. These natural materials, if realized experimentally, could offer new platforms to explore correlated phenomena driven by flat Chern band with higher Chern number."
https://arxiv.org/abs/2403.07550,2024-03-12,The post--quasi-static approximation: An analytical approach to gravitational collapse,"['L. Herrera', 'A. Di prisco', 'J. Ospino']","A semi--numerical approach proposed many years ago for describing gravitational collapse in the post--quasi--static approximation, is modified in order to avoid the numerical integration of the basic differential equations the approach is based upon. For doing that we have to impose some restrictions on the fluid distribution. More specifically, we shall assume the vanishing complexity factor condition, which allows for analytical integration of the pertinent differential equations and leads to physically interesting models. Instead, we show that neither the homologous nor the quasi--homologous evolution are acceptable since they lead to geodesic fluids, which are unsuitable for being described in the post--quasi--static approximation. Also, we prove that, within this approximation, adiabatic evolution also leads to geodesic fluids and therefore we shall consider exclusively dissipative systems. Besides the vanishing complexity factor condition, additional information is required for a full description of models. We shall propose different strategies for obtaining such an information, which are based on observables quantities (e.g. luminosity and redshift), and/or heuristic mathematical ansatz. To illustrate the method, we present two models. One model is inspired in the well known Schwarzschild interior solution, and another one is inspired in Tolman VI solution."
https://arxiv.org/abs/2403.07549,2024-03-12,Consensus under Persistence Excitation,"['Fabio Ancona', 'Mohamed Bentaibi', 'Francesco Rossi']",We prove that a first-order cooperative system of interacting agents converges to consensus if the so-called Persistence Excitation condition holds. This condition requires that the interaction function between any pair of agents satisfies an integral lower bound. The interpretation is that the interaction needs to ensure a minimal amount of service.
https://arxiv.org/abs/2403.07548,2024-03-12,Online Continual Learning For Interactive Instruction Following Agents,"['Byeonghwi Kim', 'Minhyuk Seo', 'Jonghyun Choi']","In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information during training (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state of the art in our empirical validations by noticeable margins. The project page including codes is https://github.com/snumprlab/cl-alfred."
https://arxiv.org/abs/2403.07547,2024-03-12,SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields,"['Jungho Lee', 'Dogyoon Lee', 'Minhyeok Lee', 'Donghyung Kim', 'Sangyoun Lee']","Neural radiance fields (NeRF) has attracted considerable attention for their exceptional ability in synthesizing novel views with high fidelity. However, the presence of motion blur, resulting from slight camera movements during extended shutter exposures, poses a significant challenge, potentially compromising the quality of the reconstructed 3D scenes. While recent studies have addressed this issue, they do not consider the continuous dynamics of camera movements during image acquisition, leading to inaccurate scene reconstruction. Additionally, these methods are plagued by slow training and rendering speed. To effectively handle these issues, we propose sequential motion understanding radiance fields (SMURF), a novel approach that employs neural ordinary differential equation (Neural-ODE) to model continuous camera motion and leverages the explicit volumetric representation method for faster training and robustness to motion-blurred input images. The core idea of the SMURF is continuous motion blurring kernel (CMBK), a unique module designed to model a continuous camera movements for processing blurry inputs. Our model, rigorously evaluated against benchmark datasets, demonstrates state-of-the-art performance both quantitatively and qualitatively."
https://arxiv.org/abs/2403.07546,2024-03-12,Homological stability for the Cremona groups,['Markus Szymik'],"The Cremona groups are the groups of all birational equivalences of projective spaces and, equivalently, the automorphism groups of the rational function fields. We construct highly connected spaces on which these groups act in a way that allows us to deduce that their abelianisations, and more generally, the homologies of these groups, stabilise as the dimension increases."
https://arxiv.org/abs/2403.07545,2024-03-12,Artin-Schreier quandles of involutions in absolute Galois groups,['Markus Szymik'],"We introduce a new invariant of fields that refines their real spectrum and is related to their absolute Galois group: the Artin-Schreier quandle. For formally real number fields, it is freely generated in its variety by a Cantor space of indeterminates. For Laurent series fields, we compute it in terms of the Artin-Schreier quandle of the coefficient field. This result and other examples show that, in general, there are relations."
https://arxiv.org/abs/2403.07544,2024-03-12,MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki,"['Timothee Mickus', 'Stig-Arne Grönroos', 'Joseph Attieh', 'Michele Boggia', 'Ona De Gibert', 'Shaoxiong Ji', 'Niki Andreas Lopi', 'Alessandro Raganato', 'Raúl Vázquez', 'Jörg Tiedemann']","NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled. The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality. In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular machine translation systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters. We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information. The toolkit is publicly available online."
https://arxiv.org/abs/2403.07543,2024-03-12,Gauss-Bonnet $\boldsymbol{AdS}$ planar and spherical black hole thermodynamics and holography,"['Souvik Paul', 'Sunandan Gangopadhyay', 'Ashis Saha']","In this work, we extend the study in \cite{Bilic:2022psx} incorporating the $AdS$/CFT duality to establish a relationship between the local temperatures of a large ($AdS$) spherical and a ($AdS$) planar Schwarzschild black hole near the $AdS$ boundary considering Gauss-Bonnet curvature correction in the gravitational action. We have shown the finite coupling corrections appear in the local temperature relationships due to the inclusion of Gauss-Bonnet term in the bulk. By transforming the metric into Fefferman-Graham form, we have calculated the energy of the conformal fluid at the boundary. Following the results of fluid/gravity duality, the energy of the conformal fluid at the boundary is then compared with the black body radiation energy which eventually leads us to establish the local temperature relationship between spherical and planar black holes in Gauss-Bonnet gravity near the $AdS$ boundary."
https://arxiv.org/abs/2403.07542,2024-03-12,A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions,['Quoc-Vinh Lai-Dang'],"This survey explores the adaptation of visual transformer models in Autonomous Driving, a transition inspired by their success in Natural Language Processing. Surpassing traditional Recurrent Neural Networks in tasks like sequential image processing and outperforming Convolutional Neural Networks in global context capture, as evidenced in complex scene recognition, Transformers are gaining traction in computer vision. These capabilities are crucial in Autonomous Driving for real-time, dynamic visual scene processing. Our survey provides a comprehensive overview of Vision Transformer applications in Autonomous Driving, focusing on foundational concepts such as self-attention, multi-head attention, and encoder-decoder architecture. We cover applications in object detection, segmentation, pedestrian detection, lane detection, and more, comparing their architectural merits and limitations. The survey concludes with future research directions, highlighting the growing role of Vision Transformers in Autonomous Driving."
https://arxiv.org/abs/2403.07541,2024-03-12,Process Modeling With Large Language Models,"['Humam Kourani', 'Alessandro Berti', 'Daniel Schuster', 'Wil M. P. van der Aalst']","In the realm of Business Process Management (BPM), process modeling plays a crucial role in translating complex process dynamics into comprehensible visual representations, facilitating the understanding, analysis, improvement, and automation of organizational processes. Traditional process modeling methods often require extensive expertise and can be time-consuming. This paper explores the integration of Large Language Models (LLMs) into process modeling to enhance flexibility, efficiency, and accessibility of process modeling for both expert and non-expert users. We propose a framework that leverages LLMs for the automated generation and iterative refinement of process models starting from textual descriptions. Our framework involves innovative prompting strategies for effective LLM utilization, along with a secure model generation protocol and an error-handling mechanism. Moreover, we instantiate a concrete system extending our framework. This system provides robust quality guarantees on the models generated and supports exporting them in standard modeling notations, such as the Business Process Modeling Notation (BPMN) and Petri nets. Preliminary results demonstrate the framework's ability to streamline process modeling tasks, underscoring the transformative potential of generative AI in the BPM field."
https://arxiv.org/abs/2403.07540,2024-03-12,WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic Malicious Storage Traces,"['Dionysios Diamantopolous', 'Roman Pletka', 'Slavisa Sarafijanovic', 'A. L. Narasimha Reddy', 'Haris Pozidis']","Ransomware, a fearsome and rapidly evolving cybersecurity threat, continues to inflict severe consequences on individuals and organizations worldwide. Traditional detection methods, reliant on static signatures and application behavioral patterns, are challenged by the dynamic nature of these threats. This paper introduces three primary contributions to address this challenge. First, we introduce a ransomware emulator. This tool is designed to safely mimic ransomware attacks without causing actual harm or spreading malware, making it a unique solution for studying ransomware behavior. Second, we demonstrate how we use this emulator to create storage I/O traces. These traces are then utilized to train machine-learning models. Our results show that these models are effective in detecting ransomware, highlighting the practical application of our emulator in developing responsible cybersecurity tools. Third, we show how our emulator can be used to mimic the I/O behavior of existing ransomware thereby enabling safe trace collection. Both the emulator and its application represent significant steps forward in ransomware detection in the era of machine-learning-driven cybersecurity."
https://arxiv.org/abs/2403.07539,2024-03-12,Validation of electrodeposited 241Am alpha-particle sources for use in liquified gas detectors at cryogenic temperatures,"['E. Calvo Alamillo', 'M. T. Crespo Vázquez', 'P. F. Rato Mendes', 'R. Álvarez Garrote', 'J. I. Crespo Anadón', 'C. Cuesta', 'A. De la Torre Rojo', 'I. Gil-Botella', 'I. Martín Martín', 'M. Mejuto Mendieta', 'C. Palomares', 'L. Pérez Molina', 'J. A. Soto Otón', 'A. Verdugo de Osa']","This paper describes a procedure for the validation of alpha-particle sources (exempt unsealed sources) to be used in experimental setups with liquefied gases at cryogenic temperatures (down to -196 C) and high vacuum. These setups are of interest for the development and characterization of neutrino and dark matter detectors based on liquid argon, among others. Due to the high purity requirements, the sources have to withstand high vacuum and cryogenic temperatures for extended periods. The validation procedure has been applied to 241Am sources produced by electrodeposition."
https://arxiv.org/abs/2403.07538,2024-03-12,On rainbow domination of cubic graphs,['Janez Žerovnik'],"The structure of minimal weight rainbow domination functions of cubic graphs are studied. Based on general observations for cubic graphs, generalized Petersen graphs $P(ck,k)$ are characterized whose 4- and 5-rainbow domination numbers equal the general lower bounds. As $t$-rainbow domination of cubic graphs for $t \ge 6$ is trivial, characterizations of such generalized Petersen graphs $P(ck,k)$ are known for all $t$-rainbow domination numbers.In addition, new upper bounds for 4- and 5-rainbow domination numbers that are valid for all $P(ck,k)$ are provided."
https://arxiv.org/abs/2403.07537,2024-03-12,Vortices and Factorization,"['Igor Loutsenko', 'Oksana Yermolayeva']","We review applications of factorization methods to the problem of finding stationary point vortex patterns in two-dimensional fluid mechanics. Then we present a new class of patterns related to periodic analogs of Schrodinger operators from the ``even"" bi-spectral family. We also show that patterns related to rational and soliton solutions of the KdV hierarchy constitute complete solution of the problem for certain classes of vortex systems."
https://arxiv.org/abs/2403.07536,2024-03-12,LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes,"['Julian Suk', 'Baris Imre', 'Jelmer M. Wolterink']","Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three tasks in cardiovascular hemodynamics modelling and neurodevelopmental phenotype prediction, featuring meshes of up to 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful architecture for learning with high-fidelity meshes which has the potential to enable interesting downstream applications. Our implementation is publicly available."
https://arxiv.org/abs/2403.07535,2024-03-12,Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving,"['JunDa Cheng', 'Wei Yin', 'Kaixuan Wang', 'Xiaozhi Chen', 'Shijie Wang', 'Xin Yang']","Multi-view depth estimation has achieved impressive performance over various benchmarks. However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving. In this work, we propose a new robustness benchmark to evaluate the depth estimation system under various noisy pose settings. Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail when given noisy pose settings. To address this challenge, we propose a single-view and multi-view fused depth estimation system, which adaptively integrates high-confident multi-view and single-view results for both robust and accurate depth estimations. The adaptive fusion module performs fusion by dynamically selecting high-confidence regions between two branches based on a wrapping confidence map. Thus, the system tends to choose the more reliable branch when facing textureless scenes, inaccurate calibration, dynamic objects, and other degradation or challenging conditions. Our method outperforms state-of-the-art multi-view and fusion methods under robustness testing. Furthermore, we achieve state-of-the-art performance on challenging benchmarks (KITTI and DDAD) when given accurate pose estimations. Project website: https://github.com/Junda24/AFNet/."
https://arxiv.org/abs/2403.07534,2024-03-12,Frobenius numbers associated with Diophantine triples of $x^2+y^2=z^r$ (extended version),"['Takao Komatsu', 'Neha Gupta', 'Manoj Upreti']","We give an explicit formula for the $p$-Frobenius number of triples associated with Diophantine equations $x^2+y^2=z^r$, that is, the largest positive integer that can only be represented in $p$ ways by combining the three integers of the solutions of Diophantine equations $x^2+y^2=z^r$. When $r=2$, the Frobenius number has already been given."
https://arxiv.org/abs/2403.07533,2024-03-12,Turbulent mixed convection in vertical and horizontal channels,"['Christopher J. Howland', 'Guru Sreevanshu Yerragolam', 'Roberto Verzicco', 'Detlef Lohse']","Turbulent shear flows driven by a combination of a pressure gradient and buoyancy forcing are investigated using direct numerical simulations. Specifically, we consider the setup of a differentially heated vertical channel subject to a Poiseuille-like horizontal pressure gradient. We explore the response of the system to its three control parameters: the Grashof number $Gr$, the Prandtl number $Pr$, and the Reynolds number $Re$ of the pressure-driven flow. From these input parameters, the relative strength of buoyancy driving to the pressure gradient can be quantified by the Richardson number $Ri=Gr/Re^2$. We compare the response of the mixed vertical convection configuration to that of mixed Rayleigh-Bénard convection and find a nearly identical behaviour, including an increase in wall friction at higher $Gr$ and a drop in the heat flux relative to natural convection for $Ri=O(1)$. This closely matched response is despite vastly different flow structures in the systems. No large-scale organisation is visible in visualisations of mixed vertical convection - an observation that is quantitatively confirmed by spectral analysis. This analysis, combined with a statistical description of the wall heat flux, highlights how moderate shear suppresses the growth of small-scale plumes and reduces the likelihood of extreme events in the local wall heat flux. Vice versa, starting from a pure shear flow, the addition of thermal driving enhances the drag due to the emission of thermal plumes."
https://arxiv.org/abs/2403.07532,2024-03-12,Open-World Semantic Segmentation Including Class Similarity,"['Matteo Sodano', 'Federico Magistri', 'Lucas Nunes', 'Jens Behley', 'Cyrill Stachniss']","Interpreting camera data is key for autonomously acting systems, such as autonomous vehicles. Vision systems that operate in real-world environments must be able to understand their surroundings and need the ability to deal with novel situations. This paper tackles open-world semantic segmentation, i.e., the variant of interpreting image data in which objects occur that have not been seen during training. We propose a novel approach that performs accurate closed-world semantic segmentation and, at the same time, can identify new categories without requiring any additional training data. Our approach additionally provides a similarity measure for every newly discovered class in an image to a known category, which can be useful information in downstream tasks such as planning or mapping. Through extensive experiments, we show that our model achieves state-of-the-art results on classes known from training data as well as for anomaly segmentation and can distinguish between different unknown classes."
https://arxiv.org/abs/2403.07531,2024-03-12,"How Language, Culture, and Geography shape Online Dialogue: Insights from Koo","['Amin Mekacher', 'Max Falkenberg', 'Andrea Baronchelli']","Koo is a microblogging platform based in India launched in 2020 with the explicit aim of catering to non-Western communities in their vernacular languages. With a near-complete dataset totalling over 71M posts and 399M user interactions, we show how Koo has attracted users from several countries including India, Nigeria and Brazil, but with variable levels of sustained user engagement. We highlight how Koo's interaction network has been shaped by multiple country-specific migrations and displays strong divides between linguistic and cultural communities, for instance, with English-speaking communities from India and Nigeria largely isolated from one another. Finally, we analyse the content shared by each linguistic community and identify cultural patterns that promote similar discourses across language groups. Our study raises the prospect that a multilingual and politically diverse platform like Koo may be able to cultivate vernacular communities that have, historically, not been prioritised by US-based social media platforms."
https://arxiv.org/abs/2403.07530,2024-03-12,Carbon Economics of Different Agricultural Practices for Farming Soil,"['Suganthi Pazhanivel Koushika', 'Anbalagan Krishnaveni', 'Sellaperumal Pazhanivelan', 'Alagirisamy Bharani', 'Venugopal Arunkumar', 'Perumal Devaki', 'Narayanan Muthukrishnan']","The loss of soil organic carbon (SOC) poses a severe danger to agricultural sustainability around the World. This review examines various farming practices and their impact on soil organic carbon storage. After a careful review of the literature, most of the research indicated that different farming practices, such as organic farming, cover crops, conservation tillage, and agroforestry, play vital roles in increasing the SOC content of the soil sustainably. Root exudation from cover crops increases microbial activity and helps break down complex organic compounds into organic carbon. Conservation tillage enhances the soil structure and maintains carbon storage without disturbing the soil. Agroforestry systems boost organic carbon input and fasten nutrient cycling because the trees and crops have symbiotic relationships. Intercropping and crop rotations have a role in changing the composition of plant residues and promoting carbon storage. There were many understanding on the complex interactions between soil organic carbon dynamics and agricultural practices. Based on the study, the paper reveals, the role of different agricultural practices like Carbon storage through cover crops, crop rotation, mulching Conservation tillage, conventional tillage, zero tillage and organic amendments in organic carbon storage in the soil for maximum crop yield to improve the economic condition of the cultivators."
https://arxiv.org/abs/2403.07529,2024-03-12,Comments on characterizing demand flexibility to provide power grid services,['Prabir Barooah'],"Many loads have flexibility in demand that can be used to provide ancillary services to power grids. A large body of literature exists on designing algorithms to coordinate actions of many loads to provide such a service. The topic of characterizing the flexibility of one or a collection of loads - to determine what kinds of demand deviation from the baseline is feasible - has also been studied. However, there is a large diversity in definitions of flexibility and methods proposed to characterize flexibility. As a result there are several gaps in the literature on flexibility characterization. Some approaches on flexibility characterization are based on ad-hoc approximations that lead to highly conservative estimates. In this paper we point out some of these issues and their implications, with the hope to encourage additional research to address them."
https://arxiv.org/abs/2403.07528,2024-03-12,Enhanced Monochromatic Photon Emission from Millicharged Co-Interacting Dark Matter,"['Mingxuan Du', 'Jia Liu', 'Xiao-Ping Wang', 'Tianhao Wu']","We study a millicharged co-interacting dark matter scenario, where the primary dark matter constituent is the dark photon $A'$ and the secondary component is the fermion $χ$. In this model, $χ$ interacts with $A'$ via a $U(1)'$ interaction while being millicharged with respect to normal photons. Our investigation focuses on the oscillation of $A'$ dark matter into photons within the background of $χ$ particles, revealing that the $A'-χ$ scattering rate benefits from a Bose enhancement of the $A'$ final state. As the oscillation production rate is directly linked to the scattering rate, the conversion of $A'$ dark matter into monochromatic photons experiences significant amplification owing to this Bose enhancement, especially when the scattering rate $Γ_{\rm sca}$ approaches the dark photon mass $m_{A'}$. These converted monochromatic photons are detectable through radio telescopes and can induce distortions in the Cosmic Microwave Background (CMB) spectrum. We find that the sensitivity of radio telescopes and the constraints imposed by CMB distortion on the kinetic mixing parameter are notably heightened compared to scenarios without the subdominant millicharged dark matter."
https://arxiv.org/abs/2403.07527,2024-03-12,Evaluation and thermodynamic optimization of phase diagram of lithium niobate tantalate solid solutions,"['Umar Bashir', 'Detlef Klimm', 'Michael Rusing', 'Matthias Bickermann', 'Steffen Ganschow']","The phase diagram of the lithium niobate and lithium tantalate solid solutions was investigated using experimental data from differential thermal analysis (DTA) and crystal growth. We used XRF analysis to determine the elemental composition of crystals. Based on the Neumann-Kopp rule, essential data of end members lithium niobate (LN) and lithium tantalate (LT) was created. The heats of fusion of end members given by DTA measurements of LN (103 kJ/mol at 1531 K) and LT (289 kJ/mol at 1913 K) were given as input parameters to generate the data. This data served as the basis for calculating a phase diagram for LN and LT solid solutions. Finally, based on the experimental data and thermodynamic solution model, the phase diagram was optimized in the Calphad Factsage module. We also generated thermodynamic parameters for Gibb's excess energy of the solid solution. A plot of segregation coefficient as a function of Ta concentration was derived from the phase diagram."
https://arxiv.org/abs/2403.07526,2024-03-12,Physics-Transfer Learning for Material Strength Screening,"['Yingjie Zhao', 'Zian Zhang', 'Zhiping Xu']","The strength of materials, like many problems in the natural sciences, spans multiple length and time scales, and the solution has to balance accuracy and performance. Peierls stress is one of the central concepts in crystal plasticity that measures the strength through the resistance of a dislocation to plastic flow. The determination of Peierls stress involves a multiscale nature depending on both elastic lattice responses and the energy landscape of crystal slips. Material screening by strength via the Peierls stress from first-principles calculations is computationally intractable for the nonlocal characteristics of dislocations, and not included in the state-of-the-art computational material databases. In this work, we propose a physics-transfer framework to learn the physics of crystal plasticity from empirical atomistic simulations and then predict the Peierls stress from chemically accurate density functional theory-based calculations of material parameters. Notably, the strengths of single-crystalline metals can be predicted from a few single-point calculations for the deformed lattice and on the γ surface, allowing efficient, high-throughput screening for material discovery. Uncertainty quantification is carried out to assess the accuracy of models and sources of errors, showing reduced physical and system uncertainties in the predictions by elevating the fidelity of training models. This physics-transfer framework can be generalized to other problems facing the accuracy-performance dilemma, by harnessing the hierarchy of physics in the multiscale models of materials science."
https://arxiv.org/abs/2403.07525,2024-03-12,Universal Chemical Formula Dependence of $Ab$ $Initio$ Low-Energy Effective Hamiltonian in Single-Layer Carrier Doped Cuprate Superconductors -- Study by Hierarchical Dependence Extraction Algorithm,"['Jean-Baptiste Morée', 'Ryotaro Arita']","We explore the possibility to control the superconducting (SC) transition temperature at optimal hole doping $T_{c}^{\rm opt}$ in cuprates by tuning the chemical formula (CF). $T_{c}^{\rm opt}$ can be theoretically predicted from the parameters of the \textit{ab initio} low-energy effective Hamiltonian (LEH) with one antibonding (AB) Cu$3d_{x^2-y^2}$/O$2p_σ$ orbital per Cu atom in the CuO$_2$ plane, notably the nearest neighbor hopping amplitude $|t_1|$ and the ratio $u=U/|t_1|$, where $U$ is the onsite effective Coulomb repulsion. However, the CF dependence of $|t_1|$ and $u$ is a highly nontrivial question. In this paper, we propose the universal dependence of $|t_1|$ and $u$ on the CF and structural features in hole doped cuprates with a single CuO$_2$ layer sandwiched between block layers. To do so, we perform extensive \textit{ab initio} calculations of $|t_1|$ and $u$ and analyze the results by employing a machine learning method called Hierarchical Dependence Extraction (HDE). The main results are the following: (a) $|t_1|$ has a main-order dependence on the radii $R_{\rm X}$ and $R_{\rm A}$ of the apical anion X and cation A in the block layer. ($|t_1|$ increases when $R_{\rm X}$ or $R_{\rm A}$ decreases.) (b) $u$ has a main-order dependence on the negative ionic charge $Z_{\rm X}$ of X and the hole doping $δ$ of the AB orbital. ($u$ decreases when $|Z_{\rm X}|$ increases or $δ$ increases.) We elucidate and discuss the microscopic mechanism of (a,b). We demonstrate the predictive power of the HDE by showing the consistency between (a,b) and results from previous works. The present results provide a basis for optimizing SC properties in cuprates and possibly akin materials. Also, the HDE method offers a general platform to identify dependencies between physical quantities."
https://arxiv.org/abs/2403.07524,2024-03-12,Shining Light on Periodic Dominating Sets in Bounded-Treewidth Graphs,"['Jakob Greilhuber', 'Philipp Schepper', 'Philip Wellnitz']","For the vertex selection problem $(σ,ρ)$-DomSet one is given two fixed sets $σ$ and $ρ$ of integers and the task is to decide whether we can select vertices of the input graph, such that, for every selected vertex, the number of selected neighbors is in $σ$ and, for every unselected vertex, the number of selected neighbors is in $ρ$. This framework covers Independent Set and Dominating Set for example."
https://arxiv.org/abs/2403.07523,2024-03-12,Online Misogyny Against Female Candidates in the 2022 Brazilian Elections: A Threat to Women's Political Representation?,"['Luise Kocha', 'Raji Ghawi', 'Jürgen Pfeffer', 'Janina Isabel Steinert']","Technology-facilitated gender-based violence has become a global threat to women's political representation and democracy. Understanding how online hate affects its targets is thus paramount. We analyse 10 million tweets directed at female candidates in the Brazilian election in 2022 and examine their reactions to online misogyny. Using a self-trained machine learning classifier to detect Portuguese misogynistic tweets and a quantitative analysis of the candidates' tweeting behaviour, we investigate how the number of misogynistic attacks received alters the online activity of the female candidates. We find that young and left-wing candidates and candidates with higher visibility online received significantly more attacks. Furthermore, we find that an increase in misogynistic attacks in the previous week is associated with a decrease in female candidates' tweets in the following week. This potentially threatens their equal participation in public opinion building and silences women's voices in political discourse."
https://arxiv.org/abs/2403.07522,2024-03-12,Holographic spin alignment for vector mesons,"['Xin-Li Sheng', 'Yan-Qing Zhao', 'Si-Wen Li', 'Francesco Becattini', 'Defu Hou']","We develop a general framework for studying the spin alignment $ρ_{00}$ for flavorless vector mesons by using the gauge/gravity duality. Focusing on the dilepton production through vector meson decay, we derive the relation between production rates at each spin channel and meson's spectral function, which can be evaluated by holographic models for a strongly coupled system. As examples, we study $ρ_{00}$ for $J/ψ$ and $φ$ mesons, induced by the relative motion to a thermal background, within the soft-wall model. We show that $ρ_{00}$ in the helicity frame for $J/ψ$ and $φ$ mesons have positive and negative deviations from 1/3 at $T=150$ MeV, respectively, which consequently leads to different properties for their global spin alignments."
https://arxiv.org/abs/2403.07521,2024-03-12,Cohomologies and deformations of differential algebra morphisms,"['Lei Du', 'Yanhong Bao']","This paper studies the formal deformations of differential algebra morphisms. As a consequence, we develop a cohomology theory of differential algebra morphisms to interpret the lower degree cohomology groups as formal deformations. Then, we prove the Cohomology Comparison Theorem of differential algebra morphisms, i.e., the cohomology of a morphism of differential algebras is isomorphic to the cohomology of an auxiliary differential algebra. Finally, we can give a minimal model for morphism of differential algebras with weight=0."
https://arxiv.org/abs/2403.07520,2024-03-12,Effects of diffusion and advection on predator prey dynamics in an advective patchy environment,['Qi Wang'],"In this paper, we consider a specialist predator-prey patchy model over the closed stream network. We study the dynamics and the asymptotic profiles of positive steady states according to the mortality rate of the specialist predators, advection and diffusion rates. We verify that the specialist predators can successfully invade as long as the mortality rate is sufficiently small. On the other hand, the impacts of diffusion and advection on the asymptotic profiles of positive steady states and on the concentration of the species are given."
https://arxiv.org/abs/2403.07519,2024-03-12,Time evolution of the Boltzmann entropy for a nonequilibrium dilute gas,"['P. L. Garrido', 'S. Goldstein', 'D. A. Huse', 'J. L. Lebowitz']","We investigate the time evolution of the Boltzmann entropy of a dilute gas of N particles, N>>1, as it undergoes a free expansion doubling its volume. The microstate of the system, a point in the 4N dimensional phase space, changes in time via Hamiltonian dynamics. Its entropy, at any time $t$, is given by the logarithm of the phase space volume of all the microstates giving rise to its macrostate at time $t$. The macrostates that we consider are defined by coarse graining the one-particle phase space into cells $Δ_α$. The initial and final macrostates of the system are equilibrium states in volumes $V$ and $2V$, with the same energy $E$ and particle number $N$. Their entropy per particle is given, for sufficiently large systems, by the thermodynamic entropy as a function of the particle and energy density, whose leading term is independent of the size of the $Δ_α$. The intermediate (non-equilibrium) entropy does however depend on the size of the cells $Δ_α$. Its change with time is due to (i) dispersal in physical space from free motion and to (ii) the collisions between particles which change their velocities. The former depends strongly on the size of the velocity coarse graining $Δv$: it produces entropy at a rate proportional to $Δv$. This dependence is investigated numerically and analytically for a dilute two-dimensional gas of hard discs. It becomes significant when the mean free path between collisions is of the same order or larger than the length scale of the initial spatial inhomogeneity. In the opposite limit, the rate of entropy production is essentially independent of $Δv$ and is given by the Boltzmann equation for the limit $Δv\rightarrow 0$. We show that when both processes are active the time dependence of the entropy has a scaling form involving the ratio of the rates of its production by the two processes."
https://arxiv.org/abs/2403.07518,2024-03-12,Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss,"['Xuhua Ren', 'Hengcan Shi', 'Jin Li']","Scene text recognition is an important and challenging task in computer vision. However, most prior works focus on recognizing pre-defined words, while there are various out-of-vocabulary (OOV) words in real-world applications."
https://arxiv.org/abs/2403.07517,2024-03-12,Energy versus Output Quality of Non-volatile Writes in Intermittent Computing,"['Rei Barjami', 'Antonio Miele', 'Luca Mottola']","We explore how to improve the energy performance of battery-less Internet of Things (IoT) devices at the cost of a reduction in the quality of the output. Battery-less IoT devices are extremely resource-constrained energy-harvesting devices. Due to erratic energy patterns from the ambient, their executions become intermittent; periods of active computation are interleaved by periods of recharging small energy buffers. To cross periods of energy unavailability, a device persists application and system state onto Non-Volatile Memory (NVM) in anticipation of energy failures. We purposely control the energy invested in these operations, representing a major energy overhead, when using Spin-Transfer Torque Magnetic Random-Access Memory (STT-MRAM) as NVM. As a result, we abate the corresponding overhead, yet introduce write errors. Based on 1.9+ trillion experimental data points, we illustrate whether this is a gamble worth taking, when, and where. We measure the energy consumption and quality of output obtained from the execution of nine diverse benchmarks on top of seven different platforms. Our results allow us to draw three key observations: i) the trade-off between energy saving and reduction of output quality is program-specific; ii) the same trade-off is a function of a platform's specific compute efficiency and power figures; and iii) data encoding and input size impact a program's resilience to errors. As a paradigmatic example, we reveal cases where we achieve up to 50% reduction in energy consumption with negligible effects on output quality, as opposed to settings where a minimal energy gain causes drastic drops in output quality."
https://arxiv.org/abs/2403.07516,2024-03-12,D4D: An RGBD diffusion model to boost monocular depth estimation,"['L. Papa', 'P. Russo', 'I. Amerini']","Ground-truth RGBD data are fundamental for a wide range of computer vision applications; however, those labeled samples are difficult to collect and time-consuming to produce. A common solution to overcome this lack of data is to employ graphic engines to produce synthetic proxies; however, those data do not often reflect real-world images, resulting in poor performance of the trained models at the inference step. In this paper we propose a novel training pipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion model able to generate realistic RGBD samples. We show the effectiveness of the developed solution in improving the performances of deep learning models on the monocular depth estimation task, where the correspondence between RGB and depth map is crucial to achieving accurate measurements. Our supervised training pipeline, enriched by the generated samples, outperforms synthetic and original data performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%) respectively on the indoor NYU Depth v2 and the outdoor KITTI dataset."
https://arxiv.org/abs/2403.07515,2024-03-12,Existence of Weak Solutions to a Cahn-Hilliard-Biot System,"['Helmut Abels', 'Harald Garcke', 'Jonas Haselböck']","We prove existence of weak solutions to a diffuse interface model describing the flow of a fluid through a deformable porous medium consisting of two phases. The system non-linearly couples Biot's equations for poroelasticity, including phase-field dependent material properties, with the Cahn-Hilliard equation to model the evolution of the solid, and is further augmented by a visco-elastic regularization consistent with secondary consolidation. To obtain this result, we approximate the problem in two steps, where first a semi-Galerkin ansatz is employed to show existence of weak solutions to regularized systems, for which later on compactness arguments allow limit passage. Notably, we also establish a maximal regularity theory for linear visco-elastic problems."
https://arxiv.org/abs/2403.07514,2024-03-12,Uncertainty-guided Contrastive Learning for Single Source Domain Generalisation,"['Anastasios Arsenos', 'Dimitrios Kollias', 'Evangelos Petrongonas', 'Christos Skliros', 'Stefanos Kollias']","In the context of single domain generalisation, the objective is for models that have been exclusively trained on data from a single domain to demonstrate strong performance when confronted with various unfamiliar domains. In this paper, we introduce a novel model referred to as Contrastive Uncertainty Domain Generalisation Network (CUDGNet). The key idea is to augment the source capacity in both input and label spaces through the fictitious domain generator and jointly learn the domain invariant representation of each class through contrastive learning. Extensive experiments on two Single Source Domain Generalisation (SSDG) datasets demonstrate the effectiveness of our approach, which surpasses the state-of-the-art single-DG methods by up to $7.08\%$. Our method also provides efficient uncertainty estimation at inference time from a single forward pass through the generator subnetwork."
https://arxiv.org/abs/2403.07513,2024-03-12,Spatiotemporal Representation Learning for Short and Long Medical Image Time Series,"['Chengzhi Shen', 'Martin J. Menten', 'Hrvoje Bogunović', 'Ursula Schmidt-Erfurth', 'Hendrik Scholl', 'Sobha Sivaprasad', 'Andrew Lotery', 'Daniel Rueckert', 'Paul Hager', 'Robbie Holland']","Analyzing temporal developments is crucial for the accurate prognosis of many medical conditions. Temporal changes that occur over short time scales are key to assessing the health of physiological functions, such as the cardiac cycle. Moreover, tracking longer term developments that occur over months or years in evolving processes, such as age-related macular degeneration (AMD), is essential for accurate prognosis. Despite the importance of both short and long term analysis to clinical decision making, they remain understudied in medical deep learning. State of the art methods for spatiotemporal representation learning, developed for short natural videos, prioritize the detection of temporal constants rather than temporal developments. Moreover, they do not account for varying time intervals between acquisitions, which are essential for contextualizing observed changes. To address these issues, we propose two approaches. First, we combine clip-level contrastive learning with a novel temporal embedding to adapt to irregular time series. Second, we propose masking and predicting latent frame representations of the temporal sequence. Our two approaches outperform all prior methods on temporally-dependent tasks including cardiac output estimation and three prognostic AMD tasks. Overall, this enables the automated analysis of temporal patterns which are typically overlooked in applications of deep learning to medicine."
https://arxiv.org/abs/2403.07512,2024-03-12,Is there (no) collective flow in \textit{pp} collisions?,"['Gábor Bíró', 'Leonid Serkin', 'Guy Paić', 'Gergely Gábor Barnaföldi']","The transverse momentum spectra and their multiplicity dependence serve as key tools for extracting parameters that can be compared with theoretical models. This comparison aims to establish the behaviour and nature of the system created in the collision. Over the past decade, the scientific community has extensively studied the possibility of a system analogous to quark-gluon plasma, predicted in heavy nuclei collisions, also existing in collisions involving light nuclei and proton-proton collisions. We have reanalysed the experimental data published by the ALICE Collaboration at the LHC, exploring a seemingly universal feature of transverse momentum spectra of charged particles. We have identified a specific range where the contribution of the hard part is nonexistent for the studied multiplicities. We present the dependence of the mean transverse momenta obtained in the soft and soft+hard (mixes) parts and discuss the results in the context of the ongoing controversy regarding the existence of collectivity in small systems. Finally, we also discuss possible refinements of the analyses concerning the use of statistical parameters of higher order, aimed at better distinguishing the agreement of theoretical and Monte Carlo models with the data."
https://arxiv.org/abs/2403.07511,2024-03-12,The entropy of an extended map for abelian group actions,['Yuan Lian'],"In this paper, we mainly consider on the entropy of the extended map conditional to the natural extension of a dynamical system for an Abelian group action and we calculate the entropy is zero."
https://arxiv.org/abs/2403.07510,2024-03-12,Relevance Score: A Landmark-Like Heuristic for Planning,"['Oliver Kim', 'Mohan Sridharan']","Landmarks are facts or actions that appear in all valid solutions of a planning problem. They have been used successfully to calculate heuristics that guide the search for a plan. We investigate an extension to this concept by defining a novel ""relevance score"" that helps identify facts or actions that appear in most but not all plans to achieve any given goal. We describe an approach to compute this relevance score and use it as a heuristic in the search for a plan. We experimentally compare the performance of our approach with that of a state of the art landmark-based heuristic planning approach using benchmark planning problems. While the original landmark-based heuristic leads to better performance on problems with well-defined landmarks, our approach substantially improves performance on problems that lack non-trivial landmarks."
https://arxiv.org/abs/2403.07509,2024-03-12,Conformal anomalies for (maximal) 6d conformal supergravity,"['Lorenzo Casarin', 'Christian Kennedy', 'Gabriele Tartaglino-Mazzucchelli']","We compute the conformal anomalies for 6d (2,0) conformal supergravity by direct calculation in component fields. The main novel results consist of the type-B anomaly coefficients for the gravitino and the 3-form, as well as their explicit quadratic action on some specific backgrounds. We also comment on the graviton contribution, whose Lagrangian is essentially given by the $\mathscr Q$-curvature. We confirm the expectation that, when coupling (2,0) conformal supergravity to 26 copies of the (2,0) tensor multiplet, the resulting theory is free of conformal anomalies. We also consider the conformal anomalies for its (1,0) truncation and confirm their relation with the chiral anomaly polynomial recently derived. For calculating the anomalies, we work with an Einstein on-shell background and make a factorised Ansatz for the operators governing the quadratic fluctuations. This reduces the calculation to evaluating heat-kernel coefficients of standard 2-derivative operators. We fix and check the Ansatz against the explicit evaluation of the component-field supergravity action in some cases."
https://arxiv.org/abs/2403.07508,2024-03-12,MoAI: Mixture of All Intelligence for Large Language and Vision Models,"['Byung-Kwan Lee', 'Beomchan Park', 'Chae Won Kim', 'Yong Man Ro']","The rise of large language models (LLMs) and instruction tuning has led to the current trend of instruction-tuned large language and vision models (LLVMs). This trend involves either meticulously curating numerous instruction tuning datasets tailored to specific objectives or enlarging LLVMs to manage vast amounts of vision language (VL) data. However, current LLVMs have disregarded the detailed and comprehensive real-world scene understanding available from specialized computer vision (CV) models in visual perception tasks such as segmentation, detection, scene graph generation (SGG), and optical character recognition (OCR). Instead, the existing LLVMs rely mainly on the large capacity and emergent capabilities of their LLM backbones. Therefore, we present a new LLVM, Mixture of All Intelligence (MoAI), which leverages auxiliary visual information obtained from the outputs of external segmentation, detection, SGG, and OCR models. MoAI operates through two newly introduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the outputs of the external CV models, the MoAI-Compressor aligns and condenses them to efficiently use relevant auxiliary visual information for VL tasks. MoAI-Mixer then blends three types of intelligence (1) visual features, (2) auxiliary features from the external CV models, and (3) language features by utilizing the concept of Mixture of Experts. Through this integration, MoAI significantly outperforms both open-source and closed-source LLVMs in numerous zero-shot VL tasks, particularly those related to real-world scene understanding such as object existence, positions, relations, and OCR without enlarging the model size or curating extra visual instruction tuning datasets."
https://arxiv.org/abs/2403.07507,2024-03-12,Reconstructions of Jupiter's magnetic field using physics informed neural networks,"['Philip W. Livermore', 'Leyuan Wu', 'Longwei Chen', 'Sjoerd A. L. de Ridder']","Magnetic sounding using data collected from the Juno mission can be used to provide constraints on Jupiter's interior. However, inwards continuation of reconstructions assuming zero electrical conductivity and a representation in spherical harmonics are limited by the enhancement of noise at small scales. In this paper we describe new reconstructions of Jupiter's internal magnetic field based on physics-informed neural networks and either the first 33 (PINN33) or the first 50 (PINN50) of Juno's orbits. The method can resolve local structures, and allows for weak ambient electrical currents. Compared with other methods, our reconstructions of Jupiter's magnetic field both on and above the surface are similar, and we achieve a similar fit to the Juno data. However, our models are not hampered by noise at depth, and so offer a much clearer picture of the interior structure. We estimate that the dynamo boundary is at a fractional radius of 0.8. At this depth, the magnetic field is arranged into longitudinal bands, and the great blue spot appears to be rooted in neighbouring structures of oppositely signed flux."
https://arxiv.org/abs/2403.07506,2024-03-12,"Robustness, Security, Privacy, Explainability, Efficiency, and Usability of Large Language Models for Code","['Zhou Yang', 'Zhensu Sun', 'Terry Zhuo Yue', 'Premkumar Devanbu', 'David Lo']","Large language models for code (LLM4Code), which demonstrate strong performance (e.g., high accuracy) in processing source code, have significantly transformed software engineering. Many studies separately investigate the non-functional properties of LM4Code, but there is no systematic review of how these properties are evaluated and enhanced. This paper fills this gap by thoroughly examining 146 relevant studies, thereby presenting the first systematic literature review to identify seven important properties beyond accuracy, including robustness, security, privacy, explainability, efficiency, and usability. We discuss the current state-of-the-art methods and trends, identify gaps in existing research, and present promising directions for future study."
https://arxiv.org/abs/2403.07505,2024-03-12,Rotational Evolution of Classical T Tauri Stars: Models and Observations,"['Javier Serna', 'Giovanni Pinzón', 'Jesús Hernández', 'Ezequiel Manzo-Martínez', 'Karina Mauco', 'Carlos G. Román-Zúñiga', 'Nuria Calvet', 'Cesar Briceño', 'Ricardo López-Valdivia', 'Marina Kounkel', 'Guy S. Stringfellow', 'Keivan G. Stassun', 'Marc Pinsonneault', 'Lucia Adame', 'Lyra Cao', 'Kevin Covey', 'Amelia Bayo', 'Alexandre Roman-Lopes', 'Christian Nitschelm', 'Richard R. Lane']","We developed a grid of stellar rotation models for low-mass and solar-type Classical T Tauri stars (CTTS) ($0.3M_{\odot}<M_{\ast}<1.2M_{\odot}$). These models incorporate the star-disk interaction and magnetospheric ejections to investigate the evolution of the stellar rotation rate as a function of the mass of the star $M_{\ast}$, the magnetic field ($B_{\ast}$), and stellar wind ($\dot{M}_{wind}$). We compiled and determined stellar parameters for 208 CTTS, such as projected rotational velocity $v\sin(i)$, mass accretion rate $\dot{M}_{acc}$, stellar mass $M_{\ast}$, ages, and estimated rotational periods using TESS data. We also estimated a representative value of the mass-loss rate for our sample using the $[\text{O}\text{ I}]$ spectral line. Our results confirm that $v\sin(i)$ measurements in CTTS agree with the rotation rates provided by our spin models in the accretion-powered stellar winds (APSW) picture. In addition, we used the Approximate Bayesian Computation (ABC) technique to explore the connection between the model parameters and the observational properties of CTTS. We find that the evolution of $v\sin(i)$ with age might be regulated by variations in (1) the intensity of $B_{\ast}$ and (2) the fraction of the accretion flow ejected in magnetic winds, removing angular momentum from these systems. The youngest stars in our sample ($\sim $1 Myr) show a median branching ratio $\dot{M}_{wind}/\dot{M}_{acc}\sim$ $0.16$ and median $B_{\ast}\sim$ 2000 G, in contrast to $\sim 0.01$ and 1000 G, respectively, for stars with ages $\gtrsim 3$ Myr."
https://arxiv.org/abs/2403.07504,2024-03-12,Two-dimensional phase diagram of the charge density wave in doped CsV$_3$Sb$_5$,"['Linwei Huai', 'Hongyu Li', 'Yulei Han', 'Yang Luo', 'Shuting Peng', 'Zhiyuan Wei', 'Jianchang Shen', 'Bingqian Wang', 'Yu Miao', 'Xiupeng Sun', 'Zhipeng Ou', 'Bo Liu', 'Xiaoxiao Yu', 'Ziji Xiang', 'Min-Quan Kuang', 'Zhenhua Qiao', 'Xianhui Chen', 'Junfeng He']","Kagome superconductors AV$_3$Sb$_5$ (A = K, Rb and Cs) have attracted much recent attention due to the coexistence of multiple exotic orders. Among them, the charge density wave (CDW) order has been shown to host various unconventional behaviors. Here, we investigate the CDW order by a combination of both bulk and surface doping methods. While element substitutions in bulk doping change both carriers and the crystal lattice, the surface doping primarily tunes the carrier concentration. As such, our results reveal a two-dimensional phase diagram of the CDW in doped CsV$_3$Sb$_5$. In the lightly bulk doped regime, the existence of CDW order is reversible by tuning the carrier concentration. But excessive bulk doping permanently destroys the CDW, regardless of the carrier doping level. These results provide insights to the origin of the CDW from both electronic and structural degrees of freedom. They also open an avenue for manipulating the exotic CDW order in Kagome superconductors."
https://arxiv.org/abs/2403.07503,2024-03-12,Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach,['Shuchang Yan'],"Hybrid electric vehicles (HEVs) are becoming increasingly popular because they can better combine the working characteristics of internal combustion engines and electric motors. However, the minimum fuel consumption of an HEV for a battery electrical balance case under a specific assembly condition and a specific speed curve still needs to be clarified in academia and industry. Regarding this problem, this work provides the mathematical expression of constrained optimal fuel consumption (COFC) from the perspective of constrained reinforcement learning (CRL) for the first time globally. Also, two mainstream approaches of CRL, constrained variational policy optimization (CVPO) and Lagrangian-based approaches, are utilized for the first time to obtain the vehicle's minimum fuel consumption under the battery electrical balance condition. We conduct case studies on the well-known Prius TOYOTA hybrid system (THS) under the NEDC condition; we give vital steps to implement CRL approaches and compare the performance between the CVPO and Lagrangian-based approaches. Our case study found that CVPO and Lagrangian-based approaches can obtain the lowest fuel consumption while maintaining the SOC balance constraint. The CVPO approach converges stable, but the Lagrangian-based approach can obtain the lowest fuel consumption at 3.95 L/100km, though with more significant oscillations. This result verifies the effectiveness of our proposed CRL approaches to the COFC problem."
https://arxiv.org/abs/2403.07502,2024-03-12,Short time asymptotics of the fundamental solutions for Schrödinger equations with non-smooth potentials,['Shun Takizawa'],"This paper deals with Schrödinger equations with potentials which are time-dependent non-smooth and at most quadratic growth. In the case where potentials are smooth with respect to spatial variables, fundamental solutions have explicit formulas in short time by D. Fujiwara. On the otherhand in the case where ones are non-smooth, we cannot expect that fundamental solutions have similar formula as above because dispersive estimates fail to hold in general. We show that a principal part of an asymptotic form of the fundamental solution has similar form as above even in the case where a potential is in $C^2$ with respect to spatial variables."
https://arxiv.org/abs/2403.07501,2024-03-12,Detecting Security-Relevant Methods using Multi-label Machine Learning,"['Oshando Johnson', 'Goran Piskachev', 'Ranjith Krishnamurthy', 'Eric Bodden']","To detect security vulnerabilities, static analysis tools need to be configured with security-relevant methods. Current approaches can automatically identify such methods using binary relevance machine learning approaches. However, they ignore dependencies among security-relevant methods, over-generalize and perform poorly in practice. Additionally, users have to nevertheless manually configure static analysis tools using the detected methods. Based on feedback from users and our observations, the excessive manual steps can often be tedious, error-prone and counter-intuitive."
https://arxiv.org/abs/2403.07500,2024-03-12,Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation,"['Likun Li', 'Haoqi Zeng', 'Changpeng Yang', 'Haozhe Jia', 'Di Xu']","The objective of personalization and stylization in text-to-image is to instruct a pre-trained diffusion model to analyze new concepts introduced by users and incorporate them into expected styles. Recently, parameter-efficient fine-tuning (PEFT) approaches have been widely adopted to address this task and have greatly propelled the development of this field. Despite their popularity, existing efficient fine-tuning methods still struggle to achieve effective personalization and stylization in T2I generation. To address this issue, we propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained fine-tuning for different blocks of SD, which can generate images faithful to input prompts and target identity and also with desired style. Extensive experiments demonstrate the effectiveness of the proposed method."
https://arxiv.org/abs/2403.07499,2024-03-12,The low-mass enhancement of kaon pairs in $B^+\to\bar{D}^{(*)0}K^+\bar{K}^0$ and $B^0\to D^{(*)-}K^+\bar{K}^0$ decays,"['Wen-Fei Wang', 'Li-Fei Yang', 'Ai-Jun Ma', 'Àngels Ramos']","Very recently, the Belle~II collaboration presented a measurement for the decays $B^+\to\bar{D}^{(*)0} K^+\bar{K}^0$ and $B^0\to D^{(*)-}K^+\bar{K}^0$, the bulk of observed $m(K^+ K_S^0)$ distributions showing low-mass structures in all four channels. In this work, we study the contributions of $ρ(770,1450)^+$, $a_2(1320)^+$ and $a_0(980,1450)^+$ resonances to these decay processes. The intermediate states $ρ(770,1450)^+$ are found to dominate the low-mass distribution of kaon pairs roughly contributing to half of the total branching fraction in each of the four decay channels. The contribution of the tensor $a_2(1320)^+$ meson is found to be negligible. Near the threshold of the kaon pair, the state $a_0(980)^+$ turns out to be much less important than expected, not being able to account for the enhancement of events in that energy region observed in the $B^+\to\bar{D}^{(*)0} K^+\bar{K}^0$ decays. Further studies both from the theoretical and experimental sides are needed to elucidate the role of the non-resonant contributions governing the formation of $K^+\bar{K}^0$ pairs near their threshold in these decay processes."
https://arxiv.org/abs/2403.07498,2024-03-12,FLAME: Fitting Lyα Absorption lines using Machine learning,"['Priyanka Jalan', 'Vikram Khaire', 'M. Vivek', 'Prakash Gaikwad']","We introduce FLAME, a machine learning algorithm designed to fit Voigt profiles to HI Lyman-alpha (Ly$α$) absorption lines using deep convolutional neural networks. FLAME integrates two algorithms: the first determines the number of components required to fit Ly$α$ absorption lines, and the second calculates the Doppler parameter $b$, the HI column density N$_{\rm HI}$, and the velocity separation of individual components. For the current version of FLAME, we trained it on low-redshift Ly$α$ forests observed with the Far Ultraviolet gratings of the Cosmic Origin Spectrograph (COS) aboard the Hubble Space Telescope (HST). Drawing on this data, we trained FLAME on $\sim$ $10^6$ simulated Voigt profiles, forward-modeled to Ly$α$ absorption lines observed with HST-COS, to classify lines as either single or double components and then determine Voigt profile fitting parameters. FLAME shows impressive accuracy on the simulated data by identifying more than 98% (90%) of single (double) component lines. It determines $b$ values within $\approx \pm{8}~(15)$ km s$^{-1}$ and log $N_{\rm HI}/ {\rm cm}^2$ values within $\approx \pm 0.3~(0.8)$ for 90% of the single (double) component lines. However, when applied to real data, FLAME's component classification accuracy drops by $\sim$ 10%. Despite this, there is a reasonable agreement between the $b$ and N$_{\rm HI}$ distributions obtained from traditional Voigt profile fitting methods and FLAME's predictions. Our mock HST-COS data analysis, designed to emulate real data parameters, demonstrated that FLAME could achieve consistent accuracy comparable to its performance with simulated data. This finding suggests that the drop in FLAME's accuracy when used on real data primarily arises from the difficulty of replicating the full complexity of real data in the training sample."
https://arxiv.org/abs/2403.07497,2024-03-12,Weyl mean equicontinuity and Weyl mean sensitivity of a random dynamical system,['Yuan Lian'],"In this article, we introduce the concepts of Weyl mean equicontinuity and Weyl mean sensitivity of a random dynamical system associated to an infinite countable discrete amenable group action. We obtain the dichotomy result to Weyl mean equicontinuity and Weyl mean sensitivity of a random dynamical system when the corresponding skew product transformation is minimal and Ωis finite."
https://arxiv.org/abs/2403.07496,2024-03-12,"Reheated Sub-40000 Kelvin Neutron Stars at the JWST, ELT, and TMT","['Nirmal Raj', 'Prajwal Shivanna', 'Gaurav Niraj Rachh']","Neutron stars cooling passively since their birth may be reheated in their late-stage evolution by a number of possible phenomena: rotochemical, vortex creep, crust cracking, magnetic field decay, or more exotic processes such as removal of neutrons from their Fermi seas (the nucleon Auger effect), baryon number-violating nucleon decay, and accretion of particle dark matter. Using Exposure Time Calculator tools, we show that reheating mechanisms imparting effective temperatures of 2000--40000 Kelvin may be uncovered with excellent sensitivities at the James Webb Space Telescope (JWST), the Extremely Large Telescope (ELT), and the Thirty Meter Telescope (TMT), with imaging instruments operating from visible-edge to near-infrared. With a day of exposure, they could constrain the reheating luminosity of a neutron star up to a distance of 500 pc, within which about $10^5$ (undiscovered) neutron stars lie. Detection in multiple filters could overconstrain a neutron star's surface temperature, distance from Earth, mass, and radius. Using publicly available catalogues of newly discovered pulsars at the FAST and CHIME radio telescopes and the Galactic electron distribution models YMW16 and NE2001, we estimate the pulsars' dispersion measure distance from Earth, and find that potentially 30$-$40 of these may be inspected for late-stage reheating within viable exposure times, in addition to a few hundred candidates already present in the ATNF catalogue. Whereas the coldest neutron star observed (PSR J2144$-$3933) has an upper limit on its effective temperature of about 33000 Kelvin with the Hubble Space Telescope, we show that the effective temperature may be constrained down to 20000 Kelvin with JWST-NIRCam, 15000 Kelvin at ELT-MICADO, and 9000 Kelvin with TMT-IRIS. Campaigns to measure thermal luminosities of old neutron stars would be transformative for astrophysics and fundamental physics."
https://arxiv.org/abs/2403.07495,2024-03-12,Tuning diagonal scale matrices for HMC,"['Jimmy Huy Tran', 'Tore Selland Kleppe']","Three approaches for adaptively tuning diagonal scale matrices for HMC are discussed and compared. The common practice of scaling according to estimated marginal standard deviations is taken as a benchmark. Scaling according to the mean log-target gradient (ISG), and a scaling method targeting that the frequency of when the underlying Hamiltonian dynamics crosses the respective medians should be uniform across dimensions, are taken as alternatives. Numerical studies suggest that the ISG method leads in many cases to more efficient sampling than the benchmark, in particular in cases with strong correlations or non-linear dependencies. The ISG method is also easy to implement, computationally cheap and would be relatively simple to include in automatically tuned codes as an alternative to the benchmark practice."
https://arxiv.org/abs/2403.07494,2024-03-12,SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM,"['Siting Zhu', 'Renjie Qin', 'Guangming Wang', 'Jiuming Liu', 'Hesheng Wang']","We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D Gaussian representation, that enables accurate 3D semantic mapping, robust camera tracking, and high-quality rendering in real-time. In this system, we incorporate semantic feature embedding into 3D Gaussian representation, which effectively encodes semantic information within the spatial layout of the environment for precise semantic scene representation. Furthermore, we propose feature-level loss for updating 3D Gaussian representation, enabling higher-level guidance for 3D Gaussian optimization. In addition, to reduce cumulative drift and improve reconstruction accuracy, we introduce semantic-informed bundle adjustment leveraging semantic associations for joint optimization of 3D Gaussian representation and camera poses, leading to more robust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates superior performance over existing dense semantic SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in novel-view semantic synthesis and 3D semantic mapping."
https://arxiv.org/abs/2403.07493,2024-03-12,Signed graphs in data sciences via communicability geometry,"['Fernando Diaz-Diaz', 'Ernesto Estrada']","Signed graphs are an emergent way of representing data in a variety of contexts were conflicting interactions exist. These include data from biological, ecological, and social systems. Here we propose the concept of communicability geometry for signed graphs, proving that metrics in this space, such as the communicability distance and angles, are Euclidean and spherical. We then apply these metrics to solve several problems in data analysis of signed graphs in a unified way. They include the partitioning of signed graphs, dimensionality reduction, finding hierarchies of alliances in signed networks as well as the quantification of the degree of polarization between the existing factions in systems represented by this type of graphs."
https://arxiv.org/abs/2403.07492,2024-03-12,P1 Error: A Combination of Absolute and Relative Errors,['Peichen Xie'],"We propose $\frac{|x-y|}{1+|y|}$, termed the ``P1 error"" or ``plus-1 error"", as a metric of errors. It equals half the harmonic mean of absolute error and relative error, effectively combining their advantages while mitigating their limitations. The P1 error approaches absolute error when $|y|$ is small, and approaches relative error when $|y|$ is large. An $ε$ P1 error indicates that $x$ is close to $y$ at a tolerance level of $ε$, in compliance with the ``isclose"" definition used in popular numerical libraries."
https://arxiv.org/abs/2403.07491,2024-03-12,Hybrid Data Management Architecture for Present Quantum Computing,"['Markus Zajac', 'Uta Störl']","Quantum computers promise polynomial or exponential speed-up in solving certain problems compared to classical computers. However, in practical use, there are currently a number of fundamental technical challenges. One of them concerns the loading of data into quantum computers, since they cannot access common databases. In this vision paper, we develop a hybrid data management architecture in which databases can serve as data sources for quantum algorithms. To test the architecture, we perform experiments in which we assign data points stored in a database to clusters. For cluster assignment, a quantum algorithm processes this data by determining the distances between data points and cluster centroids."
https://arxiv.org/abs/2403.07490,2024-03-12,Study of parameters affecting the cooling capacity of liquid jets by using OpenFoam as tool to solve the inverse heat transfer problem,"['Kaissar Nabbout', 'Martin Sommerfeld']","In this work, some of the parameters influencing the cooling capacity of a liquid jet impinging onto Inconel 718 and C45 plates were experimentally investigated. The experiment included a high-speed camera to record the dynamic of the jet during the cooling process while an infrared camera was used to record the temperature field at the opposite surface. Jets made of water and oil-in-water emulsion were analysed as well as the influence of the oil concentration. Other parameters studied here include initial temperature of the plate, nozzle-to-plate distance, nozzle diameter, jet velocity, and impinging angle. The cooling performance was analysed by solving a full 3D inverse heat transfer problem (IHTP) with the Conjugate Gradient Method (CGM) implemented in a new solver in OpenFoam. The basic organization and implementation of the solver is shown, followed by its validation with a made-up case. Finally, the growth of the wetting front was analysed for different oil concentrations and a combination of nozzle diameters and jet velocities for the same flow rate. For the latest, unexpected results emerged when comparing the wetting growth for the two plate materials."
https://arxiv.org/abs/2403.07489,2024-03-12,Spherical $p$-group complexes arising from finite groups of Lie type,['Kevin Iván Piterman'],"We show that the $p$-group complex of a finite group $G$ is homotopy equivalent to a wedge of spheres of dimension at most $n$ if $G$ contains a self-centralising normal subgroup $H$ which is isomorphic to a group of Lie type and Lie rank $n$ in characteristic $p$. If in addition every order-$p$ element of $G$ induces an inner or field automorphism on $H$, the $p$-group complex of $G$ is $G$-homotopy equivalent to a spherical complex obtained from the Tits building of $H$."
https://arxiv.org/abs/2403.07488,2024-03-12,A variational principle for entropy of a random dynamical system,['Yuan Lian'],"In this article, I give a definition of topological entropy for random dynamical systems associated to an infinite countable discrete amenable group action. I obtain a variational principle between the topological entropy and measurable fiber entropy of a random dynamical system."
https://arxiv.org/abs/2403.07487,2024-03-12,Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM,"['Zeyu Zhang', 'Akide Liu', 'Ian Reid', 'Richard Hartley', 'Bohan Zhuang', 'Hao Tang']","Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames. We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame. Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation. See project website https://steve-zeyu-zhang.github.io/MotionMamba/"
https://arxiv.org/abs/2403.07486,2024-03-12,XpertAI: uncovering model strategies for sub-manifolds,"['Simon Letzgus', 'Klaus-Robert Müller', 'Grégoire Montavon']","In recent years, Explainable AI (XAI) methods have facilitated profound validation and knowledge extraction from ML models. While extensively studied for classification, few XAI solutions have addressed the challenges specific to regression models. In regression, explanations need to be precisely formulated to address specific user queries (e.g.\ distinguishing between `Why is the output above 0?' and `Why is the output above 50?'). They should furthermore reflect the model's behavior on the relevant data sub-manifold. In this paper, we introduce XpertAI, a framework that disentangles the prediction strategy into multiple range-specific sub-strategies and allows the formulation of precise queries about the model (the `explanandum') as a linear combination of those sub-strategies. XpertAI is formulated generally to work alongside popular XAI attribution techniques, based on occlusion, gradient integration, or reverse propagation. Qualitative and quantitative results, demonstrate the benefits of our approach."
https://arxiv.org/abs/2403.07485,2024-03-12,PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates,"['Janina Schreiber', 'Pau Batlle', 'Damar Wicaksono', 'Michael Hecht']","We introduce a surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit. We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions."
https://arxiv.org/abs/2403.07484,2024-03-12,The Nikodym property and filters on $ω$,['Tomasz Żuchowski'],"For a free filter $F$ on $ω$, let $N_F=ω\cup\{p_F\}$, where $p_F\not\inω$, be equipped with the following topology: every element of $ω$ is isolated whereas all open neighborhoods of $p_F$ are of the form $A\cup\{p_F\}$ for $A\in F$. The aim of this paper is to study spaces of the form $N_F$ in the context of the Nikodym property of Boolean algebras. By $\mathcal{AN}$ we denote the class of all those ideals $\mathcal{I}$ on $ω$ such that for the dual filter $\mathcal{I}^*$ the space $N_{\mathcal{I}^*}$ carries a sequence $\langleμ_n\colon n\inω\rangle$ of finitely supported signed measures such that $\|μ_n\|\rightarrow\infty$ and $μ_n(A)\rightarrow 0$ for every clopen subset $A\subseteq N_{\mathcal{I}^*}$. We prove that $\mathcal{I}\in\mathcal{AN}$ if and only if there exists a density submeasure $\varphi$ on $ω$ such that $\varphi(ω)=\infty$ and $\mathcal{I}$ is contained in the exhaustive ideal $\mbox{Exh}(\varphi)$. Consequently, we get that if $\mathcal{I}\subseteq\mbox{Exh}(\varphi)$ for some density submeasure $\varphi$ on $ω$ such that $\varphi(ω)=\infty$ and $N_{\mathcal{I}^*}$ is homeomorphic to a subspace of the Stone space $St(\mathcal{A})$ of a given Boolean algebra $\mathcal{A}$, then $\mathcal{A}$ does not have the Nikodym property."
https://arxiv.org/abs/2403.07483,2024-03-12,A Deep Learning Approach to Diabetes Diagnosis,"['Zeyu Zhang', 'Khandaker Asif Ahmed', 'Md Rakibul Hasan', 'Tom Gedeon', 'Md Zakir Hossain']","Diabetes, resulting from inadequate insulin production or utilization, causes extensive harm to the body. Existing diagnostic methods are often invasive and come with drawbacks, such as cost constraints. Although there are machine learning models like Classwise k Nearest Neighbor (CkNN) and General Regression Neural Network (GRNN), they struggle with imbalanced data and result in under-performance. Leveraging advancements in sensor technology and machine learning, we propose a non-invasive diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization, incorporating data re-sampling and normalization for class balancing. Our method addresses existing challenges such as limited performance associated with traditional machine learning. Experimental results on three datasets show significant improvements in overall accuracy, sensitivity, and specificity compared to traditional methods. Notably, we achieve accuracies of 89.81% in Pima diabetes dataset, 75.49% in CDC BRFSS2015 dataset, and 95.28% in Mesra Diabetes dataset. This underscores the potential of deep learning models for robust diabetes diagnosis. See project website https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/"
https://arxiv.org/abs/2403.07482,2024-03-12,Linking Invariants for Valuations and Orderings on Fields,['Ido Efrat'],"The mod-2 arithmetic Milnor invariants, introduced by Morishita, provide a decomposition law for primes in canonical Galois extensions with unitriangular Galois groups, and contain the Legendre and Redei symbols as special cases. Morishita further proposed a notion of mod-q arithmetic Milnor invariants, where q is a prime power, for number fields containing the q-th roots of unity and satisfying certain class field theory assumptions. We extend this theory from the number field context to general fields, by introducing a notion of a linking invariant for discrete valuations and orderings. We further express it as a Magnus homomorphism coefficient, and relate it to Massey product elements in Galois cohomology."
https://arxiv.org/abs/2403.07481,2024-03-12,Generalised Graph Grammars for Natural Language Processing,"['Oliver Robert Fox', 'Giacomo Bergami']",This seminal paper proposes a new query language for graph matching and rewriting overcoming {the declarative} limitation of Cypher while outperforming {Neo4j} on graph matching and rewriting by at least one order of magnitude. We exploited columnar databases (KnoBAB) to represent graphs using the Generalised Semistructured Model.
https://arxiv.org/abs/2403.07480,2024-03-12,On non-tameness of the Ellis semigroup,['Johannes Kellendonk'],"The Ellis semigroup of a dynamical system $(X,T)$ is tame if every element is the limit of a sequence (as opposed to a net) of homeomorphisms coming from the $T$ action. This topological property is related to the cardinality of the semigroup. Non-tame Ellis semigroups have a cardinality which is that of the power set of the continuum $2^{\mathfrak c}$.The semigroup admits a minimal bilateral ideal and this ideal is a union of isomorphic copies of a group $\mathcal H$, the so-called structure group of $(X,T)$.  For almost automorphic systems the cardinality of $\mathcal H$ is at most $\mathfrak c$, that of the continuum. We show a partial converse for minimal $(X,T)$ with abelian $T$, namely  that the cardinality of the structure group is $2^{\mathfrak c}$ if the proximal relation is not transitive and the subgroup generated by differences of singular points in the maximal equicontinuous factor is not open.This refines the above statement about non-tame Ellis semigroups, as it locates a particular algebraic component of the latter which has such a large cardinality."
https://arxiv.org/abs/2403.07479,2024-03-12,A quantum oscillator interacting with a classical oscillator,"['Muhammad Sajjad', 'Andrea Russo', 'Maite Arcos', 'Andrzej Grudka', 'Jonathan Oppenheim']","We study a quantum oscillator interacting and back-reacting on a classical oscillator. This can be done consistently provided the quantum system decoheres, while the backreaction has a stochastic component which causes the classical system to undergo diffusion. Nonetheless the state of the quantum oscillator can remain pure conditioned on the trajectory of the classical oscillator. We solve the system using the classical-quantum path integral formulation, and investigate slow moving regimes of either the classical or quantum oscillator. Lastly, we study the correlators of this classicalquantum setup. We are able to identify the free correlators of the theory and compute the full partition function perturbatively up to second order. This serves as a toy model for a number of other systems in which one system can be treated as effectively classical, such as a scalar quantum field interacting with another field undergoing decoherence, or a system emitting radiation, one of which is treated classically."
https://arxiv.org/abs/2403.07478,2024-03-12,Towards Graph Foundation Models for Personalization,"['Andreas Damianou', 'Francesco Fabbri', 'Paul Gigioli', 'Marco De Nadai', 'Alice Wang', 'Enrico Palumbo', 'Mounia Lalmas']","In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions. In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval. In this paper, we present a graph-based foundation modeling approach tailored to personalization. Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types. To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of nodes that accommodates all item types, and construct the graph using co-interaction signals, which inherently transcend content specificity. To facilitate practical generalization, we further couple the HGNN with an adaptation mechanism based on a two-tower (2T) architecture, which also operates agnostically to content type. This multi-stage approach ensures high scalability; while the HGNN produces general purpose embeddings, the 2T component models in a continuous space the sheer size of user-item interaction data. Our comprehensive approach has been rigorously tested and proven effective in delivering recommendations across a diverse array of products within a real-world, industrial audio streaming platform."
https://arxiv.org/abs/2403.07477,2024-03-12,$M$-ary partition polynomials,['Błażej Żmija'],"Let $M=(m_{i})_{i=0}^{\infty}$ be a sequence of integers such that $m_{0}=1$ and $m_{i}\geq 2$ for $i\geq 1$. In this paper we study $M$-ary partition polynomials $(p_{M}(n,t))_{n=0}^{\infty}$ defined as the coefficient in the following power series expansion: \begin{align*} \prod_{i=0}^{\infty}\frac{1}{1-tq^{M_{i}}} = \sum_{n=0}^{\infty} p_{M}(n,t)q^{n}, \end{align*} where $M_{i}=\prod_{j=0}^{i}m_{j}$. In particular, we provide a detailed description of their rational roots and show, that all their complex roots have absolute values not greater than $2$. We also study arithmetic properties of $M$-ary partition polynomials. One of our main results says that if $n=a_{0}+a_{1}M_{1}+\cdots +a_{k}M_{k}$ is a (unique) representation such that $a_{j}\in\{0,\ldots ,m_{j+1}-1\}$ for every $j$, then \begin{align*} p_{M}(n,t)\equiv t^{a_{0}}\prod t^{a_{j}}f(a_{j}+1,t^{m_{j}-1}) \pmod{g_{k}(t)}, \end{align*} where $f(a,t):=\frac{t^{a}-1}{t-1}$ and $g_{k}(t):=\gcd \big(t^{m_{1}+m_{2}-1}f(m_{2},t^{m_{1}-1}),\ldots ,t^{m_{k}+m_{k+1}-1}f(m_{k+1},t^{m_{k}-1})\big)$. This is a polynomial generalisation of the well-known characterisation modulo $m$ of the sequence of $m$-ary partition."
https://arxiv.org/abs/2403.07476,2024-03-12,2-descent for Bloch--Kato Selmer groups and rational points on hyperelliptic curves II,['Netan Dogra'],"We give refined methods for proving finiteness of the Chabauty--Coleman--Kim set $X(\mathbb{Q}_2 )_2 $, when $X$ is a hyperelliptic curve with a rational Weierstrass point. The main developments are methods for computing Selmer conditions at $2$ and $\infty$ for the mod 2 Bloch--Kato Selmer group associated to the higher Chow group $\mathrm{CH}^2 (\mathrm{Jac}(X),1)$. As a result we show that most genus 2 curves in the LMFDB of Mordell--Weil rank 2 with exactly one rational Weierstrass point satsify $\# X(\mathbb{Q}_2 )_2 <\infty $. We also obtain a field-theoretic description of second descent on the Jacobian of a hyperelliptic curve (under some conditions)."
https://arxiv.org/abs/2403.07475,2024-03-12,Predicting the Risk of Ischemic Stroke in Patients with Atrial Fibrillation using Heterogeneous Drug-protein-disease Network-based Deep Learning,"['Zhiheng Lyu', 'Jiannan Yang', 'Zhongzhi Xu', 'Weilan Wang', 'Weibin Cheng', 'Kwok-Leung Tsui', 'Gary Tse', 'Qingpeng Zhang']","We develop a deep learning model, ABioSPATH, to predict the one-year risk of ischemic stroke (IS) in atrial fibrillation (AF) patients. The model integrates drug-protein-disease pathways and real-world clinical data of AF patients to generate the IS risk and potential pathways for each patient. The model uses a multilayer network to identify the mechanism of drug action and disease comorbidity propagation pathways. The model is tested on the Electronic Health Record (EHR) data of 7859 AF patients from 43 hospitals in Hong Kong. The model outperforms all baselines across all metrics and provides valuable molecular-level insights for clinical use. The model also highlights key proteins in common pathways and potential IS risks tied to less-studied drugs. The model only requires routinely collected data, without requiring expensive biomarkers to be tested."
https://arxiv.org/abs/2403.07474,2024-03-12,Optical chaos synchronisation in a cascaded injection experiment,"['Jules Mercadier', 'Yaya Doumbia', 'Stefan Bittner', 'Marc Sciamanna']","We experimentally study the synchronization of chaos generated by semiconductor lasers in a cascade injection configuration, i.e., a tunable master laser is used to generate chaos by optical injection in a transmitter laser that injects light into a receiver laser. Chaos synchronization between the transmitter and the receiver lasers is achieved with a correlation coefficient of 90 % for a measurement bandwidth up to 35 GHz. Two parameter regions of good synchronization are found, corresponding to the alignment of the oscillation frequencies of the receiver laser with either the transmitter laser or the master laser."
https://arxiv.org/abs/2403.07473,2024-03-12,Pedophysics: an open-source python package for soil geophysics,"['Gaston Mendoza Veirana', 'Philippe De Smedt', 'Jeroen Verhegge', 'Wim Cornelis']","This study introduces Pedophysics, an open-source Python package designed to facilitate solutions for users who work in the field of soil assessment using near-surface geophysical electromagnetic techniques. At the core of this software is the ability to translate geophysical data into specific soil properties (and vice-versa) using pedophysical models (PM). Pedophysical modelling techniques offer valuable insights into various realms including precision agriculture, soil health, resource prospecting, nutrient and land management, hydrogeology, and heritage conservation. In developing a tool for pedophysical modelling, some challenges emerged: selecting suitable PMs from the extensive literature, adapting these to specific conditions, and ensuring adequate data availability. While addressing these, we designed an automated workflow that implements robust PMs (selected after a throughout review), apply different modelling approaches based on soil characteristics and targeted properties, and employs pedotransfer functions and assumptions to integrate missing soil data into PMs. The capabilities of Pedophysics extend to handling complex scenarios such as fusing data from different instruments, incorporating continuous monitoring measurements, and soil calibration data. With these solutions, Pedophysics automates the process of deriving targeted soil and geophysical properties with state-of-art accuracy. Hereby, users can rely on Pedophysics to implement specific knowledge about pedophysical modeling. The software promotes global access to advanced soil geophysical solutions by being open-source and encouraging community contributions. Pedophysics is written in pure Python and has minimal dependencies. It can be easily installed from the Python Package Index (PyPI)."
https://arxiv.org/abs/2403.07472,2024-03-12,Imbalance-aware Presence-only Loss Function for Species Distribution Modeling,"['Robin Zbinden', 'Nina van Tiel', 'Marc Rußwurm', 'Devis Tuia']","In the face of significant biodiversity decline, species distribution models (SDMs) are essential for understanding the impact of climate change on species habitats by connecting environmental conditions to species occurrences. Traditionally limited by a scarcity of species observations, these models have significantly improved in performance through the integration of larger datasets provided by citizen science initiatives. However, they still suffer from the strong class imbalance between species within these datasets, often resulting in the penalization of rare species--those most critical for conservation efforts. To tackle this issue, this study assesses the effectiveness of training deep learning models using a balanced presence-only loss function on large citizen science-based datasets. We demonstrate that this imbalance-aware loss function outperforms traditional loss functions across various datasets and tasks, particularly in accurately modeling rare species with limited observations."
https://arxiv.org/abs/2403.07471,2024-03-12,On the nonconvexity of some push-forward constraints and its consequences in machine learning,"['Lucas de Lara', 'Mathis Deronzier', 'Alberto González-Sanz', 'Virgile Foy']","The push-forward operation enables one to redistribute a probability measure through a deterministic map. It plays a key role in statistics and optimization: many learning problems (notably from optimal transport, generative modeling, and algorithmic fairness) include constraints or penalties framed as push-forward conditions on the model. However, the literature lacks general theoretical insights on the (non)convexity of such constraints and its consequences on the associated learning problems. This paper aims at filling this gap. In a first part, we provide a range of sufficient and necessary conditions for the (non)convexity of two sets of functions: the maps transporting one probability measure to another; the maps inducing equal output distributions across distinct probability measures. This highlights that for most probability measures, these push-forward constraints are not convex. In a second time, we show how this result implies critical limitations on the design of convex optimization problems for learning generative models or group-fair predictors. This work will hopefully help researchers and practitioners have a better understanding of the critical impact of push-forward conditions onto convexity."
https://arxiv.org/abs/2403.07470,2024-03-12,DrPlanner: Diagnosis and Repair of Motion Planners Using Large Language Models,"['Yuanfei Lin', 'Chenran Li', 'Mingyu Ding', 'Masayoshi Tomizuka', 'Wei Zhan', 'Matthias Althoff']","Motion planners are essential for the safe operation of automated vehicles across various scenarios. However, no motion planning algorithm has achieved perfection in the literature, and improving its performance is often time-consuming and labor-intensive. To tackle the aforementioned issues, we present DrPlanner, the first framework designed to automatically diagnose and repair motion planners using large language models. Initially, we generate a structured description of the planner and its planned trajectories from both natural and programming languages. Leveraging the profound capabilities of large language models in addressing reasoning challenges, our framework returns repaired planners with detailed diagnostic descriptions. Furthermore, the framework advances iteratively with continuous feedback from the evaluation of the repaired outcomes. Our approach is validated using search-based motion planners; experimental results highlight the need of demonstrations in the prompt and the ability of our framework in identifying and rectifying elusive issues effectively."
https://arxiv.org/abs/2403.07469,2024-03-12,A Comprehensive Survey of 3D Dense Captioning: Localizing and Describing Objects in 3D Scenes,"['Ting Yu', 'Xiaojun Lin', 'Shuhui Wang', 'Weiguo Sheng', 'Qingming Huang', 'Jun Yu']","Three-Dimensional (3D) dense captioning is an emerging vision-language bridging task that aims to generate multiple detailed and accurate descriptions for 3D scenes. It presents significant potential and challenges due to its closer representation of the real world compared to 2D visual captioning, as well as complexities in data collection and processing of 3D point cloud sources. Despite the popularity and success of existing methods, there is a lack of comprehensive surveys summarizing the advancements in this field, which hinders its progress. In this paper, we provide a comprehensive review of 3D dense captioning, covering task definition, architecture classification, dataset analysis, evaluation metrics, and in-depth prosperity discussions. Based on a synthesis of previous literature, we refine a standard pipeline that serves as a common paradigm for existing methods. We also introduce a clear taxonomy of existing models, summarize technologies involved in different modules, and conduct detailed experiment analysis. Instead of a chronological order introduction, we categorize the methods into different classes to facilitate exploration and analysis of the differences and connections among existing techniques. We also provide a reading guideline to assist readers with different backgrounds and purposes in reading efficiently. Furthermore, we propose a series of promising future directions for 3D dense captioning by identifying challenges and aligning them with the development of related tasks, offering valuable insights and inspiring future research in this field. Our aim is to provide a comprehensive understanding of 3D dense captioning, foster further investigations, and contribute to the development of novel applications in multimedia and related domains."
https://arxiv.org/abs/2403.07468,2024-03-12,Holographic spin alignment of $J/ψ$ meson in magnetized plasma,"['Yan-Qing Zhao', 'Xin-Li Sheng', 'Si-Wen Li', 'Defu Hou']","We study the mass spectra and spin alignment of vector meson $J/ψ$ in a thermal magnetized background using the gauge/gravity duality. Utilizing a soft wall model for the QGP background and a massive vector field for the $J/ψ$ meson, we delve into the meson's spectral function and spin parameters $(λ_θ,\, λ_\varphi,\,λ_{θ\varphi})$ for different cases, assessing their response to variations in magnetic field strength, momentum, and temperature. We initially examine scenarios where a meson's momentum aligns parallel to the magnetic field in helicity frame. Our results reveal a magnetic field-induced positive $λ_θ^\text{H}$ for low meson momentum, transitioning to negative with increased momentum. As a comparison, we also study the case of momentum perpendicular to the magnetic field and find the direction of magnetic field does not affect the qualitative behavior for the $eB$-dependence of $λ_θ^\text{H}$. Moreover, we apply our model to real heavy-ion collisions for three different spin quantization directions. Further comparisons with experimental data show qualitative agreement for spin parameters $λ_θ$ and $λ_\varphi$ in the helicity and Collins-Soper frames."
https://arxiv.org/abs/2403.07467,2024-03-12,Zero-Sum Stochastic Games with Vanishing Stage Duration and Public Signals,['Ivan Novikov'],"We consider the behaviour of $λ$-discounted zero-sum games as the discount factor $λ$ tends to 0 (that is, the players are more and more patient), in the context of games with stage duration. In stochastic games with stage duration h, players act at times 0, h, 2h, ..., and the payoff and leaving probabilities are proportional to h. When h tends to 0, such games approximate games in continuous time. The asymptotic behavior of the values (when both $λ$ and h tend to 0) was already studied in the case of stochastic games with perfect observation of the state and in the state-blind case. We consider the same question for the case of stochastic games with imperfect observation of the state. In such games, players are given at each stage a public signal that depends only on the current state. Our main result states that there exists a stochastic game with public signals, with no limit value (as the discount factor $λ$ goes to 0) if stage duration is 1, but with a limit value when stage duration h and discount factor $λ$ both tend to 0. Informally speaking, it means that the limit value in discrete time does not exist, but the limit value in continuous time (i.e., when h tends to 0) exists. Such a situation is impossible in the case of stochastic games with perfect observation of the state."
https://arxiv.org/abs/2403.07466,2024-03-12,Norms in sinogram space and stability estimates for the Radon transform,"['Stefan Kindermann', 'Simon Hubmer']","We consider different norms for the Radon transform $Rf$ of a function $f$ and investigate under which conditions they can be estimated from above or below by some standard norms for $f$. We define Fourier-based norms for $Rf$ that can be related to Bessel-potential space norms for $f$. Furthermore, we define a variant of a total-variation norm for $Rf$ and provide conditions under which it is equivalent to the total-variation norm of $f$. As an illustration of potential applications of these results, we propose a novel nonlinear backprojection method for inverting the Radon transform."
https://arxiv.org/abs/2403.07465,2024-03-12,One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices,"['Marco Chilese', 'Richard Mitev', 'Meni Orenbach', 'Robert Thorburn', 'Ahmad Atamli', 'Ahmad-Reza Sadeghi']","Control-Flow Attestation (CFA) is a security service that allows an entity (verifier) to verify the integrity of code execution on a remote computer system (prover). Existing CFA schemes suffer from impractical assumptions, such as requiring access to the prover's internal state (e.g., memory or code), the complete Control-Flow Graph (CFG) of the prover's software, large sets of measurements, or tailor-made hardware. Moreover, current CFA schemes are inadequate for attesting embedded systems due to their high computational overhead and resource usage."
https://arxiv.org/abs/2403.07464,2024-03-12,On Ranking-based Tests of Independence,"['Myrto Limnios', 'Stéphan Clémençon']","In this paper we develop a novel nonparametric framework to test the independence of two random variables $\mathbf{X}$ and $\mathbf{Y}$ with unknown respective marginals $H(dx)$ and $G(dy)$ and joint distribution $F(dx dy)$, based on {\it Receiver Operating Characteristic} (ROC) analysis and bipartite ranking. The rationale behind our approach relies on the fact that, the independence hypothesis $\mathcal{H}\_0$ is necessarily false as soon as the optimal scoring function related to the pair of distributions $(H\otimes G,\; F)$, obtained from a bipartite ranking algorithm, has a ROC curve that deviates from the main diagonal of the unit square.We consider a wide class of rank statistics encompassing many ways of deviating from the diagonal in the ROC space to build tests of independence. Beyond its great flexibility, this new method has theoretical properties that far surpass those of its competitors. Nonasymptotic bounds for the two types of testing errors are established. From an empirical perspective, the novel procedure we promote in this paper exhibits a remarkable ability to detect small departures, of various types, from the null assumption $\mathcal{H}_0$, even in high dimension, as supported by the numerical experiments presented here."
https://arxiv.org/abs/2403.07463,2024-03-12,Backdoor Attack with Mode Mixture Latent Modification,"['Hongwei Zhang', 'Xiaoyin Xu', 'Dongsheng An', 'Xianfeng Gu', 'Min Zhang']","Backdoor attacks become a significant security concern for deep neural networks in recent years. An image classification model can be compromised if malicious backdoors are injected into it. This corruption will cause the model to function normally on clean images but predict a specific target label when triggers are present. Previous research can be categorized into two genres: poisoning a portion of the dataset with triggered images for users to train the model from scratch, or training a backdoored model alongside a triggered image generator. Both approaches require significant amount of attackable parameters for optimization to establish a connection between the trigger and the target label, which may raise suspicions as more people become aware of the existence of backdoor attacks. In this paper, we propose a backdoor attack paradigm that only requires minimal alterations (specifically, the output layer) to a clean model in order to inject the backdoor under the guise of fine-tuning. To achieve this, we leverage mode mixture samples, which are located between different modes in latent space, and introduce a novel method for conducting backdoor attacks. We evaluate the effectiveness of our method on four popular benchmark datasets: MNIST, CIFAR-10, GTSRB, and TinyImageNet."
https://arxiv.org/abs/2403.07462,2024-03-12,Compressed-sensing Lindbladian quantum tomography with trapped ions,"['Dmitrii Dobrynin', 'Lorenzo Cardarelli', 'Markus Müller', 'Alejandro Bermudez']","Characterizing the dynamics of quantum systems is a central task for the development of quantum information processors (QIPs). It serves to benchmark different devices, learn about their specific noise, and plan the next hardware upgrades. However, this task is also very challenging, for it requires a large number of measurements and time-consuming classical processing. Moreover, when interested in the time dependence of the noise, there is an additional overhead since the characterization must be performed repeatedly within the time interval of interest. To overcome this limitation while, at the same time, ordering the learned sources of noise by their relevance, we focus on the inference of the dynamical generators of the noisy dynamics using Lindbladian quantum tomography (LQT). We propose two different improvements of LQT that alleviate previous shortcomings. In the weak-noise regime of current QIPs, we manage to linearize the maximum likelihood estimation of LQT, turning the constrained optimization into a convex problem to reduce the classical computation cost and to improve its robustness. Moreover, by introducing compressed sensing techniques, we reduce the number of required measurements without sacrificing accuracy. To illustrate these improvements, we apply our LQT tools to trapped-ion experiments of single- and two-qubit gates, advancing in this way the previous state of the art."
https://arxiv.org/abs/2403.07461,2024-03-12,A time-adaptive finite element phase-field model suitable for rate-independent fracture mechanics,"['Felix Rörentrop', 'Samira Boddin', 'Dorothee Knees', 'Jörn Mosler']","The modeling of cracks is an important topic - both in engineering as well as in mathematics. Since crack propagation is characterized by a free boundary value problem (the geometry of the crack is not known beforehand, but part of the solution), approximations of the underlying sharp-interface problem based on phase-field models are often considered. Focusing on a rate-independent setting, these models are defined by a unidirectional gradient-flow of an energy functional. Since this energy functional is non-convex, the evolution of the variables such as the displacement field and the phase-field variable might be discontinuous in time leading to so-called brutal crack growth. For this reason, solution concepts have to be carefully chosen in order to predict discontinuities that are physically reasonable. One such concept is that of Balanced Viscosity solutions (BV solutions). This concept predicts physically sound energy trajectories that do not jump across energy barriers. The paper deals with a time-adaptive finite element phase-field model for rate-independent fracture which converges to BV solutions. The model is motivated by constraining the pseudo-velocity of the crack tip. The resulting constrained minimization problem is solved by the augmented Lagrangian method. Numerical examples highlight the predictive capabilities of the model and furthermore show the efficiency and the robustness of the final algorithm."
https://arxiv.org/abs/2403.07460,2024-03-12,Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index,"['Camila Fernandez', 'Chung Shue Chen', 'Chen Pierre Gaillard', 'Alonso Silva']","Time-to-event analysis is a branch of statistics that has increased in popularity during the last decades due to its many application fields, such as predictive maintenance, customer churn prediction and population lifetime estimation. In this paper, we review and compare the performance of several prediction models for time-to-event analysis. These consist of semi-parametric and parametric statistical models, in addition to machine learning approaches. Our study is carried out on three datasets and evaluated in two different scores (the integrated Brier score and concordance index). Moreover, we show how ensemble methods, which surprisingly have not yet been much studied in time-to-event analysis, can improve the prediction accuracy and enhance the robustness of the prediction performance. We conclude the analysis with a simulation experiment in which we evaluate the factors influencing the performance ranking of the methods using both scores."
https://arxiv.org/abs/2403.07459,2024-03-12,Localization-Delocalization Transitions in Non-Hermitian Aharonov-Bohm Cages,"['Xiang Li', 'Jin Liu', 'Tao Liu']","A unique feature of non-Hermitian systems is the extreme sensitivity of the eigenspectrum to boundary conditions with the emergence of the non-Hermitian skin effect (NHSE). A NHSE originates from the point-gap topology of complex eigenspectrum, where an extensive number of eigenstates are anomalously localized at the boundary driven by nonreciprocal dissipation. Two different approaches to create localization are disorder and flat-band spectrum, and their interplay can lead to the anomalous inverse Anderson localization, where the Bernoulli anti-symmetric disorder induce mobility in a full-flat band system in the presence of Aharonov-Bohm (AB) Cage. In this work, we study the localization-delocalization transitions due to the interplay of the point-gap topology, flat band and correlated disorder in the one-dimensional rhombic lattice, where both its Hermitian and non-Hermitian structures show AB cage in the presence of magnetic flux. Although it remains the coexistence of localization and delocalization for the Hermitian rhombic lattice in the presence of the random anti-symmetric disorder, it surprisingly becomes complete delocalization, accompanied by the emergence of NHSE. To further study the effects from the Bernoulli anti-symmetric disorder, we found the similar NHSE due to the interplay of the point-gap topology, correlated disorder and flat bands. Our anomalous localization-delocalization property can be experimentally tested in the classical physical platform, such as electrical circuit."
https://arxiv.org/abs/2403.07458,2024-03-12,Fixing Smart Contract Vulnerabilities: A Comparative Analysis of Literature and Developer's Practices,"['Francesco Salzano', 'Simone Scalabrino', 'Rocco Oliveto', 'Remo Pareschi']","Smart Contracts are programs running logic in the Blockchain network by executing operations through immutable transactions. The Blockchain network validates such transactions, storing them into sequential blocks of which integrity is ensured. Smart Contracts deal with value stakes, if a damaging transaction is validated, it may never be reverted, leading to unrecoverable losses. To prevent this, security aspects have been explored in several fields, with research providing catalogs of security defects, secure code recommendations, and possible solutions to fix vulnerabilities. In our study, we refer to vulnerability fixing in the ways found in the literature as guidelines. However, it is not clear to what extent developers adhere to these guidelines, nor whether there are other viable common solutions and what they are. The goal of our research is to fill knowledge gaps related to developers' observance of existing guidelines and to propose new and viable solutions to security vulnerabilities. To reach our goal, we will obtain from Solidity GitHub repositories the commits that fix vulnerabilities included in the DASP TOP 10 and we will conduct a manual analysis of fixing approaches employed by developers. Our analysis aims to determine the extent to which literature-based fixing strategies are followed. Additionally, we will identify and discuss emerging fixing techniques not currently documented in the literature. Through qualitative analysis, we will evaluate the suitability of these new fixing solutions and discriminate between valid approaches and potential mistakes."
https://arxiv.org/abs/2403.07457,2024-03-12,Energy bounds for weighted spherical codes and designs via linear programming,"['Sergiy Borodachov', 'Peter Boyvalenkov', 'Peter Dragnev', 'Douglas Hardin', 'Edward Saff', 'Maya Stoyanova']","Universal bounds for the potential energy of weighted spherical codes are obtained by linear programming. The universality is in the sense of Cohn-Kumar -- every attaining code is optimal with respect to a large class of potential functions (absolutely monotone), in the sense of Levenshtein -- there is a bound for every weighted code, and in the sense of parameters (nodes and weights) -- they are independent of the potential function. We derive a necessary condition for optimality (in the linear programming framework) of our lower bounds which is also shown to be sufficient when the potential is strictly absolutely monotone. Bounds are also obtained for the weighted energy of weighted spherical designs. We explore our bounds for several previously studied weighted spherical codes."
https://arxiv.org/abs/2403.07456,2024-03-12,A tutorial on multi-view autoencoders using the multi-view-AE library,"['Ana Lawry Aguila', 'Andre Altmann']","There has been a growing interest in recent years in modelling multiple modalities (or views) of data to for example, understand the relationship between modalities or to generate missing data. Multi-view autoencoders have gained significant traction for their adaptability and versatility in modelling multi-modal data, demonstrating an ability to tailor their approach to suit the characteristics of the data at hand. However, most multi-view autoencoders have inconsistent notation and are often implemented using different coding frameworks. To address this, we present a unified mathematical framework for multi-view autoencoders, consolidating their formulations. Moreover, we offer insights into the motivation and theoretical advantages of each model. To facilitate accessibility and practical use, we extend the documentation and functionality of the previously introduced \texttt{multi-view-AE} library. This library offers Python implementations of numerous multi-view autoencoder models, presented within a user-friendly framework. Through benchmarking experiments, we evaluate our implementations against previous ones, demonstrating comparable or superior performance. This work aims to establish a cohesive foundation for multi-modal modelling, serving as a valuable educational resource in the field."
https://arxiv.org/abs/2403.07455,2024-03-12,Novel Signatures of Radiation Reaction in Electron-Laser Sidescattering,"['Philipp Sikorski', 'Alec G. R. Thomas', 'Stepan S. Bulanov', 'Matt Zepf', 'Daniel Seipt']","In this article we investigate novel signatures of radiation reaction via the angular deflection of an electron beam colliding at 90 degrees with an intense laser pulse. Due to the radiation reaction effect, the electrons can be deflected towards the beam axis for plane wave backgrounds, which is not possible in the absence of radiation reaction effects. The magnitude and size of the deflection angle can be controlled by tailoring the laser pulse shapes. The effect is first derived analytically using the Landau-Lifshitz equation, which allows to determine the important scaling behavior with laser intensity and particle energy. We then move on to full scale 3D Monte Carlo simulations to verify the effect is observable with present day laser technology. We investigate the opportunities for an indirect observation of laser depletion in such side scattering scenarios."
https://arxiv.org/abs/2403.07454,2024-03-12,"Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings","['Henrik Häggström', 'Pedro L. C. Rodrigues', 'Geoffroy Oudoumanessah', 'Florence Forbes', 'Umberto Picchini']","Bayesian inference for complex models with an intractable likelihood can be tackled using algorithms performing many calls to computer simulators. These approaches are collectively known as ""simulation-based inference"" (SBI). Recent SBI methods have made use of neural networks (NN) to provide approximate, yet expressive constructs for the unavailable likelihood function and the posterior distribution. However, they do not generally achieve an optimal trade-off between accuracy and computational demand. In this work, we propose an alternative that provides both approximations to the likelihood and the posterior distribution, using structured mixtures of probability distributions. Our approach produces accurate posterior inference when compared to state-of-the-art NN-based SBI methods, while exhibiting a much smaller computational footprint. We illustrate our results on several benchmark models from the SBI literature."
https://arxiv.org/abs/2403.07453,2024-03-12,Humans-in-the-Building: Getting Rid of Thermostats for Optimal Thermal Comfort Control in Energy Management Systems,"['Jiali Wang', 'Yang Tang', 'Luca Schenato']","Given the widespread attention to individual thermal comfort, coupled with significant energy-saving potential inherent in energy management systems for optimizing indoor environments, this paper aims to introduce advanced ""Humans-in-the-building"" control techniques to redefine the paradigm of indoor temperature design. Firstly, we innovatively redefine the role of individuals in the control loop, establishing a model for users' thermal comfort and constructing discomfort signals based on individual preferences. Unlike traditional temperature-centric approaches, ""thermal comfort control"" prioritizes personalized comfort. Then, considering the diversity among users, we propose a novel method to determine the optimal indoor temperature range, thus minimizing discomfort for various users and reducing building energy consumption. Finally, the efficacy of the ""thermal comfort control"" approach is substantiated through simulations conducted using Matlab."
https://arxiv.org/abs/2403.07452,2024-03-12,A compact approach to higher-resolution resonant inelastic X-ray scattering detection using photoelectrons,"['Jan O. Schunck', 'Jens Buck', 'Robin Y. Engel', 'Simon R. Kruse', 'Simon Marotzke', 'Markus Scholz', 'Sanjoy K. Mahatha', 'Meng-Jie Huang', 'Henrik M. Rønnow', 'Georgi Dakovski', 'Moritz Hoesch', 'Matthias Kalläne', 'Kai Rossnagel', 'Martin Beye']","The detection of inelastically scattered soft X-rays with high energy resolution usually requires large grating spectrometers. Recently, photoelectron spectrometry for analysis of X-rays (PAX) has been rediscovered for modern spectroscopy experiments at synchrotron light sources. By converting scattered photons to electrons and using an electron energy analyser, the energy resolution for resonant inelastic X-ray scattering (RIXS) becomes decoupled from the X-ray spot size and instrument length. In this work, we develop PAX towards high energy resolution using a modern photoemission spectroscopy setup studying Ba2Cu3O4Cl2 at the Cu L3-edge. We measure a momentum transfer range of 24% of the first Brillouin zone simultaneously. Our results hint at the observation of a magnon excitation below 100 meV energy transfer and show intensity variations related to the dispersion of dd-excitations. With dedicated setups, PAX can become an alternative to the best and largest RIXS instruments, while at the same time opening new opportunities to acquire RIXS at a range of momentum transfers simultaneously and combine it with angle-resolved photoemission spectroscopy in a single instrument."
https://arxiv.org/abs/2403.07451,2024-03-12,"The minimum weight of the code of intersecting lines in ${\rm PG}(3,q)$","['Sam Adriaensen', 'Robin Simoens', 'Leo Storme']","We characterise the minimum weight codewords of the $p$-ary linear code of intersecting lines in ${\rm PG}(3,q)$, $q=p^h$, $q\geq19$, $p$ prime, $h\geq 1$. If $q$ is even, the minimum weight equals $q^3+q^2+q+1$. If $q$ is odd, the minimum weight equals $q^3+2q^2+q+1$. For $q$ even, we also characterise the codewords of second smallest weight."
https://arxiv.org/abs/2403.07450,2024-03-12,Measuring Data Similarity for Efficient Federated Learning: A Feasibility Study,"['Fernanda Famá', 'Charalampos Kalalas', 'Sandra Lagen', 'Paolo Dini']","In multiple federated learning schemes, a random subset of clients sends in each round their model updates to the server for aggregation. Although this client selection strategy aims to reduce communication overhead, it remains energy and computationally inefficient, especially when considering resource-constrained devices as clients. This is because conventional random client selection overlooks the content of exchanged information and falls short of providing a mechanism to reduce the transmission of semantically redundant data. To overcome this challenge, we propose clustering the clients with the aid of similarity metrics, where a single client from each of the formed clusters is selected in each round to participate in the federated training. To evaluate our approach, we perform an extensive feasibility study considering the use of nine statistical metrics in the clustering process. Simulation results reveal that, when considering a scenario with high data heterogeneity of clients, similarity-based clustering can reduce the number of required rounds compared to the baseline random client selection. In addition, energy consumption can be notably reduced from 23.93% to 41.61%, for those similarity metrics with an equivalent number of clients per round as the baseline random scheme."
https://arxiv.org/abs/2403.07449,2024-03-12,Single-Switch Transformer-less Power Supply for Low Temperature Plasma Jet -- 3.3 kV SiC MOSFET opportunities,"['David Florez', 'Hubert Piquet', 'Eric Bru', 'Rafael Diez']","This work presents a simple power converter, without any high voltage transformer, able to supply and control a plasma jet based on dielectric barrier discharge. The converter, operating in pulsed current mode, requires a single power switch and is fed by a low voltage DC source. It can deliver very short duration pulses to the plasma jet with high current amplitude. The operating principle is explained by means of the state plane analysis and is validated with simulations and experimental results. The equations provided allow for the calculation during the design stage of important characteristics of the plasma jet as the peak voltage and the duration of the pulses. The power can be easily adjusted during experimentation to comply with the desired appearance of the plasma jet."
https://arxiv.org/abs/2403.07448,2024-03-12,Cuprate-like Electronic Structures in Infinite-Layer Nickelates with 3D dispersion,"['X. Ding', 'Y. Fan', 'X. X. Wang', 'C. H. Li', 'Z. T. An', 'J. H. Ye', 'S. L. Tang', 'M. Y. N. Lei', 'X. T. Sun', 'N. Guo', 'Z. H. Chen', 'S. Sangphet', 'Y. L. Wang', 'H. C. Xu', 'R. Peng', 'D. L. Feng']","The discovery of superconductivity in the infinite-layer (IL) nickelates provides a new platform and angle of view to study the long-standing problem of high temperature superconductivity . Many models were proposed to understand its superconducting mechanisms based on the calculated electronic structure, and the multiple Fermi surfaces and multiple orbitals involved create complications and controversial conclusions. Over the past 5 years, the lack of direct measurements of the electronic structure has hindered the understanding of nickelate superconductors. Here, we fill this gap by preparing IL LaNiO$_2$ and La$_{0.8}$Ca$_{0.2}$NiO$_2$ thin films with superior surface quality and measuring their electronic structure by angle-resolved photoemission spectroscopy (ARPES).The Fermi surface consists of a large three-dimensional hole pocket primarily contributed by Ni-3$d_{x^2-y^2}$ states, and a small electron pocket at the Brillouin zone (BZ) corner. The hole pocket exhibits a two-dimensional character over approximately 80% of the Brillouin zone, and its Fermi surface topology and band dispersion closely resemble those observed in hole-doped cuprates, suggesting their superconducting mechanisms may be alike. Yet this hole pocket shows strong three-dimensional character near $k_z=π$, which deviates from previous calculations and adds new facets to the superconductivity in IL nickelates. The experimental electronic structure represents a pivotal step toward a microscopic understanding of the IL nickelate family and its superconductivity."
https://arxiv.org/abs/2403.07447,2024-03-12,Ab-initio variational wave functions for the time-dependent many-electron Schrödinger equation,"['Jannes Nys', 'Gabriel Pescia', 'Giuseppe Carleo']","Describing the dynamics of many-electron quantum systems is crucial for applications such as predicting electronic structures in quantum chemistry, the properties of condensed matter systems, and the behaviors of complex materials. However, the real-time evolution of non-equilibrium quantum electronic systems poses a significant challenge for theoretical and computational approaches, due to the system's exploration of a vast configuration space. This work introduces a variational approach for fermionic time-dependent wave functions, surpassing mean-field approximations by capturing many-body correlations. The proposed methodology involves parameterizing the time-evolving quantum state, enabling the approximation of the state's evolution. To account for electron correlations, we employ time-dependent Jastrow factors and backflow transformations. We also show that we can incorporate neural networks to parameterize these functions. The time-dependent variational Monte Carlo technique is employed to efficiently compute the optimal time-dependent parameters. The approach is demonstrated in three distinct systems: the solvable harmonic interaction model, the dynamics of a diatomic molecule in intense laser fields, and a quenched quantum dot. In all cases, we show clear signatures of many-body correlations in the dynamics not captured by mean-field methods. The results showcase the ability of our variational approach to accurately capture the time evolution of quantum states, providing insight into the quantum dynamics of interacting electronic systems, beyond the capabilities of mean-field."
https://arxiv.org/abs/2403.07446,2024-03-12,Compact structures for single-beam magneto-optical trapping of ytterbium,"['Julian Pick', 'Roman Schwarz', 'Jens Kruse', 'Christian Lisdat', 'Carsten Klempt']","Today's best optical lattice clocks are based on the spectroscopy of trapped alkaline-earth-like atoms such as ytterbium and strontium atoms. The development towards mobile or even space-borne clocks necessitates concepts for the compact laser-cooling and trapping of these atoms with reduced laser requirements. Here we present two compact and robust achromatic mirror structures for single-beam magneto-optical trapping of alkaline-earth-like atoms using two widely separated optical cooling frequencies. We have compared the trapping and cooling performance of a monolithic aluminium structure that generates a conventional trap geometry to a quasi-planar platform based on a periodic mirror structure for different isotopes of Yb. Compared to prior work with strontium in non-conventional traps, where only bosons were trapped on a narrow line transition, we demonstrate two-stage cooling and trapping of a fermionic alkaline-earth-like isotope in a single-beam quasi-planar structure."
https://arxiv.org/abs/2403.07445,2024-03-12,The fourth-order Schrödinger equation on lattices,['Jiawei Cheng'],"In this paper, we study the fourth-order Schrödinger equation \begin{equation*}"
https://arxiv.org/abs/2403.07444,2024-03-12,A Survey on Federated Learning in Intelligent Transportation Systems,"['Rongqing Zhang', 'Hanqiu Wang', 'Bing Li', 'Xiang Cheng', 'Liuqing Yang']","The development of Intelligent Transportation System (ITS) has brought about comprehensive urban traffic information that not only provides convenience to urban residents in their daily lives but also enhances the efficiency of urban road usage, leading to a more harmonious and sustainable urban life. Typical scenarios in ITS mainly include traffic flow prediction, traffic target recognition, and vehicular edge computing. However, most current ITS applications rely on a centralized training approach where users upload source data to a cloud server with high computing power for management and centralized training. This approach has limitations such as poor real-time performance, data silos, and difficulty in guaranteeing data privacy. To address these limitations, federated learning (FL) has been proposed as a promising solution. In this paper, we present a comprehensive review of the application of FL in ITS, with a particular focus on three key scenarios: traffic flow prediction, traffic target recognition, and vehicular edge computing. For each scenario, we provide an in-depth analysis of its key characteristics, current challenges, and specific manners in which FL is leveraged. Moreover, we discuss the benefits that FL can offer as a potential solution to the limitations of the centralized training approach currently used in ITS applications."
https://arxiv.org/abs/2403.07443,2024-03-12,Magnetic Phase Diagram and Skyrmions of the Hubbard Model on the Beta-Mn Type Lattice,['Yoshiro Kakehashi'],"Magnetic phase diagram for the Hubbard model on the Beta-Mn type lattice has been calculated as a function of the Coulomb interaction energy parameter U and the electron number per atom n by using the generalized Hartree-Fock approximation combined with the recursion method for electronic-structure calculations. The ferromagnetic state, the ferrimagnetic state, and the helimagnetic state as well as the 3Q multiple spin density waves (3QMSDW) states have been obtained on the U-n plane. Their detailed structures are examined with use of the Fourier analysis. It is shown that the calculated phase diagram and the Dzyaloshinskii-Moriya interaction elucidate the magnetic interactions for the itinerant-electron skyrmions in transition metal alloys and compounds on the Beta-Mn type lattice."
https://arxiv.org/abs/2403.07442,2024-03-12,Proxy Methods for Domain Adaptation,"['Katherine Tsai', 'Stephen R. Pfohl', 'Olawale Salaudeen', 'Nicole Chiou', 'Matt J. Kusner', ""Alexander D'Amour"", 'Sanmi Koyejo', 'Arthur Gretton']","We study the problem of domain adaptation under distribution shift, where the shift is due to a change in the distribution of an unobserved, latent variable that confounds both the covariates and the labels. In this setting, neither the covariate shift nor the label shift assumptions apply. Our approach to adaptation employs proximal causal learning, a technique for estimating causal effects in settings where proxies of unobserved confounders are available. We demonstrate that proxy variables allow for adaptation to distribution shift without explicitly recovering or modeling latent variables. We consider two settings, (i) Concept Bottleneck: an additional ''concept'' variable is observed that mediates the relationship between the covariates and labels; (ii) Multi-domain: training data from multiple source domains is available, where each source domain exhibits a different distribution over the latent confounder. We develop a two-stage kernel estimation approach to adapt to complex distribution shifts in both settings. In our experiments, we show that our approach outperforms other methods, notably those which explicitly recover the latent confounder."
https://arxiv.org/abs/2403.07441,2024-03-12,Exploring the Nuclear Shape Phase Transition in Ultra-Relativistic $^{129}$Xe+$^{129}$Xe Collisions at the LHC,"['Shujun Zhao', 'Hao-jie Xu', 'You Zhou', 'Yu-Xin Liu', 'Huichao Song']","The shape phase transition for certain isotope or isotone chains, associated with the quantum phase transition of finite nuclei, is an intriguing phenomenon in nuclear physics. A notable case is the Xe isotope chain, where the structure transits from a $γ$-soft rotor to a spherical vibrator, with the second-order shape phase transition occurring in the vicinity of $^{128-130}$Xe. In this letter, we focus on investigating the $γ$-soft deformation of $^{129}$Xe associated with the second-order shape phase transition by constructing novel correlators for ultra-relativistic $^{129}$Xe+$^{129}$Xe collisions. In particular, our iEBE-VISHNU model calculations show that the $v_2^2-[p_T]$ correlation $ρ_{2}$ and the mean transverse momentum fluctuation $Γ_{p_T}$, which were previously interpreted as the evidence for the rigid triaxial deformation of $^{129}$Xe, can also be well explained by the $γ$-soft deformation of $^{129}$Xe. We also propose two novel correlators $ρ_{4,2}$ and $ρ_{2,4}$, which carry non-trivial higher-order correlations and show unique capabilities to distinguish between the $γ$-soft and the rigid triaxial deformation of $^{129}$Xe in $^{129}$Xe+$^{129}$Xe collisions at the LHC. The present study also provides a novel way to explore the second-order shape phase transition of finite nuclei with ultra-relativistic heavy ion collisions."
https://arxiv.org/abs/2403.07440,2024-03-12,Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning,"['Yao Liang', 'Yuwei Wang', 'Yi Zeng']","Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameterization method for efficient fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA). MTLoRA aims to dynamically alter its spatial geometric structure by applying a transformation-matrix T to perform linear transformations, such as rotation, scaling, and translation, on the task-specific parameter matrix, generating new matrix feature patterns (eigenvectors) to mimic the fundamental influence of complex geometric structure feature patterns in the brain on functions, thereby enhancing the model's performance in downstream tasks. In Natural Language Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and the results reveal that MTLoRA achieves an overall performance increase of about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks, MTLoRA improves performance by an average of 0.95% and 0.31% in the DART and WebNLG tasks, respectively."
https://arxiv.org/abs/2403.07439,2024-03-12,Renewal theorems in a periodic environment,['Quentin Cormier'],"We study a renewal problem within a periodic environment, departing from the classical renewal theory by relaxing the assumption of independent and identically distributed inter-arrival times. Instead, the conditional distribution of the next arrival time, given the current one, is governed by a periodic kernel, denoted as $H$. The periodicity property of $H$ is expressed as $\mathbb{P}(T_{k+1} > t ~ |~ T_k) = H(t, T_k)$, where $H(t+T,s+T) = H(t, s)$. For a fixed time $t$, we define $N_t$ as the count of events occurring up to time $t$. The focus is on two temporal aspects: $Y_t$, the time elapsed since the last event, and $X_t$, the time until the next event occurs, given by $Y_t = t - T_{N_t}$ and $X_t = T_{N_{t}+1} - t$. The study explores the long-term behavior of the distributions of $X_t$ and $Y_t$."
https://arxiv.org/abs/2403.07438,2024-03-12,Experimental analysis of the TRC benchmark system,"['Arati Bhattu', 'Svenja Hermann', 'Nidhal Jamia', 'Florian Müller', 'Maren Scheel', 'Christoph Schwingshackl', 'H. Nevzat Özgüven', 'Malte Krack']","The Tribomechadynamics Research Challenge (TRC) was a blind prediction of the vibration behavior of a thin plate clamped on two sides using bolted joints. The first bending mode's natural frequency and damping ratio were requested as function of the amplitude, starting from the linear regime until high levels, where both frictional contact and nonlinear bending-stretching coupling become relevant. The predictions were confronted with experimental results in a companion paper; the present article addresses the experimental analysis of this benchmark system. Amplitude-dependent modal data was obtained from phase resonance and response controlled tests. An original variant of response controlled testing is proposed: Instead of a fixed frequency interval, a fixed phase interval is analyzed. This way, the high excitation levels required outside resonance, which could activate unwanted exciter nonlinearity, are avoided. Consistency of testing methods is carefully analyzed. Overall, these measures have permitted to gain high confidence in the acquired modal data. The different sources of the remaining uncertainty were further analyzed. A low reassembly-variability but a moderate time-variability were identified, where the latter is attributed to some thermal sensitivity of the system. Two nominally identical plates were analyzed, which both have an appreciable initial curvature, and a significant effect on the vibration behavior was found depending on whether the plate is aligned/misaligned with the support structure. Further, a 1:2 nonlinear modal interaction with the first torsion mode was observed, which only occurs in the aligned configurations."
https://arxiv.org/abs/2403.07437,2024-03-12,Category-Agnostic Pose Estimation for Point Clouds,"['Bowen Liu', 'Wei Liu', 'Siang Chen', 'Pengwei Xie', 'Guijin Wang']","The goal of object pose estimation is to visually determine the pose of a specific object in the RGB-D input. Unfortunately, when faced with new categories, both instance-based and category-based methods are unable to deal with unseen objects of unseen categories, which is a challenge for pose estimation. To address this issue, this paper proposes a method to introduce geometric features for pose estimation of point clouds without requiring category information. The method is based only on the patch feature of the point cloud, a geometric feature with rotation invariance. After training without category information, our method achieves as good results as other category-based methods. Our method successfully achieved pose annotation of no category information instances on the CAMERA25 dataset and ModelNet40 dataset."
https://arxiv.org/abs/2403.07436,2024-03-12,JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection,"['Hanyu Zhou', 'Zhiwei Shi', 'Hao Dong', 'Shihan Peng', 'Yi Chang', 'Luxin Yan']","Event-based moving object detection is a challenging task, where static background and moving object are mixed together. Typically, existing methods mainly align the background events to the same spatial coordinate system via motion compensation to distinguish the moving object. However, they neglect the potential spatial tailing effect of moving object events caused by excessive motion, which may affect the structure integrity of the extracted moving object. We discover that the moving object has a complete columnar structure in the point cloud composed of motion-compensated events along the timestamp. Motivated by this, we propose a novel joint spatio-temporal reasoning method for event-based moving object detection. Specifically, we first compensate the motion of background events using inertial measurement unit. In spatial reasoning stage, we project the compensated events into the same image coordinate, discretize the timestamp of events to obtain a time image that can reflect the motion confidence, and further segment the moving object through adaptive threshold on the time image. In temporal reasoning stage, we construct the events into a point cloud along timestamp, and use RANSAC algorithm to extract the columnar shape in the cloud for peeling off the background. Finally, we fuse the results from the two reasoning stages to extract the final moving object region. This joint spatio-temporal reasoning framework can effectively detect the moving object from motion confidence and geometric structure. Moreover, we conduct extensive experiments on various datasets to verify that the proposed method can improve the moving object detection accuracy by 13\%."
https://arxiv.org/abs/2403.07435,2024-03-12,Broadened-beam Uniform Rectangular Array Coefficient Design in LEO SatComs Under Quality of Service and Constant Modulus Constraints,"['Weiting Lin', 'Yuchieh Wu', 'Borching Su']","Satellite communications (SatComs) are anticipated to provide global Internet access. Low Earth orbit (LEO) satellites (SATs) have the advantage of providing higher downlink capacity owing to their smaller link budget compared with medium Earth orbit (MEO) and geostationary Earth orbit (GEO) SATs. In this paper, beam-broadening algorithms for uniform rectangular arrays (URAs) in LEO SatComs were studied. The proposed method is the first of its kind that jointly considers the path loss variation from SAT to user terminal (UT) due to the Earth's curvature to guarantee quality of service (QoS) inspired by the synthesis of isoflux radiation patterns in the literature, constant modulus constraint (CMC) favored for maximizing power amplifier (PA) efficiency, and out-of-beam radiation suppression to avoid interference. A URA design problem is formulated and decomposed into two uniform linear array (ULA) design subproblems utilizing the idea of Kronecker product beamforming to reduce the computational complexity of designing URA.The non-convex ULA subproblems are solved by a convex iterative algorithm. Simulation results reveal the advantages of the proposed method for suppressing out-of-beam radiation and achieving design criteria. In addition, channel capacity evaluation is carried out and shows that the proposed ``broadened-beam"" beamformers can offer capacities that are at least four times greater than ``narrow-beam"" beamformers employing an array steering vector when beam transition time is taken into account. The proposed method holds potential for LEO broadcasting applications such as digital video broadcasting (DVB)."
https://arxiv.org/abs/2403.07434,2024-03-12,DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated MR Images,"['Michael Götz', 'Christian Weber', 'Franciszek Binczyk', 'Joanna Polanska', 'Rafal Tarnawski', 'Barbara Bobek-Billewicz', 'Ullrich Köthe', 'Jens Kleesiek', 'Bram Stieltjes', 'Klaus H. Maier-Hein']","We propose a new method that employs transfer learning techniques to effectively correct sampling selection errors introduced by sparse annotations during supervised learning for automated tumor segmentation. The practicality of current learning-based automated tissue classification approaches is severely impeded by their dependency on manually segmented training databases that need to be recreated for each scenario of application, site, or acquisition setup. The comprehensive annotation of reference datasets can be highly labor-intensive, complex, and error-prone. The proposed method derives high-quality classifiers for the different tissue classes from sparse and unambiguous annotations and employs domain adaptation techniques for effectively correcting sampling selection errors introduced by the sparse sampling. The new approach is validated on labeled, multi-modal MR images of 19 patients with malignant gliomas and by comparative analysis on the BraTS 2013 challenge data sets. Compared to training on fully labeled data, we reduced the time for labeling and training by a factor greater than 70 and 180 respectively without sacrificing accuracy. This dramatically eases the establishment and constant extension of large annotated databases in various scenarios and imaging setups and thus represents an important step towards practical applicability of learning-based approaches in tissue classification."
https://arxiv.org/abs/2403.07433,2024-03-12,Baryonic Vortex Phase and Magnetic Field Generation in QCD with Isospin and Baryon Chemical Potentials,"['Zebin Qiu', 'Muneto Nitta']","We propose a novel baryonic vortex phase in low energy dense QCD with finite baryon and isospin chemical potentials. It is known that the homogeneous charged pion condensate takes up the ground state in the presence of the isospin chemical potential, and therein arises the Abrikosov vortex lattice with an applied magnetic field. We first demonstrate that a vortex sustaining the same quantized magnetic flux as the conventional Abrikosov vortex, carries a baryon number captured by the third homotopy group of Skyrmions, given the added modulation of the neutral pion inside the vortex core. Such a state is therefore dubbed the baryonic vortex. We reveal that when the baryon chemical potential is above a critical value, the baryonic vortex has negative tension measured from the charged pion condensation. It implies the baryonic vortex phase would emerge spontaneously without any external magnetic field, and take over the ground state at high density. This new phase contributes to the comprehension of QCD phase diagram and could be relevant to the generation of magnetic fields inside neutron stars."
https://arxiv.org/abs/2403.07432,2024-03-12,Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow,"['Hanyu Zhou', 'Yi Chang', 'Zhiwei Shi', 'Luxin Yan']","Single RGB or LiDAR is the mainstream sensor for the challenging scene flow, which relies heavily on visual features to match motion features. Compared with single modality, existing methods adopt a fusion strategy to directly fuse the cross-modal complementary knowledge in motion space. However, these direct fusion methods may suffer the modality gap due to the visual intrinsic heterogeneous nature between RGB and LiDAR, thus deteriorating motion features. We discover that event has the homogeneous nature with RGB and LiDAR in both visual and motion spaces. In this work, we bring the event as a bridge between RGB and LiDAR, and propose a novel hierarchical visual-motion fusion framework for scene flow, which explores a homogeneous space to fuse the cross-modal complementary knowledge for physical interpretation. In visual fusion, we discover that event has a complementarity (relative v.s. absolute) in luminance space with RGB for high dynamic imaging, and has a complementarity (local boundary v.s. global shape) in scene structure space with LiDAR for structure integrity. In motion fusion, we figure out that RGB, event and LiDAR are complementary (spatial-dense, temporal-dense v.s. spatiotemporal-sparse) to each other in correlation space, which motivates us to fuse their motion correlations for motion continuity. The proposed hierarchical fusion can explicitly fuse the multimodal knowledge to progressively improve scene flow from visual space to motion space. Extensive experiments have been performed to verify the superiority of the proposed method."
https://arxiv.org/abs/2403.07431,2024-03-12,Knowledge Transfer across Multiple Principal Component Analysis Studies,"['Zeyu Li', 'Kangxiang Qin', 'Yong He', 'Wang Zhou', 'Xinsheng Zhang']","Transfer learning has aroused great interest in the statistical community. In this article, we focus on knowledge transfer for unsupervised learning tasks in contrast to the supervised learning tasks in the literature. Given the transferable source populations, we propose a two-step transfer learning algorithm to extract useful information from multiple source principal component analysis (PCA) studies, thereby enhancing estimation accuracy for the target PCA task. In the first step, we integrate the shared subspace information across multiple studies by a proposed method named as Grassmannian barycenter, instead of directly performing PCA on the pooled dataset. The proposed Grassmannian barycenter method enjoys robustness and computational advantages in more general cases. Then the resulting estimator for the shared subspace from the first step is further utilized to estimate the target private subspace in the second step. Our theoretical analysis credits the gain of knowledge transfer between PCA studies to the enlarged eigenvalue gap, which is different from the existing supervised transfer learning tasks where sparsity plays the central role. In addition, we prove that the bilinear forms of the empirical spectral projectors have asymptotic normality under weaker eigenvalue gap conditions after knowledge transfer. When the set of informativesources is unknown, we endow our algorithm with the capability of useful dataset selection by solving a rectified optimization problem on the Grassmann manifold, which in turn leads to a computationally friendly rectified Grassmannian K-means procedure. In the end, extensive numerical simulation results and a real data case concerning activity recognition are reported to support our theoretical claims and to illustrate the empirical usefulness of the proposed transfer learning methods."
https://arxiv.org/abs/2403.07430,2024-03-12,Spatially oscillating correlation functions in $\left(2+1\right)$-dimensional four-fermion models: The mixing of scalar and vector modes at finite density,['Marc Winstel'],"In this work, we demonstrate that the mixing of scalar and vector condensates produces spatially oscillating, but exponentially damped correlation functions in fermionic theories at finite density and temperature. We find a regime exhibiting this oscillatory behavior in a Gross-Neveu-type model that also features vector interactions within the mean-field approximation. The existence of this regime aligns with expectations based on symmetry arguments, that are also applicable to QCD at finite baryon density. We compute the phase diagram including both homogeneous phases and regions with spatially oscillating, exponentially damped correlation functions at finite temperature and chemical potential for different strengths of the vector coupling. Furthermore, we find that inhomogeneous condensates are disfavored compared to homogeneous ones akin to previous findings without vector interactions. We show that our results are valid for a broad class of $\left(2+1\right)$-dimensional models with local four-fermion interactions."
https://arxiv.org/abs/2403.07429,2024-03-12,Dual orthogonally-polarized lasing assisted by imaginary Fermi arcs in organic microcavities,"['Teng Long', 'Jiahuan Ren', 'Peng Li', 'Feng Yun', 'Guillaume Malpuech', 'Dmitry Solnyshkov', 'Hongbing Fu', 'Feng Li', 'Qing Liao']","The polarization control of micro/nano lasers is an important topic in nanophotonics. Up to now, the simultaneous generation of two distinguishable orthogonally-polarized lasing modes from a single organic microlaser remains a critical challenge. Here, we demonstrate simultaneously orthogonally-polarized dual lasing from a microcavity filled with an organic single crystal exhibiting selective strong coupling. We show that the non-Hermiticity due to polarization-dependent losses leads to the formation of real and imaginary Fermi arcs with exceptional points. Simultaneous orthogonally-polarized lasing becomes possible thanks to the eigenstate mixing by the photonic spin-orbit coupling at the imaginary Fermi arcs. Our work provides a novel way to develop linearly-polarized lasers and paves the way for the future fundamental research in topological photonics, non-Hermitian optics, and other fields."
https://arxiv.org/abs/2403.07428,2024-03-12,Input Data Adaptive Learning (IDAL) for Sub-acute Ischemic Stroke Lesion Segmentation,"['Michael Götz', 'Christian Weber', 'Christoph Kolb', 'Klaus Maier-Hein']","In machine learning larger databases are usually associated with higher classification accuracy due to better generalization. This generalization may lead to non-optimal classifiers in some medical applications with highly variable expressions of pathologies. This paper presents a method for learning from a large training base by adaptively selecting optimal training samples for given input data. In this way heterogeneous databases are supported two-fold. First, by being able to deal with sparsely annotated data allows a quick inclusion of new data set and second, by training an input-dependent classifier. The proposed approach is evaluated using the SISS challenge. The proposed algorithm leads to a significant improvement of the classification accuracy."
https://arxiv.org/abs/2403.07427,2024-03-12,White dwarf systems: exoplanets and debris disks,['Uri Malamud'],"Although there is abundant and diverse observational evidence in support of white dwarf stars hosting planets or debris disks which form in the catastrophic destruction of various planetary bodies, the key processes that explain these observations are still being intensely investigated. The study of white dwarf planetary systems offers a unique perspective on exo-solar composition, that cannot be obtained by any other means. This chapter describes the various observational techniques that are used in order to find and characterize exo-planets and debris disks around white dwarfs. In turn, it discusses how to theoretically interpret these observations by surveying an array of various research tools and models currently employed in this field."
https://arxiv.org/abs/2403.07426,2024-03-12,Self-phoretic oscillatory motion in a harmonic trap,"['A. Alexandre', 'L. Anderson', 'T. Collin-Dufresne', 'T. Guérin', 'D. S. Dean']","We consider the motion of a harmonically trapped overdamped particle, which is submitted to a self-phoretic force, that is proportional to the gradient of a diffusive field for which the particle itself is the source. In agreement with existing results for free particles or particles in a bounded domain, we find that the system exhibits a transition between an immobile phase, where the particle stays at the center of the trap, and an oscillatory state. We perform an exact analysis giving access to the bifurcation threshold, as well as the frequency of oscillations and their amplitude near the threshold. Our analysis also characterizes the shape of two-dimensional oscillations, that take place along a circle or a straight line. Our results are confirmed by numerical simulations."
https://arxiv.org/abs/2403.07425,2024-03-12,"Modulational instability of nonuniformly damped, broad-banded waves: applications to waves in sea-ice","['Raphael Stuhlmeier', 'Conor Heffernan', 'Alberto Alberello', 'Emilian Părău']","This paper sets out to explore the modulational (or Benjamin-Feir) instability of a monochromatic wave propagating in the presence of damping such as that induced by sea-ice on the ocean surface. The fundamental wave motion is modelled using the spatial Zakharov equation, to which either uniform or non-uniform (frequency dependent) damping is added. By means of mode truncation the spatial analogue of the classical Benjamin-Feir instability can be studied analytically using dynamical systems techniques. The formulation readily yields the free surface envelope, giving insight into the physical implications of damping on the modulational instability. The evolution of an initially unstable mode is also studied numerically by integrating the damped, spatial Zakharov equation, in order to complement the analytical theory. This sheds light on the effects of damping on spectral broadening arising from this instability."
https://arxiv.org/abs/2403.07424,2024-03-12,Automated Discovery of Anomalous Features in Ultra-Large Planetary Remote Sensing Datasets using Variational Autoencoders,"['Adam Lesnikowski', 'Valentin T. Bickel', 'Daniel Angerhausen']","The NASA Lunar Reconnaissance Orbiter (LRO) has returned petabytes of lunar high spatial resolution surface imagery over the past decade, impractical for humans to fully review manually. Here we develop an automated method using a deep generative visual model that rapidly retrieves scientifically interesting examples of LRO surface imagery representing the first planetary image anomaly detector. We give quantitative experimental evidence that our method preferentially retrieves anomalous samples such as notable geological features and known human landing and spacecraft crash sites. Our method addresses a major capability gap in planetary science and presents a novel way to unlock insights hidden in ever-increasing remote sensing data archives, with numerous applications to other science domains. We publish our code and data along with this paper."
https://arxiv.org/abs/2403.07423,2024-03-12,On the locomotion of the slider within a self-adaptive beam-slider system,"['Florian Müller', 'Malte Krack']","A beam-slider system is considered whose passive self-adaption relies on an intricate locomotion process involving both frictional and unilateral contact. The system also exploits geometric nonlinearity to achieve broadband efficacy. The dynamics of the system take place on three distinct time scales: On the fast time scale of the harmonic base excitation are the vibrations and the locomotion cycle. On the slow time scale, the slider changes its position along the beam, and the overall vibration level varies. Finally, on an intermediate time scale, strong modulations of the vibration amplitude may take place. In the present work, first, an analytical approximation of the beam's response on the slow time scale is derived as function of the slider position, which is a crucial prerequisite for identifying the main drivers of the slider's locomotion. Then, the most important forms of locomotion are described and approximations of their individual contribution to the overall slider transport are estimated. Finally, the theoretical results are compared against numerical results obtained from an experimentally validated model."
https://arxiv.org/abs/2403.07422,2024-03-12,Persistent Upflows and Downflows at Active Region boundaries Observed by SUTRI and AIA,"['Yuchuan Wu', 'Zhenyong Hou', 'Wenxian Li', 'Xianyong Bai', 'Yongliang Song', 'Xiao Yang', 'Ziyao Hu', 'Yuanyong Deng', 'Kaifan Ji']","Upflows and downflows at active region (AR) boundaries have been frequently observed with spectroscopic observations at extreme ultraviolet (EUV) passbands. In this paper, we report the coexistence of upflows and downflows at the AR boundaries with imaging observations from the Solar Upper Transition Region Imager (SUTRI) and the Atmospheric Imaging Assembly (AIA). With their observations from 2022 September 21 to 2022 September 30, we find 17 persistent opposite flows occurring along the AR coronal loops. The upflows are prominent in the AIA 193 Åimages with a velocity of 50-200 km/s, while the downflows are best seen in the SUTRI 465 Åand AIA 131 Åimages with a slower velocity of tens of kilometers per second (characteristic temperatures (log T(K)) for 193 Å, 465 Åand 131 Åare 6.2, 5.7, 5.6, respectively). We also analyze the center-to-limb variation of the velocities for both upflows and downflows. The simultaneous observations of downflows and upflows can be explained by the chromosphere-corona mass-cycling process, in which the localized chromospheric plasma is impulsively heated to coronal temperature forming a upflow and then these upflows experience radiative cooling producing a downflow with the previously heated plasma returning to the lower atmosphere. In particular, the persistent downflows seen by SUTRI provide strong evidence of the cooling process in the mass cycle. For upflows associated with open loops, part of the plasma is able to escape outward and into the heliosphere as solar wind."
https://arxiv.org/abs/2403.07421,2024-03-12,On the Borel complexity and the complete metrizability of spaces of metrics,['Katsuhisa Koshino'],"Given a metrizable space $X$, let $AM(X)$ be the space of continuous bounded admissible metrics on $X$, which is endowed with the sup-metric. In this paper, we shall investigate the Borel complexity and the complete metrizability of $AM(X)$ and show that a separable metrizable space $X$ is $σ$-compact if and only if $AM(X)$ is completely metrizable."
https://arxiv.org/abs/2403.07420,2024-03-12,DragAnything: Motion Control for Anything using Entity Representation,"['Weijia Wu', 'Zhuang Li', 'Yuchao Gu', 'Rui Zhao', 'Yefei He', 'David Junhao Zhang', 'Mike Zheng Shou', 'Yan Li', 'Tingting Gao', 'Di Zhang']","We introduce DragAnything, which utilizes a entity representation to achieve motion control for any object in controllable video generation. Comparison to existing motion control methods, DragAnything offers several advantages. Firstly, trajectory-based is more userfriendly for interaction, when acquiring other guidance signals (e.g., masks, depth maps) is labor-intensive. Users only need to draw a line (trajectory) during interaction. Secondly, our entity representation serves as an open-domain embedding capable of representing any object, enabling the control of motion for diverse entities, including background. Lastly, our entity representation allows simultaneous and distinct motion control for multiple objects. Extensive experiments demonstrate that our DragAnything achieves state-of-the-art performance for FVD, FID, and User Study, particularly in terms of object motion control, where our method surpasses the previous methods (e.g., DragNUWA) by 26% in human voting."
https://arxiv.org/abs/2403.07419,2024-03-12,Quenching and flow of charm and bottom quarks via semi-leptonic decay of $D$ and $B$ mesons in Pb+Pb collisions at the LHC,"['Shu-Qing Li', 'Wen-Jing Xing', 'Shanshan Cao', 'Guang-You Qin']","Heavy flavor particles provide important probes of the microscopic structure and thermodynamic properties of the quark-gluon plasma (QGP) produced in high-energy nucleus-nucleus collisions. We study the energy loss and flow of charm and bottom quarks inside the QGP via the nuclear modification factor ($R_\mathrm{AA}$) and elliptic flow coefficient ($v_2$) of their decayed leptons in heavy-ion collisions at the LHC. The dynamical evolution of the QGP is performed using the (3+1)-dimensional viscous hydrodynamics model CLVisc; the evolution of heavy quarks inside the QGP is simulated with our improved Langevin model that takes into account both collisional and radiative energy loss of heavy quarks; the hadronization of heavy quarks is simulated via our hybrid coalescence-fragmentation model; and the semi-leptonic decay of $D$ and $B$ mesons is simulated via PYTHIA. By using the same spatial diffusion coefficient for charm and bottom quarks, we obtain smaller $R_\mathrm{AA}$ and larger $v_2$ of charm decayed leptons than bottom decayed leptons, indicating stronger energy loss of charm quarks than bottom quarks inside the QGP within our current model setup."
https://arxiv.org/abs/2403.07418,2024-03-12,"$λ$-shaped random matrices, $λ$-plane trees, and $λ$-Dyck paths","['Elia Bisi', 'Fabio Deelan Cunden']","We consider random matrices whose shape is the dilation $Nλ$ of a self-conjugate Young diagram $λ$. In the large-$N$ limit, the empirical distribution of the squared singular values converges almost surely to a probability distribution $F^λ$. The moments of $F^λ$ enumerate two combinatorial objects: $λ$-plane trees and $λ$-Dyck paths, which we introduce and show to be in bijection. We also prove that the distribution $F^λ$ is algebraic, in the sense of Rao and Edelman. In the case of fat hook shapes we provide explicit formulae for $F^λ$ and we express it as a free convolution of two measures involving a Marchenko-Pastur and a Bernoulli distribution."
https://arxiv.org/abs/2403.07417,2024-03-12,Cabello's nonlocality argument for multisetting high-dimensional systems and its experimental test,"['M. Yang', 'D. Zhang', 'L. Chen']","Recent advancements have expanded Hardy's nonlocality arguments into multisetting and multidimensional systems to enhance quantum correlations. In comparison with Hardy's nonlocal argument, Cabello's nonlocal argument (CNA) emerges as a superior choice for illustrating nonlocal features. An open question persists regarding the potential extension of CNA to arbitrary (k, d) scenarios. Here, we answer this question both in theory and experiment. Theoretically, by utilizing compatibility graphs, we construct a new logical framework for multisetting and multidimensional CNA, demonstrating an increase in the maximum successful probability with setting k and dimension d. Experimentally, by employing controllable photonic orbital angular momentum entanglement, we exhibit nonlocality with an experimentally recorded probability of 20.29% in the (2, 4) scenario and 28.72% in the (6, 2) scenario. Our work showcases a sharper contradiction between quantum mechanics and classical theory, surpassing the bound limited by the original version."
https://arxiv.org/abs/2403.07416,2024-03-12,Grain growth competition and formation of grain boundaries during solidification of hcp alloys,"['A. K. Boukellal', 'M. Sarebanzadeh', 'A. Orozco-Caballero', 'F. Sket', 'J. LLorca', 'D. Tourret']","Grain growth competition during directional solidification of a polycrystal with hexagonal (hcp) symmetry (Mg-1wt%Gd alloy) is studied by phase-field modeling, exploring the effect of the temperature gradient G on the resulting grain boundary (GB) orientation selection. Results show that selection mechanisms and scaling laws derived for cubic (fcc, bcc) crystals also apply to hcp materials (within their basal plane), provided a re-estimation of fitting parameters and re-scaling to account for the sixfold symmetry. While grain growth competition remains stochastic with rare events of unexpected elimination or side-branching along the developing GBs, we also confirm an overall transition from a geometrical limit to a favorably oriented grain limit behavior with an increase of thermal gradient within the dendritic regime, and the progressive alignment of dendrites and GBs toward the temperature gradient direction with an increase of G during the dendritic-to-cellular morphological transition. Comparisons with original thin-sample directional solidification experiments show a qualitative agreement with PF results, yet with notable discrepancies, which nonetheless can be explained based on the stochastic variability of selected GB orientations, and the statistically limited experimental sample size. Overall, our results extend the understanding of GB formation and grain growth competition during solidification of hcp materials, and the effect of thermal conditions, nonetheless concluding on the challenges of extending the current studies to three dimensions, and the need for much broader (statistically significant) data sets of GB orientation selected under well-identified solidification conditions."
https://arxiv.org/abs/2403.07415,2024-03-12,Frequency-explicit stability estimates for time-harmonic elastodynamic problems in nearly incompressible materials,"['T. Chaumont-Frelet', 'S. Nicaise']","We consider time-harmonic elastodynamic problems in heterogeneous media.cWe focus on scattering problems in the high-frequency regime and incnearly incompressible media, where the the angular frequency $ω$ and ratio of the Lamé parameters $λ/μ$ may both be large. We derive stability estimates controlling the norm of the solution by the norm of the right-hand side up to a fully-explicit constant. Crucially, under natural assumptions on the domain and coefficients, this constant increases linearly with $ω$ and is uniform in the ratio $λ/μ$."
https://arxiv.org/abs/2403.07414,2024-03-12,Strong asymptotic giant branch stars' spectral features in distant quiescent galaxies: Impact on galaxy evolution,"['Shiying Lu', 'Emanuele Daddi', 'Claudia Maraston', 'Mark Dickinson', 'Pablo Arrabal Haro', 'Raphael Gobat', 'Alvio Renzini', 'Mauro Giavalisco', 'Micaela B. Bagley', 'Antonello Calabrò', 'Yingjie Cheng', 'Alexander de la Vega', ""Chiara D'Eugenio"", 'David Elbaz', 'Steven L. Finkelstein', 'Carlos Gómez-Guijarro', 'Qiusheng Gu', 'Nimish P. Hathi', 'Marc Huertas-Company', 'Jeyhan S. Kartaltepe', 'Anton M. Koekemoer', 'Aurélien Le Bail', 'Yipeng Lyu', 'Benjamin Magnelli', 'Bahram Mobasher']","Age-dating and weighting stellar populations in galaxies at various cosmic epochs are essential steps to study galaxy formation through cosmic times. Evolutionary population synthesis models with different input physics are used towards this aim. In particular, the contribution from the thermally pulsing asymptotic-giant-branch (TP-AGB) stellar phase, which peaks for intermediate-age 0.6-2 Gyr systems, has been debated upon for decades. Here we report the detection of strong cool star signatures in the rest-frame near-infrared spectra of three young (~1 Gyr), massive (~10^10 Msun) quiescent galaxies at large look-back time, z=1-2, using JWST/NIRSpec. The co-existence of oxygen- and carbon-type absorption features, spectral edges and features from rare species such as Vanadium, and possibly Zirconium, reveal a strong contribution from TP-AGB stars. Population synthesis models with significant TP-AGB contribution reproduce the observations considerably better than those with weak TP-AGB, which are those commonly used. These findings call for revisions of published stellar population fitting results, pointing to lower masses and younger ages, with additional implications on cosmic dust production and chemical enrichment. These results will stimulate new generations of improved models informed by these and future observations."
https://arxiv.org/abs/2403.07413,2024-03-12,Learning-Augmented Algorithms with Explicit Predictors,"['Marek Elias', 'Haim Kaplan', 'Yishay Mansour', 'Shay Moran']","Recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data. These approaches have demonstrated an enhancement in performance when the predictions are accurate, while also ensuring robustness by providing worst-case guarantees when predictions fail. In this paper we focus on online problems; prior research in this context was focused on a paradigm where the predictor is pre-trained on past data and then used as a black box (to get the predictions it was trained for). In contrast, in this work, we unpack the predictor and integrate the learning problem it gives rise for within the algorithmic challenge. In particular we allow the predictor to learn as it receives larger parts of the input, with the ultimate goal of designing online learning algorithms specifically tailored for the algorithmic task at hand. Adopting this perspective, we focus on a number of fundamental problems, including caching and scheduling, which have been well-studied in the black-box setting. For each of the problems we consider, we introduce new algorithms that take advantage of explicit learning algorithms which we carefully design towards optimizing the overall performance. We demonstrate the potential of our approach by deriving performance bounds which improve over those established in previous work."
https://arxiv.org/abs/2403.07412,2024-03-12,GPU-Accelerated Vecchia Approximations of Gaussian Processes for Geospatial Data using Batched Matrix Computations,"['Qilong Pan', 'Sameh Abdulah', 'Marc G. Genton', 'David E. Keyes', 'Hatem Ltaief', 'Ying Sun']","Gaussian processes (GPs) are commonly used for geospatial analysis, but they suffer from high computational complexity when dealing with massive data. For instance, the log-likelihood function required in estimating the statistical model parameters for geospatial data is a computationally intensive procedure that involves computing the inverse of a covariance matrix with size n X n, where n represents the number of geographical locations. As a result, in the literature, studies have shifted towards approximation methods to handle larger values of n effectively while maintaining high accuracy. These methods encompass a range of techniques, including low-rank and sparse approximations. Vecchia approximation is one of the most promising methods to speed up evaluating the log-likelihood function. This study presents a parallel implementation of the Vecchia approximation, utilizing batched matrix computations on contemporary GPUs. The proposed implementation relies on batched linear algebra routines to efficiently execute individual conditional distributions in the Vecchia algorithm. We rely on the KBLAS linear algebra library to perform batched linear algebra operations, reducing the time to solution compared to the state-of-the-art parallel implementation of the likelihood estimation operation in the ExaGeoStat software by up to 700X, 833X, 1380X on 32GB GV100, 80GB A100, and 80GB H100 GPUs, respectively. We also successfully manage larger problem sizes on a single NVIDIA GPU, accommodating up to 1M locations with 80GB A100 and H100 GPUs while maintaining the necessary application accuracy. We further assess the accuracy performance of the implemented algorithm, identifying the optimal settings for the Vecchia approximation algorithm to preserve accuracy on two real geospatial datasets: soil moisture data in the Mississippi Basin area and wind speed data in the Middle East."
https://arxiv.org/abs/2403.07411,2024-03-12,Cube tilings with linear constraints,"['Dae Gwan Lee', 'Goetz E. Pfander', 'David Walnut']","We consider tilings $(\mathcal{Q},Φ)$ of $\mathbb{R}^d$ where $\mathcal{Q}$ is the $d$-dimensional unit cube and the set of translations $Φ$ is constrained to lie in a pre-determined lattice $A \mathbb{Z}^d$ in $\mathbb{R}^d$. We provide a full characterization of matrices $A$ for which such cube tilings exist when $Φ$ is a sublattice of $A\mathbb{Z}^d$ with any $d \in \mathbb{N}$ or a generic subset of $A\mathbb{Z}^d$ with $d\leq 7$. As a direct consequence of our results, we obtain a criterion for the existence of linearly constrained frequency sets, that is, $Φ\subseteq A\mathbb{Z}^d$, such that the respective set of complex exponential functions $\mathcal{E} (Φ)$ is an orthogonal Fourier basis for the space of square integrable functions supported on a parallelepiped $B\mathcal{Q}$, where $A, B \in \mathbb{R}^{d \times d}$ are nonsingular matrices given a priori. Similarly constructed Riesz bases are considered in a companion paper."
https://arxiv.org/abs/2403.07410,2024-03-13,Polylog-Competitive Deterministic Local Routing and Scheduling,"['Bernhard Haeupler', 'Shyamal Patel', 'Antti Roeyskoe', 'Cliff Stein', 'Goran Zuzic']","This paper addresses point-to-point packet routing in undirected networks, which is the most important communication primitive in most networks. The main result proves the existence of routing tables that guarantee a polylog-competitive completion-time $\textbf{deterministically}$: in any undirected network, it is possible to give each node simple stateless deterministic local forwarding rules, such that, any adversarially chosen set of packets are delivered as fast as possible, up to polylog factors."
https://arxiv.org/abs/2403.07409,2024-03-12,Universal Slepian-Wolf coding for individual sequences,['Neri Merhav'],"We establish a coding theorem and a matching converse theorem for separate encodings and joint decoding of individual sequences using finite-state machines. The achievable rate region is characterized in terms of the Lempel-Ziv (LZ) complexities, the conditional LZ complexities and the joint LZ complexity of the two source sequences. An important feature that is needed to this end, which may be interesting on its own right, is a certain asymptotic form of a chain rule for LZ complexities, which we establish in this work. The main emphasis in the achievability scheme is on the universal decoder and its properties. We then show that the achievable rate region is universally attainable by a modified version of Draper's universal incremental Slepian-Wolf (SW) coding scheme, provided that there exists a low-rate reliable feedback link."
https://arxiv.org/abs/2403.07408,2024-03-12,NightHaze: Nighttime Image Dehazing via Self-Prior Learning,"['Beibei Lin', 'Yeying Jin', 'Wending Yan', 'Wei Ye', 'Yuan Yuan', 'Robby T. Tan']","Masked autoencoder (MAE) shows that severe augmentation during training produces robust representations for high-level tasks. This paper brings the MAE-like framework to nighttime image enhancement, demonstrating that severe augmentation during training produces strong network priors that are resilient to real-world night haze degradations. We propose a novel nighttime image dehazing method with self-prior learning. Our main novelty lies in the design of severe augmentation, which allows our model to learn robust priors. Unlike MAE that uses masking, we leverage two key challenging factors of nighttime images as augmentation: light effects and noise. During training, we intentionally degrade clear images by blending them with light effects as well as by adding noise, and subsequently restore the clear images. This enables our model to learn clear background priors. By increasing the noise values to approach as high as the pixel intensity values of the glow and light effect blended images, our augmentation becomes severe, resulting in stronger priors. While our self-prior learning is considerably effective in suppressing glow and revealing details of background scenes, in some cases, there are still some undesired artifacts that remain, particularly in the forms of over-suppression. To address these artifacts, we propose a self-refinement module based on the semi-supervised teacher-student framework. Our NightHaze, especially our MAE-like self-prior learning, shows that models trained with severe augmentation effectively improve the visibility of input haze images, approaching the clarity of clear nighttime images. Extensive experiments demonstrate that our NightHaze achieves state-of-the-art performance, outperforming existing nighttime image dehazing methods by a substantial margin of 15.5% for MUSIQ and 23.5% for ClipIQA."
https://arxiv.org/abs/2403.07407,2024-03-12,In-context learning enables multimodal large language models to classify cancer pathology images,"['Dyke Ferber', 'Georg Wölflein', 'Isabella C. Wiest', 'Marta Ligero', 'Srividhya Sainath', 'Narmin Ghaffari Laleh', 'Omar S. M. El Nahhas', 'Gustav Müller-Franzes', 'Dirk Jäger', 'Daniel Truhn', 'Jakob Nikolas Kather']","Medical image classification requires labeled, task-specific datasets which are used to train deep learning networks de novo, or to fine-tune foundation models. However, this process is computationally and technically demanding. In language processing, in-context learning provides an alternative, where models learn from within prompts, bypassing the need for parameter updates. Yet, in-context learning remains underexplored in medical image analysis. Here, we systematically evaluate the model Generative Pretrained Transformer 4 with Vision capabilities (GPT-4V) on cancer image processing with in-context learning on three cancer histopathology tasks of high importance: Classification of tissue subtypes in colorectal cancer, colon polyp subtyping and breast tumor detection in lymph node sections. Our results show that in-context learning is sufficient to match or even outperform specialized neural networks trained for particular tasks, while only requiring a minimal number of samples. In summary, this study demonstrates that large vision language models trained on non-domain specific data can be applied out-of-the box to solve medical image-processing tasks in histopathology. This democratizes access of generalist AI models to medical experts without technical background especially for areas where annotated data is scarce."
https://arxiv.org/abs/2403.07406,2024-03-12,FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental Learning with Hill-Climbing,"['Eduard Hogea', 'Adrian Popescu', 'Darian Onchis', 'Grégoire Petit']","Exemplar-free class-incremental learning (EFCIL) poses significant challenges, primarily due to catastrophic forgetting, necessitating a delicate balance between stability and plasticity to accurately recognize both new and previous classes. Traditional EFCIL approaches typically skew towards either model plasticity through successive fine-tuning or stability by employing a fixed feature extractor beyond the initial incremental state. Building upon the foundational FeTrIL framework, our research extends into novel experimental domains to examine the efficacy of various oversampling techniques and dynamic optimization strategies across multiple challenging datasets and incremental settings. We specifically explore how oversampling impacts accuracy relative to feature availability and how different optimization methodologies, including dynamic recalibration and feature pool diversification, influence incremental learning outcomes. The results from these comprehensive experiments, conducted on CIFAR100, Tiny-ImageNet, and an ImageNet-Subset, under-score the superior performance of FeTrIL in balancing accuracy for both new and past classes against ten contemporary methods. Notably, our extensions reveal the nuanced impacts of oversampling and optimization on EFCIL, contributing to a more refined understanding of feature-space manipulation for class incremental learning. FeTrIL and its extended analysis in this paper FeTrIL++ pave the way for more adaptable and efficient EFCIL methodologies, promising significant improvements in handling catastrophic forgetting without the need for exemplars."
https://arxiv.org/abs/2403.07405,2024-03-12,Radon Concentration Measurement with a High-Sensitivity Radon Detector at the Yemilab,"['Kyungmin Seo', 'Hyunsoo Kim', 'Yeongduk Kim', 'Hyeyoung Lee', 'Jaison Lee', 'Moo Hyun Lee', 'Jungho So', 'Sangcheol Yoon', 'Youngsoo Yoon']","The radiation emitted from radon is a critical background in rare event search experiments conducted at the Yemi Underground Laboratory (Yemilab) in Jeongseon, Korea. A Radon Reduction System(RRS) has been developed and installed in Yemilab to reduce radon concentration in the air. The RRS primarily provides a purified air of 50 m3/h to the cleanroom used to assemble crystal detectors in the AMoRE, a neutrinoless double beta decay search experiment. RRS can reduce the radon level by a factor of 300, so a high-sensitivity radon detector was required. A highly sensitive radon detector was constructed using a 70 L chamber with a large PIN photodiode to measure radon concentration in the purified air. The radon detector shows an excellent resolution of 72 keV (FWHM) for 6.003 MeV alphas from 218Po decay and a sensitivity down to 23.8 +- 2.1 mBq/m3 with a boil-off N2 gas sample. The radon concentration level from the RRS measured by the radon detector was below 0.29 Bq/m3 with a reduction factor of about 300."
https://arxiv.org/abs/2403.07404,2024-03-12,Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning,"['Filip Szatkowski', 'Fei Yang', 'Bartłomiej Twardowski', 'Tomasz Trzciński', 'Joost van de Weijer']","Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task-wise Logits Correction (TLC), a simple method that equalizes this bias and improves the network performance for every given compute budget in the class-incremental setting. We assess the accuracy and computational cost of various continual learning techniques enhanced with early-exits and TLC across standard class-incremental learning benchmarks such as 10 split CIFAR100 and ImageNetSubset and show that TLC can achieve the accuracy of the standard methods using less than 70\% of their computations. Moreover, at full computational budget, our method outperforms the accuracy of the standard counterparts by up to 15 percentage points. Our research underscores the inherent synergy between early-exit networks and continual learning, emphasizing their practical utility in resource-constrained environments."
https://arxiv.org/abs/2403.07403,2024-03-12,From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios,"['Guoshan Liu', 'Yang Jiao', 'Jingjing Chen', 'Bin Zhu', 'Yu-Gang Jiang']","The precise recognition of food categories plays a pivotal role for intelligent health management, attracting significant research attention in recent years. Prominent benchmarks, such as Food-101 and VIREO Food-172, provide abundant food image resources that catalyze the prosperity of research in this field. Nevertheless, these datasets are well-curated from canteen scenarios and thus deviate from food appearances in daily life. This discrepancy poses great challenges in effectively transferring classifiers trained on these canteen datasets to broader daily-life scenarios encountered by humans. Toward this end, we present two new benchmarks, namely DailyFood-172 and DailyFood-16, specifically designed to curate food images from everyday meals. These two datasets are used to evaluate the transferability of approaches from the well-curated food image domain to the everyday-life food image domain. In addition, we also propose a simple yet effective baseline method named Multi-Cluster Reference Learning (MCRL) to tackle the aforementioned domain gap. MCRL is motivated by the observation that food images in daily-life scenarios exhibit greater intra-class appearance variance compared with those in well-curated benchmarks. Notably, MCRL can be seamlessly coupled with existing approaches, yielding non-trivial performance enhancements. We hope our new benchmarks can inspire the community to explore the transferability of food recognition models trained on well-curated datasets toward practical real-life applications."
https://arxiv.org/abs/2403.07402,2024-03-12,Renormalization of Complex Networks with Partition Functions,"['Sungwon Jung', 'Sang Hoon Lee', 'Jaeyoon Cho']","While renormalization groups are fundamental in physics, renormalization of complex networks remains vague in its conceptual definition and methodology. Here, we propose a novel strategy to renormalize complex networks. Rather than resorting to handling the bare structure of a network, we overlay it with a readily renormalizable physical model, which reflects real-world scenarios with a broad generality. From the renormalization of the overlying system, we extract a rigorous and simple renormalization group transformation of arbitrary networks. In this way, we obtain a transparent, model-dependent physical meaning of the network renormalization, which in our case is a scale transformation preserving the transition dynamics of low-density particles. We define the strength of a node in accordance with the physical model and trace the change of its distribution under our renormalization process. This analysis demonstrates that the strength distributions of scale-free networks remain scale-invariant, whereas those of homogeneous random networks do not."
https://arxiv.org/abs/2403.07401,2024-03-13,Constraining the Initial Mass function in the Epoch of Reionization from Astrophysical and Cosmological data,"['A. Lapi', 'G. Gandolfi', 'L. Boco', 'F. Gabrielli', 'M. Massardi', 'B. S. Haridasu', 'C. Baccigalupi', 'A. Bressan', 'L. Danese']","[abridged] We aim to constrain the stellar initial mass function (IMF) during the epoch of reionization. To this purpose, we build up a semi-empirical model for the reionization history of the Universe, based on various ingredients: the latest determination of the UV galaxy luminosity function from JWST out to redshift $z\lesssim 12$; data-inferred and simulation-driven assumptions on the redshift-dependent escape fraction of ionizing photons from primordial galaxies; a simple yet flexible parameterization of the IMF $φ(m_\star)\sim m_\star^ξ\, e^{-m_{\star,\rm c}/m_\star}$ in terms of a high-mass end slope $ξ<0$ and of a characteristic mass $m_{\star,\rm c}$ below which a flattening or a bending sets in; the PARSEC stellar evolution code to compute the UV and ionizing emission from different star's masses as a function of age and metallicity; a few physical constraints related to stellar and galaxy formation in faint galaxies at the reionization redshifts. We compare our model outcomes with the reionization observables from different astrophysical and cosmological probes, and perform Bayesian inference on the IMF parameters. We find that the IMF slope $ξ$ is within the range from $-2.8$ to $-2.3$, while appreciably flatter slopes are excluded at great significance. However, the bestfit value of the IMF characteristic mass $m_{\star,\rm c}\sim$ a few $M_\odot$ implies a suppression in the formation of small stellar masses, at variance with the IMF in the local Universe; this may be induced by the thermal background $\sim 20-30$ K provided by CMB photons at the reionization redshifts. Finally, we investigate the implications of our reconstructed IMF on the recent JWST detections of massive galaxies at and beyond the reionization epoch, showing that any putative tension with the standard cosmological framework is substantially alleviated."
https://arxiv.org/abs/2403.07400,2024-03-12,Boundedness of energy for N-body Schrödinger equations with time dependent small potentials,['Kenji Yajima'],We prove that Sobolev norms of solutions to time dependent Schrödinger equations for $d$-dimensional $N$-partcles interacting via time dependent two body potentials are bounded in time if certain Lebesgue norms of the potentials are small uniformly in time. The proof uses the scattering theory in the extended phase space which proves that all particles scatter freely in the remote past and far future.
https://arxiv.org/abs/2403.07399,2024-03-12,Automorphisms of Hilbert schemes of Cayley's K3 surfaces,['Kwangwoo Lee'],We prove that the automorphism group of Hilbert square of a Cayley's K3 surface of Picard number 2 is the free product of three cyclic groups of order two. The generators are three Beauville involutions.
https://arxiv.org/abs/2403.07398,2024-03-12,Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs,"['Tianqing Fang', 'Zeming Chen', 'Yangqiu Song', 'Antoine Bosselut']","Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations."
https://arxiv.org/abs/2403.07397,2024-03-13,Skyrmion flow in periodically modulated channels,"['Klaus Raab', 'Maurice Schmitt', 'Maarten A. Brems', 'Jan Rothörl', 'Fabian Kammerbauer', 'Sachin Krishnia', 'Mathias Kläui', 'Peter Virnau']","Magnetic skyrmions, topologically stabilized chiral magnetic textures with particle-like properties have so far primarily been studied statically. Here, we experimentally investigate the dynamics of skyrmion ensembles in metallic thin film conduits where they behave as quasi-particle fluids. By exploiting our access to the full trajectories of all fluid particles by means of time-resolved magneto-optical Kerr microscopy, we demonstrate that boundary conditions of skyrmion fluids can be tuned by modulation of the channel geometry. We observe as a function of channel width deviations from classical flow profiles even into the no- or partial-slip regime. Unlike conventional colloids, the skyrmion Hall effect can also introduce transversal flow-asymmetries and even local motion of single skyrmions against the driving force which we explore with particle-based simulations, demonstrating the unique properties of skyrmion liquid flow that uniquely deviates from previously known behavior of other quasi-particles."
https://arxiv.org/abs/2403.07396,2024-03-12,Crystal design of altermagnetism,"['Zhiyuan Zhou', 'Xingkai Cheng', 'Mengli Hu', 'Junwei Liu', 'Feng Pan', 'Cheng Song']","Symmetry plays a fundamental role in condensed matter. The unique entanglement between magnetic sublattices and alternating crystal environment in altermagnets provides a unique opportunity for designing magnetic space symmetry. There have been extensive experimental efforts concentrated on tuning the Neel vector to reconstruct altermagnetic symmetry. However, it remains challenging to modulate the altermagnetic symmetry through the crystal aspect. Here, the crystal design of altermagnetism is successfully realized, by breaking glide mirrors and magnetic mirrors of the (0001) crystallographic plane in CrSb films via crystal distortion. We establish a locking relationship between altermagnetic symmetry and the emergent Dzyaloshinskii-Moriya (DM) vectors in different CrSb films, realizing unprecedentedly room-temperature spontaneous anomalous Hall effect in an altermagnetic metal. The concept of exchange-coupling torques is broadened to include both antiferromagnetic exchange-coupling torque and DM torque. Their relationship is designable, determining electrical manipulation modes, e.g., field-assisted switching for CrSb(1-100)/Pt and field-free switching for W/CrSb(11-20). Particularly, the unprecedentedly field-free 100-percent switching of Neel vectors is realized by making these two torques parallel or antiparallel, dependent on Neel vector orientation. Besides unravelling the rich mechanisms for electrical manipulation of altermagnetism rooted in broadened concept of exchange-coupling torques, we list other material candidates and propose that crystal design of altermagnetism would bring rich designability to magnonics, topology, etc."
https://arxiv.org/abs/2403.07395,2024-03-12,Predicting the Slowing of Stellar Differential Rotation by Instability-Driven Turbulence,"['B. Tripathi', 'A. J. Barker', 'A. E. Fraser', 'P. W. Terry', 'E. G. Zweibel']","Differentially rotating stars and planets transport angular momentum internally due to turbulence at rates that have long been a challenge to predict reliably. We develop a self-consistent saturation theory, using a statistical closure approximation, for hydrodynamic turbulence driven by the axisymmetric Goldreich--Schubert--Fricke (GSF) instability at the stellar equator with radial differential rotation. This instability arises when fast thermal diffusion eliminates the stabilizing effects of buoyancy forces in a system where a stabilizing entropy gradient dominates over the destabilizing angular momentum gradient. Our turbulence closure invokes a dominant three-wave coupling between pairs of linearly unstable eigenmodes and a near-zero frequency, viscously damped eigenmode that features latitudinal jets. We derive turbulent transport rates of momentum and heat, and provide them in analytic forms. Such formulae, free of tunable model parameters, are tested against direct numerical simulations; the comparison shows good agreement. They improve upon prior quasi-linear or ``parasitic saturation"" models containing a free parameter. Given model correspondences, we also extend this theory to heat and compositional transport for axisymmetric thermohaline instability-driven turbulence in certain regimes."
https://arxiv.org/abs/2403.07394,2024-03-12,A regularization theorem for bounded-degree self-maps,['She Yang'],"Let $K$ be an algebraically closed field of arbitrary characteristic and let $X$ be an irreducible projective variety over $K$. Let $G\subseteq\text{Bir}(X)$ be a bounded-degree subgroup. We prove that there exists an irreducible projective variety $Y$ birational to $X$, such that every element of $G$ becomes an automorphism of $Y$ after the birational transformation. If $K=\mathbb{C}$, this result is stated in [Can14, Theorem 2.5] and the proof backs to [HZ96, Section 5]. The proof in [HZ96] is not purely algebraic. Inheriting the methods in [HZ96], we give a purely algebraic proof of this statement in arbitrary characteristic. We will also discuss a corollary of this result which is useful in arithmetic dynamics."
https://arxiv.org/abs/2403.07393,2024-03-12,Learning on the correct class for domain inverse problems of gravimetry,"['Yihang Chen', 'Wenbin Li']","We consider end-to-end learning approaches for inverse problems of gravimetry. Due to ill-posedness of the inverse gravimetry, the reliability of learning approaches is questionable. To deal with this problem, we propose the strategy of learning on the correct class. The well-posedness theorems are employed when designing the neural-network architecture and constructing the training set. Given the density-contrast function as a priori information, the domain of mass can be uniquely determined under certain constrains, and the domain inverse problem is a correct class of the inverse gravimetry. Under this correct class, we design the neural network for learning by mimicking the level-set formulation for the inverse gravimetry. Numerical examples illustrate that the method is able to recover mass models with non-constant density contrast."
https://arxiv.org/abs/2403.07392,2024-03-12,ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions,"['Chunlong Xia', 'Xinliang Wang', 'Feng Lv', 'Xin Hao', 'Yifeng Shi']","Although Vision Transformer (ViT) has achieved significant success in computer vision, it does not perform well in dense prediction tasks due to the lack of inner-patch information interaction and the limited diversity of feature scale. Most existing studies are devoted to designing vision-specific transformers to solve the above problems, which introduce additional pre-training costs. Therefore, we present a plain, pre-training-free, and feature-enhanced ViT backbone with Convolutional Multi-scale feature interaction, named ViT-CoMer, which facilitates bidirectional interaction between CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has the following advantages: (1) We inject spatial pyramid multi-receptive field convolutional features into the ViT architecture, which effectively alleviates the problems of limited local information interaction and single-feature representation in ViT. (2) We propose a simple and efficient CNN-Transformer bidirectional fusion interaction module that performs multi-scale fusion across hierarchical features, which is beneficial for handling dense prediction tasks. (3) We evaluate the performance of ViT-CoMer across various dense prediction tasks, different frameworks, and multiple advanced pre-training. Notably, our ViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and 62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art methods. We hope ViT-CoMer can serve as a new backbone for dense prediction tasks to facilitate future research. The code will be released at https://github.com/Traffic-X/ViT-CoMer."
https://arxiv.org/abs/2403.07391,2024-03-12,Towards adiabatic-connection interpolation model with broader applicability,"['Lucian A. Constantin', 'Szymon Śmiga', 'Fabio Della Sala']","The Adiabatic Connection Integrand Interpolation (ACII) method represents a general path for calculating correlation energies in electronic systems within the Den sity Functional Theory. ACII functionals include both exact-exchange and the second-order correlation energy, as well as an interpolating function toward the strictly-correlated electron (SCE) regime. Several interpolating functions have been proposed in the last years targeting different properties, yet an accurate ACII approach with broad applicability is sti ll missing. Recently, we have proposed an ACII functional that was made accurate for the three-dimensional (3D) uniform electron gas as well as for model metal clusters. In this work we present an ACII functional (named genISI2) which is very accurate for both three-dimensional (3D) and two-dimensional (2D) uniform electron gases and for the q uasi-2D infinite barrier model, where most of the exchange-correlation functionals fail badly, as well as for strongly correlated two-electrons systems. Using the exact-exchange Kohn-Sham orbitals, we have also assessed the genISI2 for various molecular systems, showing a superior performance with respect to the o ther ACII methods for total energies, atomization energies, and ionization potentials. The genISI2 functional can thus find application in a broad range of systems and properties."
https://arxiv.org/abs/2403.07390,2024-03-12,Learning Correction Errors via Frequency-Self Attention for Blind Image Super-Resolution,"['Haochen Sun', 'Yan Yuan', 'Lijuan Su', 'Haotian Shao']","Previous approaches for blind image super-resolution (SR) have relied on degradation estimation to restore high-resolution (HR) images from their low-resolution (LR) counterparts. However, accurate degradation estimation poses significant challenges. The SR model's incompatibility with degradation estimation methods, particularly the Correction Filter, may significantly impair performance as a result of correction errors. In this paper, we introduce a novel blind SR approach that focuses on Learning Correction Errors (LCE). Our method employs a lightweight Corrector to obtain a corrected low-resolution (CLR) image. Subsequently, within an SR network, we jointly optimize SR performance by utilizing both the original LR image and the frequency learning of the CLR image. Additionally, we propose a new Frequency-Self Attention block (FSAB) that enhances the global information utilization ability of Transformer. This block integrates both self-attention and frequency spatial attention mechanisms. Extensive ablation and comparison experiments conducted across various settings demonstrate the superiority of our method in terms of visual quality and accuracy. Our approach effectively addresses the challenges associated with degradation estimation and correction errors, paving the way for more accurate blind image SR."
https://arxiv.org/abs/2403.07389,2024-03-12,Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from Duplex to Monoplex IHC Images,"['Nicolas Brieu', 'Nicolas Triltsch', 'Philipp Wortmann', 'Dominik Winter', 'Shashank Saran', 'Marlon Rebelatto', 'Günter Schmidt']","Generative models enable the translation from a source image domain where readily trained models are available to a target domain unseen during training. While Cycle Generative Adversarial Networks (GANs) are well established, the associated cycle consistency constrain relies on that an invertible mapping exists between the two domains. This is, however, not the case for the translation between images stained with chromogenic monoplex and duplex immunohistochemistry (IHC) assays. Focusing on the translation from the latter to the first, we propose - through the introduction of a novel training design, an alternative constrain leveraging a set of immunofluorescence (IF) images as an auxiliary unpaired image domain. Quantitative and qualitative results on a downstream segmentation task show the benefit of the proposed method in comparison to baseline approaches."
https://arxiv.org/abs/2403.07388,2024-03-12,Understanding the shear modulus of dense microgel suspensions,"['Maxime Bergman', 'Yixuan Xu', 'Zhang Chi', 'Thomas G. Mason', 'Frank Scheffold']","Polymer microgels exhibit intriguing macroscopic flow properties arising from their unique microscopic structure. Microgel colloids comprise a crosslinked polymer network with a radially decaying density profile, resulting in a dense core surrounded by a fuzzy corona. Notably, microgels synthesized from poly(N-isopropylacrylamide) (PNIPAM) are thermoresponsive, capable of adjusting their size and density profile based on temperature. Above the lower critical solution temperature ($T_\text{LCST}\sim 33$ $^\circ$C), the microgel's polymer network collapses, leading to the expulsion of water through a reversible process. Conversely, below $33$ $^\circ$C, the microgel's network swells, becoming highly compressible and allowing overpacking to effective volume fractions exceeding one. Under conditions of dense packing, microgels undergo deformation in distinct stages: corona compression and faceting, interpenetration, and finally, isotropic compression. Each stage exhibits a characteristic signature in the yield stress and elastic modulus of the dense microgel suspensions. Here, we introduce a model for the linear elastic shear modulus through the minimization of a quasi-equilibrium free energy, encompassing all relevant energetic contributions. We validate our model by comparing its predictions to experimental results from oscillatory shear rheology tests on microgel suspensions at different densities and temperatures. Our findings demonstrate that combining macroscopic rheological measurements with the model allows for temperature-dependent characterization of polymer interaction parameters."
https://arxiv.org/abs/2403.07387,2024-03-12,Combinatorics of generalized parking-function polytopes,"['Margaret M. Bayer', 'Steffen Borgwardt', 'Teressa Chambers', 'Spencer Daugherty', 'Aleyah Dawkins', 'Danai Deligeorgaki', 'Hsin-Chieh Liao', 'Tyrrell McAllister', 'Angela Morrison', 'Garrett Nelson', 'Andrés R. Vindas-Meléndez']","For $\mathbf{b}=(b_1,\dots,b_n)\in \mathbb{Z}_{>0}^n$, a $\mathbf{b}$-parking function is defined to be a sequence $(β_1,\dots,β_n)$ of positive integers whose nondecreasing rearrangement $β'_1\leq β'_2\leq \cdots \leq β'_n$ satisfies $β'_i\leq b_1+\cdots + b_i$. The $\mathbf{b}$-parking-function polytope $\mathfrak{X}_n(\mathbf{b})$ is the convex hull of all $\mathbf{b}$-parking functions of length $n$ in $\mathbb{R}^n$. Geometric properties of $\mathfrak{X}_n(\mathbf{b})$ were previously explored in the specific case where $\mathbf{b}=(a,b,b,\dots,b)$ and were shown to generalize those of the classical parking-function polytope. In this work, we study $\mathfrak{X}_n(\mathbf{b})$ in full generality. We present a minimal inequality and vertex description for $\mathfrak{X}_n(\mathbf{b})$, prove it is a generalized permutahedron, and study its $h$-polynomial. Furthermore, we investigate $\mathfrak{X}_n(\mathbf{b})$ through the perspectives of building sets and polymatroids, allowing us to identify its combinatorial types and obtain bounds on its combinatorial and circuit diameters."
https://arxiv.org/abs/2403.07386,2024-03-12,Multi-source Scheduling and Resource Allocation for Age-of-Semantic-Importance Optimization in Status Update Systems,"['Lunyuan Chen', 'Jie Gong']","In recent years, semantic communication is progressively emerging as an effective means of facilitating intelligent and context-aware communication. However, current researches seldom simultaneously consider the reliability and timeliness of semantic communication, where scheduling and resource allocation (SRA) plays a crucial role. In contrast, conventional age-based approaches cannot seamlessly extend to semantic communication due to their oversight of semantic importance. To bridge this gap, we introduce a novel metric: Age of Semantic Importance (AoSI), which adaptly captures both the freshness of information and its semantic importance. Utilizing AoSI, we formulate an average AoSI minimization problem by optimizing multi-source SRA. To address this problem, we proposed a AoSI-aware joint SRA algorithm based on Deep Q-Network (DQN). Simulation results validate the effectiveness of our proposed method, demonstrating its ability to facilitate timely and reliable semantic communication."
https://arxiv.org/abs/2403.07385,2024-03-12,Phases and Duality in Fundamental Kazakov-Migdal Model on the Graph,"['So Matsuura', 'Kazutoshi Ohta']","We examine the fundamental Kazakov-Migdal (FKM) model on a generic graph, whose partition function is represented by the Ihara zeta function weighted by unitary matrices. The FKM model becomes unstable in the critical strip of the Ihara zeta function. We discover a duality between small and large couplings, associated with the functional equation of the Ihara zeta function for regular graphs. Although the duality is not precise for irregular graphs, we show that the effective action in the large coupling region can be represented by a summation of all possible Wilson loops on the graph similar to that in the small coupling region. We estimate the phase structure of the FKM model both in the small and large coupling regions by comparing it with the Gross-Witten-Wadia (GWW) model. We further validate the theoretical analysis through detailed numerical simulations."
https://arxiv.org/abs/2403.07384,2024-03-12,SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models,"['Yu Yang', 'Siddhartha Mishra', 'Jeffrey N Chiang', 'Baharan Mirzasoleiman']","Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most challenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et al., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset (Johnson et al., 2016), S2L again outperforms training on the full dataset using only 50% of the data. Notably, S2L can perform data selection using a reference model 40x smaller than the target model, proportionally reducing the cost of data selection."
https://arxiv.org/abs/2403.07383,2024-03-12,Homogeneous quandles with commutative inner automorphism groups,"['Takuya Saito', 'Sakumi Sugawara']","In this paper, we give a characterization for homogeneous quandles with commutative inner automorphism groups. In particular, it is shown that such a quandle is expressed as an abelian extension of a trivial quandle. Our construction is a generalization of the recent work by Furuki and Tamaru, which gives the construction of disconnected flat quandles."
https://arxiv.org/abs/2403.07382,2024-03-12,Plasmon-driven creation of magnetic topological structures,"['W. Al Saidi', 'R. Sbiaa', 'Y. Dusch', 'N. Tiercelin']","In the present research, we demonstrate the usage of plasmonic effects in thin film structures to control magnetic topological textures, specifically skyrmions and skyrmioniums. We investigate numerically the generation and alteration of these topological structures caused by hemisphere gold nanoparticle placed over a magnetic layer coated with a dielectric material. The electromagnetic and photothermal models are used to clarify the processes of producing heat and absorption, and the results were implemented in micromagnetic formalism to reveal the dynamics of magnetization under various conditions. Our findings demonstrate the significance of the laser pulse duration and the contact area between nanoparticles and the underlying magnetic layer in forming topological textures. In particular, we show how to generate a single skyrmion, multiple skyrmions, and skyrmioniums, and how to dynamically transition between these states. These results highlight the possibility of manipulating magnetic textures by using plasmonic effects, which presents significant opportunities for spintronics and non-conventional computer applications."
https://arxiv.org/abs/2403.07381,2024-03-12,The solenoidal Heisenberg Virasoro algebra and its simple weight modules,"['Boujemaa Agrebaoui', 'Walid Mhiri']","Let $A_n=\mathbb{C}[t_i^{\pm1},~1\leq i\leq n]$ and $\mathbf{W}(n)_μ=A_nd_μ$ the solenoidal Lie algebra introduced by Y.Billig and V.Futorny in \cite{BiFu2}, where $μ=(μ_1,\ldots,μ_n)\in\mathbb{C}^n$ is a generic vector and $$d_μ=\sum_{i=1}^nμ_it_i\frac{\partial}{\partial t_i}.$$ We consider the semi-direct product Lie algebra $\mathbf{WA}(n)_μ:=\mathbf{W}(n)_μ\ltimes A_n$."
https://arxiv.org/abs/2403.07380,2024-03-12,Gabor-guided transformer for single image deraining,"['Sijin He', 'Guangfeng Lin']","Image deraining have have gained a great deal of attention in order to address the challenges posed by the effects of harsh weather conditions on visual tasks. While convolutional neural networks (CNNs) are popular, their limitations in capturing global information may result in ineffective rain removal. Transformer-based methods with self-attention mechanisms have improved, but they tend to distort high-frequency details that are crucial for image fidelity. To solve this problem, we propose the Gabor-guided tranformer (Gabformer) for single image deraining. The focus on local texture features is enhanced by incorporating the information processed by the Gabor filter into the query vector, which also improves the robustness of the model to noise due to the properties of the filter. Extensive experiments on the benchmarks demonstrate that our method outperforms state-of-the-art approaches."
https://arxiv.org/abs/2403.07379,2024-03-12,"Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends","['Sidak Pal Singh', 'Bobby He', 'Thomas Hofmann', 'Bernhard Schölkopf']","We propose a fresh take on understanding the mechanisms of neural networks by analyzing the rich structure of parameters contained within their optimization trajectories. Towards this end, we introduce some natural notions of the complexity of optimization trajectories, both qualitative and quantitative, which reveal the inherent nuance and interplay involved between various optimization choices, such as momentum, weight decay, and batch size. We use them to provide key hallmarks about the nature of optimization in deep neural networks: when it goes right, and when it finds itself in a dead end. Further, thanks to our trajectory perspective, we uncover an intertwined behaviour of momentum and weight decay that promotes directional exploration, as well as a directional regularization behaviour of some others. We perform experiments over large-scale vision and language settings, including large language models (LLMs) with up to 12 billion parameters, to demonstrate the value of our approach."
https://arxiv.org/abs/2403.07378,2024-03-12,SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression,"['Xin Wang', 'Yu Zheng', 'Zhongwei Wan', 'Mi Zhang']","The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven models from three different LLM families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios. The source code is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM."
https://arxiv.org/abs/2403.07377,2024-03-12,Generic simplicity of ellipses,"['Luc Hillairet', 'Chris M. Judge']","We prove that the Laplace spectrum of the generic ellipse is simple, both with Neumann and Dirichlet boundary condition. We rely on the known multiplicities in the spectrum of the disk (Bourget's hypothesis) and on a refined version of our method of asymptotic separation of variables."
https://arxiv.org/abs/2403.07376,2024-03-12,NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning,"['Bingqian Lin', 'Yunshuang Nie', 'Ziming Wei', 'Jiaqi Chen', 'Shikui Ma', 'Jianhua Han', 'Hang Xu', 'Xiaojun Chang', 'Xiaodan Liang']","Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps. Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications. Code is available at https://github.com/expectorlin/NavCoT."
https://arxiv.org/abs/2403.07375,2024-03-12,A Novel Method to Constrain Tidal Quality Factor from A Non-synchronized Exoplanetary System,"['Takato Tokuno', 'Akihiko Fukui', 'Takeru K. Suzuki']","We propose a novel method to constrain the tidal quality factor, $Q'$, from a non-synchronized star-planet system consisting of a slowly rotating low-mass star and a close-in Jovian planet, taking into account the tidal interaction and the magnetic braking. On the basis of dynamical system theory, the track of the co-evolution of angular momentum for such a system exhibits the existence of a forbidden region in the $Ω_\mathrm{orb}$ -- $Ω_\mathrm{spin}$ plane , where $Ω_\mathrm{spin}$ and $Ω_\mathrm{orb}$ denote the angular velocity of the stellar spin and planetary orbit, respectively. The forbidden region is determined primarily by the strength of the tidal interaction. By comparing ($Ω_\mathrm{orb},Ω_\mathrm{spin}$) of a single star-planet system to the forbidden region, we can constrain the tidal quality factor regardless of the evolutionary history of the system. The application of this method to the star-planet system, NGTS-10 -- NGTS-10 b, gives $Q' \gtrsim 10^8$, leading to an tight upper bound on the tidal torque. Since this cannot be explained by previous theoretical predictions for non-synchronized star-planet systems, our result requires mechanisms that suppress the tidal interaction in such systems."
https://arxiv.org/abs/2403.07374,2024-03-12,Free submonoids of hyperbolic monoids,['Matthias Hamann'],"In this paper, we prove that infinite cancellative finitely generated hyperbolic monoids never contain $\mathbb N\times\mathbb N$ as a submonoid but that they contain an element of infinite order and, if they are elementary, then they also contain a free monoid of rank at least 2. As a corollary we obtain that the latter have exponential growth. We prove these results by analysing the monoid of self-embeddings of hyperbolic digraphs and proving fixed-point theorems for them."
https://arxiv.org/abs/2403.07373,2024-03-12,Calibration of VELC detectors on-board Aditya-L1 mission,"['Shalabh Mishra', 'K. Sasikumar Raja', 'Sanal Krishnan V U', 'Venkata Suresh Narra', 'Bhavana Hegde S', 'Utkarsha D.', 'Muthu Priyal V', 'Pawan Kumar S', 'Natarajan V', 'Raghavendra Prasad B', 'Jagdev Singh', 'Umesh Kamath P', 'Kathiravan S', 'Vishnu T', ' Suresha', 'Savarimuthu P', 'Jalshri H Desai', 'Rajiv Kumaran', 'Shiv Sagar', 'Sumit Kumar', 'Inderjeet Singh Bamrah', 'Amit Kumar']","Aditya-L1 is the first Indian space mission to explore the Sun and solar atmosphere with seven multi-wavelength payloads, with Visible Emission Line Coronagraph (VELC) being the prime payload. It is an internally occulted coronagraph with four channels to image the Sun at 5000 Å~ in the field of view 1.05 - 3 \rsun, and to pursue spectroscopy at 5303 Å, 7892 Å~ and 10747 Å~ channels in the FOV (1.05 - 1.5 \rsun). In addition, spectropolarimetry is planned at 10747 Å~ channel. Therefore, VELC has three sCMOS detectors and one InGaAs detector. In this article, we aim to describe the technical details and specifications of the detectors achieved by way of thermo-vacuum calibration at the CREST campus of the Indian Institute of Astrophysics, Bangalore, India. Furthermore, we report the estimated conversion gain, full-well capacity, and readout noise at different temperatures. Based on the numbers, it is thus concluded that it is essential to operate the sCMOS detectors and InGaAs detectors at $-5^{\circ}$ and $-17^{\circ}$ C, respectively, at the spacecraft level."
https://arxiv.org/abs/2403.07372,2024-03-12,Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection,"['Jiahui Fu', 'Chen Gao', 'Zitian Wang', 'Lirong Yang', 'Xiaofei Wang', 'Beipeng Mu', 'Si Liu']","Recent 3D object detectors typically utilize multi-sensor data and unify multi-modal features in the shared bird's-eye view (BEV) representation space. However, our empirical findings indicate that previous methods have limitations in generating fusion BEV features free from cross-modal conflicts. These conflicts encompass extrinsic conflicts caused by BEV feature construction and inherent conflicts stemming from heterogeneous sensor signals. Therefore, we propose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly eliminate the extrinsic/inherent conflicts in BEV space and produce improved multi-modal BEV features. Specifically, we devise a Semantic-guided Flow-based Alignment (SFA) module to resolve extrinsic conflicts via unifying spatial distribution in BEV space before fusion. Moreover, we design a Dissolved Query Recovering (DQR) mechanism to remedy inherent conflicts by preserving objectness clues that are lost in the fusion BEV feature. In general, our method maximizes the effective information utilization of each modality and leverages inter-modal complementarity. Our method achieves state-of-the-art performance in the highly competitive nuScenes 3D object detection dataset. The code is released at https://github.com/fjhzhixi/ECFusion."
https://arxiv.org/abs/2403.07371,2024-03-12,Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of Altered Diffusion Models,"['Phuong Dam', 'Jihoon Jeong', 'Anh Tran', 'Daeyoung Kim']","This study discusses the critical issues of Virtual Try-On in contemporary e-commerce and the prospective metaverse, emphasizing the challenges of preserving intricate texture details and distinctive features of the target person and the clothes in various scenarios, such as clothing texture and identity characteristics like tattoos or accessories. In addition to the fidelity of the synthesized images, the efficiency of the synthesis process presents a significant hurdle. Various existing approaches are explored, highlighting the limitations and unresolved aspects, e.g., identity information omission, uncontrollable artifacts, and low synthesis speed. It then proposes a novel diffusion-based solution that addresses garment texture preservation and user identity retention during virtual try-on. The proposed network comprises two primary modules - a warping module aligning clothing with individual features and a try-on module refining the attire and generating missing parts integrated with a mask-aware post-processing technique ensuring the integrity of the individual's identity. It demonstrates impressive results, surpassing the state-of-the-art in speed by nearly 20 times during inference, with superior fidelity in qualitative assessments. Quantitative evaluations confirm comparable performance with the recent SOTA method on the VITON-HD and Dresscode datasets."
https://arxiv.org/abs/2403.07370,2024-03-12,Superconducting switching jump induced missing first Shapiro step in Al-InSb nanosheet Josephson junctions,"['Xingjun Wu', 'Haitian Su', 'Chuanchang Zeng', 'Ji-Yin Wang', 'Shili Yan', 'Dong Pan', 'Jianhua Zhao', 'Po Zhang', 'H. Q. Xu']","The absence of odd-order Shapiro steps is one of the predicted signatures for topological superconductors. Experimentally, the missing first-order Shapiro step has been reported in several superconducting systems presumably to be topologically non-trivial, as well as in the topologically trivial regime of superconductor-semiconductor Josephson junctions. In this work, we revisit the missing first Shapiro step signature in the topologically trivial regime of Al-InSb nanosheet Josephson junctions under microwave irradiation. The missing first Shapiro step is found to be accompanied by a sharp voltage jump during the superconducting switching and reappears when the jump is softened by increasing temperature or magnetic field. The missing first Shapiro step also reappears with an increased microwave frequency. The sharp switching jump, existing without microwave irradiation, deviates from the relation given by the standard resistively shunted junction (RSJ) model. Missing Shapiro step signatures are qualitatively captured by introducing the sharp voltage jump into the RSJ model. This work reveals a common, yet overlooked, phenomenon that leads to the missing first Shapiro step, providing a new perspective on fractional Josephson experiments."
https://arxiv.org/abs/2403.07369,2024-03-12,Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery,"['Haiyang Zheng', 'Nan Pu', 'Wenjing Li', 'Nicu Sebe', 'Zhun Zhong']","In this paper, we study the problem of Generalized Category Discovery (GCD), which aims to cluster unlabeled data from both known and unknown categories using the knowledge of labeled data from known categories. Current GCD methods rely on only visual cues, which however neglect the multi-modality perceptive nature of human cognitive processes in discovering novel visual categories. To address this, we propose a two-phase TextGCD framework to accomplish multi-modality GCD by exploiting powerful Visual-Language Models. TextGCD mainly includes a retrieval-based text generation (RTG) phase and a cross-modality co-teaching (CCT) phase. First, RTG constructs a visual lexicon using category tags from diverse datasets and attributes from Large Language Models, generating descriptive texts for images in a retrieval manner. Second, CCT leverages disparities between textual and visual modalities to foster mutual learning, thereby enhancing visual GCD. In addition, we design an adaptive class aligning strategy to ensure the alignment of category perceptions between modalities as well as a soft-voting mechanism to integrate multi-modality cues. Experiments on eight datasets show the large superiority of our approach over state-of-the-art methods. Notably, our approach outperforms the best competitor, by 7.7% and 10.8% in All accuracy on ImageNet-1k and CUB, respectively."
https://arxiv.org/abs/2403.07368,2024-03-12,Groups with infinite linearly ordered products,['Vincent Bagayoko'],"We introduce a formalism for considering infinite, linearly ordered products in groups. Using this, we define infinite compositions in certain groups of formal power series and show that such series can sometimes be represented as infinite, linearly ordered, semidirect products of ordered Abelian groups."
https://arxiv.org/abs/2403.07367,2024-03-12,Observations of spiral and streamer on a candidate proto-brown dwarf,"['B. Riaz', 'D. Stamatellos', 'M. Machida']","Spirals and streamers are the hallmarks of mass accretion during the early stages of star formation. We present the first observations of a large-scale spiral and a streamer towards a very young brown dwarf candidate in its early formation stages. These observations show, for the first time, the influence of external environment that results in asymmetric mass accretion via feeding filaments onto a candidate proto-brown dwarf in the making. The impact of the streamer has produced emission in warm carbon-chain species close to the candidate proto-brown dwarf. Two contrasting scenarios, a pseudo-disk twisted by core rotation and the collision of dense cores, can both explain these structures. The former argues for the presence of a strong magnetic field in brown dwarf formation while the latter suggests that a minimal magnetic field allows large-scale spirals and clumps to form far from the candidate proto-brown dwarf."
https://arxiv.org/abs/2403.07366,2024-03-12,Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors,"['Jonghyun Lee', 'Dahuin Jung', 'Saehyung Lee', 'Junsung Park', 'Juhyeon Shin', 'Uiwon Hwang', 'Sungroh Yoon']","Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for unseen test data. The primary challenge of TTA is limited access to the entire test dataset during online updates, causing error accumulation. To mitigate it, TTA methods have utilized the model output's entropy as a confidence metric that aims to determine which samples have a lower likelihood of causing error. Through experimental studies, however, we observed the unreliability of entropy as a confidence metric for TTA under biased scenarios and theoretically revealed that it stems from the neglect of the influence of latent disentangled factors of data on predictions. Building upon these findings, we introduce a novel TTA method named Destroy Your Object (DeYO), which leverages a newly proposed confidence metric named Pseudo-Label Probability Difference (PLPD). PLPD quantifies the influence of the shape of an object on prediction by measuring the difference between predictions before and after applying an object-destructive transformation. DeYO consists of sample selection and sample weighting, which employ entropy and PLPD concurrently. For robust adaptation, DeYO prioritizes samples that dominantly incorporate shape information when making predictions. Our extensive experiments demonstrate the consistent superiority of DeYO over baseline methods across various scenarios, including biased and wild. Project page is publicly available at https://whitesnowdrop.github.io/DeYO/."
https://arxiv.org/abs/2403.07365,2024-03-12,On exactly solvable Yang-Baxter models and enhanced symmetries,"['Khalil Idiab', 'Stijn J. van Tongeren']","We study Yang-Baxter deformations of the flat space string that result in exactly solvable models, finding the Nappi-Witten model and its higher dimensional generalizations. We then consider the spectra of these models obtained by canonical quantization in light-cone gauge, and match them with an integrability-based Bethe ansatz approach. By considering a generalized light-cone gauge we can describe the model by a nontrivially Drinfel'd twisted S matrix, explicitly verifying the twisted structure expected for such deformations. Next, the reformulation of the Nappi-Witten model as a Yang-Baxter deformation shows that Yang-Baxter models can have more symmetries than suggested by the $r$ matrix defining the deformation. We discuss these enhanced symmetries in more detail for some trivial and nontrivial examples. Finally, we observe that there are nonunimodular but Weyl-invariant Yang-Baxter models of a type not previously considered."
https://arxiv.org/abs/2403.07364,2024-03-12,Hybrid Kinetics Embedding Framework for Dynamic PET Reconstruction,"['Yubo Ye', 'Huafeng Liu', 'Linwei Wang']","In dynamic positron emission tomography (PET) reconstruction, the importance of leveraging the temporal dependence of the data has been well appreciated. Current deep-learning solutions can be categorized in two groups in the way the temporal dynamics is modeled: data-driven approaches use spatiotemporal neural networks to learn the temporal dynamics of tracer kinetics from data, which relies heavily on data supervision; physics-based approaches leverage \textit{a priori} tracer kinetic models to focus on inferring their parameters, which relies heavily on the accuracy of the prior kinetic model. In this paper, we marry the strengths of these two approaches in a hybrid kinetics embedding (HyKE-Net) framework for dynamic PET reconstruction. We first introduce a novel \textit{hybrid} model of tracer kinetics consisting of a physics-based function augmented by a neural component to account for its gap to data-generating tracer kinetics, both identifiable from data. We then embed this hybrid model at the latent space of an encoding-decoding framework to enable both supervised and unsupervised identification of the hybrid kinetics and thereby dynamic PET reconstruction. Through both phantom and real-data experiments, we demonstrate the benefits of HyKE-Net -- especially in unsupervised reconstructions -- over existing physics-based and data-driven baselines as well as its ablated formulations where the embedded tracer kinetics are purely physics-based, purely neural, or hybrid but with a non-adaptable neural component."
https://arxiv.org/abs/2403.07363,2024-03-12,A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees,"['Yingtao Ren', 'Xiaomin Zhu', 'Kaiyuan Bai', 'Runtong Zhang']","Classification is essential to the applications in the field of data mining, artificial intelligence, and fault detection. There exists a strong need in developing accurate, suitable, and efficient classification methods and algorithms with broad applicability. Random forest is a general algorithm that is often used for classification under complex conditions. Although it has been widely adopted, its combination with diverse fuzzy theory is still worth exploring. In this paper, we propose the intuitionistic fuzzy random forest (IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees (IFDT). Such trees in forest use intuitionistic fuzzy information gain to select features and consider hesitation in information transmission. The proposed method enjoys the power of the randomness from bootstrapped sampling and feature selection, the flexibility of fuzzy logic and fuzzy sets, and the robustness of multiple classifier systems. Extensive experiments demonstrate that the IFRF has competitative and superior performance compared to other state-of-the-art fuzzy and ensemble algorithms. IFDT is more suitable for ensemble learning with outstanding classification accuracy. This study is the first to propose a random forest ensemble based on the intuitionistic fuzzy theory."
https://arxiv.org/abs/2403.07362,2024-03-12,Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning,"['Chongyu Fan', 'Jiancheng Liu', 'Alfred Hero', 'Sijia Liu']","The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization level to emulate worst-case scenarios, while simultaneously engaging in standard training and unlearning at the lower level, achieving a balance between data influence erasure and model utility. Our proposal offers a worst-case evaluation of MU's resilience and effectiveness. Through extensive experiments across different datasets (including CIFAR-10, 100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image classifiers and generative models), we expose critical pros and cons in existing (approximate) unlearning strategies. Our results illuminate the complex challenges of MU in practice, guiding the future development of more accurate and robust unlearning algorithms. The code is available at https://github.com/OPTML-Group/Unlearn-WorstCase."
https://arxiv.org/abs/2403.07361,2024-03-12,Migration of two Interacting Micro-Confined Deformable Drops Under an Imposed Temperature Gradient,"['Sayak Ray', 'Sudipta Ray', 'Prof. Suman Chakraborty']","A tiny drop of one liquid, suspended within another, may be set into motion aligned with an imposed thermal gradient, as influenced by thermocapillary action stemming from the gradients in interfacial tension due to the local variations in temperature. In real-world situations, however, such drops do not remain in isolation, as they interact with their neighbouring entities including other drops in the proximity as well as a nearby solid boundary, setting up a complex interplay between the confinement-mediated interactions and three-dimensional nature of the droplet dynamics. In this study, we present numerical solutions for the migration dynamics of a tightly-confined drop-couple, incorporating deformable interfaces, film flow, and Marangoni effects in the presence of dynamically evolving thermocapillary stresses induced by an imposed uniform temperature gradient. Unlike prior investigations, our work highlights the influence of the confinement towards orchestrating non-trivial features of drop migration, as dictated by an intricate coupling of the thermal and flow fields amidst the interferences of the domain boundaries. The study reveals that hydrodynamic interactions resulting from a juxtaposition of these influences deform the drops in a unique manner as compared to the characteristics evidenced from previously reported studies, causing a distortion of the local thermal fields around them. The consequent alteration in the drop velocities is shown to govern their migration in a distinctive manner, presenting unique signatures as compared to more restrictive scenarios studied previously. These findings hold significance in designing thermocapillary-driven micro-confined systems for controlling drop trajectories under an imposed thermal field, bearing far-reaching implications in a plethora of overarching applications ranging from droplet microfluidics to space technology."
https://arxiv.org/abs/2403.07360,2024-03-12,Optimization of Pressure Management Strategies for Geological CO2 Sequestration Using Surrogate Model-based Reinforcement Learning,"['Jungang Chen', 'Eduardo Gildin', 'John E. Killough']","Injecting greenhouse gas into deep underground reservoirs for permanent storage can inadvertently lead to fault reactivation, caprock fracturing and greenhouse gas leakage when the injection-induced stress exceeds the critical threshold. Extraction of pre-existing fluids at various stages of injection process, referred as pressure management, can mitigate associated risks and lessen environmental impact. However, identifying optimal pressure management strategies typically requires thousands of full-order simulations due to the need for function evaluations, making the process computationally prohibitive. This paper introduces a novel surrogate model-based reinforcement learning method for devising optimal pressure management strategies for geological CO2 sequestration efficiently. Our approach comprises two steps. Firstly, a surrogate model is developed through the embed to control method, which employs an encoder-transition-decoder structure to learn latent dynamics. Leveraging this proxy model, reinforcement learning is utilized to find an optimal strategy that maximizes economic benefits while satisfying various control constraints. The reinforcement learning agent receives the latent state space representation and immediate reward tailored for CO2 sequestration and choose real-time controls which are subject to predefined engineering constraints in order to maximize the long-term cumulative rewards. To demonstrate its effectiveness, this framework is applied to a compositional simulation model where CO2 is injected into saline aquifer. The results reveal that our surrogate model-based reinforcement learning approach significantly optimizes CO2 sequestration strategies, leading to notable economic gains compared to baseline scenarios."
https://arxiv.org/abs/2403.07359,2024-03-13,FSC: Few-point Shape Completion,"['Xianzu Wu', 'Xianfeng Wu', 'Tianyu Luan', 'Yajing Bai', 'Zhongyuan Lai', 'Junsong Yuan']","While previous studies have demonstrated successful 3D object shape completion with a sufficient number of points, they often fail in scenarios when a few points, e.g. tens of points, are observed. Surprisingly, via entropy analysis, we find that even a few points, e.g. 64 points, could retain substantial information to help recover the 3D shape of the object. To address the challenge of shape completion with very sparse point clouds, we then propose Few-point Shape Completion (FSC) model, which contains a novel dual-branch feature extractor for handling extremely sparse inputs, coupled with an extensive branch for maximal point utilization with a saliency branch for dynamic importance assignment. This model is further bolstered by a two-stage revision network that refines both the extracted features and the decoder output, enhancing the detail and authenticity of the completed point cloud. Our experiments demonstrate the feasibility of recovering 3D shapes from a few points. The proposed Few-point Shape Completion (FSC) model outperforms previous methods on both few-point inputs and many-point inputs, and shows good generalizability to different object categories."
https://arxiv.org/abs/2403.07358,2024-03-12,A novel fast iterative moment method for near-continuum flows,"['Guanghan Li', 'Chunwu Wang', 'Zhicheng Hu']","In this paper, we develop a novel fast iterative moment method for the steady-state simulation of near-continuum flows, which are modeled by the high-order moment system derived from the Boltzmann-BGK equation. The fast convergence of the present method is mainly achieved by alternately solving the moment system and the hydrodynamic equations with compatible constitutive relations and boundary conditions. To be specific, the compatible hydrodynamic equations are solved in each iteration to get improved predictions of macroscopic quantities, which are subsequently utilized to expedite the evolution of the moment system. Additionally, a semi-implicit scheme treating the collision term implicitly is introduced for the moment system. With cell-by-cell sweeping strategy, the resulting alternating iteration can be further accelerated for steady-state computation. It is also worth mentioning that such an alternating iteration works well with the nonlinear multigrid method. Numerical experiments for planar Couette flow, shock structure, and lid-driven cavity flow are carried out to investigate the performance of the proposed fast iterative moment method, and all results show wonderful efficiency and robustness."
https://arxiv.org/abs/2403.07357,2024-03-12,Real-time observation of picosecond-timescale optical quantum entanglement toward ultrafast quantum information processing,"['Akito Kawasaki', 'Hector Brunel', 'Ryuhoh Ide', 'Takumi Suzuki', 'Takahiro Kashiwazaki', 'Asuka Inoue', 'Takeshi Umeki', 'Taichi Yamashima', 'Atsushi Sakaguchi', 'Kan Takase', 'Mamoru Endo', 'Warit Asavanant', 'Akira Furusawa']","Entanglement is a fundamental resource of various optical quantum-information-processing (QIP) applications. Towards high-speed QIP system, entanglement should be encoded in short wavepackets. We report real-time observation of ultrafast optical Einstein-Podolsky-Rosen (EPR) correlation at a picosecond timescale in a continuous-wave (CW) system. Optical phase-sensitive amplification using 6-THz-bandwidth waveguide-optical-parametric amplifier enhances the effective efficiency of 70-GHz-bandwidth homodyne detectors, mainly used in 5th-generation telecommunication, enabling its use in real-time quantum-state measurement. While power measurement using frequency scanning, i.e., optical spectrum analyzer, is not performed in real-time, our observation is demonstrated through real-time amplitude measurement and can be directly employed in QIP applications. Observed EPR states show quantum correlation of 4.5 dB below shotnoise level encoded in wavepackets with 40-ps period, equivalent to 25-GHz repetition -- ${10^3}$ times faster than previous entanglement observation in CW system. The quantum correlation of 4.5 dB is already sufficient for several QIP applications, and our system can be readily extended to large-scale entanglement. Moreover, our scheme has high compatibility with optical communication technology such as wavelength-division multiplexing, and femtosecond-timescale observation is also feasible. Our demonstration is paradigm shift in accelerating accessible quantum correlation, the foundational resource of all quantum applications, from the nanosecond to picosecond timescale, enabling ultra-fast optical QIP."
https://arxiv.org/abs/2403.07356,2024-03-12,Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning,"['Mark D. McDonnell', 'Dong Gong', 'Ehsan Abbasnejad', 'Anton van den Hengel']","Continual learning requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed. It is rare, however, that the data and task changes are completely unpredictable. Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it. We show here that the combination of a large language model and an image generation model can similarly provide useful premonitions as to how a continual learning challenge might develop over time. We use the large language model to generate text descriptions of semantically related classes that might potentially appear in the data stream in future. These descriptions are then rendered using Stable Diffusion to generate new labelled image samples. The resulting synthetic dataset is employed for supervised pre-training, but is discarded prior to commencing continual learning, along with the pre-training classification head. We find that the backbone of our pre-trained networks can learn representations useful for the downstream continual learning problem, thus becoming a valuable input to any existing continual learning method. Although there are complexities arising from the domain gap between real and synthetic images, we show that pre-training models in this manner improves multiple Class Incremenal Learning (CIL) methods on fine-grained image classification benchmarks. Supporting code can be found at https://github.com/cl-premonition/premonition."
https://arxiv.org/abs/2403.07355,2024-03-12,Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems,"['Junyong Shin', 'Yujin Kang', 'Yo-Seb Jeon']","This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead."
https://arxiv.org/abs/2403.07354,2024-03-12,BID: Boundary-Interior Decoding for Unsupervised Temporal Action Localization Pre-Trainin,"['Qihang Fang', 'Chengcheng Tang', 'Shugao Ma', 'Yanchao Yang']","Skeleton-based motion representations are robust for action localization and understanding for their invariance to perspective, lighting, and occlusion, compared with images. Yet, they are often ambiguous and incomplete when taken out of context, even for human annotators. As infants discern gestures before associating them with words, actions can be conceptualized before being grounded with labels. Therefore, we propose the first unsupervised pre-training framework, Boundary-Interior Decoding (BID), that partitions a skeleton-based motion sequence into discovered semantically meaningful pre-action segments. By fine-tuning our pre-training network with a small number of annotated data, we show results out-performing SOTA methods by a large margin."
https://arxiv.org/abs/2403.07353,2024-03-13,Graph Unlearning with Efficient Partial Retraining,"['Jiahao Zhang', 'Lin Wang', 'Shijie Wang', 'Wenqi Fan']","Graph Neural Networks (GNNs) have achieved remarkable success in various real-world applications. However, GNNs may be trained on undesirable graph data, which can degrade their performance and reliability. To enable trained GNNs to efficiently unlearn unwanted data, a desirable solution is retraining-based graph unlearning, which partitions the training graph into subgraphs and trains sub-models on them, allowing fast unlearning through partial retraining. However, the graph partition process causes information loss in the training graph, resulting in the low model utility of sub-GNN models. In this paper, we propose GraphRevoker, a novel graph unlearning framework that better maintains the model utility of unlearnable GNNs. Specifically, we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation. We conduct extensive experiments to demonstrate the superiority of our proposed approach."
https://arxiv.org/abs/2403.07352,2024-03-12,Predicting the Scaling Relations between the Dark Matter Halo Mass and Observables from Generalised Profiles I: Kinematic Tracers,"['Andrew Sullivan', 'Chris Power', 'Connor Bottrell']","We investigate the relationship between a dark matter halo's mass profile and measures of the velocity dispersion of kinematic tracers within its gravitational potential. By predicting the scaling relation of the halo mass with the aperture velocity dispersion, $M_\mathrm{vir} - σ_\mathrm{ap}$, we present the expected form and dependence of this halo mass tracer on physical parameters within our analytic halo model: parameterised by the halo's negative inner logarithmic density slope, $α$, its concentration parameter, $c$, and its velocity anisotropy parameter, $β$. For these idealised halos, we obtain a general solution to the Jeans equation, which is projected over the line of sight and averaged within an aperture to form the corresponding aperture velocity dispersion profile. Through dimensional analysis, the $M_\mathrm{vir} - σ_\mathrm{ap}$ scaling relation is devised explicitly in terms of analytical bounds for these aperture velocity dispersion profiles: allowing constraints to be placed on this relation for motivated parameter choices. We predict the $M_{200} - σ_\mathrm{ap}$ and $M_{500} - σ_\mathrm{ap}$ scaling relations, each with an uncertainty of $60.5\%$ and $56.2\%$, respectively. These halo mass estimates are found to be weakly sensitive to the halo's concentration and mass scale, and most sensitive to the size of the aperture radius in which the aperture velocity dispersion is measured, the maximum value for the halo's inner slope, and the minimum and maximum values of the velocity anisotropy. Our results show that a halo's structural and kinematic profiles impose only a minor uncertainty in estimating its mass. Consequently, spectroscopic surveys aimed at constraining the halo mass using kinematic tracers can focus on characterising other, more complex sources of uncertainty and observational systematics."
https://arxiv.org/abs/2403.07351,2024-03-12,An Effective Way to Determine the Separability of Quantum State,"['Ma-Cheng Yang', 'Cong-Feng Qiao']","We propose in this work a practical approach, by virtue of correlation matrices of the generic observables, to study the long lasting tough issue of quantum separability. Some general separability conditions are set up through constructing a measurement-induced Bloch space. In essence, these conditions are established due to the self constraint in the space of quantum states. The new approach can not only reproduce many of the prevailing entanglement criteria, but also lead to even stronger results and manifest superiority for some bound entangled states. Moreover, as a by product, the new criteria are found directly transformable to the entanglement witness operators."
https://arxiv.org/abs/2403.07350,2024-03-12,KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models,"['Han Huang', 'Haitian Zhong', 'Qiang Liu', 'Shu Wu', 'Liang Wang', 'Tieniu Tan']","Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing data. We conducted experiments of different editing methods on five LVLMs, and thoroughly analyze how these methods impact the models. The results reveal strengths and deficiencies of these methods and, hopefully, provide insights into potential avenues for future research."
https://arxiv.org/abs/2403.07349,2024-03-12,On Mirror Symmetry and Irrationality of Zeta Values,"['Andreas Malmendier', 'Michael T. Schultz']","A fundamental object of study in mirror symmetry of $n$-dimensional Fano varieties is the A-side connection on small quantum cohomology. When the Picard rank is 1, the Borel transform relates the quantum differential operator of the Fano to the Picard-Fuchs operator of the associated pencil of anticanonical Calabi-Yau $(n-1)$-folds on the Fano variety. Expanding on related work by W. Yang on the Beukers-Peters pencil of K3 surfaces associated with Apéry's proof for the irrationality of $ζ(3)$, for such operators we define holomorphic prepotentials, virtual Yukawa couplings, and virtual instanton numbers, analogous to the usual ingredients of Calabi-Yau mirror symmetry. We prove that when the underlying Calabi-Yau operator is modular, the virtual Yukawa coupling is a modular form of weight-$(n+1)$, with the holomorphic prepotential as an Eichler integral. We then analyze the quantum differential operators for modular pencils of K3 surfaces arising as anticanonical linear systems for the 17 deformation classes of Fano threefolds of Picard rank-1 classified by Iskovskikh from the perspective of Golyshev \& Zagier's proof of the Gamma conjecture for such Fanos, the natural setting of Yang's work. Here, the virtual instanton numbers are proven to be periodic integers with period equal to the level of the modular subgroup. Finally, we conjecture that the geometric nature of these virtual instanton numbers can be understood in terms of genus zero Gromov-Witten invariants of certain local Calabi-Yau fourfolds."
https://arxiv.org/abs/2403.07348,2024-03-12,Linear and smooth oriented equivalence of orthogonal representations of finite groups,"['Luis Eduardo García-Hernández', 'Ben Williams']","Let $n\le 5$ be an integer, and let $Γ$ be a finite group. We prove that if $ρ, ρ': Γ\to O(n)$ are two representations that are conjugate by an orientation-preserving diffeomorphism, then they are conjugate by an element of $SO(n)$. In the process, we prove that if $G \subset O(4)$ is a finite group, then exactly one of the following is true: the elements of $G$ have a common invariant $1$-dimensional subspace in $\mathbb{R}^4$; some element of $G$ has no invariant $1$-dimensional subspace; or $G$ is conjugate to a specific group $K \subset O(4)$ of order $16$."
https://arxiv.org/abs/2403.07347,2024-03-12,Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture,"['Fei Wang', 'Dan Guo', 'Kun Li', 'Zhun Zhong', 'Meng Wang']","Video Motion Magnification (VMM) aims to reveal subtle and imperceptible motion information of objects in the macroscopic world. Prior methods directly model the motion field from the Eulerian perspective by Representation Learning that separates shape and texture or Multi-domain Learning from phase fluctuations. Inspired by the frequency spectrum, we observe that the low-frequency components with stable energy always possess spatial structure and less noise, making them suitable for modeling the subtle motion field. To this end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion Magnification with a Multi-level Isomorphic Architecture to capture multi-level high-frequency details and a stable low-frequency structure (motion field) in video space. Since high-frequency details and subtle motions are susceptible to information degradation due to their inherent subtlety and unavoidable external interference from noise, we carefully design Sparse High/Low-pass Filters to enhance the integrity of details and motion structures, and a Sparse Frequency Mixer to promote seamless recoupling. Besides, we innovatively design a contrastive regularization for this task to strengthen the model's ability to discriminate irrelevant features, reducing undesired motion magnification. Extensive experiments on both Real-world and Synthetic Datasets show that our FD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\times$ and boosts inference speed by 1.68$\times$ than the latest method. Our code is available at https://github.com/Jiafei127/FD4MM."
https://arxiv.org/abs/2403.07346,2024-03-12,Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction,"['Jianping Jiang', 'Xinyu Zhou', 'Bingxuan Wang', 'Xiaoming Deng', 'Chao Xu', 'Boxin Shi']","Reliable hand mesh reconstruction (HMR) from commonly-used color and depth sensors is challenging especially under scenarios with varied illuminations and fast motions. Event camera is a highly promising alternative for its high dynamic range and dense temporal resolution properties, but it lacks key texture appearance for hand mesh reconstruction. In this paper, we propose EvRGBHand -- the first approach for 3D hand mesh reconstruction with an event camera and an RGB camera compensating for each other. By fusing two modalities of data across time, space, and information dimensions,EvRGBHand can tackle overexposure and motion blur issues in RGB-based HMR and foreground scarcity and background overflow issues in event-based HMR. We further propose EvRGBDegrader, which allows our model to generalize effectively in challenging scenes, even when trained solely on standard scenes, thus reducing data acquisition costs. Experiments on real-world data demonstrate that EvRGBHand can effectively solve the challenging issues when using either type of camera alone via retaining the merits of both, and shows the potential of generalization to outdoor scenes and another type of event camera."
https://arxiv.org/abs/2403.07345,2024-03-12,The transition operator of a random walk perturbated by sparse potentials,"['Takuya Mine', 'Nobuo Yoshida']","We consider an operator $P_V=(1+V)P$ on $\ell^2(Z^d)$, where $P$ is the transition operator of a symmetric irreducible random walk, and $V$ is a ``sparse'' potential. We first characterize the essential spectra of this operator. Secondly, we prove that all the eigenfunctions which correspond to discrete spectra decay exponentially fast. Thirdly, we give a sufficient condition for this operator to have an absolute spectral gap at the right edge of the spectra. Finally, as an application of the absolute spectral gap and the exponential decay of the eigenfunctions, we prove a limit theorem for the random walk under the Gibbs measure associated to the potential $V$."
https://arxiv.org/abs/2403.07344,2024-03-12,Electronic Structure of Superconducting Infinite-Layer Lanthanum Nickelates,"['Wenjie Sun', 'Zhicheng Jiang', 'Chengliang Xia', 'Bo Hao', 'Yueying Li', 'Shengjun Yan', 'Maosen Wang', 'Hongquan Liu', 'Jianyang Ding', 'Jiayu Liu', 'Zhengtai Liu', 'Jishan Liu', 'Hanghui Chen', 'Dawei Shen', 'Yuefeng Nie']","Revealing the momentum-resolved electronic structure of infinite-layer nickelates is essential for understanding this new class of unconventional superconductors, but has been hindered by the formidable challenges in improving the sample quality. In this work, we report for the first time the angle-resolved photoemission spectroscopy of superconducting La$_{0.8}$Sr$_{0.2}$NiO$_{2}$ films prepared by molecular beam epitaxy and ${\mathrm{\textit{in situ}}}$ atomic-hydrogen reduction. The measured Fermi topology closely matches theoretical calculations, showing a large Ni-$d_{x^2-y^2}$ derived Fermi sheet that evolves from hole-like to electron-like along $k_{z}$, and a three-dimensional (3D) electron pocket centered at Brillouin zone corner. The Ni-$d_{x^2-y^2}$ derived bands show a mass enhancement ($m^*/m_{\rm{DFT}}$) of 2-3,while the 3D electron band shows negligible band renormalization. Moreover, the Ni-$d_{x^2-y^2}$ derived states also display a band dispersion anomaly at higher binding energy, reminiscent of the waterfall feature and kinks observed in cuprates."
https://arxiv.org/abs/2403.07343,2024-03-12,Large fluctuations in the Sky,['Sayantan Choudhury'],"Renormalization of quantum loop effects generated from large fluctuations is a hugely debatable topic of research these days which rules out the Primordial Black Hole (PBH) formation within the framework of single-field inflation. In this article, we briefly discuss that the correct implementation of regularization, renormalization, and resummation techniques in a setup described by an ultra-slow-roll phase sandwiched between two slow-roll phases in the presence of smooth or sharp transitions can lead to a stringent constraint on the PBH mass (i.e. ${\cal O}(10^{2}{\rm gm}$)), which we advertise as a new No-go theorem. Finally, we will give some of the possible way-outs using which one can evade this proposed No-go theorem and produce solar/sub-solar mass PBHs."
https://arxiv.org/abs/2403.07342,2024-03-12,Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning,"['Qiao Sun', 'Liujia Yang', 'Minghao Ma', 'Nanyang Ye', 'Qinying Gu']","Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of fine-grained sentiment analysis, aiming to extract structured sentiment triplets from unstructured textual data. Existing approaches to ASTE often complicate the task with additional structures or external data. In this research, we propose a novel tagging scheme and employ a contrastive learning approach to mitigate these challenges. The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead. Notably, even in the era of Large Language Models (LLMs), our method exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning scenarios. This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of large language models."
https://arxiv.org/abs/2403.07341,2024-03-12,Non-linear characterization of Jordan $*$-isomorphisms via maps on positive cones of $C^*$-algebras,"['Osamu Hatori', 'Shiho Oi']",We study maps between positive definite or positive semidefinite cones of unital $C^*$-algebras. We describe surjective maps that preserve
https://arxiv.org/abs/2403.07340,2024-03-12,Direct and inverse time-harmonic scattering by Dirichlet periodic curves with local perturbations,"['Guanghui Hu', 'Andreas Kirsch']","This is a continuation of the authors' previous work (A. Kirsch, Math. Meth. Appl. Sci., 45 (2022): 5737-5773.) on well-posedness of time-harmonic scattering by locally perturbed periodic curves of Dirichlet kind. The scattering interface is supposed to be given by a non-self-intersecting Lipschitz curve. We study properties of the Green's function and prove new well-posedness results for scattering of plane waves at a propagative wave number. In such a case there exist guided waves to the unperturbed problem, which are also known as Bounded States in the Continuity (BICs) in physics. In this paper uniqueness of the forward scattering follows from an orthogonal constraint condition enforcing on the total field to the unperturbed scattering problem. This constraint condition, which is also valid under the Neumann boundary condition, is derived from the singular perturbation arguments and also from the approach of approximating a plane wave by point source waves. For the inverse problem of determining the defect, we prove several uniqueness results using a finite or infinite number of point source and plane waves, depending on whether a priori information on the size and height of the defect is available."
https://arxiv.org/abs/2403.07339,2024-03-12,IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers,"['Zhanpeng Zeng', 'Karthikeyan Sankaralingam', 'Vikas Singh']","GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need -- for {\em both} training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\em low} bit-width integers, the existence of a few heavy hitter entries make it difficult to achieve efficiency gains via the exclusive use of low bit-width GEMMs alone. To address this issue, we develop a simple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\em unpack} a matrix with large integer entries into a larger matrix whose entries all lie within the representable range of arbitrarily low bit-width integers. This allows {\em equivalence} with the original GEMM, i.e., the exact result can be obtained using purely low bit-width integer GEMMs. This comes at the cost of additional operations -- we show that for many popular models, this overhead is quite small."
https://arxiv.org/abs/2403.07338,2024-03-13,D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic Communications,"['Jianhao Huang', 'Kai Yuan', 'Chuan Huang', 'Kaibin Huang']","Semantic communications (SemCom) have emerged as a new paradigm for supporting sixth-generation applications, where semantic features of data are transmitted using artificial intelligence algorithms to attain high communication efficiencies. Most existing SemCom techniques utilize deep neural networks (DNNs) to implement analog source-channel mappings, which are incompatible with existing digital communication architectures. To address this issue, this paper proposes a novel framework of digital deep joint source-channel coding (D$^2$-JSCC) targeting image transmission in SemCom. The framework features digital source and channel codings that are jointly optimized to reduce the end-to-end (E2E) distortion. First, deep source coding with an adaptive density model is designed to encode semantic features according to their distributions. Second, digital channel coding is employed to protect encoded features against channel distortion. To facilitate their joint design, the E2E distortion is characterized as a function of the source and channel rates via the analysis of the Bayesian model and Lipschitz assumption on the DNNs. Then to minimize the E2E distortion, a two-step algorithm is proposed to control the source-channel rates for a given channel signal-to-noise ratio. Simulation results reveal that the proposed framework outperforms classic deep JSCC and mitigates the cliff and leveling-off effects, which commonly exist for separation-based approaches."
https://arxiv.org/abs/2403.07337,2024-03-12,Analysis of Intelligent Reflecting Surface-Enhanced Mobility Through a Line-of-Sight State Transition Model,"['Hongtao Zhang', 'Haoyan Wei']","Rapid signal fluctuations due to blockage effects cause excessive handovers (HOs) and degrade mobility performance. By reconfiguring line-of-sight (LoS) Links through passive reflections, intelligent reflective surface (IRS) has the potential to address this issue. Due to the lack of introducing blocking effects, existing HO analyses cannot capture excessive HOs or exploit enhancements via IRSs. This paper proposes an LoS state transition model enabling analysis of mobility enhancement achieved by IRS-reconfigured LoS links, where LoS link blocking and reconfiguration utilizing IRS during user movement are explicitly modeled as stochastic processes. Specifically, the condition for blocking LoS links is characterized as a set of possible blockage locations, the distribution of available IRSs is thinned by the criteria for reconfiguring LoS links. In addition, BSs potentially handed over are categorized by probabilities of LoS states to enable HO decision analysis. By projecting distinct gains of LoS states onto a uniform equivalent distance criterion, mobility enhanced by IRS is quantified through the compact expression of HO probability. Results show the probability of dropping into non-LoS decreases by 70% when deploying IRSs with the density of 93/km$^2$, and HOs decrease by 67% under the optimal IRS distributed deployment parameter."
https://arxiv.org/abs/2403.07336,2024-03-12,Mathematical analysis and numerical comparison of energy-conservative schemes for the Zakharov equations,"['Shuto Kawai', 'Shun Sato', 'Takayasu Matsuo']","Furihata and Matsuo proposed in 2010 an energy-conserving scheme for the Zakharov equations, as an application of the discrete variational derivative method (DVDM)."
https://arxiv.org/abs/2403.07335,2024-03-12,Out-of-time-order correlator as a detector of baryonic phase structure in holographic QCD with a theta angle,"['Si-wen Li', 'Yi-peng Zhang', 'Hao-qian Li']","We study the out-of-time-order correlators (OTOC) of Skyrmion as baryon in the D0-D4/D8 model which is holographically dual to QCD with an non-zero theta angle. The baryon state is identified to the excitation of the Skyrmion which is described by a quantum mechanical system in holography. By employing the definition of OTOC in quantum mechanics, we derive the formulas and demonstrate explicitly the numerical calculations of the OTOC. Our calculation illustrates the quantum OTOC with imaginary Lyapunov coefficient indicates the possibly metastable baryonic status in the presence of the theta angle while the classical OTOC can not, thus it reveals the theta-dependent features of QCD are dominated basically by its quantum properties. Furthermore, the OTOC also reveals the baryonic phase becomes really chaotic with real Lyapunov exponent if the theta angle increases sufficiently which agrees with the unstable baryon spectrum presented in this model. In this sense, we believe the OTOC may be treated as a tool to detect the baryonic phase structure of QCD."
https://arxiv.org/abs/2403.07334,2024-03-12,From the Fokker-Planck equation to a contact Hamiltonian system,['Shin-itiro Goto'],"The Fokker-Planck equation is one of the fundamental equations in nonequilibrium statistical mechanics, and this equation is known to be derived from the Wasserstein gradient flow equation with a free energy. This gradient flow equation describes relaxation processes and is formulated on Riemannian manifolds. Meanwhile contact Hamiltonian systems are also known to describe relaxation processes. Hence a relation between these two equations is expected to be clarified, which gives a solid foundation in geometric statistical mechanics. In this paper a class of contact Hamiltonian systems is derived from a class of the Fokker-Planck equations on Riemannian manifolds. In the course of the derivation, the Fokker-Planck equation is shown to be written as a diffusion equation with a weighted Laplacian without any approximation, which enables to employ a theory of eigenvalue problems."
https://arxiv.org/abs/2403.07333,2024-03-12,$α$-Generalized No-Scale Inflation,"['Lina Wu', 'Tianjun Li', 'Junle Pei']","We propose the $α$-generalized no-scale supergravity, and study the corresponding inflationary models. With a new parameter $0<α\leq 1$, the $α$-generalized no-scale supergravity provides the continuous connections among the generic no-scale supergravity from string theory compactifications. The resulting prediction of the CMB, spectrum index $n_s$, and tensor-to-scalar ratio $r$ can be highly consistent with the latest Planck/BICEP/Keck Array observations. Notably, the models with $α\neq 1$ give a smaller ratio $r\leq 10^{-3}$, which is flexible even under the anticipated tighter observational constraints at the future experiments. Additionally, these models have the potential to generate a broad-band stochastic gravitational wave background, and thus explain the NANOGrav 15yr signal. Furthermore, they predict the formation of primordial black holes (PBHs) with various mass scales, which could account for an significant portion of dark matter relic density in the Universe."
https://arxiv.org/abs/2403.07332,2024-03-12,Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention,"['Jinhong Wang', 'Jintai Chen', 'Danny Chen', 'Jian Wu']","In clinical practice, medical image segmentation provides useful information on the contours and dimensions of target organs or tissues, facilitating improved diagnosis, analysis, and treatment. In the past few years, convolutional neural networks (CNNs) and Transformers have dominated this area, but they still suffer from either limited receptive fields or costly long-range modeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a promising paradigm for long-range dependency modeling with linear complexity. In this paper, we introduce a Large Window-based Mamba U}-shape Network, or LMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of our LMa-UNet is its utilization of large windows, excelling in locally spatial modeling compared to small kernel-based CNNs and small window-based Transformers, while maintaining superior efficiency in global modeling compared to self-attention with quadratic complexity. Additionally, we design a novel hierarchical and bidirectional Mamba block to further enhance the global and neighborhood spatial modeling capability of Mamba. Comprehensive experiments demonstrate the effectiveness and efficiency of our method and the feasibility of using large window size to achieve large receptive fields. Codes are available at https://github.com/wjh892521292/LMa-UNet."
https://arxiv.org/abs/2403.07331,2024-03-12,LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries,"['Ziqi Yin', 'Shanshan Feng', 'Shang Liu', 'Gao Cong', 'Yew Soon Ong', 'Bin Cui']","With the proliferation of spatio-textual data, Top-k KNN spatial keyword queries (TkQs), which return a list of objects based on a ranking function that evaluates both spatial and textual relevance, have found many real-life applications. Existing geo-textual indexes for TkQs use traditional retrieval models like BM25 to compute text relevance and usually exploit a simple linear function to compute spatial relevance, but its effectiveness is limited. To improve effectiveness, several deep learning models have recently been proposed, but they suffer severe efficiency issues. To the best of our knowledge, there are no efficient indexes specifically designed to accelerate the top-k search process for these deep learning models."
https://arxiv.org/abs/2403.07330,2024-03-12,The potential of QQQ in the anisotropic background,"['Jing Zhou', 'Kazem Bitaghsir Fadafan', 'Xun Chen']","In this work, we use the AdS/CFT correspondence to study the behavior of a triply heavy baryon within anisotropic backgrounds. Beginning with the total action of the three quarks, we derive the balance equation for the three-quark system and compute the separation distance and potential energy. Our results reveal a consistent decrease in both the separation distance and potential energy for the A configuration and the B configuration as the anisotropy coefficient $a$ increases. This suggests that the presence of an anisotropic background promotes the dissolution of the three-quark system. Additionally, we compare the potential energies of the A and B configurations and observe that the A configuration has a slightly smaller potential energy, suggesting greater stability compared to the B configuration."
https://arxiv.org/abs/2403.07329,2024-03-12,Unknown Domain Inconsistency Minimization for Domain Generalization,"['Seungjae Shin', 'HeeSun Bae', 'Byeonghu Na', 'Yoon-Yeong Kim', 'Il-Chul Moon']","The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain's loss sharpness. Although SAM variants have delivered significant improvements in DG, we highlight that there's still potential for improvement in generalizing to unknown domains through the exploration on data space. This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM reduces the loss landscape inconsistency between source domain and unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. In particular, by aligning the loss landscape acquired in the source domain to the loss landscape of perturbed domains, we expect to achieve generalization grounded on these flat minima for the unknown domains. Theoretically, we validate that merging SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task. In an empirical aspect, UDIM consistently outperforms SAM variants across multiple DG benchmark datasets. Notably, UDIM shows statistically significant improvements in scenarios with more restrictive domain information, underscoring UDIM's generalization capability in unseen domains. Our code is available at \url{https://github.com/SJShin-AI/UDIM}."
https://arxiv.org/abs/2403.07328,2024-03-12,"Satisfiability to Coverage in Presence of Fairness, Matroid, and Global Constraints","['Tanmay Inamdar', 'Pallavi Jain', 'Daniel Lokshtanov', 'Abhishek Sahu', 'Saket Saurabh', 'Anannya Upasana']","In MaxSAT with Cardinality Constraint problem (CC-MaxSAT), we are given a CNF-formula $Φ$, and $k \ge 0$, and the goal is to find an assignment $β$ with at most $k$ variables set to true (also called a weight $k$-assignment) such that the number of clauses satisfied by $β$ is maximized. MaxCov can be seen as a special case of CC-MaxSAT, where the formula $Φ$ is monotone, i.e., does not contain any negative literals. CC-MaxSAT and MaxCov are extremely well-studied problems in the approximation algorithms as well as parameterized complexity literature."
https://arxiv.org/abs/2403.07327,2024-03-12,Challenges in the extraction of physics beyond the Standard Model from electron scattering,"['X. G. Wang', 'A. W. Thomas']","Precise measurements of electron and positron scattering, including parity violation, offer great promise in the search for physics beyond the Standard Model. In this context it is crucial to understand the corrections which might arise from charge symmetry violation, as well as the less well known strange and charm quark distributions. Our analysis, using state of the art parton distributions, suggests that these contributions lead to corrections in the extraction of the weak couplings $g^{eq}_{AV}$ and $g^{eq}_{VA}$ of the order $(1-2)\%$, while they are as large as $4\%$ for $g^{eq}_{AA}$, at a typical scale of $Q^2 = 10\ {\rm GeV}^2$. These results underline the importance of carrying out high precision measurements, which will not only provide information on physics beyond the Standard Model but also reduce the current uncertainties on our knowledge of the strange and charm quark distributions in the proton."
https://arxiv.org/abs/2403.07326,2024-03-12,SGE: Structured Light System Based on Gray Code with an Event Camera,"['Xingyu Lu', 'Lei Sun', 'Diyang Gu', 'Zhijie Xu', 'Kaiwei Wang']","Fast and accurate depth sensing has long been a significant research challenge. Event camera, as a device that quickly responds to intensity changes, provides a new solution for structured light (SL) systems. In this paper, we introduce Gray code into event-based SL systems for the first time. Our setup includes an event camera and Digital Light Processing (DLP) projector, enabling depth estimation through high-speed projection and decoding of Gray code patterns. By employing spatio-temporal encoding for point matching, our method is immune to timestamp noise, realizing high-speed depth estimation without loss of accuracy. The binary nature of events and Gray code minimizes data redundancy, enabling us to fully utilize sensor bandwidth at 100%. Experimental results show that our approach achieves accuracy comparable to state-of-the-art scanning methods while surpassing them in data acquisition speed (up to 41 times improvement) without sacrificing accuracy. Our proposed approach offers a highly promising solution for ultra-fast, real-time, and high-precision dense depth estimation. Code and dataset will be publicly available."
https://arxiv.org/abs/2403.07325,2024-03-12,Gauge Symmetries and Conserved Currents in AdS/BCFT,['Kenta Suzuki'],"In this paper, we study massless/massive vector and $p$-form field perturbations in AdS spacetime with an end-of-the-world brane. By imposing $U(1)$ preserving Neumann boundary condition on the end-of-the-world brane, we study their spectrum and discuss their implications for dual BCFT operators. When the perturbation is massless, the dual BCFT operator is a conserved current and we show that such an operator indeed satisfies the $U(1)$ preserving conformal boundary condition. On the other hand, when the perturbation is massive, in general there exists non-vanishing perpendicular components of the dual BCFT operator, even in the massless limit. We explain this difference between massless and massive perturbations from the point of view of the bulk gauge symmetry, or equivalently from different structure of equations of motion. We also find several brane-tension-independent modes in massless perturbations, and these are understood as boundary-condition-independent modes from the dual BCFT point of view."
https://arxiv.org/abs/2403.07324,2024-03-12,Observation of room temperature gate tunable quantum confinement effect in photodoped junctionless MOSFET,"['Biswajit Khan', 'Abir Mukherjee', 'Yordan M. Georgiev', 'J. P. Colinge', 'Suprovat Ghosh', 'Samaresh Das']","In the pursuit of room temperature quantum hardware, our study introduces a gate voltage tunable quantum wire within a tri-gated n-type junctionless MOSFET. The application of gate voltage alters the parabolic potential well of the tri-gated junctionless MOSFET, enabling modification of the nanowire's potential well profile. In the presence of light, photogenerated electrons accumulate at the center of the junctionless nanowire, aligning with the modified potential well profile influenced by gate bias. These carriers at the center are far from interfaces and experience less interfacial noise. Therefore, such clean photo-doping shows clear, repeatable peaks in current for specific gate biases compared to the dark condition, considering different operating drain-to-source voltages at room temperature. We propose that photodoping-induced subband occupation of gate tunable potential well of the nanowire is the underlying phenomenon responsible for this kind of observation. This study reveals experimental findings demonstrating gate-induced switching from semi-classical to the quantum domain, followed by the optical occupancy of electronic sub-bands at room temperature. We developed a compact model based on the Nonequilibrium Green's function formalism to understand this phenomenon in our illuminated device better. This work reveals the survival of the quantum confinement effect at room temperature in such semi-classical transport."
https://arxiv.org/abs/2403.07323,2024-03-12,Discrete-Time Modeling and Handover Analysis of Intelligent Reflecting Surface-Assisted Networks,"['Hongtao Zhang', 'Haoyan Wei']","Owning to the reflection gain and double path loss featured by intelligent reflecting surface (IRS) channels, handover (HO) locations become irregular and the signal strength fluctuates sharply with variations in IRS connections during HO, the risk of HO failures (HOFs) is exacerbated and thus HO parameters require reconfiguration. However, existing HO models only assume monotonic negative exponential path loss and cannot obtain sound HO parameters. This paper proposes a discrete-time model to explicitly track the HO process with variations in IRS connections, where IRS connections and HO process are discretized as finite states by measurement intervals, and transitions between states are modeled as stochastic processes. Specifically, to capture signal fluctuations during HO, IRS connection state-dependent distributions of the user-IRS distance are modified by the correlation between measurement intervals. In addition, states of the HO process are formed with Time-to-Trigger and HO margin whose transition probabilities are integrated concerning all IRS connection states. Trigger location distributions and probabilities of HO, HOF, and ping-pong (PP) are obtained by tracing user HO states. Results show IRSs mitigate PPs by 48% but exacerbate HOFs by 90% under regular parameters. Optimal parameters are mined ensuring probabilities of HOF and PP are both less than 0.1%."
https://arxiv.org/abs/2403.07322,2024-03-12,A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models,"['Hengyuan Zhang', 'Zitao Liu', 'Chenming Shang', 'Dawei Li', 'Yong Jiang']","Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This makes teachers accept the rationale behind the prediction results and utilize them to design teaching activities and tailored learning strategies for students. However, the inherent black-box nature of deep learning techniques often poses a hurdle for teachers to fully embrace the model's prediction results. To address these challenges, we propose a Question-centric Multi-experts Contrastive Learning framework for KT called Q-MCKT."
https://arxiv.org/abs/2403.07321,2024-03-12,GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method,"['Zubair Qazi', 'William Shiao', 'Evangelos E. Papalexakis']","As natural language models like ChatGPT become increasingly prevalent in applications and services, the need for robust and accurate methods to detect their output is of paramount importance. In this paper, we present GPT Reddit Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text detection dataset designed to assess the performance of detection models in identifying generated responses from ChatGPT. The dataset consists of a diverse collection of context-prompt pairs based on Reddit, with human-generated and ChatGPT-generated responses. We provide an analysis of the dataset's characteristics, including linguistic diversity, context complexity, and response quality. To showcase the dataset's utility, we benchmark several detection methods on it, demonstrating their efficacy in distinguishing between human and ChatGPT-generated responses. This dataset serves as a resource for evaluating and advancing detection techniques in the context of ChatGPT and contributes to the ongoing efforts to ensure responsible and trustworthy AI-driven communication on the internet. Finally, we propose GpTen, a novel tensor-based GPT text detection method that is semi-supervised in nature since it only has access to human-generated text and performs on par with fully-supervised baselines."
https://arxiv.org/abs/2403.07320,2024-03-12,Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding,"['Eric Lei', 'Hamed Hassani', 'Shirin Saeedi Bidokhti']","Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal in a one-shot sense on certain sources, we show that it is highly sub-optimal on i.i.d. sequences, and in fact always recovers scalar quantization of the original source sequence. We demonstrate that the sub-optimality is due to the choice of quantization scheme in the latent space, and not the transform design. By employing lattice quantization instead of scalar quantization in the latent space, we demonstrate that Lattice Transform Coding (LTC) is able to recover optimal vector quantization at various dimensions and approach the asymptotically-achievable rate-distortion function at reasonable complexity. On general vector sources, LTC improves upon standard neural compressors in one-shot coding performance. LTC also enables neural compressors that perform block coding on i.i.d. vector sources, which yields coding gain over optimal one-shot coding."
https://arxiv.org/abs/2403.07319,2024-03-12,Efficient Diffusion Model for Image Restoration by Residual Shifting,"['Zongsheng Yue', 'Jianyi Wang', 'Chen Change Loy']","While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \textit{\textbf{even only with four sampling steps}}. Our code and model are publicly available at \url{https://github.com/zsyOAOA/ResShift}."
https://arxiv.org/abs/2403.07318,2024-03-12,Test for high-dimensional linear hypothesis of mean vectors via random integration,"['Jianghao Li', 'Shizhe Hong', 'Zhenzhen Niu', 'Zhidong Bai']","In this paper, we investigate hypothesis testing for the linear combination of mean vectors across multiple populations through the method of random integration. We have established the asymptotic distributions of the test statistics under both null and alternative hypotheses. Additionally, we provide a theoretical explanation for the special use of our test statistics in situations when the nonzero signal in the linear combination of the true mean vectors is weakly dense. Moreover, Monte-Carlo simulations are presented to evaluate the suggested test against existing high-dimensional tests. The findings from these simulations reveal that our test not only aligns with the performance of other tests in terms of size but also exhibits superior power."
https://arxiv.org/abs/2403.07317,2024-03-12,GMPC: Geometric Model Predictive Control for Wheeled Mobile Robot Trajectory Tracking,"['Jiawei Tang', 'Shuang Wu', 'Bo Lan', 'Yahui Dong', 'Yuqiang Jin', 'Guangjian Tian', 'Wen-An Zhang', 'Ling Shi']","The configuration of most robotic systems lies in continuous transformation groups. However, in mobile robot trajectory tracking, many recent works still naively utilize optimization methods for elements in vector space without considering the manifold constraint of the robot configuration. In this letter, we propose a geometric model predictive control (MPC) framework for wheeled mobile robot trajectory tracking. We first derive the error dynamics of the wheeled mobile robot trajectory tracking by considering its manifold constraint and kinematic constraint simultaneously. After that, we utilize the relationship between the Lie group and Lie algebra to convexify the tracking control problem, which enables us to solve the problem efficiently. Thanks to the Lie group formulation, our method tracks the trajectory more smoothly than existing nonlinear MPC. Simulations and physical experiments verify the effectiveness of our proposed methods. Our pure Python-based simulation platform is publicly available to benefit further research in the community."
https://arxiv.org/abs/2403.07316,2024-03-12,Simplicial complexes with many facets are vertex decomposable,"['Anton Dochtermann', 'Ritika Nair', 'Jay Schweig', 'Adam Van Tuyl', 'Russ Woodroofe']","Let $Δ$ be a pure simplicial complex on $n$ vertices having dimension $d$ and codimension $c = n-d-1$ in the simplex. Terai and Yoshida proved that if the number of facets of $Δ$ is at least $\binom{n}{c}-2c+1$, then $Δ$ is Cohen-Macaulay. We improve this result by showing that these hypotheses imply the stronger condition that $Δ$ is vertex decomposable. We give examples to show that this bound is optimal, and that the conclusion cannot be strengthened to the class of matroids or shifted complexes. We explore an application to Simon's Conjecture and discuss connections to other results from the literature."
https://arxiv.org/abs/2403.07315,2024-03-12,Secant variety and syzygies of Hilbert scheme of two points,"['Chiwon Yoon', 'Haesong Seo']","In this paper, we prove that the singular locus of $\mathrm{Sec} (X^{[2]})$ coincides with $X^{[2]}$ under the Grothendieck-Plücker embedding $X^{[2]} \hookrightarrow \mathbb{P}^N$ when $X$ is embedded by a $4$-very ample line bundle. We also prove that the embedding $X^{[2]} \hookrightarrow \mathbb{P}^N$ satisfies Green's condition $(N_p)$ when the embedding of $X$ is positive enough. As an application, we describe the geometry of a resolution of singularities from the secant bundle to $\mathrm{Sec}(X^{[2]})$ when $X$ is a surface."
https://arxiv.org/abs/2403.07314,2024-03-12,Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) for Improved User Engagement,"['Megan A. Witherow', 'Crystal Butler', 'Winston J. Shields', 'Furkan Ilgin', 'Norou Diawara', 'Janice Keener', 'John W. Harrington', 'Khan M. Iftekharuddin']","Customizable 3D avatar-based facial expression stimuli may improve user engagement in behavioral biomarker discovery and therapeutic intervention for autism, Alzheimer's disease, facial palsy, and more. However, there is a lack of customizable avatar-based stimuli with Facial Action Coding System (FACS) action unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled, customizable avatar-based expression stimuli for maintaining subjects' engagement, (2) learning-based measurements that quantify subjects' facial responses to such stimuli, and (3) validation of constructs represented by stimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS expert. To measure subjects' AUs in response to CADyFACE, we propose a novel Beta-guided Correlation and Multi-task Expression learning neural network (BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss encourages feature correlation with AUs while discouraging correlation with subject identities for improved generalization. We train BeCoME-Net for unilateral and bilateral AU detection and compare with state-of-the-art approaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty healthy adult volunteers complete expression recognition and mimicry tasks in an online feasibility study while webcam-based eye-tracking and video are collected. We test validity of multiple constructs, including face preference during recognition and AUs during mimicry."
https://arxiv.org/abs/2403.07313,2024-03-12,Cosmology with shear ratios: a joint study of weak lensing and spectroscopic redshift datasets,"['Ni Emas', 'Chris Blake', 'Rossana Ruggeri', 'Anna Porredon']","The ratio of the average tangential shear signal of different weak lensing source populations around the same lens galaxies, also known as a shear ratio, provides an important test of lensing systematics and a potential source of cosmological information. In this paper we measure shear ratios of three current weak lensing surveys -- KiDS, DES, and HSC -- using overlapping data from the Baryon Oscillation Spectroscopic Survey. We apply a Bayesian method to reduce bias in shear ratio measurement, and assess the degree to which shear ratio information improves the determination of important astrophysical parameters describing the source redshift distributions and intrinsic galaxy alignments, as well as cosmological parameters, in comparison with cosmic shear and full 3x2-pt correlations (cosmic shear, galaxy-galaxy lensing, and galaxy clustering). We consider both Fisher matrix forecasts, as well as full likelihood analyses of the data. We find that the addition of shear ratio information to cosmic shear allows the mean redshifts of the source samples and intrinsic alignment parameters to be determined significantly more accurately. Although the additional constraining power enabled by the shear ratio is less than that obtained by introducing an accurate prior in the mean source redshift using photometric redshift calibration, the shear ratio allows for a useful cross-check. The inclusion of shear ratio data consistently benefits the determination of cosmological parameters such as S_8, for which we obtain improvements up to 34%. However these improvements are less significant when shear ratio is combined with the full 3x2-pt correlations. We conclude that shear ratio tests will remain a useful source of cosmological information and cross-checks for lensing systematics, whose application will be further enhanced by upcoming datasets such as the Dark Energy Spectroscopic Instrument."
https://arxiv.org/abs/2403.07312,2024-03-12,Multi-task Manipulation Policy Modeling with Visuomotor Latent Diffusion,"['Wenhui Tan', 'Bei Liu', 'Junbo Zhang', 'Ruihua Song', 'Jianlong Fu']","Modeling a generalized visuomotor policy has been a longstanding challenge for both computer vision and robotics communities. Existing approaches often fail to efficiently leverage cross-dataset resources or rely on heavy Vision-Language models, which require substantial computational resources, thereby limiting their multi-task performance and application potential. In this paper, we introduce a novel paradigm that effectively utilizes latent modeling of manipulation skills and an efficient visuomotor latent diffusion policy, which enhances the utilizing of existing cross-embodiment and cross-environment datasets, thereby improving multi-task capabilities. Our methodology consists of two decoupled phases: action modeling and policy modeling. Firstly, we introduce a task-agnostic, embodiment-aware trajectory latent autoencoder for unified action skills modeling. This step condenses action data and observation into a condensed latent space, effectively benefiting from large-scale cross-datasets. Secondly, we propose to use a visuomotor latent diffusion policy that recovers target skill latent from noises for effective task execution. We conducted extensive experiments on two widely used benchmarks, and the results demonstrate the effectiveness of our proposed paradigms on multi-tasking and pre-training. Code is available at https://github.com/AlbertTan404/RoLD."
https://arxiv.org/abs/2403.07311,2024-03-12,Knowledge Graph Large Language Model (KG-LLM) for Link Prediction,"['Dong Shu', 'Tianle Chen', 'Mingyu Jin', 'Yiting Zhang', 'Mengnan Du', 'Yongfeng Zhang']","The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities for handling previously unseen prompts. Our experimental findings discover that integrating ICL and CoT not only augments the performance of our approach but also significantly boosts the models' generalization capacity, thereby ensuring more precise predictions in unfamiliar scenarios."
https://arxiv.org/abs/2403.07310,2024-03-12,How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance,"['Hongkang Li', 'Shuai Zhang', 'Yihua Zhang', 'Meng Wang', 'Sijia Liu', 'Pin-Yu Chen']","Group imbalance has been a known problem in empirical risk minimization (ERM), where the achieved high average accuracy is accompanied by low accuracy in a minority group. Despite algorithmic efforts to improve the minority group accuracy, a theoretical generalization analysis of ERM on individual groups remains elusive. By formulating the group imbalance problem with the Gaussian Mixture Model, this paper quantifies the impact of individual groups on the sample complexity, the convergence rate, and the average and group-level testing performance. Although our theoretical framework is centered on binary classification using a one-hidden-layer neural network, to the best of our knowledge, we provide the first theoretical analysis of the group-level generalization of ERM in addition to the commonly studied average generalization performance. Sample insights of our theoretical results include that when all group-level co-variance is in the medium regime and all mean are close to zero, the learning performance is most desirable in the sense of a small sample complexity, a fast training rate, and a high average and group-level testing accuracy. Moreover, we show that increasing the fraction of the minority group in the training data does not necessarily improve the generalization performance of the minority group. Our theoretical results are validated on both synthetic and empirical datasets, such as CelebA and CIFAR-10 in image classification."
https://arxiv.org/abs/2403.07309,2024-03-12,Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer,"['Dipesh Tamboli', 'Jiayu Chen', 'Kiran Pranesh Jotheeswaran', 'Denny Yu', 'Vaneet Aggarwal']","Sepsis, a life-threatening condition triggered by the body's exaggerated response to infection, demands urgent intervention to prevent severe complications. Existing machine learning methods for managing sepsis struggle in offline scenarios, exhibiting suboptimal performance with survival rates below 50%. This paper introduces the POSNEGDM -- ``Reinforcement Learning with Positive and Negative Demonstrations for Sequential Decision-Making"" framework utilizing an innovative transformer-based model and a feedback reinforcer to replicate expert actions while considering individual patient characteristics. A mortality classifier with 96.7\% accuracy guides treatment decisions towards positive outcomes. The POSNEGDM framework significantly improves patient survival, saving 97.39% of patients, outperforming established machine learning algorithms (Decision Transformer and Behavioral Cloning) with survival rates of 33.4% and 43.5%, respectively. Additionally, ablation studies underscore the critical role of the transformer-based decision maker and the integration of a mortality classifier in enhancing overall survival rates. In summary, our proposed approach presents a promising avenue for enhancing sepsis treatment outcomes, contributing to improved patient care and reduced healthcare costs."
https://arxiv.org/abs/2403.07308,2024-03-12,Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees,"['Shaoru Chen', 'Lekan Molu', 'Mahyar Fazlyab']","Barrier functions are a general framework for establishing a safety guarantee for a system. However, there is no general method for finding these functions. To address this shortcoming, recent approaches use self-supervised learning techniques to learn these functions using training data that are periodically generated by a verification procedure, leading to a verification-aided learning framework. Despite its immense potential in automating barrier function synthesis, the verification-aided learning framework does not have termination guarantees and may suffer from a low success rate of finding a valid barrier function in practice. In this paper, we propose a holistic approach to address these drawbacks. With a convex formulation of the barrier function synthesis, we propose to first learn an empirically well-behaved NN basis function and then apply a fine-tuning algorithm that exploits the convexity and counterexamples from the verification failure to find a valid barrier function with finite-step termination guarantees: if there exist valid barrier functions, the fine-tuning algorithm is guaranteed to find one in a finite number of iterations. We demonstrate that our fine-tuning method can significantly boost the performance of the verification-aided learning framework on examples of different scales and using various neural network verifiers."
https://arxiv.org/abs/2403.07307,2024-03-12,Asymptotic behavior of saxion-axion system in stringy quintessence model,['Min-Seok Seo'],"We study the late time behavior of the slow-roll parameter in the stringy quintessence model when axion as well as saxion is allowed to move. Even though the potential is independent of the axion at tree level, the axion can move through its coupling to the saxion and the background geometry. Then the contributions of the axion kinetic energy to the slow-roll parameter and the vacuum energy density are not negligible when the slow-roll approximation does not hold. As the dimension of the field space is doubled, the fixed point at which the time variation of the slow-roll parameter vanishes is not always stable. We find that the fixed point in the saxion-axion system is at most partially stable, in particular when the volume modulus and the axio-dilaton, the essential ingredients of the string compactification, are taken into account. It seems that as we consider more saxion-axion pairs, the stability of the fixed point becomes difficult to achieve."
https://arxiv.org/abs/2403.07306,2024-03-12,A Spin model for global flat-foldability of random origami,['Chihiro Nakajima'],"We map the problem of determining flat-foldability of the origami diagram onto the ground-state search problem of spin glass model on random graphs. If the origami diagram is locally flat-foldable around each vertex, a pre-folded diagram, showing the planar-positional relationship of the facet, can be obtained. For remaining combinatorial problem on layer ordering of facets can be described as a spin model. A spin variable is assigned for the layer-ordering of each pair of facets which have an overlap in the pre-folded diagram. The interactions to prohibit the intrusion of each facet into the other component of the same origami diagram are introduced among two or four spins. The flat-foldability of the diagram is closely related to the (non-)existence of frustrated loops on the spin model with the interactions on the random (hyper)graph."
https://arxiv.org/abs/2403.07305,2024-03-12,Integrated Communications and Localization for Massive MIMO LEO Satellite Systems,"['Li You', 'Xiaoyu Qiang', 'Yongxiang Zhu', 'Fan Jiang', 'Christos G. Tsinos', 'Wenjin Wang', 'Henk Wymeersch', 'Xiqi Gao', 'Björn Ottersten']","Integrated communications and localization (ICAL) will play an important part in future sixth generation (6G) networks for the realization of Internet of Everything (IoE) to support both global communications and seamless localization. Massive multiple-input multiple-output (MIMO) low earth orbit (LEO) satellite systems have great potential in providing wide coverage with enhanced gains, and thus are strong candidates for realizing ubiquitous ICAL. In this paper, we develop a wideband massive MIMO LEO satellite system to simultaneously support wireless communications and localization operations in the downlink. In particular, we first characterize the signal propagation properties and derive a localization performance bound. Based on these analyses, we focus on the hybrid analog/digital precoding design to achieve high communication capability and localization precision. Numerical results demonstrate that the proposed ICAL scheme supports both the wireless communication and localization operations for typical system setups."
https://arxiv.org/abs/2403.07304,2024-03-12,Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models,"['Yang Jiao', 'Shaoxiang Chen', 'Zequn Jie', 'Jingjing Chen', 'Lin Ma', 'Yu-Gang Jiang']","Large Multimodal Model (LMM) is a hot research topic in the computer vision area and has also demonstrated remarkable potential across multiple disciplinary fields. A recent trend is to further extend and enhance the perception capabilities of LMMs. The current methods follow the paradigm of adapting the visual task outputs to the format of the language model, which is the main component of a LMM. This adaptation leads to convenient development of such LMMs with minimal modifications, however, it overlooks the intrinsic characteristics of diverse visual tasks and hinders the learning of perception capabilities. To address this issue, we propose a novel LMM architecture named Lumen, a Large multimodal model with versatile vision-centric capability enhancement. We decouple the LMM's learning of perception capabilities into task-agnostic and task-specific stages. Lumen first promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks. Thus the output of the task-agnostic stage is a shared representation for all the tasks we address in this paper. Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts. Benefiting from such a decoupled design, our Lumen surpasses existing LMM-based approaches on the COCO detection benchmark with a clear margin and exhibits seamless scalability to additional visual tasks. Furthermore, we also conduct comprehensive ablation studies and generalization evaluations for deeper insights. The code will be released at https://github.com/SxJyJay/Lumen."
https://arxiv.org/abs/2403.07303,2024-03-12,Dynamic U-Net: Adaptively Calibrate Features for Abdominal Multi-organ Segmentation,"['Jin Yang', 'Daniel S. Marcus', 'Aristeidis Sotiras']","U-Net has been widely used for segmenting abdominal organs, achieving promising performance. However, when it is used for multi-organ segmentation, first, it may be limited in exploiting global long-range contextual information due to the implementation of standard convolutions. Second, the use of spatial-wise downsampling (e.g., max pooling or strided convolutions) in the encoding path may lead to the loss of deformable or discriminative details. Third, features upsampled from the higher level are concatenated with those that persevered via skip connections. However, repeated downsampling and upsampling operations lead to misalignments between them and their concatenation degrades segmentation performance. To address these limitations, we propose Dynamically Calibrated Convolution (DCC), Dynamically Calibrated Downsampling (DCD), and Dynamically Calibrated Upsampling (DCU) modules, respectively. The DCC module can utilize global inter-dependencies between spatial and channel features to calibrate these features adaptively. The DCD module enables networks to adaptively preserve deformable or discriminative features during downsampling. The DCU module can dynamically align and calibrate upsampled features to eliminate misalignments before concatenations. We integrated the proposed modules into a standard U-Net, resulting in a new architecture, termed Dynamic U-Net. This architectural design enables U-Net to dynamically adjust features for different organs. We evaluated Dynamic U-Net in two abdominal multi-organ segmentation benchmarks. Dynamic U-Net achieved statistically improved segmentation accuracy compared with standard U-Net. Our code is available at https://github.com/sotiraslab/DynamicUNet."
https://arxiv.org/abs/2403.07302,2024-03-12,The Kerr Memory Effect at Null Infinity,['Rudeep Gaur'],"We compute the memory effect due to a gravitational wave striking a Kerr black hole as seen by an observer at null infinity. This is done by working in Bondi--Sachs coordinates. It was shown by Hawking, Perry, and Strominger (HPS) that the memory effect due to a gravitational shockwave is seen as a pure BMS supertranslation from null infinity. Hence, it is of interest to compute the supertranslated Kerr solution in Bondi--Sachs coordinates. Finally, the gravitational wave is said to implant soft supertranslation hair on the event horizon of the black hole which carries superrotation charge. We will explicitly calculate the change in superrotation charge on the event horizon due to the supertranslation hair."
https://arxiv.org/abs/2403.07301,2024-03-12,Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller,"['Chuanqi Zang', 'Jiji Tang', 'Rongsheng Zhang', 'Zeng Zhao', 'Tangjie Lv', 'Mingtao Pei', 'Wei Liang']","Storytelling aims to generate reasonable and vivid narratives based on an ordered image stream. The fidelity to the image story theme and the divergence of story plots attract readers to keep reading. Previous works iteratively improved the alignment of multiple modalities but ultimately resulted in the generation of simplistic storylines for image streams. In this work, we propose a new pipeline, termed LLaMS, to generate multimodal human-level stories that are embodied in expressiveness and consistency. Specifically, by fully exploiting the commonsense knowledge within the LLM, we first employ a sequence data auto-enhancement strategy to enhance factual content expression and leverage a textual reasoning architecture for expressive story generation and prediction. Secondly, we propose SQ-Adatpter module for story illustration generation which can maintain sequence consistency. Numerical results are conducted through human evaluation to verify the superiority of proposed LLaMS. Evaluations show that LLaMS achieves state-of-the-art storytelling performance and 86% correlation and 100% consistency win rate as compared with previous SOTA methods. Furthermore, ablation experiments are conducted to verify the effectiveness of proposed sequence data enhancement and SQ-Adapter."
https://arxiv.org/abs/2403.07300,2024-03-12,Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation,"['Peiyuan Liu', 'Hang Guo', 'Tao Dai', 'Naiqi Li', 'Jigang Bao', 'Xudong Ren', 'Yong Jiang', 'Shu-Tao Xia']","Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with favorable performance as well as strong generalization abilities. Extensive experiments demonstrate the proposed method establishes a new state of the art for both long- and short-term forecasting. Code is available at \url{https://github.com/Hank0626/LLaTA}."
https://arxiv.org/abs/2403.07299,2024-03-12,Modelling response time contrasts in superconducting nanowire single photon detectors,"['Souvik Haldar', 'Arun Sehrawat', 'Krishna B. Balasubramanian']","Superconducting Nanowire Single Photon Detector (SNSPD) emerges as a potential candidate in the multiple fields requiring sensitive and fast photodetection. While nanowires of low temperature superconducting detectors are mature with commercial solutions, other material options with higher transition temperature and faster responses are currently being explored. Towards this goal, we develop a generalized numerical model that incorporates the thermodynamic properties of the superconducting material and identifies the minimum resolvable photon count for a given bias and device parameters. A phase diagram of detection and latching phases with the minimum number of photons as a function of biasing current and biasing temperature for each material system is presented. We show using the developed model that while low temperature superconducting (LTS) nanowires are more sensitive to the incident photon at different wavelengths, the ultimate limit of a single photon can be achieved using high temperature superconducting (HTS) material such as YBa2Cu3O7-δ, albeit at stringent biasing conditions. On the contrary, ultrafast response time with three orders of magnitude smaller response times can be achieved in select HTS materials making it an appealing for several practical applications."
https://arxiv.org/abs/2403.07298,2024-03-12,Multiple elliptic integrals and differential equations,"['John M. Campbell', 'M. Lawrence Glasser', 'Yajun Zhou']","We introduce and prove evaluations for families of multiple elliptic integrals by solving special types of ordinary and partial differential equations. As an application, we obtain new expressions of Ramanujan-type series of level 4 and associated singular values for the complete elliptic integral $\mathbf K$ with integrals involving $\mathbf K$."
https://arxiv.org/abs/2403.07297,2024-03-11,Optical detection of bacterial cells on stainless-steel surface with a low-magnification light microscope,"['Yuzhen Zhang', 'Zili Gao', 'Lili He']","A Rapid and cost-effective method for detecting bacterial cells on surfaces is critical to protect public health from various aspects, including food safety, clinical hygiene, and pharmacy quality. Herein, we first established an optical detection method based on a gold chip coating with 3-mercaptophenylboronic acid (3-MPBA) to capture bacterial cells, which allows for the detection and quantification of bacterial cells with a standard light microscope under low-magnification (10 fold) objective lens. Then, integrating the developed optical detection method with swab sampling to achieve to detect bacterial cells loading on stainless-steel surfaces. Using Salmonella enterica (SE1045) and Escherichia coli as model bacterial cells, we achieved a capture efficiency of up to 76.0 % for SE1045 cells and 81.1 % for E. coli cells at Log 3 CFU/mL upon the optimized conditions. Our assay showed good linear relationship between the concentrations of bacterial cells with the cell counting in images with the limit of detection (LOD) of Log 3 CFU/mL for both SE1045 and E. coli cells. A further increase in sensitivity in detecting E. coli cells was achieved through a heat treatment, enabling the LOD to be pushed as low as Log 2 CFU/mL. Furthermore, successful application was observed in assessing bacterial contamination on stainless-steel surface following integrating with swab collection, achieving a recovery rate of approximately 70 % suggests future prospects for evaluating the cleanliness of surfaces. The entire process was completed within around 2 hours, with a cost of merely 2 dollars per sample. Given a standard light microscope cost around 250 dollars, our developed method has shown great potential in practical industrial applications for bacterial contamination control on surfaces in low-resource settings."
https://arxiv.org/abs/2403.07296,2024-03-11,Advancements in Continuous Glucose Monitoring: Integrating Deep Learning and ECG Signal,"['MohammadReza Hosseinzadehketilateh', 'Banafsheh Adami', 'Nima Karimian']","This paper presents a novel approach to noninvasive hyperglycemia monitoring utilizing electrocardiograms (ECG) from an extensive database comprising 1119 subjects. Previous research on hyperglycemia or glucose detection using ECG has been constrained by challenges related to generalization and scalability, primarily due to using all subjects' ECG in training without considering unseen subjects as a critical factor for developing methods with effective generalization. We designed a deep neural network model capable of identifying significant features across various spatial locations and examining the interdependencies among different features within each convolutional layer. To expedite processing speed, we segment the ECG of each user to isolate one heartbeat or one cycle of the ECG. Our model was trained using data from 727 subjects, while 168 were used for validation. The testing phase involved 224 unseen subjects, with a dataset consisting of 9,000 segments. The result indicates that the proposed algorithm effectively detects hyperglycemia with a 91.60% area under the curve (AUC), 81.05% sensitivity, and 85.54% specificity."
https://arxiv.org/abs/2403.07295,2024-03-11,Tight error bounds for log-determinant cones without constraint qualifications,"['Ying Lin', 'Scott B. Lindstrom', 'Bruno F. Lourenço', 'Ting Kei Pong']","In this paper, without requiring any constraint qualifications, we establish tight error bounds for the log-determinant cone, which is the closure of the hypograph of the perspective function of the log-determinant function. This error bound is obtained using the recently developed framework based on one-step facial residual functions."
https://arxiv.org/abs/2403.07294,2024-03-11,Graph Data Condensation via Self-expressive Graph Structure Reconstruction,"['Zhanyu Liu', 'Chaolv Zeng', 'Guanjie Zheng']","With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR}). Our method stands out by (1) explicitly incorporating the original graph structure into the condensing process and (2) capturing the nuanced interdependencies between the condensed nodes by reconstructing an interpretable self-expressive graph structure. Extensive experiments and comprehensive analysis validate the efficacy of the proposed method across diverse GNN models and datasets. Our code is available at https://www.dropbox.com/scl/fi/2aonyp5ln5gisdqtjimu8/GCSR.zip?rlkey=11cuwfpsf54wxiiktu0klud0x&dl=0"
https://arxiv.org/abs/2403.07293,2024-03-11,Stability and Sharp Decay Estimates for 3D MHD Equations with Only Vertical Dissipation Near a Background Magnetic Field,"['Suhua Lai', 'Jiahong Wu', 'Jianwen Zhang', 'Xiaokui Zhao']","This paper is concerned with the stability and large-time behavior of 3D incompressible MHD equations with only vertical dissipation near a background magnetic field. By making full use of the dissipation generated by the background magnetic field, we first establish the global stability of the solutions in $H^3$-norm. Then, the optimal decay rates of the solutions are obtained, which are consistent with the 2D classical heat equation. Moreover, some enhanced decay rates of $(u_1,b_1)$ are also achieved. In other words, the decay estimates of the second or third component of velocity/magnetic field coincide with those of 2D heat kernel, while the first component behaves like the 3D heat kernel. This is mainly due to the divergence-free condition and the anisotropic structure. The results obtained improve the previous ones due to Lin-Wu-Zhu [24,25]."
https://arxiv.org/abs/2403.07292,2024-03-11,Continual All-in-One Adverse Weather Removal with Knowledge Replay on a Unified Network Structure,"['De Cheng', 'Yanling Ji', 'Dong Gong', 'Yan Li', 'Nannan Wang', 'Junwei Han', 'Dingwen Zhang']","In real-world applications, image degeneration caused by adverse weather is always complex and changes with different weather conditions from days and seasons. Systems in real-world environments constantly encounter adverse weather conditions that are not previously observed. Therefore, it practically requires adverse weather removal models to continually learn from incrementally collected data reflecting various degeneration types. Existing adverse weather removal approaches, for either single or multiple adverse weathers, are mainly designed for a static learning paradigm, which assumes that the data of all types of degenerations to handle can be finely collected at one time before a single-phase learning process. They thus cannot directly handle the incremental learning requirements. To address this issue, we made the earliest effort to investigate the continual all-in-one adverse weather removal task, in a setting closer to real-world applications. Specifically, we develop a novel continual learning framework with effective knowledge replay (KR) on a unified network structure. Equipped with a principal component projection and an effective knowledge distillation mechanism, the proposed KR techniques are tailored for the all-in-one weather removal task. It considers the characteristics of the image restoration task with multiple degenerations in continual learning, and the knowledge for different degenerations can be shared and accumulated in the unified network structure. Extensive experimental results demonstrate the effectiveness of the proposed method to deal with this challenging task, which performs competitively to existing dedicated or joint training image restoration methods. Our code is available at https://github.com/xiaojihh/CL_all-in-one."
https://arxiv.org/abs/2403.07291,2024-03-11,An extension of the Chudnovsky algorithm,['John M. Campbell'],"Using an infinite family of generalizations of the Chudnovsky brothers' series recently obtained via the analytic continuation of the Borwein brothers' formula for Ramanujan-type series of level 1, we apply the Gauss-Salamin-Brent iteration for $π$ to obtain a new, Ramanujan-type series that yields more digits per term relative to current world record given by an extension of the Chudnovsky algorithm from Bagis and Glasser that produces about 110 digits per term. We explicitly evaluate the required nested radicals over $\mathbb{Q}$ involved in the our summation, which yields about 153 digits per term, and we provide a practical way of implementing our higher-order version of the Chudnovsky algorithm via the the PARI/GP system. An evaluation due to Berndt and Chan for the modular $j$-invariant associated with their order-3315 extension of the Chudnovskys' Ramanujan-type series provides a key to our applications of recursions for the elliptic lambda and elliptic alpha functions."
https://arxiv.org/abs/2403.07290,2024-03-11,Learning Hierarchical Color Guidance for Depth Map Super-Resolution,"['Runmin Cong', 'Ronghui Sheng', 'Hao Wu', 'Yulan Guo', 'Yunchao Wei', 'Wangmeng Zuo', 'Yao Zhao', 'Sam Kwong']","Color information is the most commonly used prior knowledge for depth map super-resolution (DSR), which can provide high-frequency boundary guidance for detail restoration. However, its role and functionality in DSR have not been fully developed. In this paper, we rethink the utilization of color information and propose a hierarchical color guidance network to achieve DSR. On the one hand, the low-level detail embedding module is designed to supplement high-frequency color information of depth features in a residual mask manner at the low-level stages. On the other hand, the high-level abstract guidance module is proposed to maintain semantic consistency in the reconstruction process by using a semantic mask that encodes the global guidance information. The color information of these two dimensions plays a role in the front and back ends of the attention-based feature projection (AFP) module in a more comprehensive form. Simultaneously, the AFP module integrates the multi-scale content enhancement block and adaptive attention projection block to make full use of multi-scale information and adaptively project critical restoration information in an attention manner for DSR. Compared with the state-of-the-art methods on four benchmark datasets, our method achieves more competitive performance both qualitatively and quantitatively."
https://arxiv.org/abs/2403.07289,2024-03-11,Rediscovering BCE Loss for Uniform Classification,"['Qiufu Li', 'Xi Jia', 'Jiancan Zhou', 'Linlin Shen', 'Jinming Duan']","This paper introduces the concept of uniform classification, which employs a unified threshold to classify all samples rather than adaptive threshold classifying each individual sample. We also propose the uniform classification accuracy as a metric to measure the model's performance in uniform classification. Furthermore, begin with a naive loss, we mathematically derive a loss function suitable for the uniform classification, which is the BCE function integrated with a unified bias. We demonstrate the unified threshold could be learned via the bias. The extensive experiments on six classification datasets and three feature extraction models show that, compared to the SoftMax loss, the models trained with the BCE loss not only exhibit higher uniform classification accuracy but also higher sample-wise classification accuracy. In addition, the learned bias from BCE loss is very close to the unified threshold used in the uniform classification. The features extracted by the models trained with BCE loss not only possess uniformity but also demonstrate better intra-class compactness and inter-class distinctiveness, yielding superior performance on open-set tasks such as face recognition."
https://arxiv.org/abs/2403.07288,2024-03-11,Efficient and Model-Agnostic Parameter Estimation Under Privacy-Preserving Post-randomization Data,"['Qinglong Tian', 'Jiwei Zhao']","Protecting individual privacy is crucial when releasing sensitive data for public use. While data de-identification helps, it is not enough. This paper addresses parameter estimation in scenarios where data are perturbed using the Post-Randomization Method (PRAM) to enhance privacy. Existing methods for parameter estimation under PRAM data suffer from limitations like being parameter-specific, model-dependent, and lacking efficiency guarantees. We propose a novel, efficient method that overcomes these limitations. Our method is applicable to general parameters defined through estimating equations and makes no assumptions about the underlying data model. We further prove that the proposed estimator achieves the semiparametric efficiency bound, making it optimal in terms of asymptotic variance."
https://arxiv.org/abs/2403.07287,2024-03-11,Integrable systems and cluster algebras,"['Michael Gekhtman', 'Anton Izosimov']","We review several constructions of integrable systems with an underlying cluster algebra structure, in particular the Gekhtman-Shapiro-Tabachnikov-Vainshtein construction based on perfect networks and the Goncharov-Kenyon approach based on the dimer model. We also discuss results of Galashin and Pylyavskyy on integrability of T-systems."
https://arxiv.org/abs/2403.07286,2024-03-11,MENTOR: Multilingual tExt detectioN TOward leaRning by analogy,"['Hsin-Ju Lin', 'Tsu-Chun Chung', 'Ching-Chun Hsiao', 'Pin-Yu Chen', 'Wei-Chen Chiu', 'Ching-Chun Huang']","Text detection is frequently used in vision-based mobile robots when they need to interpret texts in their surroundings to perform a given task. For instance, delivery robots in multilingual cities need to be capable of doing multilingual text detection so that the robots can read traffic signs and road markings. Moreover, the target languages change from region to region, implying the need of efficiently re-training the models to recognize the novel/new languages. However, collecting and labeling training data for novel languages are cumbersome, and the efforts to re-train an existing/trained text detector are considerable. Even worse, such a routine would repeat whenever a novel language appears. This motivates us to propose a new problem setting for tackling the aforementioned challenges in a more efficient way: ""We ask for a generalizable multilingual text detection framework to detect and identify both seen and unseen language regions inside scene images without the requirement of collecting supervised training data for unseen languages as well as model re-training"". To this end, we propose ""MENTOR"", the first work to realize a learning strategy between zero-shot learning and few-shot learning for multilingual scene text detection."
https://arxiv.org/abs/2403.07285,2024-03-11,Improved Algebraic Inverter Modelling for Four-Wire Power Flow Optimization,"['Rahmat Heidari', 'Frederik Geth']","This paper discusses the modeling of inverters used in distributed energy resources in steady state. Modeling the interaction between distribution grids and inverter-based resources is crucial to understand the consequences for the network's operational and planning processes. This work highlights the limitations of existing models and emphasizes the need for better representations of inverters and their control laws in decision-making contexts. Improved steady-state grid-following and grid-forming inverter models are presented, including both three-leg and four-leg converter variants. The advantages of these improved models in mathematical optimization contexts are showcased by investigating the power quality improvement capabilities of the inverters. Numerical studies integrating the proposed inverter models in a four-wire unbalanced optimal power flow engine are presented, and trade-offs between modeling detail and computational intensity are illustrated."
https://arxiv.org/abs/2403.07284,2024-03-11,SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object Detection,"['Hongcheng Zhang', 'Liu Liang', 'Pengxin Zeng', 'Xiao Song', 'Zhe Wang']","Sparse 3D detectors have received significant attention since the query-based paradigm embraces low latency without explicit dense BEV feature construction. However, these detectors achieve worse performance than their dense counterparts. In this paper, we find the key to bridging the performance gap is to enhance the awareness of rich representations in two modalities. Here, we present a high-performance fully sparse detector for end-to-end multi-modality 3D object detection. The detector, termed SparseLIF, contains three key designs, which are (1) Perspective-Aware Query Generation (PAQG) to generate high-quality 3D queries with perspective priors, (2) RoI-Aware Sampling (RIAS) to further refine prior queries by sampling RoI features from each modality, (3) Uncertainty-Aware Fusion (UAF) to precisely quantify the uncertainty of each sensor modality and adaptively conduct final multi-modality fusion, thus achieving great robustness against sensor noises. By the time of submission (2024/03/08), SparseLIF achieves state-of-the-art performance on the nuScenes dataset, ranking 1st on both validation set and test benchmark, outperforming all state-of-the-art 3D object detectors by a notable margin. The source code will be released upon acceptance."
https://arxiv.org/abs/2403.07283,2024-03-11,A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism,"['Zhiyu Chen', 'Yu Li', 'Suochao Zhang', 'Jingbo Zhou', 'Jiwen Zhou', 'Chenfu Bao', 'Dianhai Yu']","As Large Language Models (LLMs) gain great success in real-world applications, an increasing number of users are seeking to develop and deploy their customized LLMs through cloud services. Nonetheless, in some specific domains, there are still concerns regarding cost and trade-offs between privacy issues and accuracy. In this study, we introduce a cost-effective and self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With carefully designed horizontal and vertical shaking operators, we can achieve comparable accuracy results with SOTA privacy-preserving LLM schemes using Cryptography-based or Differential Privacy-based methods. Experiments also show that with the CypherTalk framework, users can achieve reliable accuracy when using optimized shaking operator settings. To our best knowledge, this is the first work that considers cost, and trade-off between model utility and privacy in LLM scenarios."
https://arxiv.org/abs/2403.07282,2024-03-11,Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling,"['Hyungi Lee', 'Giung Nam', 'Edwin Fong', 'Juho Lee']","Transfer learning has recently shown significant performance across various tasks involving deep neural networks. In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. The nonparametric learning (NPL) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for transfer learning scenarios that may involve the distribution shift between upstream and downstream tasks. Through extensive empirical validations, we demonstrate that our approach surpasses other baselines in BMA performance."
https://arxiv.org/abs/2403.07281,2024-03-11,Locally constrained flows and geometric inequalities in hyperbolic space and sphere,"['Shanwei Ding', 'Guanghan Li']","In this paper, we prove some families of geometric inequalities in hyperbolic space and sphere by some kinds of locally constrained flows. In hyperbolic space, we obtain the geometric inequalities involving two weighted curvature integrals for static convex domains. While in sphere, a new family of ``three terms'' geometric inequalities involving two weighted curvature integrals and one quermassintegral are proved. Unlike hyperbolic spaces, we also obtain an inverse weighted geometric inequality in sphere."
https://arxiv.org/abs/2403.07280,2024-03-11,A $π_1$ obstruction to having finite index monodromy and an unusual subgroup of infinite index in $\textrm{Mod}(Σ_g)$,['Ishan Banerjee'],Let $X$ be an algebraic surface with $\mathcal{L}$ an ample line bundle on $X$.
https://arxiv.org/abs/2403.07279,2024-03-11,A Survey of Explainable Knowledge Tracing,"['Yanhong Bai', 'Jiabao Zhao', 'Tingjiang Wei', 'Qing Cai', 'Liang He']","With the long term accumulation of high quality educational data, artificial intelligence has shown excellent performance in knowledge tracing. However, due to the lack of interpretability and transparency of some algorithms, this approach will result in reduced stakeholder trust and a decreased acceptance of intelligent decisions. Therefore, algorithms need to achieve high accuracy, and users need to understand the internal operating mechanism and provide reliable explanations for decisions. This paper thoroughly analyzes the interpretability of KT algorithms. First, the concepts and common methods of explainable artificial intelligence and knowledge tracing are introduced. Next, explainable knowledge tracing models are classified into two categories: transparent models and black box models. Then, the interpretable methods used are reviewed from three stages: ante hoc interpretable methods, post hoc interpretable methods, and other dimensions. It is worth noting that current evaluation methods for explainable knowledge tracing are lacking. Hence, contrast and deletion experiments are conducted to explain the prediction results of the deep knowledge tracing model on the ASSISTment2009 by using three XAI methods. Moreover, this paper offers some insights into evaluation methods from the perspective of educational stakeholders. This paper provides a detailed and comprehensive review of the research on explainable knowledge tracing, aiming to offer some basis and inspiration for researchers interested in the interpretability of knowledge tracing."
https://arxiv.org/abs/2403.07278,2024-03-11,Distance-Dependent Evolution of Electronic States in Kagome- Honeycomb Lateral Heterostructures in FeSn,"['Tuan Anh Pham', 'Seoung-Hun Kang', 'Yasemin Ozbek', 'Mina Yoon', 'Pengpeng Zhang']","In this work, we demonstrate the formation and electronic influence of lateral heterointerfaces in FeSn containing Kagome and honeycomb layers. Lateral heterostructures offer spatially resolved property control, enabling the integration of dissimilar materials and promoting phenomena not typically observed in vertical heterostructures. Using the molecular beam epitaxy technique, we achieve a controllable synthesis of lateral heterostructures in the Kagome metal FeSn. With scanning tunneling microscopy/spectroscopy in conjunction with first-principles calculations, we provide a comprehensive understanding of the bonding motif connecting the Fe3Sn-terminated Kagome and Sn2-terminated honeycomb surfaces. More importantly, we reveal a distance-dependent evolution of the electronic states in the vicinity of the heterointerfaces. This evolution is significantly influenced by the orbital character of the flat bands. Our findings suggest an approach to modulate the electronic properties of the Kagome lattice, which should be beneficial for the development of future quantum devices."
https://arxiv.org/abs/2403.07277,2024-03-11,A Bayesian Approach to OOD Robustness in Image Classification,"['Prakhar Kaushik', 'Adam Kortylewski', 'Alan Yuille']","An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains. We address this problem in the scenario where we have access to images from the target domains but no annotations. Motivated by the challenges of the OOD-CV benchmark where we encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification. Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data. We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision. We obverse that some vMF kernels are similar between different domains, while others are not. This enables us to learn a transitional dictionary of vMF kernels that are intermediate between the source and target domains and train the generative model on this dictionary using the annotations on the source domain, followed by iterative refinement. This approach, termed Unsupervised Generative Transition (UGT), performs very well in OOD scenarios even when occlusion is present. UGT is evaluated on different OOD benchmarks including the OOD-CV dataset, several popular datasets (e.g., ImageNet-C [9]), artificial image corruptions (including adding occluders), and synthetic-to-real domain transfer, and does well in all scenarios outperforming SOTA alternatives (e.g. up to 10% top-1 accuracy on Occluded OOD-CV dataset)."
https://arxiv.org/abs/2403.07276,2024-03-11,Metastable ordered states induced by low temperature annealing of δ-Ag2/3V2O5,"['T. Kubo', 'K. Kojima', 'N. Katayama', 'T. Runčevski', 'R. E. Dinnebier', 'A. S. Gibbs', 'M. Isobe', 'H. Sawa']","In δ-Ag2/3V2O5 with charge degrees of freedom in V, it is known that the charge ordering state and physical properties of V that appear at low temperatures depend strongly on the ordering state of Ag. In this study, we focused on the Ag ions in the interlayer and studied the structure using synchrotron radiation powder diffraction in dependence on temperature. We found that when the sample is slowly cooled from room temperature and ordering occurs at the Ag sites, V4+/V5+ charge ordering of V and subsequent V4+-V4+ structural dimers are produced. Although quenching the sample from room temperature suppresses the ordering of Ag, annealing at around 160 K promotes partial ordering of Ag and allows a metastable phase to be realized. This metastable phase is maintained even when the temperature is lowered again, producing a remarkable change in low-temperature properties. These results indicate that the ordered state of Ag, which is the key to control the charge-ordered state and physical properties, can be controlled by low-temperature annealing. The results of this study may provide a methodology for the realization of metastable states in a wide range of material groups of vanadium compounds, where competition among various charge ordered states underlies the physical properties."
https://arxiv.org/abs/2403.07275,2024-03-11,Coarse-graining black holes out of equilibrium with boundary observables on time slice,['Daichi Takeda'],"In black hole thermodynamics, defining coarse-grained entropy for dynamical black holes has long been a challenge, and various proposals, such as generalized entropy, have been explored. Guided by the AdS/CFT, we introduce a new definition of coarse-grained entropy for a dynamical black hole in Lorentzian Einstein gravity. On each time slice, this entropy is defined as the horizon area of an auxiliary Euclidean black hole that shares the same mass, (angular) momenta, and asymptotic normalizable matter modes with the original Lorentzian solution. The entropy is shown to satisfy a generalized first law and, through holography, the second law as well. Furthermore, by applying this thermodynamics to several Vaidya models in AdS and flat spacetime, we discover a connection between the second law and the null energy condition."
https://arxiv.org/abs/2403.07274,2024-03-11,Achievable Rate Analysis and Optimization of Double-RIS Assisted Spatially Correlated MIMO with Statistical CSI,"['Kaizhe Xu', 'Jiajia Guo', 'Jun Zhang', 'Shi Jin', 'Shaodan Ma']","Reconfigurable intelligent surface (RIS) is a novel meta-material which can form a smart radio environment by dynamically altering reflection directions of the impinging electromagnetic waves. In the prior literature, the inter-RIS links which also contribute to the performance of the whole system are usually neglected when multiple RISs are deployed. In this paper we investigate a general double-RIS assisted multiple-input multiple-output (MIMO) wireless communication system under spatially correlated non line-of-sight propagation channels, where the cooperation of the double RISs is also considered. The design objective is to maximize the achievable ergodic rate based on full statistical channel state information (CSI). Specifically, we firstly present a closed-form asymptotic expression for the achievable ergodic rate by utilizing replica method from statistical physics. Then a full statistical CSI-enabled optimal design is proposed which avoids high pilot training overhead compared to instantaneous CSI-enabled design. To further reduce the signal processing overhead and lower the complexity for practical realization, a common-phase scheme is proposed to design the double RISs. Simulation results show that the derived asymptotic ergodic rate is quite accurate even for small-sized antenna arrays. And the proposed optimization algorithm can achieve substantial gain at the expense of a low overhead and complexity. Furthermore, the cooperative double-RIS assisted MIMO framework is proven to achieve superior ergodic rate performance and high communication reliability under harsh propagation environment."
https://arxiv.org/abs/2403.07273,2024-03-11,Lateral 2D superlattices in GaAs heterostructures with independent control of carrier density and modulation potential,"['D. Q. Wang', 'D. Reuter', 'A. D. Wieck', 'A. R. Hamilton', 'O. Klochan']","We present a new double-layer design for 2D surface superlattice systems in GaAs-AlGaAs heterostructures. Unlike previous studies, our device (1) uses an in-situ gate, which allows very short period superlattice in high mobility, shallow heterostructures; (2) enables independent control of the carrier density and the superlattice modulation potential amplitude over a wide range. We characterise this device design using low-temperature magneto-transport measurements and show that the fabrication process caused minimal damage to the system. We demonstrate the tuning of potential modulation from weak (much smaller than Fermi energy) to strong (larger than the Fermi energy) regimes."
https://arxiv.org/abs/2403.07272,2024-03-11,Error terms for the motives of discriminant complements and a Cayley-Bacharach theorem,['Ishan Banerjee'],"In this paper we prove under some simplifying hypotheses questions of Picoco and Levinson-Ullery on Cayley-Bacharach sets. Our results imply that, under suitable hypotheses Cayley-Bacharach sets lie on curves of low degree. We then use these results to estimate error terms to the normalized motive of the space of smooth degree $d$ hypersurfaces in $\mathbb{P}^n$as $d$ grows to infinity. The error term can be expressed in terms of a certain `sum over points' on plane cubic curves and the associated Hodge structure can be expressed in terms of the cohomology of the moduli space of elliptic curves. We also prove convergence of the motive of degree $d$ hypersurfaces in $\mathbb{P}^n$ as $n$ grows to infinity as well as other results on discriminant complements of high dimensional varieties."
https://arxiv.org/abs/2403.07271,2024-03-11,Anderson acceleration for iteratively reweighted $\ell_1$ algorithm,['Kexin Li'],"Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving sparse optimization problems with nonconvex and nonsmooth regularization. The development of its acceleration algorithm, often employing Nesterov acceleration, has sparked significant interest. Nevertheless, the convergence and complexity analysis of these acceleration algorithms consistently poses substantial challenges. Recently, Anderson acceleration has gained prominence owing to its exceptional performance for speeding up fixed-point iteration, with numerous recent studies applying it to gradient-based algorithms. Motivated by the powerful impact of Anderson acceleration, we propose an Anderson-accelerated IRL1 algorithm and establish its local linear convergence rate. We extend this convergence result, typically observed in smooth settings, to a nonsmooth scenario. Importantly, our theoretical results do not depend on the Kurdyka-Lojasiewicz condition, a necessary condition in existing Nesterov acceleration-based algorithms. Furthermore, to ensure global convergence, we introduce a globally convergent Anderson accelerated IRL1 algorithm by incorporating a classical nonmonotone line search condition. Experimental results indicate that our algorithm outperforms existing Nesterov acceleration-based algorithms."
https://arxiv.org/abs/2403.07270,2024-03-11,Long-term Hydrothermal Bid-based Market Simulator,"['Joaquim Dias Garcia', 'Alexandre Street', 'Mario Veiga Pereira']","Simulating long-term hydrothermal bid-based markets considering strategic agents is a challenging task. The representation of strategic agents considering inter-temporal constraints within a stochastic framework brings additional complexity to the already difficult single-period bilevel, thus, non-convex, optimal bidding problem. Thus, we propose a simulation methodology that effectively addresses these challenges for large-scale hydrothermal power systems. We demonstrate the effectiveness of the framework through a case study with real data from the large-scale Brazilian power system. In the case studies, we show the effects of market concentration in power systems and how contracts can be used to mitigate them. In particular, we show how market power might affect the current setting in Brazil. The developed method can strongly benefit policy makers, market monitors, and market designers as simulations can be used to understand existing power systems and experiment with alternative designs."
https://arxiv.org/abs/2403.07269,2024-03-11,MPS: A New Method for Selecting the Stable Closed-Loop Equilibrium Attitude-Error Quaternion of a UAV During Flight,"['Francisco M. F. R. Gonçalves', 'Ryan M. Bena', 'Konstantin I. Matveev', 'Néstor O. Pérez-Arancibia']","We present model predictive selection (MPS), a new method for selecting the stable closed-loop (CL) equilibrium attitude-error quaternion (AEQ) of an uncrewed aerial vehicle (UAV) during the execution of high-speed yaw maneuvers. In this approach, we minimize the cost of yawing measured with a performance figure of merit (PFM) that takes into account both the aerodynamic-torque control input and attitude-error state of the UAV. Specifically, this method uses a control law with a term whose sign is dynamically switched in real time to select, between two options, the torque associated with the lesser cost of rotation as predicted by a dynamical model of the UAV derived from first principles. This problem is relevant because the selection of the stable CL equilibrium AEQ significantly impacts the performance of a UAV during high-speed rotational flight, from both the power and control-error perspectives. To test and demonstrate the functionality and performance of the proposed method, we present data collected during one hundred real-time high-speed yaw-tracking flight experiments. These results highlight the superior capabilities of the proposed MPS-based scheme when compared to a benchmark controller commonly used in aerial robotics, as the PFM used to quantify the cost of flight is reduced by 60.30 %, on average. To our best knowledge, these are the first flight-test results that thoroughly demonstrate, evaluate, and compare the performance of a real-time controller capable of selecting the stable CL equilibrium AEQ during operation."
https://arxiv.org/abs/2403.07268,2024-03-11,Topological valley plasmons in twisted monolayer-double graphene moiré superlattices,"['Weiwei Luo', 'Jiang Fan', 'Alexey B. Kuzmenko', 'Wei Cai', 'Jingjun Xu']","In topological photonics, artificial photonic structures are constructed for realizing nontrivial unidirectional propagation of photonic information. On the other hand, moiré superlattices are emerging as an important avenue for engineering quantum materials with novel properties. In this paper, we combine these two aspects and demonstrate theoretically that moiré superlattices of small-angle twisted monolayer-bilayer graphene provide a natural platform for valley protected plasmons. Particularly, a complete plasmonic bandgap appears stemming from the distinct optical conductivities of the ABA and ABC stacked triangular domains. Moreover, the plasmonic crystals exhibit nonzero valley Chern numbers and unidirectional transport of plasmonic edge states protected from inter-valley scattering is presented."
https://arxiv.org/abs/2403.07267,2024-03-11,An Exactly Solvable Model for Single-Lane Unidirectional Ant Traffic,"['Ngo Phuoc Nguyen Ngoc', 'Huynh Anh Thi', 'Nguyen Van Vinh']","In 2002, Chowdhury et al. introduced a simplified model aimed at depicting the dynamics of single-lane unidirectional ant traffic. Despite efforts, an exact solution for the stationary state of this ant-trail model remains elusive. The primary challenge arises from inherent fluctuations in the total amount of pheromone along the ant-trail. These fluctuations significantly influence ant movement. Consequently, unlike conventional models involving driven interacting particles, the average velocity exhibits non-monotonic behavior with ant density. In this study, we propose an exactly solvable model for ant traffic whose dynamics is based on the one in Chowdhury et al.'s model, and then discuss the circumstances under which the non-monotonic trend of average velocity arises."
https://arxiv.org/abs/2403.07266,2024-03-11,Search for CP-violating Neutrino Non-Standard Interactions with the NOvA Experiment,"[' NOvA Collaboration', 'M. A. Acero', 'B. Acharya', 'P. Adamson', 'L. Aliaga', 'N. Anfimov', 'A. Antoshkin', 'E. Arrieta-Diaz', 'L. Asquith', 'A. Aurisano', 'A. Back', 'N. Balashov', 'P. Baldi', 'B. A. Bambah', 'A. Bat', 'K. Bays', 'R. Bernstein', 'T. J. C. Bezerra', 'V. Bhatnagar', 'D. Bhattarai', 'B. Bhuyan', 'J. Bian', 'A. C. Booth', 'R. Bowles', 'B. Brahma']","This Letter reports a search for charge-parity (CP) symmetry violating non-standard interactions (NSI) of neutrinos with matter using the NOvA Experiment, and examines their effects on the determination of the standard oscillation parameters. Data from $ν_μ(\barν_μ)\rightarrowν_μ(\barν_μ)$ and $ν_μ(\barν_μ)\rightarrowν_{e}(\barν_{e})$ oscillation channels are used to measure the effect of the NSI parameters $\varepsilon_{eμ}$ and $\varepsilon_{eτ}$. With 90% C.L. the magnitudes of the NSI couplings are constrained to be $|\varepsilon_{eμ}| \, \lesssim 0.3$ and $|\varepsilon_{eτ}| \, \lesssim 0.4$. A degeneracy at $|\varepsilon_{eτ}| \, \approx 1.8$ is reported, and we observe that the presence of NSI limits sensitivity to the standard CP phase $δ_{\tiny\text{CP}}$."
https://arxiv.org/abs/2403.07265,2024-03-11,Self-supervised Contrastive Learning for Implicit Collaborative Filtering,"['Shipeng Song', 'Bin Liu', 'Fei Teng', 'Tianrui Li']","Contrastive learning-based recommendation algorithms have significantly advanced the field of self-supervised recommendation, particularly with BPR as a representative ranking prediction task that dominates implicit collaborative filtering. However, the presence of false-positive and false-negative examples in recommendation systems hampers accurate preference learning. In this study, we propose a simple self-supervised contrastive learning framework that leverages positive feature augmentation and negative label augmentation to improve the self-supervisory signal. Theoretical analysis demonstrates that our learning method is equivalent to maximizing the likelihood estimation with latent variables representing user interest centers. Additionally, we establish an efficient negative label augmentation technique that samples unlabeled examples with a probability linearly dependent on their relative ranking positions, enabling efficient augmentation in constant time complexity. Through validation on multiple datasets, we illustrate the significant improvements our method achieves over the widely used BPR optimization objective while maintaining comparable runtime."
https://arxiv.org/abs/2403.07264,2024-03-11,Near-Interpolators: Rapid Norm Growth and the Trade-Off between Interpolation and Generalization,"['Yutong Wang', 'Rishi Sonthalia', 'Wei Hu']","We study the generalization capability of nearly-interpolating linear regressors: $\boldsymbolβ$'s whose training error $τ$ is positive but small, i.e., below the noise floor. Under a random matrix theoretic assumption on the data distribution and an eigendecay assumption on the data covariance matrix $\boldsymbolΣ$, we demonstrate that any near-interpolator exhibits rapid norm growth: for $τ$ fixed, $\boldsymbolβ$ has squared $\ell_2$-norm $\mathbb{E}[\|{\boldsymbolβ}\|_{2}^{2}] = Ω(n^α)$ where $n$ is the number of samples and $α>1$ is the exponent of the eigendecay, i.e., $λ_i(\boldsymbolΣ) \sim i^{-α}$. This implies that existing data-independent norm-based bounds are necessarily loose. On the other hand, in the same regime we precisely characterize the asymptotic trade-off between interpolation and generalization. Our characterization reveals that larger norm scaling exponents $α$ correspond to worse trade-offs between interpolation and generalization. We verify empirically that a similar phenomenon holds for nearly-interpolating shallow neural networks."
https://arxiv.org/abs/2403.07263,2024-03-11,Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction,"['Alexander Timans', 'Christoph-Nikolas Straehle', 'Kaspar Sakmann', 'Eric Nalisnick']","Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving. We consider quantifying such uncertainty for multi-object detection. In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes. One challenge in doing so is that bounding box predictions are conditioned on the object's class label. Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals for the bounding boxes. This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, ensuring their usefulness when maximal safety assurances are required. Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage across sizes. Validating our two-step approach on real-world datasets for 2D bounding box localization, we find that desired coverage levels are satisfied with actionably tight predictive uncertainty intervals."
https://arxiv.org/abs/2403.07262,2024-03-11,Advantage-Aware Policy Optimization for Offline Reinforcement Learning,"['Yunpeng Qing', 'Shunyu liu', 'Jingyuan Cong', 'Kaixuan Chen', 'Yihe Zhou', 'Mingli Song']","Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variational Auto-Encoder (CVAE) to disentangle the action distributions of intertwined behavior policies by modeling the advantage values of all training data as conditional variables. Then the agent can follow such disentangled action distribution constraints to optimize the advantage-aware policy towards high advantage values. Extensive experiments conducted on both the single-quality and mixed-quality datasets of the D4RL benchmark demonstrate that A2PO yields results superior to state-of-the-art counterparts. Our code will be made publicly available."
https://arxiv.org/abs/2403.07261,2024-03-11,Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation,"['Chengxing Jia', 'Fuxiang Zhang', 'Yi-Chen Li', 'Chen-Xiao Gao', 'Xu-Hui Liu', 'Lei Yuan', 'Zongzhang Zhang', 'Yang Yu']","Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task representations is to adopt contrastive learning using multi-task offline data. The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies. We observed that learned task representations from previous OMRL methods tend to correlate spuriously with the behavior policy instead of reflecting the essential characteristics of the task, resulting in unfavorable out-of-distribution generalization. To alleviate this issue, we introduce a novel algorithm to disentangle the impact of behavior policy from task representation learning through a process called adversarial data augmentation. Specifically, the objective of adversarial data augmentation is not merely to generate data analogous to offline data distribution; instead, it aims to create adversarial examples designed to confound learned task representations and lead to incorrect task identification. Our experiments show that learning from such adversarial samples significantly enhances the robustness and effectiveness of the task identification process and realizes satisfactory out-of-distribution generalization."
https://arxiv.org/abs/2403.07260,2024-03-11,CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation,['Yumeng Fu'],"Emotion recognition in conversation (ERC) is a task which predicts the emotion of an utterance in the context of a conversation. It tightly depends on dialogue context, speaker identity information, multiparty dialogue scenario and so on. However, the state-of-the-art method (instructERC) solely identifying speaker, and ignores commonsense knowledge(i.e., reaction of the listeners and intention of the speaker, etc.) behind speakers during a conversation, which can deeply mine speaker information. To this end, we propose a novel joint large language models with commonsense knowledge framework for emotion recognition in conversation, namely CKERC.We design prompts to generate interlocutors' commonsense based on historical utterances with large language model. And we use the interlocutor commonsense identification task for LLM pre-training to fine-tune speaker implicit clues information.By solving above challenge, our method achieve state-of-the-art.We extensive experiment on three widely-used datasets, i.e., IEMOCAP, MELD, EmoryNLP, demonstrate our method superiority. Also, we conduct in-depth analysis and further demonstrate the effectiveness of commonsense knowledge in ERC task in large language model."
https://arxiv.org/abs/2403.07259,2024-03-11,Near-optimal convergence of the full orthogonalization method,"['Tyler Chen', 'Gérard Meurant']","We establish a near-optimality guarantee for the full orthogonalization method (FOM), showing that the overall convergence of FOM is nearly as good as GMRES. In particular, we prove that at every iteration $k$, there exists an iteration $j\leq k$ for which the FOM residual norm at iteration $j$ is no more than $\sqrt{k+1}$ times larger than the GMRES residual norm at iteration $k$. This bound is sharp, and it has implications for algorithms for approximating the action of a matrix function on a vector."
https://arxiv.org/abs/2403.07258,2024-03-11,Harmonic metrics for Higgs bundles of rank 3 in the Hitchin section,['Hitoshi Fujioka'],"Given a tuple of holomorphic differentials on a Riemann surface, one can define a Higgs bundle in the Hitchin section and a natural symmetric pairing of the Higgs bundle. We study whether a Higgs bundle of rank 3 in the Hitchin section has a compatible harmonic metric when the spectral curve is a 2-sheeted branched covering of the Riemann surface. In particular, we give a condition for Higgs bundles in the Hitchin section on $\mathbb{C}$ or $\mathbb{C}^*$ to have compatible harmonic metrics."
https://arxiv.org/abs/2403.07257,2024-03-11,The Dawn of AI-Native EDA: Promises and Challenges of Large Circuit Models,"['Lei Chen', 'Yiqi Chen', 'Zhufei Chu', 'Wenji Fang', 'Tsung-Yi Ho', 'Yu Huang', 'Sadaf Khan', 'Min Li', 'Xingquan Li', 'Yun Liang', 'Yibo Lin', 'Jinwei Liu', 'Yi Liu', 'Guojie Luo', 'Zhengyuan Shi', 'Guangyu Sun', 'Dimitrios Tsaras', 'Runsheng Wang', 'Ziyi Wang', 'Xinming Wei', 'Zhiyao Xie', 'Qiang Xu', 'Chenhao Xue', 'Evangeline F. Y. Young', 'Bei Yu']","Within the Electronic Design Automation (EDA) domain, AI-driven solutions have emerged as formidable tools, yet they typically augment rather than redefine existing methodologies. These solutions often repurpose deep learning models from other domains, such as vision, text, and graph analytics, applying them to circuit design without tailoring to the unique complexities of electronic circuits. Such an AI4EDA approach falls short of achieving a holistic design synthesis and understanding, overlooking the intricate interplay of electrical, logical, and physical facets of circuit data. This perspective paper argues for a paradigm shift from AI4EDA towards AI-native EDA, integrating AI at the core of the design process. Pivotal to this vision is the development of a multimodal circuit representation learning technique, poised to provide a comprehensive understanding by harmonizing and extracting insights from varied data sources, such as functional specifications, RTL designs, circuit netlists, and physical layouts."
https://arxiv.org/abs/2403.07256,2024-03-11,Sharp one-point estimates and Minkowski content for the scaling limit of three-dimensional loop-erased random walk,"['Sarai Hernandez-Torres', 'Xinyi Li', 'Daisuke Shiraishi']","In this work, we consider the scaling limit of loop-erased random walk (LERW) in three dimensions and prove that the limiting occupation measure is equivalent to its $β$-dimensional Minkowski content, where $β\in (1, 5/3]$ is its Hausdorff dimension. In doing this we also establish the existence of the two-point function and provide some sharp estimates on one-point function and ball-hitting probabilities for 3D LERW in any scale, which is a considerable improvement of previous results."
https://arxiv.org/abs/2403.07255,2024-03-11,Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication,"['Yongjeong Oh', 'Jaehong Jo', 'Byonghyo Shim', 'Yo-Seb Jeon']","In this paper, we present a novel approach for joint activity detection (AD), channel estimation (CE), and data detection (DD) in uplink grant-free non-orthogonal multiple access (NOMA) systems. Our approach employs an iterative and parallel interference removal strategy inspired by parallel interference cancellation (PIC), enhanced with deep learning to jointly tackle the AD, CE, and DD problems. Based on this approach, we develop three PIC frameworks, each of which is designed for either coherent or non-coherence schemes. The first framework performs joint AD and CE using received pilot signals in the coherent scheme. Building upon this framework, the second framework utilizes both the received pilot and data signals for CE, further enhancing the performances of AD, CE, and DD in the coherent scheme. The third framework is designed to accommodate the non-coherent scheme involving a small number of data bits, which simultaneously performs AD and DD. Through joint loss functions and interference cancellation modules, our approach supports end-to-end training, contributing to enhanced performances of AD, CE, and DD for both coherent and non-coherent schemes. Simulation results demonstrate the superiority of our approach over traditional techniques, exhibiting enhanced performances of AD, CE, and DD while maintaining lower computational complexity."
https://arxiv.org/abs/2403.07254,2024-03-11,Negative orbital Hall effect in Germanium,"['E. Santos', 'J. E. Abrao', 'J. L. Costa', 'J. G. S. Santos', 'J. B. S. Mendes', 'A. Azevedo']","Our investigation reveals a groundbreaking discovery of a negative inverse orbital Hall effect (IOHE) in Ge thin films. We employed the innovative orbital pumping technique where spin-orbital coupled current is injected into Ge films using YIG/Pt(2)/Ge($t_{Ge}$) and YIG/W(2)/Ge($t_{Ge}$) heterostructures. Through comprehensive analysis, we observe significant reductions in the signals generated by coherent (RF-driven) and incoherent (thermal-driven) spin-orbital pumping techniques. These reductions are attributed to the presence of a remarkable strong negative IOHE in Ge, showing its magnitude comparable to the spin-to-charge signal in Pt. Our findings reveal that although the spin-to-charge conversion in Ge is negligible, the orbital-to-charge conversion exhibits large magnitude. Our results are innovative and pioneering in the investigation of negative IOHE by the injection of spin-orbital currents."
https://arxiv.org/abs/2403.07253,2024-03-11,Bourgain's counterexample in the sequential convergence problem for the Schrödinger equation,"['Chu-Hee Cho', 'Daniel Eceizabarrena']","We study the problem of pointwise convegence for the Schrödinger operator on $\mathbb R^n$ along time sequences. We show that the sharp counterexample to the sequential Schrödinger maximal estimate given recently by Li, Wang and Yan based in the construction by Lucà and Rogers can also be achieved with the construction of Bourgain, and we extend it to the fractal setting."
https://arxiv.org/abs/2403.07252,2024-03-11,Serre functors and complete torsion pairs,"['Zhe Han', 'Ping He']","Given a torsion pair $(\mathcal{T},\mathcal{F})$ in an abelian category $\mathcal{A}$, there is a t-structure $(\mathcal{U}_\mathcal{T},\mathcal{V}_\mathcal{T})$ determined by $\mathcal{T}$ on the derived category $D^b(\mathcal{A})$. The existence of derived equivalence between heart $\mathcal{B}$ of the t-structure and $\mathcal{A}$ which naturally extends the embedding $\mathcal{B}\to D^b(\mathcal{A})$ is determined by the completeness of the torsion pair [6]. When $\mathcal{A}$ is the module category of a finite-dimensional hereditary algebra and $\mathcal{U}_\mathcal{T}$ is closed under Serre functor, then there exists a triangle equivalence $D^b(\mathcal{B})\to D^b(\mathcal{A})$ [21]. In this case, we give a straightforward proof of the fact torsion pair $(\mathcal{T},\mathcal{F})$ is complete if and only if $\mathcal{U}_\mathcal{T}$ is closed under the Serre functor."
https://arxiv.org/abs/2403.07251,2024-03-11,Spatially resolved random telegraph fluctuations of a single trap at the Si/SiO2 interface,"['Megan Cowie', 'Procopios C. Constantinou', 'Neil J. Curson', 'Taylor J. Z. Stock', 'Peter Grutter']","We use electrostatic force microscopy to spatially resolve random telegraph noise at the Si/SiO$_2$ interface. Our measurements demonstrate that two-state fluctuations are localized at interfacial traps, with bias-dependent rates and amplitudes. These two-level systems lead to correlated carrier number and mobility fluctuations with a range of characteristic timescales; taken together as an ensemble, they give rise to a $1/f$ power spectral trend. Such individual defect fluctuations at the Si/SiO$_2$ interface impair the performance and reliability of nanoscale semiconductor devices, and will be a significant source of noise in semiconductor-based quantum sensors and computers. The fluctuations measured here are associated with a four-fold competition of rates, including slow two-state switching on the order of seconds and, in one state, fast switching on the order of nanoseconds which is associated with energy loss."
https://arxiv.org/abs/2403.07250,2024-03-11,"Integrating Gauss-Bonnet Corrections in Quantum Field Theory: Implications for Torsion, Gauge Bosons, and the Hubble Constant","['Zirui Hu', 'Haoxuan Sun']","We explore matter fields containing Gauss-Bonnet correction terms within the framework of renormalizable quantum field theory. By revising the gauge model with a charged scalar multiplier and two sets of fermion families within a flat universe model using torsion, we introduce Gauss-Bonnet corrections into the action to investigate field equations within the context of supersymmetric mixed inflationary models. After analytically computing the modified gauge boson field equations through the incorporation of Gauss-Bonnet theory into the fundamental field equations, we derive the torsion characteristics and energy-momentum tensor properties of a flat universe. Our analysis can extend to more specific Gauss-Bonnet correction models, enabling the derivation of gravitational characteristic equations with practical applications. This streamlines Gauss-Bonnet models, assesses the model's sensitivity to correction parameters, and explores high-sensitivity models with observational significance. Furthermore, in this derivation process, we identify that Gauss-Bonnet correction terms can directly impact the Hubble constant through Einstein's equations, indicating the potential for verifying Gauss-Bonnet theory through astronomical observations."
https://arxiv.org/abs/2403.07249,2024-03-11,Toward An Analytic Theory of Intrinsic Robustness for Dexterous Grasping,"['Albert H. Li', 'Preston Culbertson', 'Aaron D. Ames']","Conventional approaches to grasp planning require perfect knowledge of an object's pose and geometry. Uncertainties in these quantities induce uncertainties in the quality of planned grasps, which can lead to failure. Classically, grasp robustness refers to the ability to resist external disturbances after grasping an object. In contrast, this work studies robustness to intrinsic sources of uncertainty like object pose or geometry affecting grasp planning before execution. To do so, we develop a novel analytic theory of grasping that reasons about this intrinsic robustness by characterizing the effect of friction cone uncertainty on a grasp's force closure status. As a result, we show the Ferrari-Canny metric -- which measures the size of external disturbances a grasp can reject -- bounds the friction cone uncertainty a grasp can tolerate, and thus also measures intrinsic robustness. In tandem, we show that the recently proposed min-weight metric lower bounds the Ferrari-Canny metric, justifying it as a computationally-efficient, uncertainty-aware alternative. We validate this theory on hardware experiments versus a competitive baseline and demonstrate superior performance. Finally, we use our theory to develop an analytic notion of probabilistic force closure, which we show in simulation generates grasps that can incorporate uncertainty distributions over an object's geometry."
https://arxiv.org/abs/2403.07248,2024-03-11,Atomicity and Abstraction for Cross-Blockchain Interactions,"['Huaixi Lu', 'Akshay Jajoo', 'Kedar S. Namjoshi']","A blockchain facilitates secure and atomic transactions between mutually untrusting parties on that chain. Today, there are multiple blockchains with differing interfaces and security properties. Programming in this multi-blockchain world is hindered by the lack of general and convenient abstractions for cross-chain communication and computation. Current cross-chain communication bridges have varied and low-level interfaces, making it difficult to develop portable applications. Current methods for multi-chain atomic transactions are limited in scope to cryptocurrency swaps."
https://arxiv.org/abs/2403.07247,2024-03-11,GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation,"['Linrui Dai', 'Rongzhao Zhang', 'Zhongzhen Huang', 'Xiaofan Zhang']","The annotation burden and extensive labor for gathering a large medical dataset with images and corresponding labels are rarely cost-effective and highly intimidating. This results in a lack of abundant training data that undermines downstream tasks and partially contributes to the challenge image analysis faces in the medical field. As a workaround, given the recent success of generative neural models, it is now possible to synthesize image datasets at a high fidelity guided by external constraints. This paper explores this possibility and presents \textbf{GuideGen}: a pipeline that jointly generates CT images and tissue masks for abdominal organs and colorectal cancer conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to fit the discrete distribution of mask labels and generate low-resolution 3D tissue masks. Secondly, our Conditional Image Generator autoregressively generates CT slices conditioned on a corresponding mask slice to incorporate both style information and anatomical guidance. This pipeline guarantees high fidelity and variability as well as exact alignment between generated CT volumes and tissue masks. Both qualitative and quantitative experiments on 3D abdominal CTs demonstrate a high performance of our proposed pipeline, thereby proving our method can serve as a dataset generator and provide potential benefits to downstream tasks. It is hoped that our work will offer a promising solution on the multimodality generation of CT and its anatomical mask. Our source code is publicly available at https://github.com/OvO1111/JointImageGeneration."
https://arxiv.org/abs/2403.07246,2024-03-11,Towards Zero-shot Human-Object Interaction Detection via Vision-Language Integration,"['Weiying Xue', 'Qi Liu', 'Qiwei Xiong', 'Yuxiao Wang', 'Zhenao Wei', 'Xiaofen Xing', 'Xiangmin Xu']","Human-object interaction (HOI) detection aims to locate human-object pairs and identify their interaction categories in images. Most existing methods primarily focus on supervised learning, which relies on extensive manual HOI annotations. In this paper, we propose a novel framework, termed Knowledge Integration to HOI (KI2HOI), that effectively integrates the knowledge of visual-language model to improve zero-shot HOI detection. Specifically, the verb feature learning module is designed based on visual semantics, by employing the verb extraction decoder to convert corresponding verb queries into interaction-specific category representations. We develop an effective additive self-attention mechanism to generate more comprehensive visual representations. Moreover, the innovative interaction representation decoder effectively extracts informative regions by integrating spatial and visual feature information through a cross-attention mechanism. To deal with zero-shot learning in low-data, we leverage a priori knowledge from the CLIP text encoder to initialize the linear classifier for enhanced interaction understanding. Extensive experiments conducted on the mainstream HICO-DET and V-COCO datasets demonstrate that our model outperforms the previous methods in various zero-shot and full-supervised settings."
https://arxiv.org/abs/2403.07245,2024-03-11,Dataset Condensation for Time Series Classification via Dual Domain Matching,"['Zhanyu Liu', 'Ke Hao', 'Guanjie Zheng', 'Yanwei Yu']","Time series data has been demonstrated to be crucial in various research fields. The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network. Recently, a technique named \textit{Dataset Condensation} has emerged as a solution to this problem. This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification. However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain. In this paper, we propose a novel framework named Dataset \textit{\textbf{Cond}}ensation for \textit{\textbf{T}}ime \textit{\textbf{S}}eries \textit{\textbf{C}}lassification via Dual Domain Matching (\textbf{CondTSC}) which focuses on the time series classification dataset condensation task. Different from previous methods, our proposed framework aims to generate a condensed dataset that matches the surrogate objectives in both the time and frequency domains. Specifically, CondTSC incorporates multi-view data augmentation, dual domain training, and dual surrogate objectives to enhance the dataset condensation process in the time and frequency domains. Through extensive experiments, we demonstrate the effectiveness of our proposed framework, which outperforms other baselines and learns a condensed synthetic dataset that exhibits desirable characteristics such as conforming to the distribution of the original data."
https://arxiv.org/abs/2403.07244,2024-03-11,Time-Efficient Light-Field Acquisition Using Coded Aperture and Events,"['Shuji Habuchi', 'Keita Takahashi', 'Chihiro Tsutake', 'Toshiaki Fujii', 'Hajime Nagahara']","We propose a computational imaging method for time-efficient light-field acquisition that combines a coded aperture with an event-based camera. Different from the conventional coded-aperture imaging method, our method applies a sequence of coding patterns during a single exposure for an image frame. The parallax information, which is related to the differences in coding patterns, is recorded as events. The image frame and events, all of which are measured in a single exposure, are jointly used to computationally reconstruct a light field. We also designed an algorithm pipeline for our method that is end-to-end trainable on the basis of deep optics and compatible with real camera hardware. We experimentally showed that our method can achieve more accurate reconstruction than several other imaging methods with a single exposure. We also developed a hardware prototype with the potential to complete the measurement on the camera within 22 msec and demonstrated that light fields from real 3-D scenes can be obtained with convincing visual quality. Our software and supplementary video are available from our project website."
https://arxiv.org/abs/2403.07243,2024-03-11,Si Metasurface Supporting Multiple Quasi-BICs for Degenerate Four-Wave Mixing,"['Gianni Q. Moretti', 'Thomas Weber', 'Thomas Possmayer', 'Emiliano Cortés', 'Leonardo de S. Menezes', 'Andrea V. Bragas', 'Stefan A. Maier', 'Andreas Tittl', 'Gustavo Grinblat']","Dielectric metasurfaces supporting quasi-bound states in the continuum (qBICs) enable high field enhancement with narrow-linewidth resonances in the visible and near-infrared ranges. The resonance emerges when distorting the meta-atom's geometry away from a symmetry-protected BIC condition and, usually, a given design can sustain one or two of these states. In this work, we introduce a silicon-on-silica metasurface that simultaneously supports up to four qBIC resonances in the near-infrared region. This is achieved by combining multiple symmetry-breaking distortions on an elliptical cylinder array. By pumping two of these resonances, the nonlinear process of degenerate four-wave mixing is experimentally realized. By comparing the nonlinear response with that of an unpatterned silicon film, the near-field enhancement inside the nanostructured dielectric is revealed. The presented results demonstrate independent geometric control of multiple qBICs and their interaction trough wave mixing processes, opening new research pathways in nanophotonics, with potential applications in information multiplexing, multi-wavelength sensing and nonlinear imaging."
https://arxiv.org/abs/2403.07242,2024-03-11,Designing high-fidelity two-qubit gates between fluxonium qubits,"['Emma L. Rosenfeld', 'Connor T. Hann', 'David I. Schuster', 'Matthew H. Matheny', 'Aashish A. Clerk']","We take a bottom-up, first-principles approach to design a two-qubit gate between fluxonium qubits for minimal error, speed, and control simplicity. Our proposed architecture consists of two fluxoniums coupled via a linear resonator. Using a linear coupler introduces the possibility of material optimization for suppressing its loss, enables efficient driving of state-selective transitions through its large charge zero point fluctuation, reduces sensitivity to junction aging, and partially mitigates coherent coupling to two-level systems. Crucially, a resonator-as-coupler approach also suggests a clear path to increased connectivity between fluxonium qubits, by reducing capacitive loading when the coupler has a high impedance. After performing analytic and numeric analyses of the circuit Hamiltonian and gate dynamics, we tune circuit parameters to destructively interfere sources of coherent error, revealing an efficient, fourth-order scaling of coherent error with gate duration. For component properties from the literature, we predict an open-system average CZ gate infidelity of $1.86 \times 10^{-4}$ in 70ns."
https://arxiv.org/abs/2403.07241,2024-03-11,Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations,"['Chenyu You', 'Yifei Min', 'Weicheng Dai', 'Jasjeet S. Sekhon', 'Lawrence Staib', 'James S. Duncan']","Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any group annotation. To this end, we systematically study the existence of spurious correlation on CLIP and CILP+ERM. We first, following recent work on Deep Feature Reweighting (DFR), verify that last-layer retraining can greatly improve group robustness on pretrained CLIP. In view of them, we advocate a lightweight representation calibration method for fine-tuning CLIP, by first generating a calibration set using the pretrained CLIP, and then calibrating representations of samples within this set through contrastive learning, all without the need for group labels. Extensive experiments and in-depth visualizations on several benchmarks validate the effectiveness of our proposals, largely reducing reliance and significantly boosting the model generalization."
https://arxiv.org/abs/2403.07240,2024-03-11,Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space Learning,"['Chuangchuang Tan', 'Yao Zhao', 'Shikui Wei', 'Guanghua Gu', 'Ping Liu', 'Yunchao Wei']","This research addresses the challenge of developing a universal deepfake detector that can effectively identify unseen deepfake images despite limited training data. Existing frequency-based paradigms have relied on frequency-level artifacts introduced during the up-sampling in GAN pipelines to detect forgeries. However, the rapid advancements in synthesis technology have led to specific artifacts for each generation model. Consequently, these detectors have exhibited a lack of proficiency in learning the frequency domain and tend to overfit to the artifacts present in the training data, leading to suboptimal performance on unseen sources. To address this issue, we introduce a novel frequency-aware approach called FreqNet, centered around frequency domain learning, specifically designed to enhance the generalizability of deepfake detectors. Our method forces the detector to continuously focus on high-frequency information, exploiting high-frequency representation of features across spatial and channel dimensions. Additionally, we incorporate a straightforward frequency domain learning module to learn source-agnostic features. It involves convolutional layers applied to both the phase spectrum and amplitude spectrum between the Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (iFFT). Extensive experimentation involving 17 GANs demonstrates the effectiveness of our proposed method, showcasing state-of-the-art performance (+9.8\%) while requiring fewer parameters. The code is available at {\cred \url{https://github.com/chuangchuangtan/FreqNet-DeepfakeDetection}}."
https://arxiv.org/abs/2403.07239,2024-03-11,The Primal Pathwidth SETH,['Michael Lampis'],"Motivated by the importance of dynamic programming (DP) in parameterized complexity, we consider several fine-grained questions, such as the following examples: (i) can Dominating Set be solved in time $(3-ε)^{pw}n^{O(1)}$? (where $pw$ is the pathwidth) (ii) can Coloring be solved in time $pw^{(1-ε)pw}n^{O(1)}$? (iii) can a short reconfiguration between two size-$k$ independent sets be found in time $n^{(1-ε)k}$? Such questions are well-studied: in some cases the answer is No under the SETH, while in others coarse-grained lower bounds are known under the ETH. Even though questions such as the above seem ""morally equivalent"" as they all ask if a simple DP can be improved, the problems concerned have wildly varying time complexities, ranging from single-exponential FPT to XNLP-complete."
https://arxiv.org/abs/2403.07238,2024-03-11,Towards Full Automation of Geometry Extraction for Biomechanical Analysis of Abdominal Aortic Aneurysm; Neural Network-Based versus Classical Methodologies,"['Farah Alkhatib', 'Mostafa Jamshidian', 'Donatien Le Liepvre', 'Florian Bernard', 'Ludovic Minvielle', 'Adam Wittek', 'Karol Miller']","In this study we investigated the impact of image segmentation methods on the results of stress computation in the wall of abdominal aortic aneurysms (AAAs). We compared wall stress distributions and magnitudes calculated from geometry models obtained from classical semi-automated segmentation versus automated neural network-based segmentation. Ten different AAA contrast-enhanced computed tomography (CT) images were semi-automatically segmented by an analyst, taking, depending on the quality of an image, between 15 and 40 minutes of human effort per patient. The same images were automatically segmented using PRAEVAorta 2, commercial software by NUREA (https://www.nurea-soft.com/), developed based on artificial intelligence (AI) algorithms, requiring only 1-2 minutes of computer time per patient. Aneurysm wall stress calculations performed using the BioPARR software (https://bioparr.mech.uwa.edu.au/) revealed that, compared to the classical semi-automated segmentation, the automatic neural network-based segmentation leads to equivalent stress distributions, and slightly higher peak and 99th percentile maximum principal stress values. This difference is due to consistently larger lumen surface areas in automatically segmented models as compared to classical semi-automated segmentations, resulting in greater total pressure load on the wall. Our findings are a steppingstone toward a fully automated pipeline for biomechanical analysis of AAAs, starting with CT scans and concluding with wall stress assessment, while at the same time highlighting the critical importance of the repeatable and accurate segmentation of the lumen, the difficult problem often underestimated by the literature."
https://arxiv.org/abs/2403.07237,2024-03-11,Cosmological implications from some proposals in Quantum Gravity,['D. Mata-Pacheco'],"In this thesis we present the cosmological applications of some proposals from Quantum Gravity. Namely, we will explore classical and quantum cosmological implications of the Generalized Uncertainty Principle (GUP), the Hořava-Lifshitz (HL) theory of gravity and the Swampland Conjectures. Furthermore, we will also present a detailed analysis of Lorentzian Vacuum Transitions in various contexts."
https://arxiv.org/abs/2403.07236,2024-03-11,Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Binary Outcome Model,['Sarah Moon'],"It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. This problem of aggregation becomes relevant when the researcher wants to learn individual-level relationships, but only has access to data that has been aggregated. In this paper, I develop a methodology to partially identify linear combinations of conditional average outcomes from aggregate data when the outcome of interest is binary, while imposing as few restrictions on the underlying data generating process as possible. I construct identified sets using an optimization program that allows for researchers to impose additional shape restrictions. I also provide consistency results and construct an inference procedure that is valid with aggregate data, which only provides marginal information about each variable. I apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful. This suggests that to obtain useful information from aggregate data sets about individual-level relationships, researchers must impose further assumptions that are carefully justified."
https://arxiv.org/abs/2403.07235,2024-03-11,"Hessian estimates for shrinkers, expanders, translators, and rotators of the Lagrangian Mean Curvature Flow","['Arunima Bhattacharya', 'Jeremy Wall']","In this paper, we prove interior Hessian estimates for shrinkers, expanders, translators, and rotators of the Lagrangian mean curvature flow under the assumption that the Lagrangian phase is hypercritical. We further extend our results to a broader class of Lagrangian mean curvature type equations."
https://arxiv.org/abs/2403.07234,2024-03-11,It's All About Your Sketch: Democratising Sketch Control in Diffusion Models,"['Subhadeep Koley', 'Ayan Kumar Bhunia', 'Deeptanshu Sekhri', 'Aneeshan Sain', 'Pinaki Nath Chowdhury', 'Tao Xiang', 'Yi-Zhe Song']","This paper unravels the potential of sketches for diffusion models, addressing the deceptive promise of direct sketch control in generative AI. We importantly democratise the process, enabling amateur sketches to generate precise images, living up to the commitment of ""what you sketch is what you get"". A pilot study underscores the necessity, revealing that deformities in existing models stem from spatial-conditioning. To rectify this, we propose an abstraction-aware framework, utilising a sketch adapter, adaptive time-step sampling, and discriminative guidance from a pre-trained fine-grained sketch-based image retrieval model, working synergistically to reinforce fine-grained sketch-photo association. Our approach operates seamlessly during inference without the need for textual prompts; a simple, rough sketch akin to what you and I can create suffices! We welcome everyone to examine results presented in the paper and its supplementary. Contributions include democratising sketch control, introducing an abstraction-aware framework, and leveraging discriminative guidance, validated through extensive experiments."
https://arxiv.org/abs/2403.07233,2024-03-11,"Exploring Multiscale Quantum Media: High-Precision Efficient Numerical Solution of the Fractional Schrödinger equation, Eigenfunctions with Physical Potentials, and Fractionally-Enhanced Quantum Tunneling","['Joshua M. Lewis', 'Lincoln D. Carr']","Fractional evolution equations lack generally accessible and well-converged codes excepting anomalous diffusion. A particular equation of strong interest to the growing intersection of applied mathematics and quantum information science and technology is the fractional Schrödinger equation, which describes sub-and super-dispersive behavior of quantum wavefunctions induced by multiscale media. We derive a computationally efficient sixth-order split-step numerical method to converge the eigenfunctions of the FSE to arbitrary numerical precision for arbitrary fractional order derivative. We demonstrate applications of this code to machine precision for classic quantum problems such as the finite well and harmonic oscillator, which take surprising twists due to the non-local nature of the fractional derivative. For example, the evanescent wave tails in the finite well take a Mittag-Leffer-like form which decay much slower than the well-known exponential from integer-order derivative wave theories, enhancing penetration into the barrier and therefore quantum tunneling rates. We call this effect \emph{fractionally enhanced quantum tunneling}. This work includes an open source code for communities from quantum experimentalists to applied mathematicians to easily and efficiently explore the solutions of the fractional Schrödinger equation in a wide variety of practical potentials for potential realization in quantum tunneling enhancement and other quantum applications."
https://arxiv.org/abs/2403.07232,2024-03-11,Tractable Joint Prediction and Planning over Discrete Behavior Modes for Urban Driving,"['Adam Villaflor', 'Brian Yang', 'Huangyuan Su', 'Katerina Fragkiadaki', 'John Dolan', 'Jeff Schneider']","Significant progress has been made in training multimodal trajectory forecasting models for autonomous driving. However, effectively integrating these models with downstream planners and model-based control approaches is still an open problem. Although these models have conventionally been evaluated for open-loop prediction, we show that they can be used to parameterize autoregressive closed-loop models without retraining. We consider recent trajectory prediction approaches which leverage learned anchor embeddings to predict multiple trajectories, finding that these anchor embeddings can parameterize discrete and distinct modes representing high-level driving behaviors. We propose to perform fully reactive closed-loop planning over these discrete latent modes, allowing us to tractably model the causal interactions between agents at each step. We validate our approach on a suite of more dynamic merging scenarios, finding that our approach avoids the $\textit{frozen robot problem}$ which is pervasive in conventional planners. Our approach also outperforms the previous state-of-the-art in CARLA on challenging dense traffic scenarios when evaluated at realistic speeds."
https://arxiv.org/abs/2403.07231,2024-03-11,Learn and Search: An Elegant Technique for Object Lookup using Contrastive Learning,"['Chandan Kumar', 'Jansel Herrera-Gerena', 'John Just', 'Matthew Darr', 'Ali Jannesari']","The rapid proliferation of digital content and the ever-growing need for precise object recognition and segmentation have driven the advancement of cutting-edge techniques in the field of object classification and segmentation. This paper introduces ""Learn and Search"", a novel approach for object lookup that leverages the power of contrastive learning to enhance the efficiency and effectiveness of retrieval systems."
https://arxiv.org/abs/2403.07230,2024-03-11,Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences,"['Pulkit Pattnaik', 'Rishabh Maheshwary', 'Kelechi Ogueji', 'Vikas Yadav', 'Sathwik Tejaswi Madhusudhan']","Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set, highlighting its effectiveness. More specifically, Curry-DPO achieves a score of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs with similar parameter size. Curry-DPO also achieves the highest adjusted win rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and 87.9% respectively) in our experiments, with notable gains of upto 7.5% when compared to standard DPO technique."
https://arxiv.org/abs/2403.07229,2024-03-11,Characterizations of homomorphisms among unital completely positive maps,['Andre Kornell'],"We prove that a unital completely positive map between finite-dimensional C*-algebras is a homomorphism if and only if it is completely entropy-nonincreasing, where the relevant notion of entropy is a variant of von Neumann entropy. As an intermediate step, we prove that a unital completely positive map between finite-dimensional C*-algebras is a homomorphism if and only if its adjusted Choi operator is a projection. Both equivalences generalize familiar facts about stochastic maps between finite sets."
https://arxiv.org/abs/2403.07228,2024-03-11,Physics-constrained Active Learning for Soil Moisture Estimation and Optimal Sensor Placement,"['Jianxin Xie', 'Bing Yao', 'Zheyu Jiang']","Soil moisture is a crucial hydrological state variable that has significant importance to the global environment and agriculture. Precise monitoring of soil moisture in crop fields is critical to reducing agricultural drought and improving crop yield. In-situ soil moisture sensors, which are buried at pre-determined depths and distributed across the field, are promising solutions for monitoring soil moisture. However, high-density sensor deployment is neither economically feasible nor practical. Thus, to achieve a higher spatial resolution of soil moisture dynamics using a limited number of sensors, we integrate a physics-based agro-hydrological model based on Richards' equation in a physics-constrained deep learning framework to accurately predict soil moisture dynamics in the soil's root zone. This approach ensures that soil moisture estimates align well with sensor observations while obeying physical laws at the same time. Furthermore, to strategically identify the locations for sensor placement, we introduce a novel active learning framework that combines space-filling design and physics residual-based sampling to maximize data acquisition potential with limited sensors. Our numerical results demonstrate that integrating Physics-constrained Deep Learning (P-DL) with an active learning strategy within a unified framework--named the Physics-constrained Active Learning (P-DAL) framework--significantly improves the predictive accuracy and effectiveness of field-scale soil moisture monitoring using in-situ sensors."
https://arxiv.org/abs/2403.07227,2024-03-11,Noisy Computing of the Threshold Function,"['Ziao Wang', 'Nadim Ghaddar', 'Banghua Zhu', 'Lele Wang']","Let $\mathsf{TH}_k$ denote the $k$-out-of-$n$ threshold function: given $n$ input Boolean variables, the output is $1$ if and only if at least $k$ of the inputs are $1$. We consider the problem of computing the $\mathsf{TH}_k$ function using noisy readings of the Boolean variables, where each reading is incorrect with some fixed and known probability $p \in (0,1/2)$. As our main result, we show that, when $k = o(n)$, it is both sufficient and necessary to use $$(1 \pm o(1)) \frac{n\log \frac{k}δ}{D_{\mathsf{KL}}(p || 1-p)}$$ queries in expectation to compute the $\mathsf{TH}_k$ function with a vanishing error probability $δ= o(1)$, where $D_{\mathsf{KL}}(p || 1-p)$ denotes the Kullback-Leibler divergence between $\mathsf{Bern}(p)$ and $\mathsf{Bern}(1-p)$ distributions. In particular, this says that $(1 \pm o(1)) \frac{n\log \frac{1}δ}{D_{\mathsf{KL}}(p || 1-p)}$ queries in expectation are both sufficient and necessary to compute the $\mathsf{OR}$ and $\mathsf{AND}$ functions of $n$ Boolean variables. Compared to previous work, our result tightens the dependence on $p$ in both the upper and lower bounds."
https://arxiv.org/abs/2403.07226,2024-03-11,The order-theoretical foundation for data flow security,['Luigi Logrippo'],"Some theories on data flow security are based on order-theoretical concepts, most commonly on lattice concepts. This paper presents a correspondence between security concepts and partial order concepts, by which the former become an application of the latter. The formalization involves concepts of data flow, equivalence classes of entities that can access the same data, and labels. Efficient, well-known algorithms to obtain one of these from one of the others are presented. Security concepts such as secrecy (also called confidentiality), integrity and conflict can be expressed in this theory. Further, it is shown that complex tuple labels used in the literature to express security levels can be translated into equivalent set labels. A consequence is that any network's data flow or access control relationships can be defined by assigning simple set labels to the entities. Finally, it is shown how several partial orders can be combined when different data flows must coexist."
https://arxiv.org/abs/2403.07225,2024-03-11,Stereo-NEC: Enhancing Stereo Visual-Inertial SLAM Initialization with Normal Epipolar Constraints,"['Weihan Wang', 'Chieh Chou', 'Ganesh Sevagamoorthy', 'Kevin Chen', 'Zheng Chen', 'Ziyue Feng', 'Youjie Xia', 'Feiyang Cai', 'Yi Xu', 'Philippos Mordohai']","We propose an accurate and robust initialization approach for stereo visual-inertial SLAM systems. Unlike the current state-of-the-art method, which heavily relies on the accuracy of a pure visual SLAM system to estimate inertial variables without updating camera poses, potentially compromising accuracy and robustness, our approach offers a different solution. We realize the crucial impact of precise gyroscope bias estimation on rotation accuracy. This, in turn, affects trajectory accuracy due to the accumulation of translation errors. To address this, we first independently estimate the gyroscope bias and use it to formulate a maximum a posteriori problem for further refinement. After this refinement, we proceed to update the rotation estimation by performing IMU integration with gyroscope bias removed from gyroscope measurements. We then leverage robust and accurate rotation estimates to enhance translation estimation via 3-DoF bundle adjustment. Moreover, we introduce a novel approach for determining the success of the initialization by evaluating the residual of the normal epipolar constraint. Extensive evaluations on the EuRoC dataset illustrate that our method excels in accuracy and robustness. It outperforms ORB-SLAM3, the current leading stereo visual-inertial initialization method, in terms of absolute trajectory error and relative rotation error, while maintaining competitive computational speed. Notably, even with 5 keyframes for initialization, our method consistently surpasses the state-of-the-art approach using 10 keyframes in rotation accuracy."
https://arxiv.org/abs/2403.07224,2024-03-11,"Free-Floating Planets, Survivor Planets, Captured Planets and Binary Planets from Stellar Flybys","['Fangyuan Yu', 'Dong Lai']","In star clusters, close stellar encounters can strongly impact the architecture of a planetary system or even destroy it. We present a systematic study on the effects of stellar flybys on two-planet systems. When such a system (with a modest initial planet semi-major axis ratio, $a_1/a_2=0.6-0.8$) experiences stellar flybys, one or both planets can be ejected, forming free-floating planets (FFPs), captured planets (CPs) around the flyby star, and free-floating binary planets (BPs); the remaining single-surviving-planets (SSPs) can have their orbital radii and eccentricities greatly changed. Through numerical experiments, we calculate the formation fractions (or branching ratios) of FFPs, SSPs, CPs and BPs as a function of the pericenter separation of the flyby (in units of the initial planet semi-major axis), and use them to derive the analytical expressions that can be used to compute the formation rates of FFPs, SSPs, CPs and BPs in general cluster environments. We find that the production rates of FFPs and SSPs are similar, while the rate for CPs is a factor of a few smaller. The formation fraction of free-floating BPs depends strongly on $a_1/a_2$ of the initial systems and on the planet masses. For Jupiter-mass planets, the formation fraction of BPs is always less than $1\%$ (for systems with $a_1/a_2=0.8$) and typically much smaller ($\lesssim 0.2\%$ for $a_1/a_2\lesssim 0.7$). The fraction remains less than $1\%$ when considering $4M_{\rm J}$ planets. Overall, when averaging over all possible flybys, the production rate of BPs is less than $0.1\%$ of that for FFPs. We also derive the velocity distribution of FFPs produced by stellar flybys, and the orbital semi-major axis and eccentricity distributions of SSPs, CPs and (rare) free-floating BPs. These results can find applications in future studies of exotic planets and planetary systems."
https://arxiv.org/abs/2403.07223,2024-03-11,3D Uncertain Distance Field Mapping using GMM and GP,"['Qianqian Zou', 'Monika Sester']","In this study, we address the challenge of constructing continuous three-dimensional (3D) models that accurately represent uncertain surfaces, derived from noisy and incomplete LiDAR scanning data. Building upon our prior work, which utilized the Gaussian Process (GP) and Gaussian Mixture Model (GMM) for structured building models, we introduce a more generalized approach tailored for complex surfaces in urban scenes, where four-dimensional (4D) GMM Regression and GP with derivative observations are applied. A Hierarchical GMM (HGMM) is employed to optimize the number of GMM components and speed up the GMM training. With the prior map obtained from HGMM, GP inference is followed for the refinement of the final map. Our approach models the implicit surface of the geo-object and enables the inference of the regions that are not completely covered by measurements. The integration of GMM and GP yields well-calibrated uncertainty estimates alongside the surface model, enhancing both accuracy and reliability. The proposed method is evaluated on the real data collected by a mobile mapping system. Compared to the performance in mapping accuracy and uncertainty quantification of other methods such as Gaussian Process Implicit Surface map (GPIS) and log-Gaussian Process Implicit Surface map (Log-GPIS), the proposed method achieves lower RMSEs, higher log-likelihood values and fewer computational costs for the evaluated datasets."
https://arxiv.org/abs/2403.07222,2024-03-11,You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image Retrieval,"['Subhadeep Koley', 'Ayan Kumar Bhunia', 'Aneeshan Sain', 'Pinaki Nath Chowdhury', 'Tao Xiang', 'Yi-Zhe Song']","Two primary input modalities prevail in image retrieval: sketch and text. While text is widely used for inter-category retrieval tasks, sketches have been established as the sole preferred modality for fine-grained image retrieval due to their ability to capture intricate visual details. In this paper, we question the reliance on sketches alone for fine-grained image retrieval by simultaneously exploring the fine-grained representation capabilities of both sketch and text, orchestrating a duet between the two. The end result enables precise retrievals previously unattainable, allowing users to pose ever-finer queries and incorporate attributes like colour and contextual cues from text. For this purpose, we introduce a novel compositionality framework, effectively combining sketches and text using pre-trained CLIP models, while eliminating the need for extensive fine-grained textual descriptions. Last but not least, our system extends to novel applications in composite image retrieval, domain attribute transfer, and fine-grained generation, providing solutions for various real-world scenarios."
https://arxiv.org/abs/2403.07221,2024-03-11,LookupFFN: Making Transformers Compute-lite for CPU inference,"['Zhanpeng Zeng', 'Michael Davies', 'Pranav Pulijala', 'Karthikeyan Sankaralingam', 'Vikas Singh']","While GPU clusters are the de facto choice for training large deep neural network (DNN) models today, several reasons including ease of workflow, security and cost have led to efforts investigating whether CPUs may be viable for inference in routine use in many sectors of the industry. But the imbalance between the compute capabilities of GPUs and CPUs is huge. Motivated by these considerations, we study a module which is a workhorse within modern DNN architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent to which it can be made compute- (or FLOP-) lite. Specifically, we propose an alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by the recent studies of using Locality Sensitive Hashing (LSH) to approximate FFNs. Our formulation recasts most essential operations as a memory look-up, leveraging the trade-off between the two resources on any platform: compute and memory (since CPUs offer it in abundance). For RoBERTa language model pretraining, our formulation achieves similar performance compared to GEMM based FFNs, while dramatically reducing the required FLOP. Our development is complemented with a detailed hardware profiling of strategies that will maximize efficiency -- not just on contemporary hardware but on products that will be offered in the near/medium term future. Code is avaiable at \url{https://github.com/mlpen/LookupFFN}."
https://arxiv.org/abs/2403.07220,2024-03-11,ACMI: An index for exposed coal mapping using Landsat imagery,"['Zhen Yang', 'Jianyong Zhang', 'Yanchuang Zhao']","Remotely sensing the spatial distribution of exposed coal (EC) is significant for understanding the footprints of mining activities. However, widely applicable methods for the identification of EC surfaces remain inadequate because the choices of recent methods confront the diverse EC types and backgrounds. Therefore, this study proposed a new Automated Coal Mapping Index (ACMI) which was empirically formulated by an iterative process of identifying parameters that maximize the separability of EC and non-EC surfaces. The performance of ACMI was tested in six study areas worldwide with different landscape types and coal types. Based on the visual inspection, ACMI was more effective in highlighting EC surfaces and suppressing non-EC surfaces than the existing methods. Compared with the sample points obtained through direct interpretation, ACMI obtained better EC mapping results than previous methods with the F1 score and overall accuracy (OA) no less than 0.91 and 93.20% across all the selected Landsat images of the study areas, respectively. In addition, ACMI was demonstrated to have a stable optimal threshold and 0 can serve as its default threshold. The default threshold makes EC mapping using ACMI an automated process. The new index has the potential to support a variety of mining-activity-related studies, such as the identification of mining disturbances and illegal mining detection at multi-spatial-temporal scales."
https://arxiv.org/abs/2403.07219,2024-03-11,Monocular Microscope to CT Registration using Pose Estimation of the Incus for Augmented Reality Cochlear Implant Surgery,"['Yike Zhang', 'Eduardo Davalos', 'Dingjie Su', 'Ange Lou', 'Jack H. Noble']","For those experiencing severe-to-profound sensorineural hearing loss, the cochlear implant (CI) is the preferred treatment. Augmented reality (AR) aided surgery can potentially improve CI procedures and hearing outcomes. Typically, AR solutions for image-guided surgery rely on optical tracking systems to register pre-operative planning information to the display so that hidden anatomy or other important information can be overlayed and co-registered with the view of the surgical scene. In this paper, our goal is to develop a method that permits direct 2D-to-3D registration of the microscope video to the pre-operative Computed Tomography (CT) scan without the need for external tracking equipment. Our proposed solution involves using surface mapping of a portion of the incus in surgical recordings and determining the pose of this structure relative to the surgical microscope by performing pose estimation via the perspective-n-point (PnP) algorithm. This registration can then be applied to pre-operative segmentations of other anatomy-of-interest, as well as the planned electrode insertion trajectory to co-register this information for the AR display. Our results demonstrate the accuracy with an average rotation error of less than 25 degrees and a translation error of less than 2 mm, 3 mm, and 0.55% for the x, y, and z axes, respectively. Our proposed method has the potential to be applicable and generalized to other surgical procedures while only needing a monocular microscope during intra-operation."
https://arxiv.org/abs/2403.07218,2024-03-11,SoK: Can Trajectory Generation Combine Privacy and Utility?,"['Erik Buchholz', 'Alsharif Abuadbba', 'Shuo Wang', 'Surya Nepal', 'Salil S. Kanhere']","While location trajectories represent a valuable data source for analyses and location-based services, they can reveal sensitive information, such as political and religious preferences. Differentially private publication mechanisms have been proposed to allow for analyses under rigorous privacy guarantees. However, the traditional protection schemes suffer from a limiting privacy-utility trade-off and are vulnerable to correlation and reconstruction attacks. Synthetic trajectory data generation and release represent a promising alternative to protection algorithms. While initial proposals achieve remarkable utility, they fail to provide rigorous privacy guarantees. This paper proposes a framework for designing a privacy-preserving trajectory publication approach by defining five design goals, particularly stressing the importance of choosing an appropriate Unit of Privacy. Based on this framework, we briefly discuss the existing trajectory protection approaches, emphasising their shortcomings. This work focuses on the systematisation of the state-of-the-art generative models for trajectories in the context of the proposed framework. We find that no existing solution satisfies all requirements. Thus, we perform an experimental study evaluating the applicability of six sequential generative models to the trajectory domain. Finally, we conclude that a generative trajectory model providing semantic guarantees remains an open research question and propose concrete next steps for future research."
https://arxiv.org/abs/2403.07217,2024-03-11,Arrow Relations in Lattices of Integer Partitions,"[""Asma'a Almazaydeh"", 'Mike Behrisch', 'Edith Vargas-García', 'Andreas Wachtel']","We give a complete characterisation of the single and double arrow relations of the standard context $K(L_n)$ of the lattice $L_n$ of partitions of any positive integer $n$ under the dominance order, thereby addressing an open question of Ganter, 2022."
https://arxiv.org/abs/2403.07216,2024-03-11,Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control,"['Mike Timmerman', 'Aryan Patel', 'Tim Reinhart']","The paper presents a technique using reinforcement learning (RL) to adapt the control gains of a quadcopter controller. Specifically, we employed Proximal Policy Optimization (PPO) to train a policy which adapts the gains of a cascaded feedback controller in-flight. The primary goal of this controller is to minimize tracking error while following a specified trajectory. The paper's key objective is to analyze the effectiveness of the adaptive gain policy and compare it to the performance of a static gain control algorithm, where the Integral Squared Error and Integral Time Squared Error are used as metrics. The results show that the adaptive gain scheme achieves over 40$\%$ decrease in tracking error as compared to the static gain controller."
https://arxiv.org/abs/2403.07215,2024-03-11,Anomalous magnetic flux via junction twist-angle in a triplet-superconducting transmon qubit,"['Sebastián Domínguez-Calderón', 'Harley D. Scammell']","Superconducting transmon qubits with strong anharmonicity and insensitivity to offset charge are highly desirable for low-error implementation. In this work we propose a c-axis junction, comprising triplet superconductors, and set at a relative twist angle. Invoking spin-orbit coupling and spin polarization, which are known to occur in the material platform of choice, we examine the resulting transmon Hamiltonian. This junction allows for direct control of the single and double Cooper pair tunneling strength, and most remarkably, an anomalous magnetic flux -- i.e. a phase offset equivalent to magnetic flux, yet in zero magnetic field. Having control over these three parameters -- single and double pair tunneling and anomalous flux -- allows for optimal design of the transmon qubit. Interestingly, in this architecture, the anomalous flux is determined by the twist angle of the junction, thereby offering a novel zero-field functionality. Our key results rely on symmetry arguments, for concreteness we demonstrate the implementation of our concept using a model of moiré graphene-based c-axis junctions."
https://arxiv.org/abs/2403.07214,2024-03-11,Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers,"['Subhadeep Koley', 'Ayan Kumar Bhunia', 'Aneeshan Sain', 'Pinaki Nath Chowdhury', 'Tao Xiang', 'Yi-Zhe Song']","This paper, for the first time, explores text-to-image diffusion models for Zero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal discovery: the capacity of text-to-image diffusion models to seamlessly bridge the gap between sketches and photos. This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies. In order to harness pre-trained diffusion models effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual prompts. For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained). Then we employ visual and textual prompts to guide the model's feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations. Extensive experiments on several benchmark datasets validate significant performance improvements."
https://arxiv.org/abs/2403.07213,2024-03-11,Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits,"['Yu Xia', 'Fang Kong', 'Tong Yu', 'Liya Guo', 'Ryan A. Rossi', 'Sungchul Kim', 'Shuai Li']","Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs. Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs. Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend in model performances as the model is iteratively finetuned, leading to less accurate predictions and suboptimal model selections."
https://arxiv.org/abs/2403.07212,2024-03-11,On convex comparison for exterior Bernoulli problems with discontinuous anisotropy,"['William M Feldman', 'Norbert Pozar']",We give a new proof of a convex comparison principle for exterior Bernoulli free boundary problems with discontinuous anisotropy.
https://arxiv.org/abs/2403.07211,2024-03-11,Schauder-type estimates for fully nonlinear degenerate elliptic equations,['Thialita M. Nascimento'],"In this paper, we examine regularity estimates for solutions to fully nonlinear, degenerated elliptic equations, at interior vanishing source points. At these points, we obtain Schauder-type regularity estimates, which depend on the Hölder-like source-ellipticity vanishing rate."
https://arxiv.org/abs/2403.07210,2024-03-11,Evaluation of Eye Tracking Signal Quality for Virtual Reality Applications: A Case Study in the Meta Quest Pro,"['Samantha Aziz', 'Dillon J Lohr', 'Lee Friedman', 'Oleg Komogortsev']","We present an extensive, in-depth analysis of the eye tracking capabilities of the Meta Quest Pro virtual reality headset using a dataset of eye movement recordings collected from 78 participants. In addition to presenting classical signal quality metrics--spatial accuracy, spatial precision and linearity--in ideal settings, we also study the impact of background luminance and headset slippage on device performance. We additionally present a user-centered analysis of eye tracking signal quality, where we highlight the potential differences in user experience as a function of device performance. This work contributes to a growing understanding of eye tracking signal quality in virtual reality headsets, where the performance of applications such as gaze-based interaction, foveated rendering, and social gaze are directly dependent on the quality of eye tracking signal."
https://arxiv.org/abs/2403.07209,2024-03-11,The entropic doubling constant and robustness of Gaussian codebooks for additive-noise channels,"['Lampros Gavalakis', 'Ioannis Kontoyiannis', 'Mokshay Madiman']","Entropy comparison inequalities are obtained for the differential entropy $h(X+Y)$ of the sum of two independent random vectors $X,Y$, when one is replaced by a Gaussian. For identically distributed random vectors $X,Y$, these are closely related to bounds on the entropic doubling constant, which quantifies the entropy increase when adding an independent copy of a random vector to itself. Consequences of both large and small doubling are explored. For the former, lower bounds are deduced on the entropy increase when adding an independent Gaussian, while for the latter, a qualitative stability result for the entropy power inequality is obtained. In the more general case of non-identically distributed random vectors $X,Y$, a Gaussian comparison inequality with interesting implications for channel coding is established: For additive-noise channels with a power constraint, Gaussian codebooks come within a $\frac{\sf snr}{3{\sf snr}+2}$ factor of capacity. In the low-SNR regime this improves the half-a-bit additive bound of Zamir and Erez (2004). Analogous results are obtained for additive-noise multiple access channels, and for linear, additive-noise MIMO channels."
https://arxiv.org/abs/2403.07208,2024-03-11,Exploring iterative and non-iterative Fourier series-based methods of control optimization in application to a discontinuous capsule drive model,"['Sandra Zarychta', 'Marek Balcerzak', 'Jerzy Wojewoda']","The paper explains iterative and non-iterative approaches to control optimization with use of the Fourier series-based method. Both variants of the presented algorithm are used to numerically approximate optimal control of a discontinuous pendulum capsule drive. Firstly, the general algorithm and its two realizations (iterative and non-iterative) are presented. It is shown that the iterative variant assures non-decreasing quality of solutions in subsequent repetitions of the procedure and the background of such guarantees is explained. A numerical example follows: control of a self-propelled capsule drive is optimized using both approaches. Results are compared and discussed. It is expected that the presented methods can be useful in optimal control estimation for complex systems, particularly discontinuous ones."
https://arxiv.org/abs/2403.07207,2024-03-11,Tracking Dynamic Gaussian Density with a Theoretically Optimal Sliding Window Approach,"['Yinsong Wang', 'Yu Ding', 'Shahin Shahrampour']","Dynamic density estimation is ubiquitous in many applications, including computer vision and signal processing. One popular method to tackle this problem is the ""sliding window"" kernel density estimator. There exist various implementations of this method that use heuristically defined weight sequences for the observed data. The weight sequence, however, is a key aspect of the estimator affecting the tracking performance significantly. In this work, we study the exact mean integrated squared error (MISE) of ""sliding window"" Gaussian Kernel Density Estimators for evolving Gaussian densities. We provide a principled guide for choosing the optimal weight sequence by theoretically characterizing the exact MISE, which can be formulated as constrained quadratic programming. We present empirical evidence with synthetic datasets to show that our weighting scheme indeed improves the tracking performance compared to heuristic approaches."
https://arxiv.org/abs/2403.07206,2024-03-11,Non-Standard Version of Egorov Algebra of Generalized Functions,['Todor D. Todorov'],"We consider a non-standard version of Egorov's algebra of generalized functions, with improved properties of the generalized scalars and embedding of the Schwartz distributions compared with the original standard Egorov's version. The embedding of distributions is similar to, but different from author's works in the past and independently done by Hans Vernaeve."
https://arxiv.org/abs/2403.07205,2024-03-11,Asmptotic properties of the Stokes flow in an exterior domain with slowly decaying initial data and its application to the Navier-Stokes equations,"['Tongkeun Chang', 'Bum Ja Jin']","In this paper, we study the decay rate of the Stokes flow in an exterior domain with a slowly decaying initial data ${\bf u}_0(x)=O(|x|^{-\al}), 0<\al\leq n$. %which is not $L^1$ integrable. As an application we find the unique strong solution of the Navier-Stokes equations corresponding to a slowly decaying initial data. We also derive the pointwise decay estimate of the Navier-Stokes flow. Our decay rates will be optimal compared with the decay rates of the heat flow."
https://arxiv.org/abs/2403.07204,2024-03-11,Crystal Chute Moves on Pipe Dreams,"['Sarah Gold', 'Elizabeth Milićević', 'Yuxuan Sun']","Schubert polynomials represent a basis for the cohomology of the complete flag variety and thus play a central role in geometry and combinatorics. In this context, Schubert polynomials are generating functions over various combinatorial objects, such as rc-graphs or reduced pipe dreams. By restricting Bergeron and Billey's chute moves on rc-graphs, we define a Demazure crystal structure on the monomials of a Schubert polynomial. As a consequence, we provide a method for decomposing Schubert polynomials as sums of key polynomials, complementing related work of Assaf and Schilling via reduced factorizations with cutoff, as well as Lenart's coplactic operators on biwords."
https://arxiv.org/abs/2403.07203,2024-03-11,How to Handle Sketch-Abstraction in Sketch-Based Image Retrieval?,"['Subhadeep Koley', 'Ayan Kumar Bhunia', 'Aneeshan Sain', 'Pinaki Nath Chowdhury', 'Tao Xiang', 'Yi-Zhe Song']","In this paper, we propose a novel abstraction-aware sketch-based image retrieval framework capable of handling sketch abstraction at varied levels. Prior works had mainly focused on tackling sub-factors such as drawing style and order, we instead attempt to model abstraction as a whole, and propose feature-level and retrieval granularity-level designs so that the system builds into its DNA the necessary means to interpret abstraction. On learning abstraction-aware features, we for the first-time harness the rich semantic embedding of pre-trained StyleGAN model, together with a novel abstraction-level mapper that deciphers the level of abstraction and dynamically selects appropriate dimensions in the feature matrix correspondingly, to construct a feature matrix embedding that can be freely traversed to accommodate different levels of abstraction. For granularity-level abstraction understanding, we dictate that the retrieval model should not treat all abstraction-levels equally and introduce a differentiable surrogate Acc.@q loss to inject that understanding into the system. Different to the gold-standard triplet loss, our Acc.@q loss uniquely allows a sketch to narrow/broaden its focus in terms of how stringent the evaluation should be - the more abstract a sketch, the less stringent (higher $q$). Extensive experiments depict our method to outperform existing state-of-the-arts in standard SBIR tasks along with challenging scenarios like early retrieval, forensic sketch-photo matching, and style-invariant retrieval."
https://arxiv.org/abs/2403.07202,2024-03-11,SPAWNing Structural Priming Predictions from a Cognitively Motivated Parser,"['Grusha Prasad', 'Tal Linzen']","Structural priming is a widely used psycholinguistic paradigm to study human sentence representations. In this work we propose a framework for using empirical priming patterns to build a theory characterizing the structural representations humans construct when processing sentences. This framework uses a new cognitively motivated parser, SPAWN, to generate quantitative priming predictions from theoretical syntax and evaluate these predictions with empirical human behavior. As a case study, we apply this framework to study reduced relative clause representations in English. We use SPAWN to generate priming predictions from two theoretical accounts which make different assumptions about the structure of relative clauses. We find that the predictions from only one of these theories (Participial-Phase) align with empirical priming patterns, thus highlighting which assumptions about relative clause better capture human sentence representations."
https://arxiv.org/abs/2403.07201,2024-03-11,A multi-cohort study on prediction of acute brain dysfunction states using selective state space models,"['Brandon Silva', 'Miguel Contreras', 'Sabyasachi Bandyopadhyay', 'Yuanfang Ren', 'Ziyuan Guan', 'Jeremy Balch', 'Kia Khezeli', 'Tezcan Ozrazgat Baslanti', 'Ben Shickel', 'Azra Bihorac', 'Parisa Rashidi']","Assessing acute brain dysfunction (ABD), including delirium and coma in the intensive care unit (ICU), is a critical challenge due to its prevalence and severe implications for patient outcomes. Current diagnostic methods rely on infrequent clinical observations, which can only determine a patient's ABD status after onset. Our research attempts to solve these problems by harnessing Electronic Health Records (EHR) data to develop automated methods for ABD prediction for patients in the ICU. Existing models solely predict a single state (e.g., either delirium or coma), require at least 24 hours of observation data to make predictions, do not dynamically predict fluctuating ABD conditions during ICU stay (typically a one-time prediction), and use small sample size, proprietary single-hospital datasets. Our research fills these gaps in the existing literature by dynamically predicting delirium, coma, and mortality for 12-hour intervals throughout an ICU stay and validating on two public datasets. Our research also introduces the concept of dynamically predicting critical transitions from non-ABD to ABD and between different ABD states in real time, which could be clinically more informative for the hospital staff. We compared the predictive performance of two state-of-the-art neural network models, the MAMBA selective state space model and the Longformer Transformer model. Using the MAMBA model, we achieved a mean area under the receiving operator characteristic curve (AUROC) of 0.95 on outcome prediction of ABD for 12-hour intervals. The model achieves a mean AUROC of 0.79 when predicting transitions between ABD states. Our study uses a curated dataset from the University of Florida Health Shands Hospital for internal validation and two publicly available datasets, MIMIC-IV and eICU, for external validation, demonstrating robustness across ICU stays from 203 hospitals and 140,945 patients."
https://arxiv.org/abs/2403.07200,2024-03-11,Computing $p$-presentation distances is hard,"['Håvard Bakke Bjerkevik', 'Magnus Bakke Botnan']","Recently, $p$-presentation distances for $p\in [1,\infty]$ were introduced for merge trees and multiparameter persistence modules as more sensitive variations of the respective interleaving distances ($p=\infty$). It is well-known that computing the interleaving distance is NP-hard in both cases. We extend this result by showing that computing the $p$-presentation distance is NP-hard for all $p\in [1,\infty)$ for both merge trees and $t$-parameter persistence modules for any $t\geq 2$. Though the details differ, both proofs follow the same novel strategy, suggesting that our approach can be adapted to proving the NP-hardness of other distances based on sums or $p$-norms."
https://arxiv.org/abs/2403.07199,2024-03-11,iRoCo: Intuitive Robot Control From Anywhere Using a Smartwatch,"['Fabian C Weigend', 'Xiao Liu', 'Shubham Sonawani', 'Neelesh Kumar', 'Venugopal Vasudevan', 'Heni Ben Amor']","This paper introduces iRoCo (intuitive Robot Control) - a framework for ubiquitous human-robot collaboration using a single smartwatch and smartphone. By integrating probabilistic differentiable filters, iRoCo optimizes a combination of precise robot control and unrestricted user movement from ubiquitous devices. We demonstrate and evaluate the effectiveness of iRoCo in practical teleoperation and drone piloting applications. Comparative analysis shows no significant difference between task performance with iRoCo and gold-standard control systems in teleoperation tasks. Additionally, iRoCo users complete drone piloting tasks 32\% faster than with a traditional remote control and report less frustration in a subjective load index questionnaire. Our findings strongly suggest that iRoCo is a promising new approach for intuitive robot control through smartwatches and smartphones from anywhere, at any time. The code is available at www.github.com/wearable-motion-capture"
https://arxiv.org/abs/2403.07198,2024-03-11,Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions,"['Lan Wang', 'Vishnu Boddeti', 'Sernam Lim']","We introduce a novel text-to-pose video editing method, ReimaginedAct. While existing video editing tasks are limited to changes in attributes, backgrounds, and styles, our method aims to predict open-ended human action changes in video. Moreover, our method can accept not only direct instructional text prompts but also `what if' questions to predict possible action changes. ReimaginedAct comprises video understanding, reasoning, and editing modules. First, an LLM is utilized initially to obtain a plausible answer for the instruction or question, which is then used for (1) prompting Grounded-SAM to produce bounding boxes of relevant individuals and (2) retrieving a set of pose videos that we have collected for editing human actions. The retrieved pose videos and the detected individuals are then utilized to alter the poses extracted from the original video. We also employ a timestep blending module to ensure the edited video retains its original content except where necessary modifications are needed. To facilitate research in text-to-pose video editing, we introduce a new evaluation dataset, WhatifVideo-1.0. This dataset includes videos of different scenarios spanning a range of difficulty levels, along with questions and text prompts. Experimental results demonstrate that existing video editing methods struggle with human action editing, while our approach can achieve effective action editing and even imaginary editing from counterfactual questions."
https://arxiv.org/abs/2403.07197,2024-03-11,Simulating Quantum Circuits by Model Counting,"['Jingyi Mei', 'Marcello Bonsangue', 'Alfons Laarman']","Quantum circuit compilation comprises many computationally hard reasoning tasks that nonetheless lie inside #$\mathbf{P}$ and its decision counterpart in $\mathbf{PP}$. The classical simulation of general quantum circuits is a core example. We show for the first time that a strong simulation of universal quantum circuits can be efficiently tackled through weighted model counting by providing a linear encoding of Clifford+T circuits. To achieve this, we exploit the stabilizer formalism by Knill, Gottesmann, and Aaronson and the fact that stabilizer states form a basis for density operators. With an open-source simulator implementation, we demonstrate empirically that model counting often outperforms state-of-the-art simulation techniques based on the ZX calculus and decision diagrams. Our work paves the way to apply the existing array of powerful classical reasoning tools to realize efficient quantum circuit compilation; one of the obstacles on the road towards quantum supremacy."
https://arxiv.org/abs/2403.07196,2024-03-11,Cold Quasar Investigation: Comparing Star Formation Rates to Black Hole Growth,"['Sasha Mintz', 'Brandon Coleman', 'Allison Kirkpatrick']","Cold quasars are a rare population of luminous, unobscured quasars associated with host galaxies that have a high star formation rate. We aimed to study the host galaxies of sixty four of these cold quasars in order to probe how the supermassive black holes and host galaxies were coevolving. We compiled data from the XXL survey and crossmatched with the VHS, WISE, and HerMES surveys to obtain multiwavelength photometry spanning the Xray to the infrared and including optical spectroscopy. From the data, we calculated the supermassive black hole masses using broad emission from the magnesium II and hydrogen beta lines. We compared this with the stellar mass of the entire galaxy and find that the black holes are significantly more massive than would be predicted by local relations, indicating that the majority of black hole growth precedes the bulk of the the stellar mass formation. In addition to this, we created a spectral energy distribution for each galaxy to calculate the star formation rate. We compared the star formation rate with the black hole accretion rate and find that the stellar mass is rapidly increasing at a relative rate faster than the black hole growth, supporting the picture where the black hole grows first."
https://arxiv.org/abs/2403.07195,2024-03-11,A symplectic Hilbert-Smith conjecture,['Egor Shelukhin'],"We prove new cases of the Hilbert-Smith conjecture for actions by natural homeomorphisms in symplectic topology. Specifically, we prove that the group of $p$-adic integers $\mathbb Z_p$ does not admit non-trivial continuous actions by Hamiltonian homeomorphisms, the $C^0$ limits of Hamiltonian diffeomorphisms, on symplectically aspherical symplectic manifolds. For a class of symplectic manifolds, including all standard symplectic tori, we deduce that a locally compact group acting faithfully by homeomorphisms in the $C^0$ closure of time-one maps of symplectic isotopies must be a Lie group. Our methods of proof differ from prior approaches to the question and involve barcodes and power operations in Floer cohomology. They also apply to other natural metrics in symplectic topology, notably Hofer's metric. An appendix by Leonid Polterovich uses this to deduce obstructions on Hamiltonian actions by semi-simple $p$-adic analytic groups."
https://arxiv.org/abs/2403.07193,2024-03-11,"CuentosIE: can a chatbot about ""tales with a message"" help to teach emotional intelligence?","['Antonio Ferrández', 'Rocío Lavigne-Cerván', 'Jesús Peral', 'Ignasi Navarro-Soria', 'Ángel Lloret', 'David Gil', 'Carmen Rocamora']","In this article, we present CuentosIE (TalesEI: chatbot of tales with a message to develop Emotional Intelligence), an educational chatbot on emotions that also provides teachers and psychologists with a tool to monitor their students/patients through indicators and data compiled by CuentosIE. The use of ""tales with a message"" is justified by their simplicity and easy understanding, thanks to their moral or associated metaphors. The main contributions of CuentosIE are the selection, collection, and classification of a set of highly specialized tales, as well as the provision of tools (searching, reading comprehension, chatting, recommending, and classifying) that are useful for both educating users about emotions and monitoring their emotional development. The preliminary evaluation of the tool has obtained encouraging results, which provides an affirmative answer to the question posed in the title of the article."
https://arxiv.org/abs/2403.07192,2024-03-11,Accelerating Interface Adaptation with User-Friendly Priors,"['Benjamin A. Christie', 'Heramb Nemlekar', 'Dylan P. Losey']","Robots often need to convey information to human users. For example, robots can leverage visual, auditory, and haptic interfaces to display their intent or express their internal state. In some scenarios there are socially agreed upon conventions for what these signals mean: e.g., a red light indicates an autonomous car is slowing down. But as robots develop new capabilities and seek to convey more complex data, the meaning behind their signals is not always mutually understood: one user might think a flashing light indicates the autonomous car is an aggressive driver, while another user might think the same signal means the autonomous car is defensive. In this paper we enable robots to adapt their interfaces to the current user so that the human's personalized interpretation is aligned with the robot's meaning. We start with an information theoretic end-to-end approach, which automatically tunes the interface policy to optimize the correlation between human and robot. But to ensure that this learning policy is intuitive -- and to accelerate how quickly the interface adapts to the human -- we recognize that humans have priors over how interfaces should function. For instance, humans expect interface signals to be proportional and convex. Our approach biases the robot's interface towards these priors, resulting in signals that are adapted to the current user while still following social expectations. Our simulations and user study results across $15$ participants suggest that these priors improve robot-to-human communication. See videos here: https://youtu.be/Re3OLg57hp8"
https://arxiv.org/abs/2403.07191,2024-03-11,"$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model","['Yufeng Zhang', 'Liyu Chen', 'Boyi Liu', 'Yingxiang Yang', 'Qiwen Cui', 'Yunzhe Tao', 'Hongxia Yang']","Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms. To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers. We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO)."
https://arxiv.org/abs/2403.07190,2024-03-11,Exploring gender differences in the Force Concept Inventory using a random effects meta-analysis of international studies,"['Purwoko Haryadi Santoso', 'Bayu Setiaji', ' Wahyudi', 'Johan Syahbrudin', 'Syamsul Bahri', ' Fathurrahman', 'A. Suci Rizky Ananda', 'Yusuf Sodhiqin']","The force concept inventory (FCI) is one of the research-based assessments (RBAs) established by the physics education research (PER) community to measure students' understanding of Newtonian mechanics. Former works have often recorded the notion of gendered mean FCI scores favoring male students notably in the North America (NA) based studies. Nevertheless, these performance gaps remain inconclusive and unexplored outside the NA context. This paper aims to fill this gap by meta-analyzing the mean FCI scores between gender based on the existing PER literature beyond the NA context. We analyzed the magnitude and direction on the mean FCI scores between gender on the basis of primary international studies published over the last two decades. We also explored the moderating impact of international study characteristics on the meta-analytic findings by performing a subgroup analysis to study the different study regions stratified by two subgroups (NA vs non-NA authors). Thirty-eight studies reporting the mean FCI scores by gender were included in the present meta-analysis. We employed Hedges' g statistic to estimate to what degree the mean FCI scores may be different between male and female students on each study. Under a random effects model, we meta-analyzed the findings and conducted a subgroup analysis to answer the research questions. In summary, our meta-analysis indicated a significantly positive and moderate amount of gendered mean FCI scores in favor of male students both in NA- and non-NA based regions, and the performance gaps were wider in the NA-based studies. Suggestions are discussed for promoting gender fairness in the FCI when interpreting its scores for teaching, learning, and forthcoming studies."
https://arxiv.org/abs/2403.07189,2024-03-11,A multiscale cavity method for sublinear-rank symmetric matrix factorization,"['Jean Barbier', 'Justin Ko', 'Anas A. Rahman']","We consider a statistical model for symmetric matrix factorization with additive Gaussian noise in the high-dimensional regime where the rank $M$ of the signal matrix to infer scales with its size $N$ as $M = o(N^{1/10})$. Allowing for a $N$-dependent rank offers new challenges and requires new methods. Working in the Bayesian-optimal setting, we show that whenever the signal has i.i.d. entries the limiting mutual information between signal and data is given by a variational formula involving a rank-one replica symmetric potential. In other words, from the information-theoretic perspective, the case of a (slowly) growing rank is the same as when $M = 1$ (namely, the standard spiked Wigner model). The proof is primarily based on a novel multiscale cavity method allowing for growing rank along with some information-theoretic identities on worst noise for the Gaussian vector channel. We believe that the cavity method developed here will play a role in the analysis of a broader class of inference and spin models where the degrees of freedom are large arrays instead of vectors."
https://arxiv.org/abs/2403.07188,2024-03-11,Spin-orbit coupling in symmetric and mixed spin-symmetry,"['Ayaka Usui', 'Abel Rojo-Francàs', 'James Schloss', 'Bruno Juliá-Díaz']","Synthetically spin-orbit coupling in cold atoms couples the pseudo-spin and spatial degrees of freedom, and therefore the inherent spin symmetry of the system plays an important role. In systems of two pseudo-spin degrees, two particles configure symmetric states and anti-symmetric states, but the spin symmetry can be mixed for more particles. We study the role of mixed spin symmetry in the presence of spin-orbit coupling and consider the system of three bosons with two hyper-fine states trapped in a harmonic potential. We investigate the ground state and the energy spectrum by implementing exact diagonalization. It is found that the interplay between spin-orbit coupling and repulsive interactions between anti-aligned pseudo-spins increases the population of the unaligned spin components in the ground state. The emergence of the mixed spin symmetric states compensates for the rise of the interaction energy. With the aligned interaction on, the avoided crossing between the ground state and the first excited state is observed only for small interaction, and this causes shape changes in the spin populations. Furthermore, we find that the pair correlation of the ground state shows similarly to that of Tonks-Girardeau gas even for relatively small contact interactions and such strong interaction feature is enhanced by the spin-orbit coupling."
https://arxiv.org/abs/2403.07187,2024-03-11,UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation,"['Junhong Shen', 'Tanya Marwah', 'Ameet Talwalkar']","We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of few-shot transfer to different PDE families, coefficients, and resolutions."
https://arxiv.org/abs/2403.07186,2024-03-11,Spin-orbit correlations in the nucleon in the large-$N_{c}$ limit,"['June-Young Kim', 'Ho-Yeon Won', 'Hyun-Chul Kim', 'Christian Weiss']","We study the twist-3 spin-orbit correlations of quarks described by the nucleon matrix elements of the parity-odd rank-2 tensor QCD operator (the parity-odd partner of the QCD energy-momentum tensor). Our treatment is based on the effective dynamics emerging from the spontaneous breaking of chiral symmetry and the mean-field picture of the nucleon in the large-$N_c$ limit. The twist-3 QCD operators are converted to effective operators, in which the QCD interactions are replaced by spin-flavor-dependent chiral interactions of the quarks with the pion field. We compute the nucleon matrix elements of the twist-3 effective operators and discuss the role of the chiral interactions in the spin-orbit correlations. We derive the first-quantized representation in the mean-field picture and develop a quantum-mechanical interpretation. The chiral interactions give rise to new spin-orbit couplings and qualitatively change the correlations compared to the quark model picture. We also derive the twist-3 matrix elements in the topological soliton picture where the quarks are integrated out (skyrmion). The methods used here can be extended to other QCD operators describing higher-twist nucleon structure and generalized parton distributions."
https://arxiv.org/abs/2403.07185,2024-03-11,Uncertainty in Graph Neural Networks: A Survey,"['Fangxin Wang', 'Yuqing Liu', 'Kay Liu', 'Yibo Wang', 'Sourav Medya', 'Philip S. Yu']","Graph Neural Networks (GNNs) have been extensively used in various real-world applications. However, the predictive uncertainty of GNNs stemming from diverse sources such as inherent randomness in data and model training errors can lead to unstable and erroneous predictions. Therefore, identifying, quantifying, and utilizing uncertainty are essential to enhance the performance of the model for the downstream tasks as well as the reliability of the GNN predictions. This survey aims to provide a comprehensive overview of the GNNs from the perspective of uncertainty with an emphasis on its integration in graph learning. We compare and summarize existing graph uncertainty theory and methods, alongside the corresponding downstream tasks. Thereby, we bridge the gap between theory and practice, meanwhile connecting different GNN communities. Moreover, our work provides valuable insights into promising directions in this field."
https://arxiv.org/abs/2403.07184,2024-03-11,Timepix3: single-pixel multi-hit energy-measurement behavior,"['Hubertus Bromberger', 'David Pennicard', 'Rafael Ballabriga', 'Sebastian Trippel', 'Jochen Küpper']","The event-driven hybrid-pixel detector readout chip, Timepix3, has the ability to simultaneously measure the time of an event on the nanosecond timescale and the energy deposited in the sensor. However, the behaviour of the system when two events are recorded in quick succession of each other on the same pixel was not studied in detail previously. We present experimental measurements, circuit simulations, and an empirical model for the impact of a preceding event on this energy measurements, which can result in a loss as high as 70~\%. Accounting for this effect enables more precise compensation, particularly for phenomena like timewalk. This results in significant improvements in time resolution -- in the best case, multiple tens of nanoseconds -- when two events happen in rapid succession."
https://arxiv.org/abs/2403.07183,2024-03-11,Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews,"['Weixin Liang', 'Zachary Izzo', 'Yaohui Zhang', 'Haley Lepp', 'Hancheng Cao', 'Xuandong Zhao', 'Lingjiao Chen', 'Haotian Ye', 'Sheng Liu', 'Zhi Huang', 'Daniel A. McFarland', 'James Y. Zou']","We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices."
https://arxiv.org/abs/2403.07182,2024-03-11,MAP-Elites with Transverse Assessment for Multimodal Problems in Creative Domains,"['Marvin Zammit', 'Antonios Liapis', 'Georgios N. Yannakakis']","The recent advances in language-based generative models have paved the way for the orchestration of multiple generators of different artefact types (text, image, audio, etc.) into one system. Presently, many open-source pre-trained models combine text with other modalities, thus enabling shared vector embeddings to be compared across different generators. Within this context we propose a novel approach to handle multimodal creative tasks using Quality Diversity evolution. Our contribution is a variation of the MAP-Elites algorithm, MAP-Elites with Transverse Assessment (MEliTA), which is tailored for multimodal creative tasks and leverages deep learned models that assess coherence across modalities. MEliTA decouples the artefacts' modalities and promotes cross-pollination between elites. As a test bed for this algorithm, we generate text descriptions and cover images for a hypothetical video game and assign each artefact a unique modality-specific behavioural characteristic. Results indicate that MEliTA can improve text-to-image mappings within the solution space, compared to a baseline MAP-Elites algorithm that strictly treats each image-text pair as one solution. Our approach represents a significant step forward in multimodal bottom-up orchestration and lays the groundwork for more complex systems coordinating multimodal creative agents in the future."
https://arxiv.org/abs/2403.07181,2024-03-11,Zig-zag Eulerian polynomials,"['T. Kyle Petersen', 'Yan Zhuang']","For any finite partially ordered set $P$, the $P$-Eulerian polynomial is the generating function for the descent number over the set of linear extensions of $P$, and is closely related to the order polynomial of $P$ arising in the theory of $P$-partitions. Here we study the $P$-Eulerian polynomial where $P$ is a naturally labeled zig-zag poset; we call these $\textit{zig-zag Eulerian polynomials}$. A result of Brändén implies that these polynomials are gamma-nonnegative, and hence their coefficients are symmetric and unimodal. The zig-zag Eulerian polynomials and the associated order polynomials have appeared fleetingly in the literature in a wide variety of contexts$\unicode{x2014}$e.g., in the study of polytopes, magic labelings of graphs, and Kekulé structures$\unicode{x2014}$but they do not appear to have been studied systematically."
https://arxiv.org/abs/2403.07180,2024-03-11,Study of the Impact of the Big Data Era on Accounting and Auditing,"['Yuxiang Sun', 'Jingyi Li', 'Mengdie Lu', 'Zongying Guo']","Big data revolutionizes accounting and auditing, offering deep insights but also introducing challenges like data privacy and security. With data from IoT, social media, and transactions, traditional practices are evolving. Professionals must adapt to these changes, utilizing AI and machine learning for efficient data analysis and anomaly detection. Key to overcoming these challenges are enhanced analytics tools, continuous learning, and industry collaboration. By addressing these areas, the accounting and auditing fields can harness big data's potential while ensuring accuracy, transparency, and integrity in financial reporting. Keywords: Big Data, Accounting, Audit, Data Privacy, AI, Machine Learning, Transparency."
https://arxiv.org/abs/2403.07179,2024-03-11,3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs,"['Huaisheng Zhu', 'Teng Xiao', 'Vasant G Honavar']","Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the latent molecular graph space using a diffusion model. The results of our extensive experiments on several datasets demonstrate that 3M-Diffusion can generate high-quality, novel and diverse molecular graphs that semantically match the textual description provided."
https://arxiv.org/abs/2403.07178,2024-03-11,Phase transitions and minimal interfaces on manifolds with conical singularities,"['Daniel Grieser', 'Sina Held', 'Hannes Uecker', 'Boris Vertman']","Using $Γ$-convergence, we study the Cahn-Hilliard problem with interface width parameter $\varepsilon > 0$ for phase transitions on manifolds with conical singularities. We prove that minimizers of the corresponding energy functional exist and converge, as $\varepsilon \to 0$, to a function that takes only two values with an interface along a hypersurface that has minimal area among those satisfying a volume constraint. In a numerical example, we use continuation and bifurcation methods to study families of critical points at small $\varepsilon > 0$ on 2D elliptical cones, parameterized by height and ellipticity of the base. Some of these critical points are minimizers with interfaces crossing the cone tip. On the other hand, we prove that interfaces which are minimizers of the perimeter functional, corresponding to $\varepsilon = 0$, never pass through the cone tip for general cones with angle less than $2π$. Thus tip minimizers for finite $\varepsilon > 0$ must become saddles as $\varepsilon \to 0$, and we numerically identify the associated bifurcation, finding a delicate interplay of $\varepsilon > 0$ and the cone parameters in our example."
https://arxiv.org/abs/2403.07177,2024-03-11,Collusive Outcomes Without Collusion,"['Inkoo Cho', 'Noah Williams']","We develop a model of algorithmic pricing that shuts down every channel for explicit or implicit collusion while still generating collusive outcomes. We analyze the dynamics of a duopoly market where both firms use pricing algorithms consisting of a parameterized family of model specifications. The firms update both the parameters and the weights on models to adapt endogenously to market outcomes. We show that the market experiences recurrent episodes where both firms set prices at collusive levels. We analytically characterize the dynamics of the model, using large deviation theory to explain the recurrent episodes of collusive outcomes. Our results show that collusive outcomes may be a recurrent feature of algorithmic environments with complementarities and endogenous adaptation, providing a challenge for competition policy."
https://arxiv.org/abs/2403.07176,2024-03-11,Pressure effects on the electronic structure and magnetic properties of infinite-layer nickelates,"['Shekhar Sharma', 'Myung-Chul Jung', 'Harrison LaBollita', 'Antia S. Botana']","Motivated by the discovery of superconductivity in infinite-layer nickelates RNiO$_2$ (R= rare-earth), and the subsequent enhancement of their T$_c$ with pressure, we investigate the evolution of the electronic structure and magnetic properties of this family of materials via first-principles calculations employing hydrostatic and chemical pressure as tuning knobs. Overall, our analysis shows that pressure tends to increase the R-$5d$ self-doping effect, as well as the Ni-$d _{x^{2}-y^{2}}$ bandwidth, the $e_g$ energy splitting, the charge transfer energy, and the superexchange ($J$). Using the energy scale of $J$ as a predictor of superconducting tendencies, we anticipate that pressure can indeed be a feasible means to further increase the T$_c$ in this family of materials."
https://arxiv.org/abs/2403.07175,2024-03-11,Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing,"['Akshat Gupta', 'Gopala Anumanchipalli']","Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME."
https://arxiv.org/abs/2403.07174,2024-03-11,Exploring the Impact of Extra Dimensions on Neutron Star Structure and Equation of State,"['Debabrata Deb', 'Manjari Bagchi', 'Sarmistha Banik']","In this work, we explore the impact of higher dimensional spacetime on the stellar structure and thermodynamic properties of neutron stars. Utilizing the density-dependent relativistic hadron field theory, we introduce modifications to incorporate the influence of higher dimensionality, a novel approach not explored in existing literature to our best knowledge. Our methodology involves solving the essential stellar structure equations in D-dimensional spacetime ($D \geq 4$), starting with the modification of the Einstein-Hilbert action, derivation of the Einstein field equation in D dimensions, and application of the resulting exterior Schwarzschild spacetime metric for D-dimension. Our findings reveal that with incremental dimensions, the central density $ρ_{c} G_D$ and central pressure $p_c G_D$ gradually increase, leading to progressively stiffer neutron matter. Incremental dimensionality also results in a gradual increase in the maximum mass attained, limited to our study between $D=4$ and $D=6$, as no maximum mass value is obtained for $D>6$. We consistently observe the criteria $dM/dρ_c>0$ fulfilled up to the maximum mass point, supported by stability analysis against infinitesimal radial pulsations. The validity of our solution is confirmed through causality conditions, ensuring that the matter sound speed remains within the speed of light for all cases. Additionally, our examination indicates that the total mass-to-radius ratio for all discussed D-dimensional cases comfortably resides within the modified Buchdahl limit, which exhibits the physical validity of achieved results."
https://arxiv.org/abs/2403.07173,2024-03-11,Mixed virtual element approximation for the five-field formulation of the steady Boussinesq problem with temperature-dependent parameters,['Zeinab Gharibi'],"In this work, we develop recent research on the fully mixed virtual element method (mixed-VEM) based on the Banach space for the stationary Boussinesq equation to suggest and analyze a new mixed-VEM for the stationary two-dimensional Boussinesq equation with temperature-dependent parameters in terms of the pseudostress, vorticity, velocity, pseudoheat vector and temperature fields. The well-posedness of the continuous formulation is analyzed utilizing a fixed-point strategy, a smallness assumption on the data, and some additional regularities on the solution. The discretization for the mentioned variables is based on the coupling $\mathbb{H}(\mathbf{div}_{6/5})$ -- and $\mathbf{H}(\mathrm{div}_{6/5})$ -- conforming virtual element techniques. The proposed scheme is rewritten as an equivalent fixed point operator equation, so that its existence and stability estimates have been proven. In addition, an a priori convergence analysis is established by utilizing the Céa estimate and a suitable assumption on data for all variables in their natural norms showing an optimal rate of convergence. Finally, several numerical examples are presented to illustrate the performance of the proposed method."
https://arxiv.org/abs/2403.07172,2024-03-11,To Be or not to Be: the role of rotation in modeling Galactic Be X-ray Binaries,"['Kyle Akira Rocha', 'Vicky Kalogera', 'Zoheyr Doctor', 'Jeff J. Andrews', 'Meng Sun', 'Seth Gossage', 'Simone S. Bavera', 'Tassos Fragos', 'Konstantinos Kovlakas', 'Matthias U. Kruckow', 'Devina Misra', 'Philipp M. Srivastava', 'Zepei Xing', 'Emmanouil Zapartas']","Be X-ray binaries (Be-XRBs) are crucial in understanding high-mass X-ray binaries, featuring a rapidly rotating Be star and a neutron star companion in an eccentric orbit, intermittently accreting material from the Be star's decretion disk. Originating from binary stellar evolution, Be-XRBs are of significant interest to binary population synthesis (BPS) studies, encapsulating the physics of supernovae, common envelope, and mass transfer (MT). Using the POSYDON BPS code, employing pre-computed grids of detailed binary stellar evolution models, we investigate the Galactic Be-XRB population. POSYDON incorporates stellar rotation self-consistently during MT phases, enabling a detailed examination of the rotational distribution of Be stars. Our fiducial BPS and Be-XRB model align well with the orbital properties of Galactic Be-XRBs, emphasizing the role of rotational constraints. Our modeling reveals a bimodal rotational distribution of Be-XRB-like systems, in excellent agreement with literature values. All Be-XRBs undergo an MT phase before the first compact object forms, with over half experiencing a second MT phase from a stripped helium companion (Case BB). Computing rotationally-limited MT efficiencies and applying them to our population, we find that the majority of Be-XRBs have undergone highly non-conservative MT (beta ~ 0.15). Our study underscores the importance of detailed angular momentum modeling during MT in interpreting Be-XRB populations, emphasizing this population as a key probe for the stability and efficiency of MT in interacting binaries."
https://arxiv.org/abs/2403.07171,2024-03-11,Representing rational integers by generalized quadratic forms over quadratic fields,"['Ondřej Chwiedziuk', 'Matěj Doležálek', 'Emma Pěchoučková', 'Zdeněk Pezlar', 'Om Prakash', 'Giuliano Romeo', 'Anna Růžičková', 'Mikuláš Zindulka']",We investigate generalized quadratic forms with values in the set of rational integers over quadratic fields. We characterize the real quadratic fields which admit a positive definite binary generalized form of this type representing every positive integer. We also show that there are only finitely many such fields where a ternary generalized form with these properties exists.
https://arxiv.org/abs/2403.07170,2024-03-12,"Cyclical Long Memory: Decoupling, Modulation, and Modeling","['Stefanos Kechagias', 'Vladas Pipiras', 'Pavlos Zoubouloglou']","A new model for general cyclical long memory is introduced, by means of random modulation of certain bivariate long memory time series. This construction essentially decouples the two key features of cyclical long memory: quasi-periodicity and long-term persistence. It further allows for a general cyclical phase in cyclical long memory time series. Several choices for suitable bivariate long memory series are discussed, including a parametric fractionally integrated vector ARMA model. The parametric models introduced in this work have explicit autocovariance functions that can be used readily in simulation, estimation, and other tasks."
https://arxiv.org/abs/2403.07169,2024-03-11,Magnon bands and transverse transport in a proposed two-dimensional $Cu_2F_5$ ferrimagnet,"['P. G. de Oliveira', 'A. S. T. Pires']","The copper fluoride $Cu_2F_5$ is a proposed stable compound that can be seen as a layered magnetic lattice of $S=1$ and $S=1/2$ sites, corresponding to copper ions. Intending to cast light on the transport properties of ferrimagnetic magnons, we use the linear spin wave approach to study the magnon band structure of the 2D lattice in a ferrimagnetic off-plane order, as well as the transverse transport of magnons in the crystal bulk. That transverse (Hall-like) transport can be induced by a magnetic field or temperature gradient, and within the linear response theory is generated by the Berry curvature of the eigenstates. As in most of the cases for magnons, the Berry curvature here is related to Dzyaloshinskii-Moriya interactions between next-near-neighbors. The band structure of the system is non-degenerate and the transport coefficients are non-null. We also determine the condition for two transport coefficients to change sign in response to temperature."
https://arxiv.org/abs/2403.07168,2024-03-11,Elliptic analogue of Vershik-Kerov limit shape,"['Andrey Grekov', 'Nikita Nekrasov']","We review the limit shape problem for the Plancherel measure and its generalizations found in supersymmetric gauge theory instanton count. We focus on the measure, interpolating between the Plancherel measure and uniform measure, a U(1) case of N=2* gauge theory. We give the formula for its limit shape in terms of elliptic functions, generalizing the trigonometric ``arcsin'' law of Vershik-Kerov and Logan-Schepp."
https://arxiv.org/abs/2403.07167,2024-03-11,Stationary phase analysis of ambient noise cross-correlations: Focusing on non-ballistic arrivals,"['Yunyue Elita Li', 'Feng Zhu', 'Jizhong Yang']","Stacked cross-correlation functions have become ubiquitous in the ambient seismic imaging and monitoring community as approximations to the Green's function between two receivers. While theoretical understanding of this approximation to the ballistic arrivals is well established, the equivalent analysis for the non-ballistic arrivals is alarmingly inadequate compared to the exponential growth of its applications. To provide a fundamental understanding of the cross-correlation functions beyond the ballistic arrivals, we derive analytical stationary phase solutions for ambient noise cross-correlations with a focus on non-ballistic arrivals. We establish the mathematical and corresponding physical conditions that drastically differentiate the non-ballistic arrivals in the stacked cross-correlation and the actual Green's functions. In ambient noise environments, the coda waves due to random medium scatterings of an impulsive source cannot be distinguished from the cross-talk artifacts due to overlapping random noise sources. Therefore, changes in the non-ballistic arrivals cannot be uniquely attributed to changes in the medium or changes in the noise source environment without additional constraints. The theoretical results demand that interpreting large-elapse-time arrivals in the stacked cross-correlation functions as coda waves for deterministic information about the propagation medium should be conducted only after the source influence is sufficiently ruled out. Once the source influence is eliminated, the stationary phase solutions for scattering waves provide a solid basis for extracting reliable scattering information from the noise correlation functions for higher-resolution imaging and monitoring."
https://arxiv.org/abs/2403.07166,2024-03-11,"Small Price Changes, Sales Volume, and Menu Cost","['Doron Sayag', 'Avichai Snir', 'Daniel Levy']","The finding of small price changes in many retail price datasets is often viewed as a puzzle. We show that a possible explanation for the presence of small price changes is related to sales volume, an observation that has been overlooked in the existing literature. Analyzing a large retail scanner price dataset that contains information on both prices and sales volume, we find that small price changes are more frequent when products sales volume is high. This finding holds across product categories, within product categories, and for individual products. It is also robust to various sensitivity analyses such as measurement errors, the definition of small price changes, the inclusion of measures of price synchronization, the size of producers, the time horizon used to compute the average sales volume, the revenues, the competition, shoppers characteristics, etc."
https://arxiv.org/abs/2403.07165,2024-03-11,Localized interfacial Phonon Modes at the Electronic Axion Domain Wall,"['Abhinava Chatterjee', 'Mourad Oudich', 'Yun Jing', 'Chao-Xing Liu']","The most salient feature of electronic topological states of matter is the existence of exotic electronic modes localized at the surface or interface of a sample. In this work, in an electronic topological system, we demonstrate the existence of localized phonon modes at the domain wall between topologically trivial and non-trivial regions, in addition to the localized interfacial electronic states. In particular, we consider a theoretical model for the Dirac semimetal with a gap opened by external strains and study the phonon dynamics, which couples to electronic degrees of freedom via strong electron-phonon interaction. By treating the phonon modes as a pseudo-gauge field, we find that the axion type of terms for phonon dynamics can emerge in gapped Dirac semimetal model and lead to interfacial phonon modes localized at the domain wall between trivial and non-trivial regimes that possess the axion parameters 0 and π, respectively. We also discuss the physical properties and possible experimental probe of such interfacial phonon modes."
https://arxiv.org/abs/2403.07164,2024-03-11,Killing versus branching: Unexplored facets of diffusive relaxation,"['P. Garbaczewski', 'M. Zaba']","We analyze the relaxation dynamics of Feynman-Kac path integral kernel functions in terms of branching diffusion processes with killing. This sheds new light on the admissible path-wise description of the relaxation to equilibrium for conditioned Brownian motions, and diffusion processes with absorbing boundaries, where Feynman-Kac kernels appear as the building blocks of inferred transition probability density functions."
https://arxiv.org/abs/2403.07163,2024-03-11,Water Isotope Separation using Deep Learning and a Catalytically Active Ultrathin Membrane,"['Jinu Jeong', 'Chenxing Liang', 'Narayana Aluru']","Water isotope separation, specifically separating heavy from light water, is a socially significant issue due to the usage of heavy water in applications such as nuclear magnetic resonance, nuclear power, and spectroscopy. Separation of heavy water from light water is difficult due to very similar physical and chemical properties between the isotopes. We show that a catalytically active ultrathin membrane (e.g., a nanopore in MoS2) can enable chemical exchange processes and physicochemical mechanisms that lead to efficient separation of deuterium from hydrogen, quantified as the D2O and deuterium separation ratio of 4.5 and 1.73, respectively. The separation process is inherently multiscale in nature with the shorter times representing chemical exchange processes and the longer timescales representing the transport phenomena. To bridge the timescales, we employ a deep learning methodology which uses short time scale ab-initio molecular dynamics data for training and extends the timescales to classical molecular dynamics regime to demonstrate isotope separation and reveal the underlying complex physicochemical processes."
https://arxiv.org/abs/2403.07162,2024-03-11,Digital Twin Evolution for Sustainable Smart Ecosystems,"['Istvan David', 'Judith Michael', 'Dominik Bork']","Smart ecosystems are the drivers of modern society. They control critical infrastructures, ensuring their stable and sustainable operation. Smart ecosystems are governed by digital twins -- real-time virtual representations of physical infrastructure. To support the open-ended and reactive traits of smart ecosystems, digital twins need to be able to evolve in reaction to changing conditions. However, digital twin evolution is particularly challenging due to the intertwined nature of physical and software components. As a consequence, software practitioners find a substantial body of knowledge on software evolution hard to apply in digital twin evolution scenarios. In this article, we provide software practitioners with tangible leads toward understanding and managing the evolutionary concerns of digital twins. By that, we aim to bridge a significant gap in leveraging software engineering practices to develop robust smart ecosystems."
https://arxiv.org/abs/2403.07161,2024-03-11,How closed is cosmology?,"['Sean Gryb', 'David Sloan']","Classical cosmology exhibits a particular kind of scaling symmetry. The dynamics of the invariants of this symmetry forms a system that exhibits many of the features of open systems such as the non-conservation of mechanical energy and the focusing of measures along the dynamical flow. From these properties, we show that important dynamical features emerge that are not present in closed systems. In particular, a large and physically plausible class of cosmological models give rise to a natural arrow of time. We then argue that the appropriate notion of closure in cosmology is dynamical closure - that a system can be integrated without reference to external factors. This is realised in physical systems in terms of the algebraic closure of the equations of motion such that the system is autonomous. Remarkably, in a growing class of models it can be shown that the autonomous system obtained remains regular and can be integrated through the big bang."
https://arxiv.org/abs/2403.07160,2024-03-11,Essential self-adjointness of $\left(Δ^2 +c|x|^{-4}\right)\big|_{C_0^{\infty}(\mathbb{R}^n \backslash \{0\})}$,"['Fritz Gesztesy', 'Markus Hunziker']","Let $n\in\mathbb{N}, n\geq 2$. We prove that the strongly singular differential operator \[\left(Δ^2 +c|x|^{-4}\right)\big|_{C_0^{\infty}(\mathbb{R}^n \backslash \{0\})}, \quad c \in \mathbb{R}, \] is essentially self-adjoint in $L^2(\mathbb{R}^n; d^n x)$ if and only if \[c\geq \begin{cases}3(n+2)(6-n)&\mbox{for $2\leq n\leq 5$};\\[5pt] {\displaystyle -\frac{n(n+4)(n-4)(n-8)}{16}}&\mbox{for $n\geq 6$}.\end{cases}\]"
https://arxiv.org/abs/2403.07159,2024-03-11,Existence for a Nonlocal Multi-Species Advection Diffusion Equation,"['Elaine Cozzi', 'Zachary Radke']","We establish short-time existence of bounded, smooth non-negative solutions to a multi-species advection diffusion equation for a wide class of singular interaction kernels. We also give conditions on the interaction matrix, whose coefficients determine species attraction or repulsion, which ensure global existence of solutions."
https://arxiv.org/abs/2403.07158,2024-03-11,Sample Splitting and Assessing Goodness-of-fit of Time Series,"['Richard A. Davis', 'Leon Fernandes']","A fundamental and often final step in time series modeling is to assess the quality of fit of a proposed model to the data. Since the underlying distribution of the innovations that generate a model is often not prescribed, goodness-of-fit tests typically take the form of testing the fitted residuals for serial independence. However, these fitted residuals are inherently dependent since they are based on the same parameter estimates and thus standard tests of serial independence, such as those based on the autocorrelation function (ACF) or distance correlation function (ADCF) of the fitted residuals need to be adjusted. The sample splitting procedure in Pfister et al.~(2018) is one such fix for the case of models for independent data, but fails to work in the dependent setting. In this paper sample splitting is leveraged in the time series setting to perform tests of serial dependence of fitted residuals using the ACF and ADCF. Here the first $f_n$ of the data points are used to estimate the parameters of the model and then using these parameter estimates, the last $l_n$ of the data points are used to compute the estimated residuals. Tests for serial independence are then based on these $l_n$ residuals. As long as the overlap between the $f_n$ and $l_n$ data splits is asymptotically 1/2, the ACF and ADCF tests of serial independence tests often have the same limit distributions as though the underlying residuals are indeed iid. In particular if the first half of the data is used to estimate the parameters and the estimated residuals are computed for the entire data set based on these parameter estimates, then the ACF and ADCF can have the same limit distributions as though the residuals were iid. This procedure ameliorates the need for adjustment in the construction of confidence bounds for both the ACF and ADCF in goodness-of-fit testing."
https://arxiv.org/abs/2403.07157,2024-03-11,Freely 2-periodic knots have two canonical components,"['Keegan Boyle', 'Nicholas Rouse']","We prove that the SL(2, C) character variety of a hyperbolic, freely 2-periodic knot has two canonical components. We also prove that the hyperbolic torsion polynomial of such a knot satisfies a factorization condition which seems to be particularly effective at identifying freely 2-periodic knots."
https://arxiv.org/abs/2403.07156,2024-03-11,On Uniqueness of Participation Factors,"['Tianwei Xia', 'Kai Sun']","In modal analysis and control of a nonlinear dynamical system, participation factors of state variables with respect to a mode of interest serve as pivotal tools for stability studies. Linear participation factors are uniquely determined by the mode's shape and composition, which are defined by the right and left eigenvectors of the linearized model. For nonlinear participation factors as well as five other variants of participation factors, this paper finds the sufficient conditions for them to be unique against scaling factors on the shape and composition of a mode. Besides, the similarity between the scaling factor and perturbation amplitude is also discussed."
https://arxiv.org/abs/2403.07155,2024-03-11,A Real-time Dyson Expansion Scheme: Efficient Inclusion of Dynamical Correlations in Non-equilibrium Spectral Propertie,"['Cian Reeves', 'Vojtech Vlcek']","Time resolved photoemission spectroscopy is the key technique to probe the real-time non-equilibrium dynamics of electronic states. Theoretical predictions of the time dependent spectral function for realistic systems is however, a challenge. Employing the Kadanoff-Baym equations to find this quantity results in a cubic scaling in the total number of time steps, quickly becoming prohibitive and often fail quantitatively and even qualitatively. In comparison, mean field methods have more favorable numerical scaling both in the number number of time steps and in the complexity associated with the cost of a evolving for a single time step, however they miss key spectral properties such as emergent spectral features. Here we present a scheme that allows for the inclusion of dynamical correlations to the spectral function while maintaining the same scaling in the number of time steps as for mean field approaches, while capturing the emergent physics. Further, the scheme can be efficiently implemented on top of equilibrium real-time many-body perturbation theory schemes and codes. We see excellent agreement with exact results for test systems. Furthermore we exemplify the method on a periodic system and demonstrate clear evidence that our proposed scheme produces complex spectral features including excitonic band replicas, features that are not observed using static mean field approaches."
https://arxiv.org/abs/2403.07154,2024-03-11,Phononic bright and dark states: Investigating multi-mode light-matter interactions with a single trapped ion,"['Harry Parke', 'Robin Thomm', 'Alan C. Santos', 'André Cidrim', 'Gerard Higgins', 'Marion Mallweger', 'Natalia Kuk', 'Shalina Salim', 'Romain Bachelard', 'Celso J. Villas-Boas', 'Markus Hennrich']","Interference underpins some of the most practical and impactful properties of both the classical and quantum worlds. In this work we experimentally investigate a new formalism to describe interference effects, based on collective states which have enhanced or suppressed coupling to a two-level system. We employ a single trapped ion, whose electronic state is coupled to two of the ion's motional modes in order to simulate a multi-mode light-matter interaction. We observe the emergence of phononic bright and dark states for both a single phonon and a superposition of coherent states and demonstrate that a view of interference which is based solely on their decomposition in the collective basis is able to intuitively describe their coupling to a single atom. This work also marks the first time that multi-mode bright and dark states have been formed with the bounded motion of a single trapped ion and we highlight the potential of the methods discussed here for use in quantum information processing."
https://arxiv.org/abs/2403.07153,2024-03-11,2023 Low-Power Computer Vision Challenge (LPCVC) Summary,"['Leo Chen', 'Benjamin Boardley', 'Ping Hu', 'Yiru Wang', 'Yifan Pu', 'Xin Jin', 'Yongqiang Yao', 'Ruihao Gong', 'Bo Li', 'Gao Huang', 'Xianglong Liu', 'Zifu Wan', 'Xinwang Chen', 'Ning Liu', 'Ziyi Zhang', 'Dongping Liu', 'Ruijie Shan', 'Zhengping Che', 'Fachao Zhang', 'Xiaofeng Mou', 'Jian Tang', 'Maxim Chuprov', 'Ivan Malofeev', 'Alexander Goncharenko', 'Andrey Shcherbin']","This article describes the 2023 IEEE Low-Power Computer Vision Challenge (LPCVC). Since 2015, LPCVC has been an international competition devoted to tackling the challenge of computer vision (CV) on edge devices. Most CV researchers focus on improving accuracy, at the expense of ever-growing sizes of machine models. LPCVC balances accuracy with resource requirements. Winners must achieve high accuracy with short execution time when their CV solutions run on an embedded device, such as Raspberry PI or Nvidia Jetson Nano. The vision problem for 2023 LPCVC is segmentation of images acquired by Unmanned Aerial Vehicles (UAVs, also called drones) after disasters. The 2023 LPCVC attracted 60 international teams that submitted 676 solutions during the submission window of one month. This article explains the setup of the competition and highlights the winners' methods that improve accuracy and shorten execution time."
https://arxiv.org/abs/2403.07152,2024-03-11,Success functions in large contests,"['Yaron Azrieli', 'Christopher P. Chambers']","We consider contests with a large set (continuum) of participants and axiomatize contest success functions that arise when performance is composed of both effort and a random element, and when winners are those whose performance exceeds a cutoff determined by a market clearing condition. A co-monotonicity property is essentially all that is needed for a representation in the general case, but significantly stronger conditions must hold to obtain an additive structure. We illustrate the usefulness of this framework by revisiting some of the classic questions in the contests literature."
https://arxiv.org/abs/2403.07151,2024-03-11,Don't Forget What I did?: Assessing Client Contributions in Federated Learning,"['Bishwamittra Ghosh', 'Debabrota Basu', 'Fu Huazhu', 'Wang Yuan', 'Renuga Kanagavelu', 'Jiang Jin Peng', 'Liu Yong', 'Goh Siow Mong Rick', 'Wei Qingsong']","Federated Learning (FL) is a collaborative machine learning (ML) approach, where multiple clients participate in training an ML model without exposing the private data. Fair and accurate assessment of client contributions is an important problem in FL to facilitate incentive allocation and encouraging diverse clients to participate in a unified model training. Existing methods for assessing client contribution adopts co-operative game-theoretic concepts, such as Shapley values, but under simplified assumptions. In this paper, we propose a history-aware game-theoretic framework, called FLContrib, to assess client contributions when a subset of (potentially non-i.i.d.) clients participate in each epoch of FL training. By exploiting the FL training process and linearity of Shapley value, we develop FLContrib that yields a historical timeline of client contributions as FL training progresses over epochs. Additionally, to assess client contribution under limited computational budget, we propose a scheduling procedure that considers a two-sided fairness criteria to perform expensive Shapley value computation only in a subset of training epochs. In experiments, we demonstrate a controlled trade-off between the correctness and efficiency of client contributions assessed via FLContrib. To demonstrate the benefits of history-aware client contributions, we apply FLContrib to detect dishonest clients conducting data poisoning in FL training."
https://arxiv.org/abs/2403.07150,2024-03-11,Breaking Political Filter Bubbles via Social Comparison,"['Nouran Soliman', 'Motahhare Eslami', 'Karrie Karahalios']","Online social platforms allow users to filter out content they do not like. According to selective exposure theory, people tend to view content they agree with more to get more self-assurance. This causes people to live in ideological filter bubbles. We report on a user study that encourages users to break the political filter bubble of their Twitter feed by reading more diverse viewpoints through social comparison. The user study is conducted using political-bias analyzing and Twitter-mirroring tools to compare the political slant of what a user reads and what other Twitter users read about a topic, and in general. The results show that social comparison can have a great impact on users' reading behavior by motivating them to read viewpoints from the opposing political party."
https://arxiv.org/abs/2403.07149,2024-03-11,Advancing Hyperspectral Targeted Alpha Therapy with Adversarial Machine Learning,"['Jim Zhao', 'Greg Leadman']","Targeted Alpha Therapy (TAT) has emerged as a promising modality for the treatment of various malignancies, leveraging the high linear energy transfer (LET) and short range of alpha particles to selectively irradiate cancer cells while sparing healthy tissue. Monitoring and optimizing TAT delivery is crucial for its clinical success. Hyper-spectral Single Photon Imaging (HSPI) presents a novel and versatile approach for the real-time assessment of TAT in vivo. This study introduces a comprehensive framework for HSPI in TAT, encompassing spectral unmixing, quantitative dosimetry, and spatiotemporal visualization. We report the development of a dedicated HSPI system tailored to alpha-emitting radionuclides, enabling the simultaneous acquisition of high-resolution spectral data and single-photon localization. Utilizing advanced spectral unmixing algorithms, we demonstrate the discrimination of alpha-induced scintillation from background fluorescence, facilitating precise alpha particle tracking with adversarial machine learning."
https://arxiv.org/abs/2403.07148,2024-03-11,Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities,"['Konstantinos Emmanouilidis', 'René Vidal', 'Nicolas Loizou']","The Stochastic Extragradient (SEG) method is one of the most popular algorithms for solving finite-sum min-max optimization and variational inequality problems (VIPs) appearing in various machine learning tasks. However, existing convergence analyses of SEG focus on its with-replacement variants, while practical implementations of the method randomly reshuffle components and sequentially use them. Unlike the well-studied with-replacement variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical guarantees. In this work, we provide a convergence analysis of SEG-RR for three classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone. We derive conditions under which SEG-RR achieves a faster convergence rate than the uniform with-replacement sampling SEG. In the monotone setting, our analysis of SEG-RR guarantees convergence to an arbitrary accuracy without large batch sizes, a strong requirement needed in the classical with-replacement SEG. As a byproduct of our results, we provide convergence guarantees for Shuffle Once SEG (shuffles the data only at the beginning of the algorithm) and the Incremental Extragradient (does not shuffle the data). We supplement our analysis with experiments validating empirically the superior performance of SEG-RR over the classical with-replacement sampling SEG."
https://arxiv.org/abs/2403.07147,2024-03-11,Metabolomic profiles in Jamaican children with and without autism spectrum disorder,"['Akram Yazdani', 'Maureen Samms-Vaughan', 'Sepideh Saroukhani', 'Jan Bressler', 'Manouchehr Hessabi', 'Amirali Tahanan', 'Megan L. Grove', 'Tanja Gangnus', 'Vasanta Putluri', 'Abu Hena Mostafa Kamal', 'Nagireddy Putluri', 'Katherine A. Loveland', 'Mohammad H. Rahbar']","Autism spectrum disorder (ASD) is a complex neurodevelopmental condition with a wide range of behavioral and cognitive impairments. While genetic and environmental factors are known to contribute to its etiology, the underlying metabolic perturbations associated with ASD which can potentially connect genetic and environmental factors, remain poorly understood. Therefore, we conducted a metabolomic case-control study and performed a comprehensive analysis to identify significant alterations in metabolite profiles between children with ASD and typically developing (TD) controls. The objective of this study is to elucidate potential metabolomic signatures associated with ASD in children and identify specific metabolites that may serve as biomarkers for the disorder. We conducted metabolomic profiling on plasma samples from participants in the second phase of Epidemiological Research on Autism in Jamaica, a cohort of 200 children with ASD and 200 TD controls (2-8 years old). Using high-throughput liquid chromatography-mass spectrometry techniques, we performed a targeted metabolite analysis, encompassing amino acids, lipids, carbohydrates, and other key metabolic compounds. After quality control and imputation of missing values, we performed univariable and multivariable analysis using normalized metabolites while adjusting for covariates, age, sex, socioeconomic status, and child's parish of birth. Our findings revealed unique metabolic patterns in children with ASD for four metabolites compared to TD controls. Notably, three of these metabolites were fatty acids, including myristoleic acid, eicosatetraenoic acid, and octadecenoic acid. Additionally, the amino acid sarcosine exhibited a significant association with ASD. These findings highlight the role of metabolites in the etiology of ASD and suggest opportunities for the development of targeted interventions."
https://arxiv.org/abs/2403.07146,2024-03-11,Two novel pure-state coherence measures in quantifying coherence,"['Manis Hazra', 'Debabrata Goswami']","In the resource theory of coherence, the quantification of quantum-state coherence is an important task. In this regard, the key ingredients are the various coherence monotones (or measures). There are few coherence-monotone classes that solely depend on other coherence measures defined for all the pure states; in other words, they rely on the pure state coherence measures (PSCM). Here, we set forth two such novel PSCMs, and validate each of them through the fulfillment of all four necessary conditions. In addition, we delve into the most recent (as per our knowledge) coherence-monotone class based on the innovative idea of quantifying coherence in terms of pure-state coherence, further redefine it, and, through the study of convexity under mixing, justify why this coherence monotone class cannot be treated as a coherence-measure class in general."
https://arxiv.org/abs/2403.07145,2024-03-11,Electrically Programmable Pixelated Graphene-Integrated Plasmonic Metasurfaces for Coherent Mid-Infrared Emission,"['Xiu Liu', 'Yibai Zhong', 'Zexiao Wang', 'Tianyi Huang', 'Sen Lin', 'Jingyi Zou', 'Haozhe Wang', 'Zhien Wang', 'Zhuo Li', 'Xiao Luo', 'Rui Cheng', 'Jiayu Li', 'Hyeong Seok Yun', 'Han Wang', 'Jing Kong', 'Xu Zhang', 'Sheng Shen']","Active metasurfaces have recently emerged as compact, lightweight, and efficient platforms for dynamic control of electromagnetic fields and optical responses. However, the complexities associated with their post-fabrication tunability significantly hinder their widespread applications, especially for the mid-infrared range due to material scarcity and design intricacy. Here, we experimentally demonstrate highly dynamic, pixelated modulations of coherent mid-infrared emission based on an electrically programmable plasmonic metasurface integrated with graphene field effect transistors (Gr-FETs). The ultrabroad infrared transparency of graphene allows for free-form control over plasmonic meta-atoms, thus achieving coherent mid-infrared states across a broad range of wavelengths and polarizations. The spatial temperature modulation generated by Gr-FETs is effectively synergized with the emissivity control by the localized surface plasmon polaritons from gold nanoantennas. This integrated temperature-emissivity modulation of metasurfaces is systematically extended to form a pixelated 2D array, envisioning new approaches toward scalable 2D electrical wiring for densely packed, independently controlled pixels."
https://arxiv.org/abs/2403.07144,2024-03-11,Thought Graph: Generating Thought Process for Biological Reasoning,"['Chi-Yang Hsu', 'Kyle Cox', 'Jiawei Xu', 'Zhen Tan', 'Tianhua Zhai', 'Mengzhou Hu', 'Dexter Pratt', 'Tianlong Chen', 'Ziniu Hu', 'Ying Ding']","We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28% and LLM baselines by 5.38% based on cosine similarity to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine."
https://arxiv.org/abs/2403.07143,2024-03-11,"New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production",['Shiliang Zuo'],"This work studies the repeated principal-agent problem from an online learning perspective. The principal's goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent's type (i.e., the agent's cost and production functions)."
https://arxiv.org/abs/2403.07142,2024-03-11,One Category One Prompt: Dataset Distillation using Diffusion Models,"['Ali Abbasi', 'Ashkan Shahbazi', 'Hamed Pirsiavash', 'Soheil Kolouri']","The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging recent advancements in generative text-to-image foundation models. Our approach utilizes textual inversion, a technique for fine-tuning text-to-image generative models, to create concise and informative representations for large datasets. By employing these learned text prompts, we can efficiently store and infer new samples for introducing data variability within a fixed memory budget. We show the effectiveness of our method through extensive experiments across various computer vision benchmark datasets with different memory budgets."
https://arxiv.org/abs/2403.07141,2024-03-11,Effect of Ir growth pressure on the domain wall dynamics in Ta/Pt/Co/Ir/Ta stacks,"['P. Domenichini', 'J. Brock', 'J. Curiale', 'A. B. Kolton']","The dynamical response of magnetic domain walls to external magnetic fields in ultra-thin multilayer magnetic films is determined not only by the composition and thickness of the layers but also by the growth conditions. Growth conditions can induce significant structural changes inside the layers and at the interfaces between them, affecting in particular the dynamics of domain walls, their mobility, elastic tension, and the pinning forces acting on them. In this work, we focus specifically on the effect of Ir layer growth pressure in Ta/Pt/Co/Ir/Ta ultra-thin multilayers films. Measurements of the DC magnetic properties, domain wall velocity and domain morphology in the creep regime for both constant and alternating field pulses, were performed for a batch of samples where the Ir layer was grown at different pressures. We find that the saturation magnetization, the effective anisotropy constant and the domain wall surface tension grow with increasing pressure and saturate at a threshold pressure, while the Dzyaloshinskii-Moriya field and the strength of the disorder remain practically unaltered over the range of pressures considered."
https://arxiv.org/abs/2403.07140,2024-03-11,Properties of Dynamical Black Hole Entropy,"['Manus R. Visser', 'Zihan Yan']","We study the first law for non-stationary perturbations of a stationary black hole whose event horizon is a Killing horizon, that relates the first-order change in the mass and angular momentum to the change in the entropy of an arbitrary horizon cross-section. Recently, Hollands, Wald and Zhang [1] have shown that the dynamical black hole entropy that satisfies this first law, for general relativity, is $S_{\text{dyn}}=(1-v\partial_v)S_{\text{BH}}$, where $v$ is the affine parameter of the null horizon generators and $S_{\text{BH}}$ is the Bekenstein-Hawking entropy, and for general diffeomorphism covariant theories of gravity $S_{\text{dyn}}=(1-v\partial_v)S_{\text{Wall}}$, where $S_{\text{Wall}}$ is the Wall entropy. They obtained the first law by applying the Noether charge method to non-stationary perturbations and arbitrary cross-sections. In this formalism, the dynamical black hole entropy is defined as an ""improved"" Noether charge, which is unambiguous to first order in the perturbation. In the present article we provide a pedagogical derivation of the physical process version of the non-stationary first law for general relativity by integrating the linearised Raychaudhuri equation between two arbitrary horizon cross-sections. Moreover, we generalise the derivation of the first law in [1] to non-minimally coupled matter fields, using boost weight arguments rather than Killing field arguments, and we relax some of the gauge conditions on the perturbations by allowing for non-zero variations of the horizon Killing field and surface gravity. Finally, for $f(\text{Riemann})$ theories of gravity we show explicitly using Gaussian null coordinates that the improved Noether charge is $S_{\text{dyn}}=(1-v\partial_v)S_{\text{Wall}}$, which is a non-trivial check of [1]."
https://arxiv.org/abs/2403.07139,2024-03-11,Chern Characteristics and Todd-Hirzebruch Identities for Transpolar Pairs of Toric Spaces,"['Per Berglund', 'Tristan Hübsch']","Standard toric geometry methods used to construct Calabi-Yau varieties may be extended to complete intersections in non-Fano varieties encoded by star triangulating non-convex polytopes. Similarly, mirror symmetry is conjectured to hold in terms of a transpolar duality generalizing the original construction of Batyrev and Borisov. The associated mirror pairs naturally include certain flip-folded, multi-layered multihedral objects, inclusively named VEX multitopes, and a correspondingly generalized transpolar duality. These self-overlaying VEX multitopes, long since known in pre-symplectic geometry, are found to correspond to certain non-algebraic but smooth toric spaces with Chern classes that satisfy the standard Todd-Hirzebruch identities. The computation of diffeomorphism invariants, including characteristic submanifold intersection numbers, corroborates their recent inclusion in the connected web of Calabi-Yau spaces and associated string compactifications: They arise together with the standard (Fano/reflexive polytope) constructions, within deformation families of generalized complete intersections in products of projective spaces."
https://arxiv.org/abs/2403.07138,2024-03-11,Sets of Cross Numbers of Sequences over Finite Abelian Groups,"['Aqsa Bashir', 'Wolfgang A. Schmid']","Let $G$ be a finite abelian group with $\exp(G)$ the exponent of $G$. Then $\mathsf W(G)$ denotes the set of cross numbers of minimal zero-sum sequences over $G$ and $\mathsf w(G)$ denotes the set of all cross numbers of non-trivial zero-sum free sequences over $G$. It is clear that $\mathsf W(G)$ and $\mathsf w(G)$ are bounded subsets of $\frac{1}{\exp(G)}\mathbb{N}$ with maximum $ \mathsf K(G)$ and $\mathsf k(G)$, respectively (here $\mathsf{K}(G)$ and $\mathsf{k}(G)$ denote the large and the small cross number of $G$, respectively). We give results on the structure of $\mathsf W(G)$ and $\mathsf w(G)$. We first show that both sets contain long arithmetic progressions and that only close to the maximum there might be some gaps. Then, we provide groups for which $\mathsf W(G)$ and $\mathsf w(G)$ actually are arithmetic progressions, and argue that this is rather a rare phenomenon. Finally, we provide some results in case there are gaps."
https://arxiv.org/abs/2403.07137,2024-03-11,Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution,"['Alexandre de Oliveira Bezerra', 'Rodrigo Goncalves Mateus', 'Vanessa Ap. de Moraes Weber', 'Fabricio de Lima Weber', 'Yasmin Alves de Arruda', 'Rodrigo da Costa Gomes', 'Gabriel Toshio Hirokawa Higa', 'Hemerson Pistori']",Assessing the biotype of cattle through human visual inspection is a very common and important practice in precision cattle breeding. This paper presents the results of a correlation analysis between scores produced by humans for Nelore cattle and a variety of measurements that can be derived from images or other instruments. It also presents a study using the k-means algorithm to generate new ways of clustering a batch of cattle using the measurements that most correlate with the animal's body weight and visual scores.
https://arxiv.org/abs/2403.07136,2024-03-11,On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency,"['David Cheikhi', 'Daniel Russo']","Identifying the trade-offs between model-based and model-free methods is a central question in reinforcement learning. Value-based methods offer substantial computational advantages and are sometimes just as statistically efficient as model-based methods. However, focusing on the core problem of policy evaluation, we show information about the transition dynamics may be impossible to represent in the space of value functions. We explore this through a series of case studies focused on structures that arises in many important problems. In several, there is no information loss and value-based methods are as statistically efficient as model based ones. In other closely-related examples, information loss is severe and value-based methods are severely outperformed. A deeper investigation points to the limitations of the representational power as the driver of the inefficiency, as opposed to failure in algorithm design."
https://arxiv.org/abs/2403.07135,2024-03-11,Noninvasive cavity-based charge diagnostic for plasma accelerators,"['Simon Bohlen', 'Olena Kononenko', 'Jan-Patrick Schwinkendorf', 'Florian Grüner', 'Dirk Lipka', 'Martin Meisel', 'Charlotte Palmer', 'Theresa Staufer', 'Kristjan Põder', 'Jens Osterhoff']","The charge contained in an electron bunch is one of the most important parameters in accelerator physics. Several techniques to measure the electron bunch charge exist. However, many conventional charge diagnostics face serious drawbacks when applied to plasma accelerators. For example, integrating current transformers (ICTs or toroids) have been shown to be sensitive to the electromagnetic pulses (EMP) originating from the plasma, whereas scintillating screens are sensitive to background radiation such as betatron radiation or bremsstrahlung and only allow for a destructive measurement of the bunch charge. We show measurements with a noninvasive, cavity-based charge diagnostic (the DaMon), which demonstrate its high sensitivity, high dynamic range and resistance towards EMP. The measurements are compared to both an ICT and an absolutely calibrated scintillating screen."
https://arxiv.org/abs/2403.07134,2024-03-11,COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization,"['Aozhong Zhang', 'Zi Yang', 'Naigang Wang', 'Yingyong Qin', 'Jack Xin', 'Xin Li', 'Penghang Yin']","Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding operations. We update these variables in a carefully designed greedy order, significantly enhancing the accuracy. COMQ achieves remarkable results in quantizing 4-bit Vision Transformers, with a negligible loss of less than 1% in Top-1 accuracy. In 4-bit INT quantization of convolutional neural networks, COMQ maintains near-lossless accuracy with a minimal drop of merely 0.3% in Top-1 accuracy."
https://arxiv.org/abs/2403.07133,2024-03-11,A formula for the volume of two-bridge knots,['Julien Marche'],"We give a closed formula for the volume of a two-bridge knot, more precisely for its Bloch invariant. We obtain this formula without triangulating the complement: instead, we derive it from the Hopf formula for the second homology of the fundamental group of the complement and a systematic use of Fox derivatives."
https://arxiv.org/abs/2403.07132,2024-03-11,A New Machine Learning Dataset of Bulldog Nostril Images for Stenosis Degree Classification,"['Gabriel Toshio Hirokawa Higa', 'Joyce Katiuccia Medeiros Ramos Carvalho', 'Paolo Brito Pascoalini Zanoni', 'Gisele Braziliano de Andrade', 'Hemerson Pistori']","Brachycephaly, a conformation trait in some dog breeds, causes BOAS, a respiratory disorder that affects the health and welfare of the dogs with various symptoms. In this paper, a new annotated dataset composed of 190 images of bulldogs' nostrils is presented. Three degrees of stenosis are approximately equally represented in the dataset: mild, moderate and severe stenosis. The dataset also comprises a small quantity of non stenotic nostril images. To the best of our knowledge, this is the first image dataset addressing this problem. Furthermore, deep learning is investigated as an alternative to automatically infer stenosis degree using nostril images. In this work, several neural networks were tested: ResNet50, MobileNetV3, DenseNet201, SwinV2 and MaxViT. For this evaluation, the problem was modeled in two different ways: first, as a three-class classification problem (mild or open, moderate, and severe); second, as a binary classification problem, with severe stenosis as target. For the multiclass classification, a maximum median f-score of 53.77\% was achieved by the MobileNetV3. For binary classification, a maximum median f-score of 72.08\% has been reached by ResNet50, indicating that the problem is challenging but possibly tractable."
https://arxiv.org/abs/2403.07131,2024-03-11,Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation,"['Steve Paul', 'Nathan Maurer', 'Souma Chowdhury']","Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite graph matching methods. These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA. However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex. This raises the question: can these heuristics be learned? To this end, this paper particularly develops a Graph Reinforcement Learning (GRL) framework to learn the heuristics or incentives for a bipartite graph matching approach to MRTA. Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite graph that connects the set of tasks to the set of robots. The original capsule attention network architecture is fundamentally modified by adding encoding of robots' state graph, and two Multihead Attention based decoders whose output are used to construct a LogNormal distribution matrix from which positive bigraph weights can be drawn. The performance of this new bigraph matching approach augmented with a GRL-derived incentive is found to be at par with the original bigraph matching approach that used expert-specified heuristics, with the former offering notable robustness benefits. During training, the learned incentive policy is found to get initially closer to the expert-specified incentive and then slightly deviate from its trend."
https://arxiv.org/abs/2403.07130,2024-03-11,Selecting energy-momentum trace dependent gravity theories with LSS,"['Jonas Pinheiro da Silva', 'Hermano Velten']","We study scalar cosmological perturbations in $f(R, T)$ modified gravity theories being $T$ the trace of the energy-momentum tensor. We provide detailed equations for the matter energy density contrast. We solve then numerically to promote a comparison with available large scale structure (LSS) formation observational data on $f σ_8$ and also addressing the $S_8$ tension. We identify $f(R,T)$ models that lead either to growth enhancement or suppression. Since recent results in the literature indicate a preference for the latter feature, this type of analysis is quite useful to select viable modifications of gravity. We studied class of such $f(R,T)$ models are either ruled out or severely restricted."
https://arxiv.org/abs/2403.07129,2024-03-11,RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing using Residual Policy Learning,"['Raphael Trumpp', 'Ehsan Javanmardi', 'Jin Nakazato', 'Manabu Tsukada', 'Marco Caccamo']","The interactive decision-making in multi-agent autonomous racing offers insights valuable beyond the domain of self-driving cars. Mapless online path planning is particularly of practical appeal but poses a challenge for safely overtaking opponents due to the limited planning horizon. Accordingly, this paper introduces RaceMOP, a novel method for mapless online path planning designed for multi-agent racing of F1TENTH cars. Unlike classical planners that depend on predefined racing lines, RaceMOP operates without a map, relying solely on local observations to overtake other race cars at high speed. Our approach combines an artificial potential field method as a base policy with residual policy learning to introduce long-horizon planning capabilities. We advance the field by introducing a novel approach for policy fusion with the residual policy directly in probability space. Our experiments for twelve simulated racetracks validate that RaceMOP is capable of long-horizon decision-making with robust collision avoidance during overtaking maneuvers. RaceMOP demonstrates superior handling over existing mapless planners while generalizing to unknown racetracks, paving the way for further use of our method in robotics. We make the open-source code for RaceMOP available at http://github.com/raphajaner/racemop."
https://arxiv.org/abs/2403.07128,2024-03-11,FAX: Scalable and Differentiable Federated Primitives in JAX,"['Keith Rush', 'Zachary Charles', 'Zachary Garrett']","We present FAX, a JAX-based library designed to support large-scale distributed and federated computations in both data center and cross-device applications. FAX leverages JAX's sharding mechanisms to enable native targeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX embeds building blocks for federated computations as primitives in JAX. This enables three key benefits. First, FAX computations can be translated to XLA HLO. Second, FAX provides a full implementation of federated automatic differentiation, greatly simplifying the expression of federated computations. Last, FAX computations can be interpreted out to existing production cross-device federated compute systems. We show that FAX provides an easily programmable, performant, and scalable framework for federated computations in the data center. FAX is available at https://github.com/google-research/google-research/tree/master/fax ."
https://arxiv.org/abs/2403.07127,2024-03-11,"Dimensional Regularization and Two-Loop Vacuum Polarization Operator: Master Integrals, Analytic Results and Energy Shifts","['S. Laporta', 'U. D. Jentschura']","We present a complete reevaluation of the irreducible two-loop vacuum-polarization correction to the photon propagator in quantum electrodynamics, i.e. with an electron-positron pair in the fermion propagators. The integration is carried out by reducing the integrations to a limited set of master integrals, which are calculated using integration-by-parts identities. Dimensional regularization is used in D = 4-2*epsilon dimensions, and on-mass shell renormalization is employed. The one-loop effect is given to order epsilon, to be combined with the 1/epsilon divergence of the two-loop amplitude. Master integrals are given. Final evaluations of two-loop energy shifts for 1S, 2S, and 2P states are done analytically, and results are presented, with an emphasis on muonic hydrogen. For relativistic Dirac-Coulomb reference states, higher-order coefficients are obtained for the Zalpha-expansion. We compare the results obtained to the existing literature."
https://arxiv.org/abs/2403.07126,2024-03-11,Heterogeneous Image-based Classification Using Distributional Data Analysis,"['Alec Reinhardt', 'Newsha Nikzad', 'Raven J. Hollis', 'Galia Jacobson', 'Millicent A. Roach', 'Mohamed Badawy', 'Peter Chul Park', 'Laura Beretta', 'Prasun K Jalal', 'David T. Fuentes', 'Eugene J. Koay', 'Suprateek Kundu']","Diagnostic imaging has gained prominence as potential biomarkers for early detection and diagnosis in a diverse array of disorders including cancer. However, existing methods routinely face challenges arising from various factors such as image heterogeneity. We develop a novel imaging-based distributional data analysis (DDA) approach that incorporates the probability (quantile) distribution of the pixel-level features as covariates. The proposed approach uses a smoothed quantile distribution (via a suitable basis representation) as functional predictors in a scalar-on-functional quantile regression model. Some distinctive features of the proposed approach include the ability to: (i) account for heterogeneity within the image; (ii) incorporate granular information spanning the entire distribution; and (iii) tackle variability in image sizes for unregistered images in cancer applications. Our primary goal is risk prediction in Hepatocellular carcinoma that is achieved via predicting the change in tumor grades at post-diagnostic visits using pre-diagnostic enhancement pattern mapping (EPM) images of the liver. Along the way, the proposed DDA approach is also used for case versus control diagnosis and risk stratification objectives. Our analysis reveals that when coupled with global structural radiomics features derived from the corresponding T1-MRI scans, the proposed smoothed quantile distributions derived from EPM images showed considerable improvements in sensitivity and comparable specificity in contrast to classification based on routinely used summary measures that do not account for image heterogeneity. Given that there are limited predictive modeling approaches based on heterogeneous images in cancer, the proposed method is expected to provide considerable advantages in image-based early detection and risk prediction."
https://arxiv.org/abs/2403.07125,2024-03-11,Learning-Aided Control of Robotic Tether-Net with Maneuverable Nodes to Capture Large Space Debris,"['Achira Boonrath', 'Feng Liu', 'Elenora M. Botta', 'Souma Chowdhury']","Maneuverable tether-net systems launched from an unmanned spacecraft offer a promising solution for the active removal of large space debris. Guaranteeing the successful capture of such space debris is dependent on the ability to reliably maneuver the tether-net system -- a flexible, many-DoF (thus complex) system -- for a wide range of launch scenarios. Here, scenarios are defined by the relative location of the debris with respect to the chaser spacecraft. This paper represents and solves this problem as a hierarchically decentralized implementation of robotic trajectory planning and control and demonstrates the effectiveness of the approach when applied to two different tether-net systems, with 4 and 8 maneuverable units (MUs), respectively. Reinforcement learning (policy gradient) is used to design the centralized trajectory planner that, based on the relative location of the target debris at the launch of the net, computes the final aiming positions of each MU, from which their trajectory can be derived. Each MU then seeks to follow its assigned trajectory by using a decentralized PID controller that outputs the MU's thrust vector and is informed by noisy sensor feedback (for realism) of its relative location. System performance is assessed in terms of capture success and overall fuel consumption by the MUs. Reward shaping and surrogate models are used to respectively guide and speed up the RL process. Simulation-based experiments show that this approach allows the successful capture of debris at fuel costs that are notably lower than nominal baselines, including in scenarios where the debris is significantly off-centered compared to the approaching chaser spacecraft."
https://arxiv.org/abs/2403.07124,2024-03-11,Stochastic gradient descent-based inference for dynamic network models with attractors,"['Hancong Pan', 'Xiaojing Zhu', 'Cantay Caliskan', 'Dino P. Christenson', 'Konstantinos Spiliopoulos', 'Dylan Walker', 'Eric D. Kolaczyk']","In Coevolving Latent Space Networks with Attractors (CLSNA) models, nodes in a latent space represent social actors, and edges indicate their dynamic interactions. Attractors are added at the latent level to capture the notion of attractive and repulsive forces between nodes, borrowing from dynamical systems theory. However, CLSNA reliance on MCMC estimation makes scaling difficult, and the requirement for nodes to be present throughout the study period limit practical applications. We address these issues by (i) introducing a Stochastic gradient descent (SGD) parameter estimation method, (ii) developing a novel approach for uncertainty quantification using SGD, and (iii) extending the model to allow nodes to join and leave over time. Simulation results show that our extensions result in little loss of accuracy compared to MCMC, but can scale to much larger networks. We apply our approach to the longitudinal social networks of members of US Congress on the social media platform X. Accounting for node dynamics overcomes selection bias in the network and uncovers uniquely and increasingly repulsive forces within the Republican Party."
https://arxiv.org/abs/2403.07123,2024-03-11,On certain harmonic zeta functions,"['Mümün Can', 'Levent Kargın', 'Mehmet Cenkci', 'Ayhan Dil']","This study deals with certain harmonic zeta functions, one of them occurs in the study of the multiplication property of the harmonic Hurwitz zeta function. The values at the negative even integers are found and Laurent expansions at poles are described. Closed-form expressions are derived for the Stieltjes constants that occur in Laurent expansions in a neighborhood of s=1. Moreover, as a bonus, it is obtained that the values at the positive odd integers of three harmonic zeta functions can be expressed in closed-form evaluations in terms of zeta values and log-sine integrals."
https://arxiv.org/abs/2403.07122,2024-03-11,Am I the Odd One? Exploring (In)Congruencies in the Realism of Avatars and Virtual Others in Virtual Reality,"['David Mal', 'Nina Döllinger', 'Erik Wolf', 'Stephan Wenninger', 'Mario Botsch', 'Carolin Wienrich', 'Marc Erich Latoschik']","Virtual humans play a pivotal role in social virtual environments, shaping users' VR experiences. The diversity in available options and users' preferences can result in a heterogeneous mix of appearances among a group of virtual humans. The resulting variety in higher-order anthropomorphic and realistic cues introduces multiple (in)congruencies, eventually impacting the plausibility of the experience. In this work, we consider the impact of (in)congruencies in the realism of a group of virtual humans, including co-located others and one's self-avatar. In a 2 x 3 mixed design, participants embodied either (1) a personalized realistic or (2) a customized stylized self-avatar across three consecutive VR exposures in which they were accompanied by a group of virtual others being either (1) all realistic, (2) all stylized, or (3) mixed. Our results indicate groups of virtual others of higher realism, i.e., potentially more congruent with participants' real-world experiences and expectations, were considered more human-like, increasing the feeling of co-presence and the impression of interaction possibilities. (In)congruencies concerning the homogeneity of the group did not cause considerable effects. Furthermore, our results indicate that a self-avatar's congruence with the participant's real-world experiences concerning their own physical body yielded notable benefits for virtual body ownership and self-identification for realistic personalized avatars. Notably, the incongruence between a stylized self-avatar and a group of realistic virtual others resulted in diminished ratings of self-location and self-identification. We conclude on the implications of our findings and discuss our results within current theories of VR experiences, considering (in)congruent visual cues and their impact on the perception of virtual others, self-representation, and spatial presence."
https://arxiv.org/abs/2403.07121,2024-03-11,The phase of the electromagnetic form factor of the pion,"['Enrique Ruiz Arriola', 'Pablo Sanchez-Puertas']","We employ a dispersion relation that allows to recover the phase of the electromagnetic form factor of the pion from its absolute value above threshold. Compared to alternative approaches building on the phase, this approach builds on experimental input directly accessible at colliders. Employing the precise datasets from the $e^+e^-\toπ^+π^-$ reaction, we obtain the phase of the electromagnetic form factor up to 2.5 GeV, well beyond standard dispersive approaches. In addition, we separate the isovector and isoscalar components, that allows to extract the $P$-wave $ππ$ phase shift. We also provide relevant results, including the radius of the form factor and bounds in the spacelike region."
https://arxiv.org/abs/2403.07120,2024-03-11,Comparing Task Graph Scheduling Algorithms: An Adversarial Approach,"['Jared Coleman', 'Bhaskar Krishnamachari']","Scheduling a task graph representing an application over a heterogeneous network of computers is a fundamental problem in distributed computing. It is known to be not only NP-hard but also not polynomial-time approximable within a constant factor. As a result, many heuristic algorithms have been proposed over the past few decades. Yet it remains largely unclear how these algorithms compare to each other in terms of the quality of schedules they produce. We identify gaps in the traditional benchmarking approach to comparing task scheduling algorithms and propose a simulated annealing-based adversarial analysis approach called PISA to help address them. We also introduce SAGA, a new open-source library for comparing task scheduling algorithms. We use SAGA to benchmark 15 algorithms on 16 datasets and PISA to compare the algorithms in a pairwise manner. Algorithms that appear to perform similarly on benchmarking datasets are shown to perform very differently on adversarially chosen problem instances. Interestingly, the results indicate that this is true even when the adversarial search is constrained to selecting among well-structured, application-specific problem instances. This work represents an important step towards a more general understanding of the performance boundaries between task scheduling algorithms on different families of problem instances."
https://arxiv.org/abs/2403.07119,2024-03-11,Solvability of a class of systems of quadratic integral equations,"['Yuming Chen', 'Vitali Vougalter']","The article is devoted to the existence of solutions of a certain system of quadratic integral equations in H^1(R, R^N). We show the existence of a perturbed solution by using a fixed point technique in the Sobolev space on the real line."
https://arxiv.org/abs/2403.07118,2024-03-11,Narrating Causal Graphs with Large Language Models,"['Atharva Phatak', 'Vijay K. Mago', 'Ameeta Agrawal', 'Aravind Inbasekaran', 'Philippe J. Giabbanelli']","The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts. In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges. The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing. Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings. Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings. Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few examples as compared to fine-tuning via a large curated dataset."
https://arxiv.org/abs/2403.07117,2024-03-11,Spin-polarized Specular Andreev Reflections in Altermagnets,"['Yutaro Nagae', 'Andreas P. Schnyder', 'Satoshi Ikegaya']","We propose a multi-terminal device consisting of an s-wave superconductor coupled to an altermagnet, to generate highly correlated spin currents via Cooper pair splitting. Remarkably, we find that the correlated spin currents are induced by specular Andreev reflections in the altermagnet, an effect that has up to now been predicted to occur only in a very limited number of systems, e.g., Dirac/Weyl materials coupled to superconductors. We demonstrate that positive non-local charge currents and positive noise cross-correlations are unambiguous fingerprints of the specular Andreev reflections in our proposed device."
https://arxiv.org/abs/2403.07116,2024-03-11,Simulation-Based Segmentation of Blood Vessels in Cerebral 3D OCTA Images,"['Bastian Wittmann', 'Lukas Glandorf', 'Johannes C. Paetzold', 'Tamaz Amiranashvili', 'Thomas Wälchli', 'Daniel Razansky', 'Bjoern Menze']","Segmentation of blood vessels in murine cerebral 3D OCTA images is foundational for in vivo quantitative analysis of the effects of neurovascular disorders, such as stroke or Alzheimer's, on the vascular network. However, to accurately segment blood vessels with state-of-the-art deep learning methods, a vast amount of voxel-level annotations is required. Since cerebral 3D OCTA images are typically plagued by artifacts and generally have a low signal-to-noise ratio, acquiring manual annotations poses an especially cumbersome and time-consuming task. To alleviate the need for manual annotations, we propose utilizing synthetic data to supervise segmentation algorithms. To this end, we extract patches from vessel graphs and transform them into synthetic cerebral 3D OCTA images paired with their matching ground truth labels by simulating the most dominant 3D OCTA artifacts. In extensive experiments, we demonstrate that our approach achieves competitive results, enabling annotation-free blood vessel segmentation in cerebral 3D OCTA images."
https://arxiv.org/abs/2403.07115,2024-03-11,Operator size growth in Lindbladian SYK,"['Jiasheng Liu', 'Rene Meyer', 'Zhuo-Yu Xian']","We investigate the growth of operator size in the Lindbladian SYK model with $q$-body interaction terms and linear jump terms at finite dissipation strength. We compute the operator size as well as its distribution numerically at finite $q$ and analytically at large $q$. With dissipative (productive) jump terms, the size converges to a value smaller (larger) than half the number of Majorana fermions. At weak dissipation, the evolution of operator size displays a quadratic-exponential-plateau behavior. The plateau value is determined by the ratios between the coupling of the interaction and the linear jump term in the large $q$ limit. The operator size distribution remains localized in the finite size region even at late times, contrasting with the unitary case. Moreover, we also derived the time-independent orthogonal basis for operator expansion which exhibits the ``operator size concentration'' at finite dissipation. Finally, we observe that the uncertainty relation for operator size growth is saturated at large $q$, leading to a classical dynamics of the operator size growth with dissipation."
https://arxiv.org/abs/2403.07114,2024-03-11,Generalized many-body approach for near-field radiative heat transfer between nonspherical particles,"['Lindsay P. Walter', 'Mathieu Francoeur']","A generalized fluctuational electrodynamics-based many-body approach for calculating near-field radiative heat transfer (NFRHT) between nonspherical dipoles is proposed. The geometric parameters of nonspherical dipoles are implemented in the definition of the self-term of the free-space Green's function and, conversely to previous many-body models of NFRHT, dipole polarizability is defined a posteriori from the free-space Green's function solution such that polarizability calculation is an optional post-processing step rather than a required input. Both strong and weak forms of the generalized many-body approach are presented. It is shown that the approximate weak form is less computationally expensive but is only applicable to small particles characterized by size parameters less than ~0.24. The generalized many-body method is compared against an analytical solution for NFRHT between two spheroidal dipoles. Acceptable agreement is obtained, and the discrepancies are ascribed to differences in approximations for multiple reflections and from the way in which particle orientation is implemented in each method. The generalized many-body method is then applied to analyze the near-field spectral conductance between two SiC ellipsoidal dipoles. Results reveal that changes in the orientation of one of the ellipsoidal dipoles lead to active tuning of localized surface phonon resonance by up to three orders of magnitude. Finally, the spectral radiative thermal conductivity of a metamaterial composed of 1000 SiO2 ellipsoidal particles is studied. The metamaterial displays anisotropic radiative thermal conductivity, with differences in the value at resonance up to 2.8 times between different directions. The generalized many-body model of NFRHT presented in this paper may be used to develop particle-based metamaterials with novel, engineered radiative thermal properties."
https://arxiv.org/abs/2403.07113,2024-03-11,Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies,['Nieves Crasto'],"Object detection, a pivotal task in computer vision, is frequently hindered by dataset imbalances, particularly the under-explored issue of foreground-foreground class imbalance. This lack of attention to foreground-foreground class imbalance becomes even more pronounced in the context of single-stage detectors. This study introduces a benchmarking framework utilizing the YOLOv5 single-stage detector to address the problem of foreground-foreground class imbalance. We crafted a novel 10-class long-tailed dataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common real-world detection scenarios with a limited number of object classes. Against this backdrop, we scrutinized three established techniques: sampling, loss weighing, and data augmentation. Our comparative analysis reveals that sampling and loss reweighing methods, while shown to be beneficial in two-stage detector settings, do not translate as effectively in improving YOLOv5's performance on the COCO-ZIPF dataset. On the other hand, data augmentation methods, specifically mosaic and mixup, significantly enhance the model's mean Average Precision (mAP), by introducing more variability and complexity into the training data. (Code available: https://github.com/craston/object_detection_cib)"
https://arxiv.org/abs/2403.07112,2024-03-11,Parameterized Task Graph Scheduling Algorithm for Comparing Algorithmic Components,"['Jared Coleman', 'Ravi Vivek Agrawal', 'Ebrahim Hirani', 'Bhaskar Krishnamachari']","Scheduling distributed applications modeled as directed, acyclic task graphs to run on heterogeneous compute networks is a fundamental (NP-Hard) problem in distributed computing for which many heuristic algorithms have been proposed over the past decades. Many of these algorithms fall under the list-scheduling paradigm, whereby the algorithm first computes priorities for the tasks and then schedules them greedily to the compute node that minimizes some cost function. Thus, many algorithms differ from each other only in a few key components (e.g., the way they prioritize tasks, their cost functions, where the algorithms consider inserting tasks into a partially complete schedule, etc.). In this paper, we propose a generalized parametric list-scheduling algorithm that allows mixing and matching different algorithmic components to produce 72 unique algorithms. We benchmark these algorithms on four datasets to study the individual and combined effects of different algorithmic components on performance and runtime."
https://arxiv.org/abs/2403.07111,2024-03-11,Many-Body Localization in the Age of Classical Computing,"['Piotr Sierant', 'Maciej Lewenstein', 'Antonello Scardicchio', 'Lev Vidmar', 'Jakub Zakrzewski']","Statistical mechanics provides a framework for describing the physics of large, complex many-body systems using only a few macroscopic parameters to determine the state of the system. For isolated quantum many-body systems, such a description is achieved via the eigenstate thermalization hypothesis (ETH), which links thermalization, ergodicity and quantum chaotic behavior. However, tendency towards thermalization is not observed at finite system sizes and evolution times in a robust many-body localization (MBL) regime found numerically and experimentally in the dynamics of interacting many-body systems at strong disorder. Although the phenomenology of the MBL regime is well-established, the central question remains unanswered: under what conditions does the MBL regime give rise to an MBL phase in which the thermalization does not occur even in the asymptotic limit of infinite system size and evolution time?"
https://arxiv.org/abs/2403.07110,2024-03-11,Multimode-cavity picture of non-Markovian waveguide QED,"['Dario Cilluffo', 'Luca Ferialdi', 'G. Massimo Palma', 'Giuseppe Calajò', 'Francesco Ciccarello']","We introduce a picture to describe and intrepret waveguide-QED problems in the non-Markovian regime of long photonic retardation times resulting in delayed coherent feedback. The framework is based on an intuitive spatial decomposition of the waveguide into blocks. Among these, the block directly coupled to the atoms embodies an effective lossy multimode cavity leaking into the rest of the waveguide, in turn embodying an effective white-noise bath. The dynamics can be approximated by retaining only a finite number of cavity modes that yet eventually grows with the time delay. This description captures the atomic as well as the field's dynamics, even with many excitations, in both emission and scattering processes. As an application, we show that the recently identified non-Markovian steady states can be understood by retaining very few or even only one cavity modes."
https://arxiv.org/abs/2403.07109,2024-03-11,Accretion properties of X-ray AGN: Evidence for radiation-regulated obscuration with redshift-dependent host galaxy contribution,"['Brivael Laloux', 'Antonis Georgakakis', 'David M. Alexander', 'Johannes Buchner', 'Carolina Andonie', 'Nischal Acharya', 'James Aird', 'Alba V. Alonso-Tetilla', 'Angela Bongiorno', 'Ryan C. Hickox', 'Andrea Lapi', 'Blessing Musiimenta', 'Cristina Ramos Almeida', 'Carolin Vellforth', 'Francesco Shankar']","We adopt a Bayesian X-ray spectral approach to investigate the accretion properties of unobscured ($20<\log(N_{\rm H}/{\rm cm}^{-2}<22$) and obscured ($22< \log(N_{\rm H}/{\rm cm}^{-2}<24$) active galactic nuclei (AGN) to shed light on the orientation vs evolution scenarios for the origin of the obscuring material. For a sample of 3882 X-ray-selected AGN from the Chandra COSMOS Legacy, AEGIS and CDFS extragalactic surveys, we constrain their stellar masses, $M_\star$, intrinsic X-ray luminosities, $L_{\rm X}$, obscuring column densities, $N_{\rm H}$, and specific accretion rates $λ\propto L_{\rm X}/M_\star$. By combining these observables within a Bayesian non-parametric approach, we infer, for the first time, the specific accretion rate distribution (SARD) of obscured and unobscured AGN to $z\approx3$, i.e. the probability of a galaxy with mass $M_\star$ at redshift $z$ hosting an AGN with column density $N_{\rm H}$ and specific accretion rate $λ$. Our findings indicate that (1) both obscured and unobscured SARDs share similar shapes, shifting towards higher accretion rates with redshift, (2) unobscured SARDs exhibit a systematic offset towards higher $λ$ compared to obscured SARD for all redshift intervals, (3) the obscured AGN fraction declines sharply at $\logλ_{\rm break} \sim-2$ for $z <0.5$, but shifts to higher $λ$ values with increasing redshift, (4) the incidence of AGN within the theoretically unstable blow-out region of the $λ-N_{\rm H}$ plane increases with redshift. These observations provide compelling evidence for AGN ""downsizing"" and radiation-regulated nuclear-scale obscuration with an increasing host galaxy contribution towards higher redshifts."
https://arxiv.org/abs/2403.07108,2024-03-11,Enhancement of $p$-wave dark matter annihilation by quasi-bound states,"['Martin Beneke', 'Tobias Binder', 'Lorenzo De Ros', 'Mathias Garny']","We scrutinize the Sommerfeld enhancement in dark matter pair annihilation for $p$-wave and higher-$\ell$ partial waves. For the Yukawa potential these feature a super-resonant Breit-Wigner peak in their velocity-dependence close to Sommerfeld resonances as well as a universal scaling with velocity for all $\ell\geq 1$ that differs from the $s$-wave case. We provide a quantum mechanical explanation for these phenomena in terms of quasi-bound states sustained by the centrifugal barrier of the partial-wave potential, and give approximate WKB expressions capturing the main effects. The impact of quasi-bound states is exemplified for wino dark matter and models with light mediators, with a focus on indirect detection signals. We note that quasi-bound states can also explain similar peaks in the bound-state formation and self-scattering cross sections."
https://arxiv.org/abs/2403.07107,2024-03-11,Particle Trapping and Acceleration in Turbulent Post-flare Coronal Loops,"['Fabio Bacchini', 'Wenzhi Ruan', 'Rony Keppens']","We present a study of energetic-electron trapping and acceleration in the Kelvin-Helmholtz-induced magnetohydrodynamic (MHD) turbulence of post-flare loops in the solar corona. Using the particle-tracing capabilities of MPI-AMRVAC 3.0, we evolve ensembles of test electrons (i.e. without feedback to the underlying MHD) inside the turbulent looptop, using the guiding-center approximation. With the MHD looptop model of Ruan et al. 2018, we investigate the relation between turbulence and particle trapping inside the looptop structure, showing that better-developed turbulent cascades result in more efficient trapping primarily due to mirror effects. We then quantify the electron acceleration in the time-evolving MHD turbulence, and find that ideal-MHD processes inside the looptop can produce nonthermal particle spectra from an initial Maxwellian distribution. Electrons in this turbulence are preferentially accelerated by mirror effects in the direction perpendicular to the local magnetic field while remaining confined within small regions of space between magnetic islands. Assuming dominance of Bremsstrahlung radiation mechanisms, we employ the resulting information from accelerated electrons (combined with the MHD background) to construct HXR spectra of the post-flare loop that include nonthermal-particle contributions. Our results pave the way to constructing more realistic simulations of radiative coronal structure for comparison with current and future observations."
https://arxiv.org/abs/2403.07106,2024-03-11,Dimension matters: precision and incompatibility in multi-parameter quantum estimation models,"['Alessandro Candeloro', 'Zahra Pazhotan', 'Matteo G. A. Paris']","We study the role of probe dimension in determining the bounds of precision and the level of incompatibility in multi-parameter quantum estimation problems. In particular, we focus on the paradigmatic case of unitary encoding generated by $\mathfrak{su}(2)$ and compare precision and incompatibility in the estimation of the same parameters across representations of different dimensions. For two- and three-parameter unitary models, we prove that if the dimension of the probe is smaller than the number of parameters, then simultaneous estimation is not possible (the quantum Fisher matrix is singular). If the dimension is equal to the number of parameters, estimation is possible but the model exhibits maximal (asymptotic) incompatibility. However, for larger dimensions, there is always a state for which the incompatibility vanishes, and the symmetric Cramér-Rao bound is achievable. We also critically examine the performance of the so-called asymptotic incompatibility (AI) in characterizing the difference between the Holevo-Cramér-Rao bound and the Symmetric Logarithmic Derivative (SLD) one, showing that the AI measure alone may fail to adequately quantify this gap. Assessing the determinant of the Quantum Fisher Information Matrix (QFIM) is crucial for a precise characterization of the model's nature. Nonetheless, the AI measure still plays a relevant role since it encapsulates the non-classicality of the model in one scalar quantity rather than in a matrix form (i.e., the Uhlmann curvature)."
https://arxiv.org/abs/2403.07105,2024-03-11,A slice classification neural network for automated classification of axial PET/CT slices from a multi-centric lymphoma dataset,"['Shadab Ahamed', 'Yixi Xu', 'Ingrid Bloise', 'Joo H. O', 'Carlos F. Uribe', 'Rahul Dodhia', 'Juan L. Ferres', 'Arman Rahmim']","Automated slice classification is clinically relevant since it can be incorporated into medical image segmentation workflows as a preprocessing step that would flag slices with a higher probability of containing tumors, thereby directing physicians attention to the important slices. In this work, we train a ResNet-18 network to classify axial slices of lymphoma PET/CT images (collected from two institutions) depending on whether the slice intercepted a tumor (positive slice) in the 3D image or if the slice did not (negative slice). Various instances of the network were trained on 2D axial datasets created in different ways: (i) slice-level split and (ii) patient-level split; inputs of different types were used: (i) only PET slices and (ii) concatenated PET and CT slices; and different training strategies were employed: (i) center-aware (CAW) and (ii) center-agnostic (CAG). Model performances were compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC), and various binary classification metrics. We observe and describe a performance overestimation in the case of slice-level split as compared to the patient-level split training. The model trained using patient-level split data with the network input containing only PET slices in the CAG training regime was the best performing/generalizing model on a majority of metrics. Our models were additionally more closely compared using the sensitivity metric on the positive slices from their respective test sets."
https://arxiv.org/abs/2403.07104,2024-03-11,Shrinkage MMSE estimators of covariances beyond the zero-mean and stationary variance assumptions,"['Olivier Flasseur', 'Eric Thiébaut', 'Loïc Denis', 'Maud Langlois']","We tackle covariance estimation in low-sample scenarios, employing a structured covariance matrix with shrinkage methods. These involve convexly combining a low-bias/high-variance empirical estimate with a biased regularization estimator, striking a bias-variance trade-off. Literature provides optimal settings of the regularization amount through risk minimization between the true covariance and its shrunk counterpart. Such estimators were derived for zero-mean statistics with i.i.d. diagonal regularization matrices accounting for the average sample variance solely. We extend these results to regularization matrices accounting for the sample variances both for centered and non-centered samples. In the latter case, the empirical estimate of the true mean is incorporated into our shrinkage estimators. Introducing confidence weights into the statistics also enhance estimator robustness against outliers. We compare our estimators to other shrinkage methods both on numerical simulations and on real data to solve a detection problem in astronomy."
https://arxiv.org/abs/2403.07103,2024-03-11,Between the Extremes: A JWST Spectroscopic Benchmark for High Redshift Galaxies Using ~500 Confirmed Sources at $z\geqslant5$,"['Guido Roberts-Borsani', 'Tommaso Treu', 'Alice Shapley', 'Adriano Fontana', 'Laura Pentericci', 'Marco Castellano', 'Takahiro Morishita', 'Pietro Bergamini', 'Piero Rosati']","The exceptional spectra of the most luminous $z>10$ sources observed so far have challenged our understanding of early galaxy evolution, requiring a new observational benchmark for meaningful interpretation. As such, we construct spectroscopic templates representative of high-redshift, star-forming populations, using 482 confirmed sources at $z=5.0-12.9$ with JWST/NIRSpec prism observations, and report on their average properties. We find $z=5-11$ galaxies are dominated by blue UV continuum slopes ($β=-2.3$ to $-2.7$) and inverse Balmer jumps, characteristic of dust-poor and young systems, with a shift towards bluer slopes and younger ages with redshift. The evolution is mirrored by ubiquitous CIII] detections across all redshifts (EW$_{0}=5-14$ Å), which increase in strength towards early times. Rest-frame optical lines reveal elevated ratios ($O32=7-31$, $R23=5-8$, and $Ne3O2=1-2$) and subsolar metallicities (log O/H$=7.3-7.9$), typical of ionization conditions and metallicities rarely observed in $z\sim0$ populations. Within our sample, we identify 57 Ly$α$-emitters which we stack and compare to a matched sample of non-emitters. The former are characterized by more extreme ionizing conditions with enhanced CIII], CIV, and HeII+[OIII] line emission, younger stellar populations from inverse Balmer jumps, and a more pristine ISM seen through bluer UV slopes and elevated rest-frame optical line ratios. The novel comparison illustrates important intrinsic differences between the two populations, with implications for Ly$α$ visibility. The spectral templates derived here represent a new observational benchmark with which to interpret high-redshift sources, lifting our constraints on their global properties to unprecedented heights and extending out to the earliest of cosmic times."
https://arxiv.org/abs/2403.07102,2024-03-11,Homotopy type of shellable $q$-complexes and their homology groups,"['Sudhir R. Ghorpade', 'Rakhi Pratihar', 'Tovohery H. Randrianarisoa', 'Hugues Verdure', 'Glen Wilson']","The theory of shellable simplicial complexes brings together combinatorics, algebra, and topology in a remarkable way. Initially introduced by Alder for $q$-simplicial complexes, recent work of Ghorpade, Pratihar, and Randrianarisoa extends the study of shellability to $q$-matroid complexes and determines singular homology groups for a subclass of these $q$-simplicial complexes. In this paper, we determine the homotopy type of shellable $q$-simplicial complexes. Moreover, we establish the shellability of order complexes from lexicographically shellable $q$-simplicial complexes, that include the $q$-matroid complexes. This results in a comprehensive determination of the homology groups for any lexicographically shellable $q$-complexes."
https://arxiv.org/abs/2403.07101,2024-03-11,Advanced-Step Real-time Iterations with Four Levels -- New Error Bounds and Fast Implementation in acados,"['Jonathan Frey', 'Armin Nurkanovic', 'Moritz Diehl']","The Real-Time Iteration (RTI) is an online nonlinear model predictive control algorithm that performs a single Sequential Quadratic Programming (SQP) per sampling time. The algorithm is split into a preparation and a feedback phase, where the latter one performs as little computations as possible solving a single prepared quadratic program. To further improve the accuracy of this method, the Advanced-Step RTI (AS-RTI) performs additional Multi-Level Iterations (MLI) in the preparation phase, such as inexact or zero-order SQP iterations on a problem with a predicted state estimate. This paper extends and streamlines the existing analysis of AS-RTI, such as analyzing MLI of level A and B for the first time, and significantly simplifying the proofs for levels C and D. Moreover, this paper provides an efficient open-source implementation in acados, making it widely accessible to practitioners."
https://arxiv.org/abs/2403.07100,2024-03-11,Equivariant Variational Quantum Eigensolver to detect Phase Transitions through Energy Level Crossings,"['Giulio Crognaletti', 'Giovanni Di Bartolomeo', 'Michele Vischi', 'Luciano Loris Viteritti']","Level spectroscopy stands as a powerful method for identifying the transition point that delineates distinct quantum phases. Since each quantum phase exhibits a characteristic sequence of excited states, the crossing of energy levels between low-lying excited states offers a reliable mean to estimate the phase transition point. While approaches like the Variational Quantum Eigensolver are useful for approximating ground states of interacting systems using quantum computing, capturing low-energy excitations remains challenging. In our study, we introduce an equivariant quantum circuit that preserves the total spin and the translational symmetry to accurately describe singlet and triplet excited states in the $J_1$-$J_2$ Heisenberg model on a chain, which are crucial for characterizing its transition point. Additionally, we assess the impact of noise on the variational state, showing that conventional mitigation techniques like Zero Noise Extrapolation reliably restore its physical properties."
https://arxiv.org/abs/2403.07099,2024-03-11,Rate-independent continuous inhibitory chemical reaction networks are Turing-universal,"['Kim Calabrese', 'David Doty']","We study the model of continuous chemical reaction networks (CRNs), consisting of reactions such as $A+B \to C+D$ that can transform some continuous, nonnegative real-valued quantity (called a *concentration*) of chemical species $A$ and $B$ into equal concentrations of $C$ and $D$. Such a reaction can occur from any state in which both reactants $A$ and $B$ are present, i.e., have positive concentration. We modify the model to allow *inhibitors*, for instance, reaction $A+B \to^{I} C+D$ can occur only if the reactants $A$ and $B$ are present and the inhibitor $I$ is absent. The computational power of non-inhibitory CRNs has been studied. For instance, the reaction $X_1+X_2 \to Y$ can be thought to compute the function $f(x_1,x_2) = \min(x_1,x_2)$. Under an ""adversarial"" model in which reaction rates can vary arbitrarily over time, it was found that exactly the continuous, piecewise linear functions can be computed, ruling out even simple functions such as $f(x) = x^2$. In contrast, in this paper we show that inhibitory CRNs can compute any computable function $f:\mathbb{N}\to\mathbb{N}$."
https://arxiv.org/abs/2403.07098,2024-03-11,A universal tale of determinants and Gröbner bases,['Aldo Conca'],"In 1965 Buchberger defined Gröbner bases and an algorithm to compute them. Despite a slow start, already in the eighties Gröbner bases had become the main device for symbolic computations involving polynomials as well as a theoretical tool for the investigation of ideals and varieties via the so-called Gröbner deformation techniques. Rings and algebraic varieties defined by means of determinants are among the most classical objects in commutative algebra, algebraic geometry and invariant theory. By the end of the the eighties the time was ripe for the computation of Gröbner bases of determinantal ideals. We will tell the tale of how (universal) Gröbner bases of determinantal ideals were identified and the key role played by Bernd Sturmfels and his collaborators in this enterprise."
https://arxiv.org/abs/2403.07097,2024-03-11,Searching for New Fundamental Interactions via Isotopic Shifts in Molecular Lattice Clocks,"['E. Tiberi', 'M. Borkowski', 'B. Iritani', 'R. Moszynski', 'T. Zelevinsky']","Precision measurements with ultracold atoms and molecules are primed to probe beyond-the-Standard Model physics. Isotopologues of homonuclear molecules are a natural testbed for new Yukawa-type mass-dependent forces at nanometer scales, complementing existing mesoscopic-body and neutron scattering experiments. Here we propose using isotopic shift measurements in molecular lattice clocks to constrain these new interactions from new massive scalar particles in keV$/c^2$ range: the new interaction would impart an extra isotopic shift to molecular levels on top of one predicted by the Standard Model. For the strontium dimer, a Hz-level agreement between experiment and theory could constrain the coupling of the new particles to hadrons by up to an order-of-magnitude over the most stringent existing experiments."
https://arxiv.org/abs/2403.07096,2024-03-11,Gagliardo-Nirenberg inequality via a new pointwise estimate,"['Karol Lesnik', 'Tomas Roskovec', 'Filip Soudsky']","We prove a new type of pointwise estimate of the Kalamajska-Mazya-Shaposhnikova type, where sparse averaging operators replace the maximal operator. It allows us to extend the Gagliardo-Nirenberg interpolation inequality to all rearrangement invariant Banach function spaces without any assumptions on their upper Boyd index, i.e. omitting problems caused by unboundedness of maximal operator on spaces close to $L^1$. In particular, we remove unnecessary assumptions from the Gagliardo-Nirenberg inequality in the setting of Orlicz and Lorentz spaces. The applied method is new in this context and may be seen as a kind of sparse domination technique fitted to the context of rearrangement invariant Banach function spaces."
https://arxiv.org/abs/2403.07095,2024-03-11,Overcoming the Paradox of Certified Training with Gaussian Smoothing,"['Stefan Balauca', 'Mark Niklas Müller', 'Yuhao Mao', 'Maximilian Baader', 'Marc Fischer', 'Martin Vechev']","Training neural networks with high certified accuracy against adversarial examples remains an open problem despite significant efforts. While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods perform worse than looser relaxations. Prior work hypothesized that this is caused by the discontinuity and perturbation sensitivity of the loss surface induced by these tighter relaxations. In this work, we show theoretically that Gaussian Loss Smoothing can alleviate both of these issues. We confirm this empirically by proposing a certified training method combining PGPE, an algorithm computing gradients of a smoothed loss, with different convex relaxations. When using this training method, we observe that tighter bounds indeed lead to strictly better networks that can outperform state-of-the-art methods on the same network. While scaling PGPE-based training remains challenging due to high computational cost, our results clearly demonstrate the promise of Gaussian Loss Smoothing for training certifiably robust neural networks."
https://arxiv.org/abs/2403.07094,2024-03-11,FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning,"['Xiang Meng', 'Wenyu Chen', 'Riade Benbaki', 'Rahul Mazumder']","The increasing computational demands of modern neural networks present deployment challenges on resource-constrained devices. Network pruning offers a solution to reduce model size and computational cost while maintaining performance. However, most current pruning methods focus primarily on improving sparsity by reducing the number of nonzero parameters, often neglecting other deployment costs such as inference time, which are closely related to the number of floating-point operations (FLOPs). In this paper, we propose FALCON, a novel combinatorial-optimization-based framework for network pruning that jointly takes into account model accuracy (fidelity), FLOPs, and sparsity constraints. A main building block of our approach is an integer linear program (ILP) that simultaneously handles FLOP and sparsity constraints. We present a novel algorithm to approximately solve the ILP. We propose a novel first-order method for our optimization framework which makes use of our ILP solver. Using problem structure (e.g., the low-rank structure of approx. Hessian), we can address instances with millions of parameters. Our experiments demonstrate that FALCON achieves superior accuracy compared to other pruning approaches within a fixed FLOP budget. For instance, for ResNet50 with 20% of the total FLOPs retained, our approach improves the accuracy by 48% relative to state-of-the-art. Furthermore, in gradual pruning settings with re-training between pruning steps, our framework outperforms existing pruning methods, emphasizing the significance of incorporating both FLOP and sparsity constraints for effective network pruning."
https://arxiv.org/abs/2403.07093,2024-03-11,Luminosity and Beam-Induced Background Studies for the Cool Copper Collider,"['Dimitrios Ntounis', 'Emilio Alessandro Nanni', 'Caterina Vernieri']","A high-energy electron-positron collider has been widely recognized by the particle physics community to be the next crucial step for detailed studies of the Higgs boson and other fundamental particles and processes. Several proposals for such colliders, either linear or circular, are currently under evaluation. Any such collider will be required to reach high lumimosities, in order to collect enough data at a reasonable time scale, while at the same time coping with high rates of background particles produced from beam-beam interactions during the collisions. In this paper, we analyze the luminosity and beam-beam interaction characteristics of the Cool Copper Collider (C$^3$) and perform a comparison with other linear collider proposals. We conclude that C$^3$ can reach the same or higher collision rates as the other proposals, without having to cope with higher beam-induced background fluxes. Thus, C$^3$ emerges as an attractive option for a future electron-positron collider, benefiting from the collective advancements in beam delivery and final focus system technologies developed by other linear collider initiatives."
https://arxiv.org/abs/2403.07092,2024-03-11,A cascaded deep network for automated tumor detection and segmentation in clinical PET imaging of diffuse large B-cell lymphoma,"['Shadab Ahamed', 'Natalia Dubljevic', 'Ingrid Bloise', 'Claire Gowdy', 'Patrick Martineau', 'Don Wilson', 'Carlos F. Uribe', 'Arman Rahmim', 'Fereshteh Yousefirizi']","Accurate detection and segmentation of diffuse large B-cell lymphoma (DLBCL) from PET images has important implications for estimation of total metabolic tumor volume, radiomics analysis, surgical intervention and radiotherapy. Manual segmentation of tumors in whole-body PET images is time-consuming, labor-intensive and operator-dependent. In this work, we develop and validate a fast and efficient three-step cascaded deep learning model for automated detection and segmentation of DLBCL tumors from PET images. As compared to a single end-to-end network for segmentation of tumors in whole-body PET images, our three-step model is more effective (improves 3D Dice score from 58.9% to 78.1%) since each of its specialized modules, namely the slice classifier, the tumor detector and the tumor segmentor, can be trained independently to a high degree of skill to carry out a specific task, rather than a single network with suboptimal performance on overall segmentation."
https://arxiv.org/abs/2403.07091,2024-03-11,Sim-to-Real gap in RL: Use Case with TIAGo and Isaac Sim/Gym,"['Jaume Albardaner', 'Alberto San Miguel', 'Néstor García', 'Magí Dalmau']","This paper explores policy-learning approaches in the context of sim-to-real transfer for robotic manipulation using a TIAGo mobile manipulator, focusing on two state-of-art simulators, Isaac Gym and Isaac Sim, both developed by Nvidia. Control architectures are discussed, with a particular emphasis on achieving collision-less movement in both simulation and the real environment. Presented results demonstrate successful sim-to-real transfer, showcasing similar movements executed by an RL-trained model in both simulated and real setups."
https://arxiv.org/abs/2403.07090,2024-03-11,Time Series Analysis of Key Societal Events as Reflected in Complex Social Media Data Streams,"['Andy Skumanich', 'Han Kyul Kim']","Social media platforms hold valuable insights, yet extracting essential information can be challenging. Traditional top-down approaches often struggle to capture critical signals in rapidly changing events. As global events evolve swiftly, social media narratives, including instances of disinformation, become significant sources of insights. To address the need for an inductive strategy, we explore a niche social media platform GAB and an established messaging service Telegram, to develop methodologies applicable on a broader scale. This study investigates narrative evolution on these platforms using quantitative corpus-based discourse analysis techniques. Our approach is a novel mode to study multiple social media domains to distil key information which may be obscured otherwise, allowing for useful and actionable insights. The paper details the technical and methodological aspects of gathering and preprocessing GAB and Telegram data for a keyness (Log Ratio) metric analysis, identifying crucial nouns and verbs for deeper exploration. Empirically, this approach is applied to a case study of a well defined event that had global impact: the 2023 Wagner mutiny. The main findings are: (1) the time line can be deconstructed to provide useful data features allowing for improved interpretation; (2) a methodology is applied which provides a basis for generalization. The key contribution is an approach, that in some cases, provides the ability to capture the dynamic narrative shifts over time with elevated confidence. The approach can augment near-real-time assessment of key social movements, allowing for informed governance choices. This research is important because it lays out a useful methodology for time series relevant info-culling, which can enable proactive modes for positive social engagement."
https://arxiv.org/abs/2403.07089,2024-03-11,Graph learning methods to extract empathy supporting regions in a naturalistic stimuli fMRI,"['Sasanka GRS', 'Ayushi Agrawal', 'Santosh Nannuru', 'Kavita Vemuri']","Functional MRI (fMRI) research, employing naturalistic stimuli like movies, explores brain network interactions in complex cognitive processes such as empathy. The empathy network encompasses multiple brain areas, including the Insula, PFC, ACC, and parietal regions. Our novel processing pipeline applies graph learning methods to whole-brain timeseries signals, incorporating high-pass filtering, voxel-level clustering, and windowed graph learning with a sparsity-based approach. The study involves two short movies shown to 14 healthy volunteers, considering 54 regions extracted from the AAL Atlas. The sparsity-based graph learning consistently outperforms, achieving over 88% accuracy in capturing emotion contagion variations. Temporal analysis reveals a gradual induction of empathy, supported by the method's effectiveness in capturing dynamic connectomes through graph clustering. Edge-weight dynamics analysis underscores sparsity-based learning's superiority, while connectome-network analysis highlights the pivotal role of the Insula, Amygdala, and Thalamus in empathy. Spectral filtering analysis emphasizes the band-pass filter's significance in isolating regions linked to emotional and empathetic processing during empathy HIGH states. Key regions like Amygdala, Insula, and Angular Gyrus consistently activate, supporting their critical role in immediate emotional responses. Strong similarities across movies in graph cluster labels, connectome-network analysis, and spectral filtering-based analyses reveal robust neural correlates of empathy. These findings advance our understanding of empathy-related neural dynamics and identify specific regions in empathetic responses, offering insights for targeted interventions and treatments associated with empathetic processing."
https://arxiv.org/abs/2403.07088,2024-03-11,SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation,"['Yanming Liu', 'Xinyue Peng', 'Jiannan Cao', 'Le Dai', 'Xingzu Liu', 'Weihao Liu', 'Mingbang Wang']","Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but low computational devices while leave the parameters containing general information on the high computational devices."
https://arxiv.org/abs/2403.07087,2024-03-11,LSTM-Based Text Generation: A Study on Historical Datasets,"['Mustafa Abbas Hussein Hussein', 'Serkan Savaş']","This paper presents an exploration of Long Short-Term Memory (LSTM) networks in the realm of text generation, focusing on the utilization of historical datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical texts. The study demonstrates that LSTM-based models, when trained on historical datasets, can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time. The finding presents models that are highly accurate and efficient in predicting text from works of Nietzsche, with low loss values and a training time of 100 iterations. The accuracy of the model is 0.9521, indicating high accuracy. The loss of the model is 0.2518, indicating its effectiveness. The accuracy of the model in predicting text from the work of Shakespeare is 0.9125, indicating a low error rate. The training time of the model is 100, mirroring the efficiency of the Nietzsche dataset. This efficiency demonstrates the effectiveness of the model design and training methodology, especially when handling complex literary texts. This research contributes to the field of natural language processing by showcasing the versatility of LSTM networks in text generation and offering a pathway for future explorations in historical linguistics and beyond."
https://arxiv.org/abs/2403.07086,2024-03-11,Dual Role of Accretion Disk Winds as X-ray Obscurers and UV Line Absorbers in AGN,"['Keigo Fukumura', 'Missagh Mehdipour', 'Ehud Behar', 'Chris Shrader', 'Mauro Dadina', 'Demosthenes Kazanas', 'Stefano Marchesi', 'Francesco Tombesi']","X-ray obscuration of active galactic nuclei (AGNs) is considered in the context of ionized winds of stratified structure launched from accretion disks. We argue that a Compton-thick layer of a large-scale disk wind can obscure continuum X-rays and also lead to broad UV absorption such as in the blue wing of Civ; the former originates from the inner wind while the latter from the outer wind as a dual role. Motivated by a number of observational evidence showing strong AGN obscuration phenomena in Seyfert 1 AGNs, we demonstrate in this work, by utilizing a physically-motivated wind model coupled to post-process radiative transfer calculations, that an extended disk wind under certain physical conditions (e.g. morphology and density) could naturally cause a sufficient obscuration qualitatively consistent with UV/X-ray observations. Predicted UV/X-ray correlation is also presented as a consequence of variable spatial size of the wind in this scenario."
https://arxiv.org/abs/2403.07085,2024-03-11,Enveloping balls of Szlenk derivations,"['Tomasz Kochanek', 'Marek Miarka']","For certain reflexive Banach spaces with unconditional bases, we provide estimates for the radii of the enveloping balls of the $\varepsilon$-Szlenk derivations of the dual unit ball."
https://arxiv.org/abs/2403.07084,2024-03-11,"Is the effective potential, effective for dynamics?","['Nathan Herring', 'Shuyang Cao', 'Daniel Boyanovsky']","We critically examine the applicability of the effective potential within dynamical situations and find, in short, that the answer is negative. An important caveat of the use of an effective potential in dynamical equations of motion is an explicit violation of energy conservation."
https://arxiv.org/abs/2403.07083,2024-03-11,HST Survey of the Orion Nebula Cluster in ACS/Visible and WFC3/IR Bands. IV. A Bayesian multicolor study of stellar parameters in the ONC,"['Giovanni M. Strampelli', 'Massimo Robberto', 'Laurent Pueyo', 'Mario Gennaro', 'Carlo F. Manara', 'Elena Sabbi', 'Antonio Aparicio']","We have performed a comprehensive study of the Orion Nebula Cluster (ONC) combining the photometric data obtained by the two \textit{HST} Treasury programs that targeted this region. To consistently analyze the rich dataset obtained in a wide variety of filters, we adopted a Bayesian approach to fit the Spectral Energy Distribution of the sources, deriving mass, age, extinction, distance, and accretion for each source in the region. The three dimensional study of mass distribution for bona-fide cluster members shows that mass segregation in the ONC extends to sub-solar masses, while the age distribution strongly supports the idea that star formation in the ONC is best described by a major episode of star formation that happened $\sim 1$ Myr ago. For masses $\gtrsim 0.1$ \Msun, our derived empirical initial mass function (IMF) is in good agreement with a Chabrier system IMF. Both the accretion luminosity (\Lacc) and mass accretion rates (\dMacc) are best described by broken power-law relations. This suggests that for the majority of young circumstellar disks in this cluster the excess emission may be dominated by X-ray-driven photoevaporation by the central star rather than external photoevaporation. If this is the case, the slopes of the power-law relations may be largely determined by the initial conditions set at the onset of the star formation process, which may be quite similar between regions that eventually form clusters of different sizes."
https://arxiv.org/abs/2403.07082,2024-03-11,Exploring the Impact of ChatGPT on Student Interactions in Computer-Supported Collaborative Learning,"['Han Kyul Kim', 'Shriniwas Nayak', 'Aleyeh Roknaldin', 'Xiaoci Zhang', 'Marlon Twyman', 'Stephen Lu']","The growing popularity of generative AI, particularly ChatGPT, has sparked both enthusiasm and caution among practitioners and researchers in education. To effectively harness the full potential of ChatGPT in educational contexts, it is crucial to analyze its impact and suitability for different educational purposes. This paper takes an initial step in exploring the applicability of ChatGPT in a computer-supported collaborative learning (CSCL) environment. Using statistical analysis, we validate the shifts in student interactions during an asynchronous group brainstorming session by introducing ChatGPT as an instantaneous question-answering agent."
https://arxiv.org/abs/2403.07081,2024-03-11,Viscous current-induced forces,"['Vladimir U. Nazarov', 'Tchavdar N. Todorov', 'E. K. U. Gross']","We study the motion (translational, vibrational, and rotational) of a diatomic impurity immersed in an electron liquid and exposed to electronic current. An approach based on the linear response time-dependent density functional theory combined with the Ehrenfest dynamics leads to a system of linear algebraic equations, which account for the competing and counteracting effects of the current-induced force (electron wind) and the electronic friction. These forces, by means of the dynamic exchange-correlation kernel $f_{xc}({\bf r},{\bf r}',ω)$, include the electronic viscosity contribution. Starting from the ground state at the equilibrium inter-nuclear distance and applying a current pulse, we observe three phases of the motion: (I) acceleration due to the prevalence of the current-induced force, (II) stabilization upon balancing of the two forces, and (III) deceleration due to the friction after the end of the pulse. The viscous contribution to the force largely increases the acceleration (deceleration) at the first (third) phase of the process. For the aluminium HEG electron density, we find this correction to amount to up to 70% of the total electron wind and friction effects."
https://arxiv.org/abs/2403.07080,2024-03-11,Kazhdan-Lusztig map and Langlands duality,['Anlong Chua'],"Let $G$ be a connected reductive group over $\mathbb{C}$ with Weyl group $W$. Following a suggestion of Bezrukavnikov, we define a map from two-sided cells to conjugacy classes in $W$ using the geometry of the affine flag variety. This is an affine analog of the classical story of two-sided cells of $W$, special nilpotent orbits and special representations of $W$. The proof involves studying Springer representations appearing in the cohomology of affine Springer fibers. This relies on tools developed by Yun in his papers on Global Springer theory. Our result provides evidence for a conjecture of Lusztig on strata in a connected reductive group."
https://arxiv.org/abs/2403.07079,2024-03-11,Holography and Regge Phases with $U(1)$ Charge,"['Giulia Fardelli', 'A. Liam Fitzpatrick', 'Wei Li']","We use holography to study the large spin $J$ limit of the spectrum of low energy states with charge $Q$ under a $U(1)$ conserved current in CFTs in $d>2$ dimensions, with a focus on $d=3$ and $d=4$. For $Q=2$, the spectrum of such states is known to be universal and properly captured by the long-distance limit of holographic theories, regardless of whether the CFT itself is holographic. We study in detail the holographic description of such states at $Q>2$, by considering the contribution to the energies of $Q$ scalar particles coming from single photon and graviton exchange in the bulk of AdS; in some cases, scalar exchange and bulk contact terms are also included. For a range of finite values of $Q$ and $J$, we numerically diagonalize the Hamiltonian for such states and examine the resulting spectrum and wavefunctions as a function of the dimension $Δ$ of the charge-one operator and the central charges $c_{\mathcal{T}}, c_{\mathcal{J}}$ of the stress tensor and U(1) current, finding multiple regions in parameter space with qualitatively different behavior. We discuss the extension of these results to the regime of parametrically large charge $Q$, as well as to what extent such results are expected to hold universally, beyond the limit of holographic CFTs. We compare our holographic computations to results from the conformal bootstrap for the $3d$ O(2) model at $Q=3$ and $Q=4$ and find excellent agreement."
https://arxiv.org/abs/2403.07078,2024-03-11,"Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning","['Fuseinin Mumuni', 'Alhassan Mumuni']","We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem. Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural network and cognitive architectures offer powerful new dimensions towards alleviating these problems. Prior knowledge is represented in appropriate forms and incorporated in deep learning frameworks to improve performance. Brain-inspired cognition methods use computational models that mimic the human mind to enhance intelligent behavior in artificial agents and autonomous robots. Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience-that is, to deepen human understanding on how the brain works in general, and how it handles these problems."
https://arxiv.org/abs/2403.07077,2024-03-11,"Porous aluminium decorated with rhodium nanoparticles, preparation and use as platform for UV plasmonics","['Shrobona Banerje', 'Luca Mattarozzi', 'Nicolo Maccaferri', 'Sandro Cattarin', 'Shukun Weng', 'Ali Douaki', 'German Lanzavecchia', 'Anastasiia Sapunova', 'Francesco DAmico', 'Qifei Ma', 'Yanqui Zou', 'Roman Krahne', 'Janina Kneipp', 'Denis Garoli']","There is a high current interest for novel plasmonic platforms and materials able to extend their applicability into the ultraviolet (UV) region of the electromagnetic spectrum. In the UV it is possible to explore spectral properties of biomolecules with small cross section in the visible spectral range. However, most used metals in plasmonics have their resonances at wavelengths > 350 nm. Aluminum and rhodium are two interesting candidate materials for UV plasmonics, and in this work we developed a simple and low-cost preparation of functional substrates based on nanoporous aluminum decorated with rhodium nanoparticles. We demonstrate that these functionalized nanoporous metal films can be exploited as plasmonic materials suitable for enhanced UV Raman spectroscopy"
https://arxiv.org/abs/2403.07076,2024-03-11,Mapping High-level Semantic Regions in Indoor Environments without Object Recognition,"['Roberto Bigazzi', 'Lorenzo Baraldi', 'Shreyas Kousik', 'Rita Cucchiara', 'Marco Pavone']","Robots require a semantic understanding of their surroundings to operate in an efficient and explainable way in human environments. In the literature, there has been an extensive focus on object labeling and exhaustive scene graph generation; less effort has been focused on the task of purely identifying and mapping large semantic regions. The present work proposes a method for semantic region mapping via embodied navigation in indoor environments, generating a high-level representation of the knowledge of the agent. To enable region identification, the method uses a vision-to-language model to provide scene information for mapping. By projecting egocentric scene understanding into the global frame, the proposed method generates a semantic map as a distribution over possible region labels at each location. This mapping procedure is paired with a trained navigation policy to enable autonomous map generation. The proposed method significantly outperforms a variety of baselines, including an object-based system and a pretrained scene classifier, in experiments in a photorealistic simulator."
https://arxiv.org/abs/2403.07075,2024-03-11,H2CO and CS in diffuse clouds: Excitation and abundance,"['Maryvonne Gerin', 'Harvey Liszt', 'Jerome Pety', 'Alexandre Faure']","To provide constraints on the chemical processes responsible for the observed columns of organic species, we used NOEMA to observe the sight line toward NRAO150 in the 2mm spectral window. We targeted the low excitation lines of o-H2CO 2(1,1)-1(1,0) and p-H2CO 2(0,2)-1(0,1) as well as the nearby transitions of CS(3-2) and c-C3H2. We combined these data with previous observations to determine the excitation conditions, column densities, and abundances relative to H2 in the different velocity components. We performed non-LTE radiative transfer calculations including collision cross sections with ortho and para H2 and with electrons. New collision cross sections with electrons were computed for ortho and para formaldehyde. The c-C3H2 line profiles are very similar to those of HCO+ and CCH, while the CS absorption features are narrower and mostly concentrated in two main velocity components at V = -17 and -10 km/s. H2CO absorption lines present an intermediate pattern with absorption in all velocity components but larger opacities in the two main velocity components. The ortho-to-para ratios of H2CO and c-C3H2 are consistent with the statistical value of 3. While the excitation temperature of all c-C3H2 velocity components is consistent with the CMB, the two strong components detected in CS show a clear excess over the CMB indicating that CS resides at higher densities than other species along this particular sightline, n(H2) ~ 2500 cm-3 while n(H2) < 500 cm-3 for the other velocity components. We detected faint absorption from o-H213CO and C34S allowing us to derive isotopic ratios: o-H2CO/o-H213CO = 61 and C32S/C34S = 24. The excitation of the 4.8GHz line of formaldehyde is sensitive to the electron fraction and its excitation temperature is predicted to be lower than the CMB at low and moderate electron fractions, x(e)< 6E-5, and to rise above the CMB at high electron fractions, > 1e-4."
https://arxiv.org/abs/2403.07074,2024-03-11,A novel Bayesian approach for decomposing the radio emission of quasars: I. Modelling the radio excess in red quasars,"['B. -H. Yue', 'P. N. Best', 'K. J. Duncan', 'G. Calistro-Rivera', 'L. K. Morabito', 'J. W. Petley', 'I. Prandoni', 'H. J. A. Röttgering', 'D. J. B. Smith']","Studies show that both radio jets from the active galactic nuclei (AGN) and the star formation (SF) activity in quasar host galaxies contribute to the quasar radio emission; yet their relative contributions across the population remain unclear. Here, we present an improved parametric model that allows us to statistically separate the SF and AGN components in observed quasar radio flux density distributions, and investigate how their relative contributions evolve with AGN bolometric luminosity ($L_\mathrm{bol}$) and redshift ($z$) using a fully Bayesian method. Based on the newest data from LOFAR Two-Metre Sky Survey Data Release 2, our model gives robust fitting results out to $z\sim4$, showing a quasar host galaxy SFR evolution that increases with bolometric luminosity and with redshift out to $z\sim4$. This differs from the global cosmic SFR density, perhaps due to the importance of galaxy mergers. The prevalence of radio AGN emissions increases with quasar luminosity, but has little dependence on redshift. Furthermore, our new methodology and large sample size allow us to subdivide our dataset to investigate the role of other parameters. Specifically, in this paper, we explore quasar colour and demonstrate that the radio excess in red quasars is due to an enhancement in AGN-related emission, since the host galaxy SF contribution to the total radio emission is independent of quasar colour. We also find evidence that this radio enhancement occurs mostly in quasars with weak or intermediate radio power."
https://arxiv.org/abs/2403.07073,2024-03-11,VLEIBot: A New 45-mg Swimming Microrobot Driven by a Bioinspired Anguilliform Propulsor,"['Elijah K. Blankenship', 'Conor K. Trygstad', 'Francisco M. F. R. Gonçalves', 'Néstor O. Pérez-Arancibia']","This paper presents the VLEIBot^* (Very Little Eel-Inspired roBot), a 45-mg/23-mm^3 microrobotic swimmer that is propelled by a bioinspired anguilliform propulsor. The propulsor is excited by a single 6-mg high-work-density (HWD) microactuator and undulates periodically due to wave propagation phenomena generated by fluid-structure interaction (FSI) during swimming. The microactuator is composed of a carbon-fiber beam, which functions as a leaf spring, and shape-memory alloy (SMA) wires, which deform cyclically when excited periodically using Joule heating. The VLEIBot can swim at speeds as high as 15.1mm * s^{-1} (0.33 Bl * s^{-1}}) when driven with a heuristically-optimized propulsor. To improve maneuverability, we evolved the VLEIBot design into the 90-mg/47-mm^3 VLEIBot^+, which is driven by two propulsors and fully controllable in the two-dimensional (2D) space. The VLEIBot^+ can swim at speeds as high as 16.1mm * s^{-1} (0.35 Bl * s^{-1}), when driven with heuristically-optimized propulsors, and achieves turning rates as high as 0.28 rad * s^{-1}, when tracking path references. The measured root-mean-square (RMS) values of the tracking errors are as low as 4 mm."
https://arxiv.org/abs/2403.07072,2024-03-11,Explainable Learning with Gaussian Processes,"['Kurt Butler', 'Guanchao Feng', 'Petar M. Djuric']","The field of explainable artificial intelligence (XAI) attempts to develop methods that provide insight into how complicated machine learning methods make predictions. Many methods of explanation have focused on the concept of feature attribution, a decomposition of the model's prediction into individual contributions corresponding to each input feature. In this work, we explore the problem of feature attribution in the context of Gaussian process regression (GPR). We take a principled approach to defining attributions under model uncertainty, extending the existing literature. We show that although GPR is a highly flexible and non-parametric approach, we can derive interpretable, closed-form expressions for the feature attributions. When using integrated gradients as an attribution method, we show that the attributions of a GPR model also follow a Gaussian process distribution, which quantifies the uncertainty in attribution arising from uncertainty in the model. We demonstrate, both through theory and experimentation, the versatility and robustness of this approach. We also show that, when applicable, the exact expressions for GPR attributions are both more accurate and less computationally expensive than the approximations currently used in practice. The source code for this project is freely available under MIT license at https://github.com/KurtButler/2024_attributions_paper."
https://arxiv.org/abs/2403.07071,2024-03-11,LISO: Lidar-only Self-Supervised 3D Object Detection,"['Stefan Baur', 'Frank Moosmann', 'Andreas Geiger']","3D object detection is one of the most important components in any Self-Driving stack, but current state-of-the-art (SOTA) lidar object detectors require costly & slow manual annotation of 3D bounding boxes to perform well. Recently, several methods emerged to generate pseudo ground truth without human supervision, however, all of these methods have various drawbacks: Some methods require sensor rigs with full camera coverage and accurate calibration, partly supplemented by an auxiliary optical flow engine. Others require expensive high-precision localization to find objects that disappeared over multiple drives. We introduce a novel self-supervised method to train SOTA lidar object detection networks which works on unlabeled sequences of lidar point clouds only, which we call trajectory-regularized self-training. It utilizes a SOTA self-supervised lidar scene flow network under the hood to generate, track, and iteratively refine pseudo ground truth. We demonstrate the effectiveness of our approach for multiple SOTA object detection networks across multiple real-world datasets. Code will be released."
https://arxiv.org/abs/2403.07070,2024-03-11,"Retail Central Bank Digital Currency: Motivations, Opportunities, and Mistakes","['Geoffrey Goodell', 'Hazem Danny Al-Nakib', 'Tomaso Aste']","Nations around the world are conducting research into the design of central bank digital currency (CBDC), a new, digital form of money that would be issued by central banks alongside cash and central bank reserves. Retail CBDC would be used by individuals and businesses as form of money suitable for routine commerce. An important motivating factor in the development of retail CBDC is the decline of the popularity of central bank money for retail purchases and the increasing use of digital money created by the private sector for such purposes. The debate about how retail CBDC would be designed and implemented has led to many proposals, which have sparked considerable debate about business models, regulatory frameworks, and the socio-technical role of money in general. Here, we present a critical analysis of the existing proposals. We examine their motivations and themes, as well as their underlying assumptions. We also offer a reflection of the opportunity that retail CBDC represents and suggest a way forward in furtherance of the public interest."
https://arxiv.org/abs/2403.07069,2024-03-11,The quantum Hall effect under the influence of gravity and inertia: A unified approach,"['Alexandre Landry', 'Fayçal Hammad', 'Reza Saadati']","The quantum Hall effect under the influence of gravity and inertia is studied in a unified way. We make use of an algebraic approach, as opposed to an analytic approach. We examine how both the integer and the fractional quantum Hall effects behave under a combined influence of gravity and inertia using a unified Hamiltonian. For that purpose, we first re-derive, using the purely algebraic method, the energy spectrum of charged particles moving in a plane perpendicular to a constant and uniform magnetic field either (i) under the influence of a nonlinear gravitational potential or (ii) under the influence of a constant rotation. The general Hamiltonian for describing the combined effect of gravity, rotation and inertia on the electrons of a Hall sample is then built and the eigenstates are obtained. The electrons mutual Coulomb interaction that gives rise to the familiar fractional quantum Hall effect is also discussed within a such a combination."
https://arxiv.org/abs/2403.07068,2024-03-11,Multiset tomography: Optimizing quantum measurements by partitioning multisets of observables,"['Otto Veltheim', 'Esko Keski-Vakkuri']","Quantum tomography approaches typically consider a set of observables which we wish to measure, design a measurement scheme which measures each of the observables and then repeats the measurements as many times as necessary. We show that instead of considering only the simple set of observables, one should consider a multiset of the observables taking into account the required repetitions, to minimize the number of measurements. This leads to a graph theoretic multicolouring problem. We show that multiset tomography offers at most quadratic improvement but it is achievable. Furthermore, despite the NP-hard optimal colouring problem, the multiset approach with greedy colouring algorithms already offers asymptotically quadratic improvement in test cases."
https://arxiv.org/abs/2403.07067,2024-03-11,Bayesian Bell regression model for fitting of overdispersed count data with application,"['Ameer Musa Imran Alhseeni', 'Hossein Bevrani']","The Bell regression model (BRM) is a statistical model that is often used in the analysis of count data that exhibits overdispersion. In this study, we propose a Bayesian analysis of the BRM and offer a new perspective on its application. Specifically, we introduce a G-prior distribution for Bayesian inference in BRM, in addition to a flat-normal prior distribution. To compare the performance of the proposed prior distributions, we conduct a simulation study and demonstrate that the G-prior distribution provides superior estimation results for the BRM. Furthermore, we apply the methodology to real data and compare the BRM to the Poisson regression model using various model selection criteria. Our results provide valuable insights into the use of Bayesian methods for estimation and inference of the BRM and highlight the importance of considering the choice of prior distribution in the analysis of count data."
https://arxiv.org/abs/2403.07066,2024-03-11,Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models,"['Philip Harris', 'Michael Kagan', 'Jeffrey Krupa', 'Benedikt Maier', 'Nathaniel Woodward']","Self-Supervised Learning (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks. However, SSL strategies must be adapted to the type of training data and downstream tasks required. We propose RS3L, a novel simulation-based SSL strategy that employs a method of re-simulation to drive data augmentation for contrastive learning. By intervening in the middle of the simulation process and re-running simulation components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator. Using experiments from high-energy physics, we explore how this strategy may enable the development of a foundation model; we show how R3SL pre-training enables powerful performance in downstream tasks such as discrimination of a variety of objects and uncertainty mitigation. In addition to our results, we make the RS3L dataset publicly available for further studies on how to improve SSL strategies."
https://arxiv.org/abs/2403.07065,2024-03-11,On classical de Sitter solutions and parametric control,"['David Andriot', 'Fabian Ruehle']","Finding string backgrounds with de Sitter spacetime, where all approximations and corrections are controlled, is an open problem. We revisit the search for de Sitter solutions in the classical regime for specific type IIB supergravity compactifications on group manifolds, an under-explored corner of the landscape that offers an interesting testing ground for swampland conjectures. While the supergravity de Sitter solutions we obtain numerically are ambiguous in terms of their classicality, we find an analytic scaling that makes four out of six compactification radii, as well as the overall volume, arbitrarily large. This potentially provides parametric control over corrections. If we could show that these solutions, or others to be found, are fully classical, they would constitute a counterexample to conjectures stating that asymptotic de Sitter solutions do not exist. We discuss this point in great detail."
https://arxiv.org/abs/2403.07064,2024-03-11,Regge Limit of One-Loop String Amplitudes,"['Pinaki Banerjee', 'Lorenz Eberhardt', 'Sebastian Mizera']","We study the high-energy limit of $2 \to 2$ one-loop string amplitudes at fixed momentum transfer. For the closed string, the high-energy behaviour of the amplitudes can be determined from Regge theory just like in field theory, as was first discussed by Amati, Ciafaloni and Veneziano. However, field theory intuition partially breaks down for the open-string amplitude, where amplitudes can exhibit surprising asymptotics in the high-energy limit depending on the topology of the diagram. We call this phenomenon Regge attenuation. We extract Regge limits by a combination of unitarity cuts and saddle-point analysis. We show that the leading contribution of the planar open-string amplitude is sufficiently simple that we can extract it at any loop order. This allows us to resum the genus expansion in a certain limit and demonstrate that the leading Regge trajectory remains linear in that limit."
https://arxiv.org/abs/2403.07063,2024-03-11,Probing Lorentz Invariance Violation with Absorption of Astrophysical Gamma-rays by Solar Photons,"['Justin D. Finke', 'Parshad Patel']","We compute in detail the absorption optical depth for astrophysical $γ$-ray photons interacting with solar photons to produce electron positron pairs. This effect is greatest for $γ$-ray sources at small angular distances from the Sun, reaching optical depths as high as $τ_{γγ}\sim 10^{-2}$. We also calculate this effect including modifications to the absorption cross section threshold from subluminal Lorentz invariance violation (LIV). We show for the first time that subluminal LIV can lead to increases or decreases in $τ_{γγ}$ compared to the non-LIV case. We show that, at least in principle, LIV can be probed with this effect with observations of $γ$-ray sources near the Sun at $\gtrsim20$ TeV by HAWC or LHAASO, although a measurement will be extremely difficult due to the small size of the effect."
https://arxiv.org/abs/2403.07062,2024-03-11,CANUCS: An Updated Mass and Magnification Model of Abell 370 with JWST,"['Rachel Gledhill', 'Victoria Strait', 'Guillaume Desprez', 'Gregor Rihtaršič', 'Maruša Bradač', 'Gabriel Brammer', 'Chris J. Willott', 'Nicholas Martis', 'Marcin Sawicki', 'Gaël Noirot', 'Ghassan T. E. Sarrouh', 'Adam Muzzin']","We report an updated mass and magnification model of galaxy cluster Abell 370 using new NIRCam and NIRISS data from the CAnadian NIRISS Unbiased Cluster Survey (CANUCS). Using Lenstool and a combination of archival HST and MUSE data with new JWST data as constraints, we derive an improved gravitational lensing model and extract magnifications of background galaxies with uncertainties. Using our best fit model, we perform a search for new multiply imaged systems via predicted positions. We report no new multiply imaged systems with identifiable redshifts, likely due to already very deep HST and Spitzer data, but confirm a $z\sim8$ multiply imaged system by measuring its redshift with NIRISS and NIRSpec spectra. We find that the overall shape of the critical curve for a source at $z = 9.0$ is similar to previous models of Abell 370, with small changes. We investigate the $z\sim8$ galaxy with two images observable with an apparent magnitude in the F125W band of $26.0\pm0.2$ and $25.6\pm0.1$. After correcting for the magnifications of the images, 7.2$^{+0.2}_{-1.2}$ and 8.7$^{+0.4}_{-0.4}$, we use SED fitting to find an intrinsic stellar mass of log($M^*/M_{\odot})$ = 7.35$^{+0.04}_{-0.05}$, intrinsic SFR of 3.5$^{+2.2}_{-1.4}$ M$_{\odot}$/yr, and $M_{UV}$ of -21.3$^{+0.2}_{-0.2}$, which is close to the knee of the luminosity function at that redshift. Our model, and corresponding magnification, shear, and convergence maps are available on request and will be made publicly available on MAST in a CANUCS data release (DOI: 10.17909/ph4n-6n76)."
https://arxiv.org/abs/2403.07061,2024-03-13,Simulating Meson Scattering on Spin Quantum Simulators,"['Elizabeth R. Bennewitz', 'Brayden Ware', 'Alexander Schuckert', 'Alessio Lerose', 'Federica M. Surace', 'Ron Belyansky', 'William Morong', 'De Luo', 'Arinjoy De', 'Kate S. Collins', 'Or Katz', 'Christopher Monroe', 'Zohreh Davoudi', 'Alexey V. Gorshkov']","Studying high-energy collisions of composite particles, such as hadrons and nuclei, is an outstanding goal for quantum simulators. However, preparation of hadronic wave packets has posed a significant challenge, due to the complexity of hadrons and the precise structure of wave packets. This has limited demonstrations of hadron scattering on quantum simulators to date. Observations of confinement and composite excitations in quantum spin systems have opened up the possibility to explore scattering dynamics in spin models. In this article, we develop two methods to create entangled spin states corresponding to wave packets of composite particles in analog quantum simulators of Ising spin Hamiltonians. One wave-packet preparation method uses the blockade effect enabled by beyond-nearest-neighbor Ising spin interactions. The other method utilizes a quantum-bus-mediated exchange, such as the native spin-phonon coupling in trapped-ion arrays. With a focus on trapped-ion simulators, we numerically benchmark both methods and show that high-fidelity wave packets can be achieved in near-term experiments. We numerically study scattering of wave packets for experimentally realizable parameters in the Ising model and find inelastic-scattering regimes, corresponding to particle production in the scattering event, with prominent and distinct experimental signals. Our proposal, therefore, demonstrates the potential of observing inelastic scattering in near-term quantum simulators."
https://arxiv.org/abs/2403.07060,2024-03-11,Scrutinising evidence for the triggering of Active Galactic Nuclei in the outskirts of massive galaxy clusters at $z\approx1$,"['Iván Muñoz Rodríguez', 'Antonis Georgakakis', 'Francesco Shankar', 'Ángel Ruiz', 'Silvia Bonoli', 'Johan Comparat', 'Elias Koulouridis', 'Andrea Lapi', 'Cristina Ramos Almeida']","Environmental effects are believed to play an important yet poorly understood role in triggering accretion events onto the supermassive black holes (SMBHs) of galaxies (Active Galactic Nuclei; AGN). Massive clusters, which represent the densest structures in the Universe, provide an excellent laboratory to isolate environmental effects and study their impact on black hole growth. In this work, we critically review observational evidence for the preferential activation of SMBHs in the outskirts of galaxy clusters. We develop a semi-empirical model under the assumption that the incidence of AGN in galaxies is independent of environment. We demonstrate that the model is broadly consistent with recent observations on the AGN halo occupation at $z=0.2$, although it may overpredict satellite AGN in massive halos at that low redshift. We then use this model to interpret the projected radial distribution of X-ray sources around high redshift ($z\approx1$) massive ($>5 \times 10^{14} \, M_\odot$) clusters, which show excess counts outside their virial radius. Such an excess naturally arises in our model as a result of sample variance. Up to 20% of the simulated projected radial distributions show excess counts similar to the observations, which are however, because of background/foreground AGN and hence, not physically associated with the cluster. Our analysis emphasises the importance of projection effects and shows that current observations of $z\approx1$ clusters remain inconclusive on the activation of SMBHs during infall."
https://arxiv.org/abs/2403.07059,2024-03-11,Better than classical? The subtle art of benchmarking quantum machine learning models,"['Joseph Bowles', 'Shahnawaz Ahmed', 'Maria Schuld']","Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that ""quantumness"" may not be the crucial ingredient for the small learning tasks considered here. Our benchmarks also unlock investigations beyond simplistic leaderboard comparisons, and we identify five important questions for quantum model design that follow from our results."
https://arxiv.org/abs/2403.07058,2024-03-11,The ALMA Survey of 70 $μ$m Dark High-mass Clumps in Early Stages (ASHES). XI. Statistical Study of Early Fragmentation,"['Kaho Morii', 'Patricio Sanhueza', 'Qizhou Zhang', 'Fumitaka Nakamura', 'Shanghuo Li', 'Giovanni Sabatini', 'Fernando A. Olguin', 'Henrik Beuther', 'Daniel Tafoya', 'Natsuko Izumi', ""Ken'ichi Tatematsu"", 'Takeshi Sakai']","Fragmentation during the early stages of high-mass star formation is crucial for understanding the formation of high-mass clusters. We investigated fragmentation within thirty-nine high-mass star-forming clumps as part of the Atacama Large Millimeter/submillimeter Array (ALMA) Survey of 70 $μ$m Dark High-mass Clumps in Early Stages (ASHES). Considering projection effects, we have estimated core separations for 839 cores identified from the continuum emission and found mean values between 0.08 and 0.32 pc within each clump. We find compatibility of the observed core separations and masses with the thermal Jeans length and mass, respectively. We also present sub-clump structures revealed by the 7 m-array continuum emission. Comparison of the Jeans parameters using clump and sub-clump densities with the separation and masses of gravitationally bound cores suggests that they can be explained by clump fragmentation, implying the simultaneous formation of sub-clumps and cores within rather than a step-by-step hierarchical fragmentation. The number of cores in each clump positively correlates with the clump surface density and the number expected from the thermal Jeans fragmentation. We also find that the higher the fraction of protostellar cores, the larger the dynamic range of the core mass, implying that the cores are growing in mass as the clump evolves. The ASHES sample exhibits various fragmentation patterns: aligned, scattered, clustered, and sub-clustered. Using the Q-parameter, which can help to distinguish between centrally condensed and subclustered spatial core distributions, we finally find that in the early evolutionary stages of high-mass star formation, cores tend to follow a subclustered distribution."
https://arxiv.org/abs/2403.07057,2024-03-11,On the survival of the long-lived inner disk of PDS 70,"['Paola Pinilla', 'Myriam Benisty', 'Rens Waters', 'Jaehan Bae', 'Stefano Facchini']","PDS 70 remains as the best laboratory to investigate the influence of giant planet formation on the structure of the parental disk. One of the most intriguing discoveries is the detection of a resolved inner disk from ALMA observations, extending up to the orbit of PDS 70b. This inner disk is challenging to explain because most of the dust particles are expected to be trapped at the outer edge of the gap open by PDS 70b and PDS 70c. By performing dust evolution models and radiative transfer simulations that match the gas disk masses obtained from recent thermochemical models of PDS 70, we find that when the minimum grain size in the models is larger than 0.1$μ$m, there is an efficient filtration of dust particles, and the inner disk is depleted during the first million-year of dust evolution. Therefore, to maintain an inner disk, the minimum grain size in the models needs to be smaller than 0.1$μ$m. Only when grains are that small, they are diffused and dragged along with the gas throughout the planets' gap. The small grains transported in the inner disk grow and drift therein, but the constant reservoir of dust particles that are trapped in the outer edge of the gap and that are continuously fragmenting allows refilling the inner disk on million-year timescales. Our flux predictions at millimetre wavelength of these models agree with ALMA observations. These models predict a spectral index of 3.2 in the outer disk and 3.6 in the inner disk. Our simple analytical calculations show what the inner disk water emission recently observed with JWST may originate from these ice-coated small grains that flow through the gap, grow and drift towards the innermost disk regions, reaching the water snowline. These models may mirror the history and evolution of our Solar System where Jupiter and Saturn played a crucial role shaping the architecture and properties of planets in our Solar System."
https://arxiv.org/abs/2403.07056,2024-03-11,Gravitational back-reaction is the Holographic Dual of Magic,"['ChunJun Cao', 'Gong Cheng', 'Alioscia Hamma', 'Lorenzo Leone', 'William Munizzi', 'Savatore F. E. Oliviero']","We study interplay between magic and entanglement in quantum many-body systems. We show that non-local magic which is supported by the quantum correlations is lower bounded by the flatness of entanglement spectrum and upper bounded by the amount of entanglement in the system. We then argue that a smoothed version of non-local magic bounds the hardness of classical simulations for incompressible states. In conformal field theories, we conjecture that the non-local magic should scale linearly with entanglement entropy but sublinearly when an approximation of the state is allowed. We support the conjectures using both analytical arguments based on unitary distillation and numerical data from an Ising CFT. If the CFT has a holographic dual, then we prove that the non-local magic vanishes if and only if there is no gravitational back-reaction. Furthermore, we show that non-local magic approximately equals the rate of change of minimal surface area in response to the change of the tension of cosmic branes in the bulk."
https://arxiv.org/abs/2403.07055,2024-03-11,"Orbital angular momentum of Bloch electrons: equilibrium formulation, magneto-electric phenomena, and the orbital Hall effect","['Rhonald Burgos Atencia', 'Amit Agarwal', 'Dimitrie Culcer']","The investigation of orbital angular momentum (OAM) of delocalised Bloch electrons has significantly advanced our understanding of magnetic, transport, and optical phenomena in crystals, drawing widespread interest across various materials science domains, from metals and semiconductors to topological and magnetic materials. Here, we review OAM dynamics in depth, mainly focusing on key concepts and non-equilibrium systems, laying the groundwork for the thriving field of {\it orbitronics}. We begin by dissecting the conventional understanding of equilibrium OAM and identify two primary contributions. The {\it atomic}-OAM contribution is rooted in the angular momentum of parent atomic orbitals and is prevalent in inversion-symmetric systems. The other {\it itinerant}-OAM contribution arises from the rotation of the electron wave packet around its center of mass and it is dominant in systems lacking inversion symmetry. Following this, we explore recent theoretical and experimental developments in out-of-equilibrium systems. We specifically focus on the generation of an OAM density via the orbital magneto-electric, or Edelstein effect, the generation of an OAM current via the orbital Hall effect, the orbital torque resulting from them, along with their reciprocal non-equilibrium counterparts -- the inverse orbital Edelstein and inverse orbital Hall effects, as well as OAM conservation. Beyond discussing the current excitement and challenges in this rapidly evolving field, we highlighted the future prospects of {\it orbitronics}."
https://arxiv.org/abs/2403.07054,2024-03-11,Minimal Fractional Topological Insulator in half-filled conjugate moiré Chern bands,"['Chao-Ming Jian', 'Cenke Xu']","We propose a ""minimal"" fractional topological insulator (mFTI), motivated by the recent experimental report on the signatures of FTI at total filling factor $ν_{\rm tot} = 3$ in a transition metal dichalcogenide (TMD) moiré system. The observed FTI at $ν_{\rm tot} = 3$ is likely given by a topological state living in a pair of half-filled conjugate Chern bands with Chern numbers $C=\pm 1$ on top of another pair of fully-filled conjugate Chern bands. We propose the mFTI as a strong candidate topological state in the half-filled conjugate Chern bands. The mFTI is characterized by the following features: (1) It is a fully gapped topological order with 32 Abelian anyons (including the electron); (2) the minimally-charged anyon carries electric charge $e^\ast = e/2$, together with the spin-Hall conductivity, implying that the mFTI's robust edge state remains gapless whenever time-reversal symmetry and charge conversation are present; (3) the mFTI is ""minimal"" in the sense that it has the smallest total quantum dimension (a metric for the topological order's complexity) within all the topological orders that can potentially be realized at the same electron filling of the system and with the same Hall transports; (4) the mFTI is the common descendant of multiple valley-decoupled ""product topological orders"" with larger quantum dimensions. It is also a symmetry-enriched topological order promoted from multiple symmetry-protected topological states, after gauging part of their symmetries."
https://arxiv.org/abs/2403.07053,2024-03-11,Dark Matter-induced electron excitations in silicon and germanium with Deep Learning,"['Riccardo Catena', 'Einar Urdshals']","We train a deep neural network (DNN) to output rates of dark matter (DM) induced electron excitations in silicon and germanium detectors. Our DNN provides a massive speedup of around $5$ orders of magnitude relative to existing methods (i.e. QEdark-EFT), allowing for extensive parameter scans in the event of an observed DM signal. The network is also lighter and simpler to use than alternative computational frameworks based on a direct calculation of the DM-induced excitation rate. The DNN can be downloaded $\href{https://github.com/urdshals/DEDD}{\text{here}}$."
https://arxiv.org/abs/2403.07052,2024-03-11,Instantaneous Response and Quantum Geometry of Insulators,"['Nishchhal Verma', 'Raquel Queiroz']","We present the time-dependent Quantum Geometric Tensor (tQGT) as a comprehensive tool for capturing the geometric character of insulators observable within linear response. We show that tQGT describes the zero-point motion of bound electrons and acts as a generating function for generalized sum rules of electronic conductivity. Therefore, tQGT enables a systematic and basis-independent framework to compute the instantaneous response of insulators, including optical mass, orbital angular momentum, and the dielectric constant in low-energy effective theories. It allows for a consistent approximation across these quantities upon restricting the number of occupied and unoccupied states in an effective low-energy description of an infinite quantum system. We outline how quantum geometry can be generated in periodic systems by lattice interference and examine spectral weight transfer from small frequencies to high frequencies by creating geometrically frustrated flat bands."
https://arxiv.org/abs/2403.07051,2024-03-11,Lorentzian contours for tree-level string amplitudes,"['Lorenz Eberhardt', 'Sebastian Mizera']","We engineer compact contours on the moduli spaces of genus-zero Riemann surfaces that achieve analytic continuation from Euclidean to Lorentzian worldsheets. These generalized Pochhammer contours are based on the combinatorics of associahedra and make the analytic properties of tree-level amplitudes entirely manifest for any number and type of external strings. We use them in practice to perform first numerical computations of open and closed string amplitudes directly in the physical kinematics for $n=4,5,6,7,8,9$. We provide a code that allows anyone to do such computations."
https://arxiv.org/abs/2403.07050,2024-03-13,Cosmological Amplitudes in Power-Law FRW Universe,"['Bingchu Fan', 'Zhong-Zhi Xianyu']","The correlators of large-scale fluctuations belong to the most important observables in modern cosmology. Recently, there have been considerable efforts in analytically understanding the cosmological correlators and the related wavefunction coefficients, which we collectively call cosmological amplitudes. In this work, we provide a set of simple rules to directly write down analytical answers for arbitrary tree-level amplitudes of conformal scalars with time-dependent interactions in power-law FRW universe. With the recently proposed family-tree decomposition method, we identify an over-complete set of multivariate hypergeometric functions, called family trees, to which all tree-level conformal scalar amplitudes can be easily reduced. Our method yields series expansions and monodromies of family trees in various kinematic limits, together with a large number of functional identities. The family trees are in a sense generalizations of polylogarithms and do reduce to polylogarithmic expressions for the cubic coupling in inflationary limit. We further show that all family trees can be decomposed into linear chains by taking shuffle products of all subfamilies, with which we find simple connection between bulk time integrals and boundary energy integrals."
https://arxiv.org/abs/2403.07049,2024-03-11,Firewalls at exponentially late times,"['Andreas Blommaert', 'Chang-Han Chen', 'Yasunori Nomura']","We consider a version of the typical state firewall setup recently reintroduced by Stanford and Yang, who found that wormholes may create firewalls. We examine a late-time double scaling limit in JT gravity in which one can resum the expansion in the number of wormholes, and we use this to study the exact distribution of interior slices at times exponential in the entropy. We consider a thermofield double with and without early perturbations on a boundary. These perturbations can appear on interior slices as dangerous high energy shocks. For exponentially late times, wormholes tend to teleport the particles created by perturbations and render the interior more dangerous. In states with many perturbations separated by large times, the probability of a safe interior is exponentially small. Such states thus almost certainly have firewalls at the horizon, even though they would be safe without wormholes. With perturbation, even in the safest state we conceive, the odds of encountering a firewall are fifty-fifty. One interpretation of the phenomena found here is that wormholes can change time-ordered contours into effective out-of-time-ordered folds, making shockwaves appear in unexpected places."
https://arxiv.org/abs/2403.07048,2024-03-11,The Peak Frequency and Luminosity of Synchrotron Emitting Shocks: from Non-Relativistic to Ultra-Relativistic Explosions,"['Ben Margalit', 'Eliot Quataert']","Synchrotron emission is ubiquitous in explosive astrophysical events -- it is a natural byproduct of shocks formed when matter expelled by the explosion collides with ambient material. This emission is well-observed in various classes of transients, and is often interpreted within a canonical `equipartition' framework that allows physical properties of the shock to be inferred from the frequency and luminosity at which the observed spectral energy distribution (SED) peaks. This framework has been remarkably successful in explaining observations of radio supernovae. It has also been used for trans-relativistic explosions, where the shock velocities approach the speed of light. However, the conventional framework does not incorporate relativistic effects. Neither does it account for thermal electrons, which have been shown to be important for high-velocity shocks. In this paper we describe a revised framework that accounts for these two effects, and is applicable to non-relativistic, trans-relativistic, and ultra-relativistic explosions. We show that accounting for these effects can dramatically change the inferred parameters of high-velocity shocks, and in particular -- that the shock velocity, ambient density, and total energy are overestimated by the conventional non-relativistic framework. We delineate the phase-space where such modifications are important in terms of observationally measurable parameters. We also find a novel upper limit on the peak synchrotron luminosity of shock-powered transients, which is remarkably consistent with existing observations. Finally, we discuss a prediction of the model -- that the SED will qualitatively change as a function of shock velocity -- and show that this is broadly consistent with data for representative events (e.g., SN1998bw, AT2018cow, CSS161010, AT2020xnd)."
https://arxiv.org/abs/2403.07047,2024-03-11,Non-thermal emission from mildly relativistic dynamical ejecta of neutron star mergers: spectrum and sky image,"['Gilad Sadeh', 'Noya Linder', 'Eli Waxman']","Binary neutron star mergers are expected to produce fast dynamical ejecta, with mildly relativistic velocities extending to $β=v/c>0.6$. In a preceding paper, we derived an analytic description of the time-dependent radio to X-ray synchrotron flux produced by collisionless shocks driven by such fast ejecta into the interstellar medium, for spherical ejecta with broken power-law mass (or energy) distributions, $M(>γβ)\propto(γβ)^{-s}$ with $s=s_\text{KN}$ at $γβ<γ_0β_0$ and $s=s_\text{ft}$ at $γβ>γ_0β_0$ (where $γ$ is the Lorentz factor). Here, we extend our analysis and provide analytic expressions for the self-absorption frequency, the cooling frequency, and the observed angular size of the emitting region (which appears as a ring in the sky). For parameter values characteristic of merger calculation results -- a ""shallow"" mass distribution, $1<s_\text{KN}<3$, for the bulk of the ejecta (at $γβ\approx0.2$), and a steep, $s_\text{ft}>5$, ""fast tail"" mass distribution -- the analytic results reproduce well (to tens of percent accuracy) the results of detailed numeric calculations, a significant improvement over earlier order-of-magnitude estimates (based on extrapolations of results valid for $γβ\ll1$)."
https://arxiv.org/abs/2403.07046,2024-03-11,Lagrangian Perturbation Theory for Biased Tracers: Halo numbers are conserved?,"['Peter Espenshade', 'Jaiyul Yoo']","The Lagrangian perturbation theory (LPT) provides a simple yet powerful way of computing the nonlinear matter power spectrum, and it has been applied to biased tracers such as halos and galaxies. The number conservation of matter particles allows a simple relation between the fluctuations at the initial and the late times, which is essential in deriving the exact expression for the nonlinear matter power spectrum. Biased tracers in contrast evolve through mergers and accretion, violating the assumption of number conservation. Furthermore, a sample of halos with a narrow mass bin at initial time evolves to become halos with very different masses at late time, making it difficult to directly connect predictions of the LPT to real observations, as observed samples have similar properties. Here we use $N$-body simulations to test these core predictions of the LPT for biased tracers and demonstrate that the LPT predictions for halos overestimates the power spectrum at $z=0$ by a factor of three for the mass bin sample $Δ\log M_h~(h^{-1}M_{\odot})= 0.5$ at~$z\simeq 3$, while the predictions can be made consistent if we impose by hand the number conservation of halos in the simulations throughout the entire evolution. In reality, LPT modeling of biased tracers involves marginalization of unknown bias parameters, alleviating the problem. We discuss the implications for field-level models based on the LPT applications."
https://arxiv.org/abs/2403.07045,2024-03-11,The ďAlembert solution in hyperboloidal foliations,"['Juan A. Valiente Kroon', 'Lidia J. Gomes Da Silva']","We explicitly construct the analogue of the ďAlembert solution to the 1+1 wave equation in an hyperboloidal setting. This hyperboloidal ďAlembert solution is used, in turn, to gain intuition into the behaviour of solutions to the wave equation in a hyperboloidal foliation and to explain some apparently anomalous behaviour observed in numerically constructed solutions discussed in the literature."
https://arxiv.org/abs/2403.07044,2024-03-11,The Effect of Quark-antiquark Confinement on the Deflection Angle by the NED Black Hole,"['Erdem Sucu', 'Ali Övgün']","In this study, we explore the influence of quark-antiquark confinement on the deflection angle within the framework of nonlinear electrodynamic (NED) black holes. To achieve this, we establish the appropriate optical spacetime metric and subsequently determine the Gaussian optical curvature. Utilizing the Gauss-Bonnet theorem, we investigate the impact of quark-antiquark confinement on the deflection angle exhibited by NED black holes. Additionally, we delve into the effects of a cold non-magnetized plasma medium and also axion-plasmon on gravitational lensing. Our findings highlight the significance of the axion-plasmon effect on the optical properties of NED black holes, particularly its influence on gravitational lensing. This exploration is particularly relevant in the context of the axion's potential role as a dark matter candidate. The multifaceted interplay between quark-antiquark confinement, nonlinear electrodynamics, and plasma dynamics provides a nuanced understanding of gravitational lensing phenomena. These insights contribute to ongoing research in dark matter studies and offer avenues for further theoretical and observational investigations in astrophysics."
https://arxiv.org/abs/2403.07043,2024-03-11,A Collision Cone Approach for Control Barrier Functions,"['Manan Tayal', 'Bhavya Giri Goswami', 'Karthik Rajgopal', 'Rajpal Singh', 'Tejas Rao', 'Jishnu Keshavan', 'Pushpak Jagtap', 'Shishir Kolathaya']","This work presents a unified approach for collision avoidance using Collision-Cone Control Barrier Functions (CBFs) in both ground (UGV) and aerial (UAV) unmanned vehicles. We propose a novel CBF formulation inspired by collision cones, to ensure safety by constraining the relative velocity between the vehicle and the obstacle to always point away from each other. The efficacy of this approach is demonstrated through simulations and hardware implementations on the TurtleBot, Stoch-Jeep, and Crazyflie 2.1 quadrotor robot, showcasing its effectiveness in avoiding collisions with dynamic obstacles in both ground and aerial settings. The real-time controller is developed using CBF Quadratic Programs (CBF-QPs). Comparative analysis with the state-of-the-art CBFs highlights the less conservative nature of the proposed approach. Overall, this research contributes to a novel control formation that can give a guarantee for collision avoidance in unmanned vehicles by modifying the control inputs from existing path-planning controllers."
https://arxiv.org/abs/2403.07042,2024-03-11,Supercomputer model of finite-dimensional quantum electrodynamics applications,"['Wanshun Li', 'Hui-hui Miao', 'Yuri Igorevich Ozhigov']","A general scheme is given for supercomputer simulation of quantum processes, which are described by various modifications of finite-dimensional cavity quantum electrodynamics models, including Jaynes-Cummings-Hubbard model and Tavis-Cummings-Hubbard model. Conclusions and recommendations are illustrated using two examples: approximate model of hydrogen bonding and model of photon motion on a two-dimensional plane."
https://arxiv.org/abs/2403.07041,2024-03-11,Ant Colony Sampling with GFlowNets for Combinatorial Optimization,"['Minsu Kim', 'Sanghyeok Choi', 'Jiwoo Son', 'Hyeonah Kim', 'Jinkyoo Park', 'Yoshua Bengio']","This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology. GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input graph instances. Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS. Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems. The source code is available at \url{https://github.com/ai4co/gfacs}."
https://arxiv.org/abs/2403.07040,2024-03-11,All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract),"['Xiangguo Sun', 'Hong Cheng', 'Jia Li', 'Bo Liu', 'Jihong Guan']","This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. KDD 23) The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP's prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initialization for multiple tasks. Experiments show our method's effectiveness in enhancing model performance across different graph tasks."
https://arxiv.org/abs/2403.07039,2024-03-11,From English to ASIC: Hardware Implementation with Large Language Model,"['Emil Goh', 'Maoyang Xiang', 'I-Chyn Wey', 'T. Hui Teo']","In the realm of ASIC engineering, the landscape has been significantly reshaped by the rapid development of LLM, paralleled by an increase in the complexity of modern digital circuits. This complexity has escalated the requirements for HDL coding, necessitating a higher degree of precision and sophistication. However, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications. To address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the HDL code dataset has been developed. The fine-tuning aims to enhance models' proficiency in generating precise and efficient ASIC design, while the dataset reshuffling is intended to broaden the scope and improve the quality of training material. The model demonstrated significant improvements compared to the base model, with approximately 10% to 20% increase in accuracy across a wide range of temperature for the pass@1 metric. This approach is expected to facilitate a simplified and more efficient LLM-assisted framework for complex circuit design, leveraging their capabilities to meet the sophisticated demands of HDL coding and thus streamlining the ASIC development process."
https://arxiv.org/abs/2403.07038,2024-03-11,Leveraging graph neural networks for supporting Automatic Triage of Patients,"['Annamaria Defilippo', 'Pierangelo Veltri', ""Pietro Lio'"", 'Pietro Hiram Guzzi']","Patient triage plays a crucial role in emergency departments, ensuring timely and appropriate care based on correctly evaluating the emergency grade of patient conditions."
https://arxiv.org/abs/2403.07037,2024-03-11,Surface activation of Hastalex by vacuum argon plasma for cytocompatibility enhancement,"['Nikola Slepickova Kasalkova', 'Silvie Rimpelova', 'Cyril Vacek', 'Dominik Fajstavr', 'Vaclav Svorcik', 'Petr Sajdl', 'Petr Slepicka']","Here, we present surface analysis and biocompatibility evaluation of novel composite material based on graphene oxide traded as Hastalex. First, the surface morphology and elemental analysis of the pristine material were examined by atomic force and scanning electron microscopies, and by energy-dispersive and X-ray photoelectron spectroscopies, respectively. The Hastalex surface was then modified by plasma, 3 and 8 W with exposure times up to 240 s, the impact of which on the material surface wettability and morphology was further evaluated. In addition, the material aging was studied at room and elevated temperatures. Significant changes in surface roughness, morphology, and area were detected at the nanometre scale after plasma exposure. An increase in oxygen content due to the plasma exposure was observed both for 3 and 8 W. The plasma treatment had an outstanding effect on the cytocompatibility of Hastalex foil treated at both input powers of 3 and 8 W. The cell number of human MRC 5 fibroblasts on Hastalex foils exposed to plasma increased significantly compared to pristine Hastalex and even to tissue culture polystyrene. The plasma exposure also affected the fibroblasts cell growth and shape."
https://arxiv.org/abs/2403.07036,2024-03-11,A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge,"['Hasanul Mahmud', 'Peng Kang', 'Kevin Desai', 'Palden Lama', 'Sushil Prasad']","Reducing inference time and energy usage while maintaining prediction accuracy has become a significant concern for deep neural networks (DNN) inference on resource-constrained edge devices. To address this problem, we propose a novel approach based on ""converting"" autoencoder and lightweight DNNs. This improves upon recent work such as early-exiting framework and DNN partitioning. Early-exiting frameworks spend different amounts of computation power for different input data depending upon their complexity. However, they can be inefficient in real-world scenarios that deal with many hard image samples. On the other hand, DNN partitioning algorithms that utilize the computation power of both the cloud and edge devices can be affected by network delays and intermittent connections between the cloud and the edge. We present CBNet, a low-latency and energy-efficient DNN inference framework tailored for edge devices. It utilizes a ""converting"" autoencoder to efficiently transform hard images into easy ones, which are subsequently processed by a lightweight DNN for inference. To the best of our knowledge, such autoencoder has not been proposed earlier. Our experimental results using three popular image-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and an instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x speedup in inference latency and 79% reduction in energy usage compared to competing techniques while maintaining similar or higher accuracy."
https://arxiv.org/abs/2403.07035,2024-03-11,Multiple Population Alternate Evolution Neural Architecture Search,"['Juan Zou', 'Han Chu', 'Yizhang Xia', 'Junwen Xu', 'Yuan Liu', 'Zhanglu Hou']","The effectiveness of Evolutionary Neural Architecture Search (ENAS) is influenced by the design of the search space. Nevertheless, common methods including the global search space, scalable search space and hierarchical search space have certain limitations. Specifically, the global search space requires a significant amount of computational resources and time, the scalable search space sacrifices the diversity of network structures and the hierarchical search space increases the search cost in exchange for network diversity. To address above limitation, we propose a novel paradigm of searching neural network architectures and design the Multiple Population Alternate Evolution Neural Architecture Search (MPAE), which can achieve module diversity with a smaller search cost. MPAE converts the search space into L interconnected units and sequentially searches the units, then the above search of the entire network be cycled several times to reduce the impact of previous units on subsequent units. To accelerate the population evolution process, we also propose the the population migration mechanism establishes an excellent migration archive and transfers the excellent knowledge and experience in the migration archive to new populations. The proposed method requires only 0.3 GPU days to search a neural network on the CIFAR dataset and achieves the state-of-the-art results."
https://arxiv.org/abs/2403.07034,2024-03-11,Repetitive Infection Spreading and Directed Evolution in the Susceptible-Infected-Recovered-Susceptible Model,"['Hidetsugu Sakaguchi', 'Keito Yamasaki']","We study two simple mathematical models of the epidemic. At first, we study the repetitive infection spreading in a simplified SIRS model including the effect of the decay of the acquired immune. The model is an intermediate model of the SIRS model including the recruitment and death terms and the SIR model in which the recovered population is assumed to be never infected again. When the decay rate δof the immune is sufficiently small, the multiple infection spreading occurs in spikes. The model equation can be reduced to be a map when the decay rate δis sufficiently small, and the spike-like multiple infection spreading is reproduced in the mapping. The period-doubling bifurcation and chaos are found in the simplified SIRS model with seasonal variation. The nonlinear phenomena are reproduced by the map. Next, we study coupled SIRS equations for the directed evolution where the mutation is expressed with a diffusion-type term. A kind of reaction-diffusion equation is derived by the continuum approximation for the infected population I. The reaction-diffusion equation with the linear dependence of infection rate on the type space has an exact Gaussian solution with a time-dependent average and variance. The propagation of the Gaussian pulse corresponds to the successive transitions of the dominant variant."
https://arxiv.org/abs/2403.07033,2024-03-11,Interpreting What Typical Fault Signals Look Like via Prototype-matching,"['Qian Chen', 'Xingjian Dong', 'Zhike Peng']","Neural networks, with powerful nonlinear mapping and classification capabilities, are widely applied in mechanical fault diagnosis to ensure safety. However, being typical black-box models, their application is limited in high-reliability-required scenarios. To understand the classification logic and explain what typical fault signals look like, the prototype matching network (PMN) is proposed by combining the human-inherent prototype-matching with autoencoder (AE). The PMN matches AE-extracted feature with each prototype and selects the most similar prototype as the prediction result. It has three interpreting paths on classification logic, fault prototypes, and matching contributions. Conventional diagnosis and domain generalization experiments demonstrate its competitive diagnostic performance and distinguished advantages in representation learning. Besides, the learned typical fault signals (i.e., sample-level prototypes) showcase the ability for denoising and extracting subtle key features that experts find challenging to capture. This ability broadens human understanding and provides a promising solution from interpretability research to AI-for-Science."
https://arxiv.org/abs/2403.07032,2024-03-11,STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning for Real-world Scene Flow,"['Zhiyang Lu', 'Qinghan Chen', 'Ming Cheng']","Scene flow prediction is a crucial underlying task in understanding dynamic scenes as it offers fundamental motion information. However, contemporary scene flow methods encounter three major challenges. Firstly, flow estimation solely based on local receptive fields lacks long-dependency matching of point pairs. To address this issue, we propose global attentive flow embedding to match all-to-all point pairs in both feature space and Euclidean space, providing global initialization before local refinement. Secondly, there are deformations existing in non-rigid objects after warping, which leads to variations in the spatiotemporal relation between the consecutive frames. For a more precise estimation of residual flow, a spatial temporal feature re-embedding module is devised to acquire the sequence features after deformation. Furthermore, previous methods perform poor generalization due to the significant domain gap between the synthesized and LiDAR-scanned datasets. We leverage novel domain adaptive losses to effectively bridge the gap of motion inference from synthetic to real-world. Experiments demonstrate that our approach achieves state-of-the-art performance across various datasets, with particularly outstanding results on real-world LiDAR-scanned datasets. Our code is available at https://github.com/O-VIGIA/StarFlow."
https://arxiv.org/abs/2403.07031,2024-03-11,The Cram Method for Efficient Simultaneous Learning and Evaluation,"['Zeyang Jia', 'Kosuke Imai', 'Michael Lingzhi Li']","We introduce the ""cram"" method, a general and efficient approach to simultaneous learning and evaluation using a generic machine learning (ML) algorithm. In a single pass of batched data, the proposed method repeatedly trains an ML algorithm and tests its empirical performance. Because it utilizes the entire sample for both learning and evaluation, cramming is significantly more data-efficient than sample-splitting. The cram method also naturally accommodates online learning algorithms, making its implementation computationally efficient. To demonstrate the power of the cram method, we consider the standard policy learning setting where cramming is applied to the same data to both develop an individualized treatment rule (ITR) and estimate the average outcome that would result if the learned ITR were to be deployed. We show that under a minimal set of assumptions, the resulting crammed evaluation estimator is consistent and asymptotically normal. While our asymptotic results require a relatively weak stabilization condition of ML algorithm, we develop a simple, generic method that can be used with any policy learning algorithm to satisfy this condition. Our extensive simulation studies show that, when compared to sample-splitting, cramming reduces the evaluation standard error by more than 40% while improving the performance of learned policy. We also apply the cram method to a randomized clinical trial to demonstrate its applicability to real-world problems. Finally, we briefly discuss future extensions of the cram method to other learning and evaluation settings."
https://arxiv.org/abs/2403.07030,2024-03-10,AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation,"['Zihao Tang', 'Zheqi Lv', 'Shengyu Zhang', 'Yifan Zhou', 'Xinyu Duan', 'Fei Wu', 'Kun Kuang']","Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers' training data and real-world scenarios (student domain). The degradation stems from the portions of teachers' knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students' performance. Hence, selectively transferring teachers' appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-domain data with the teacher domain and leverages a generative method to progressively trade off the learning process between OOD knowledge distillation and domain-specific information learning via mixup learning. Extensive experiments in 3 datasets and 8 settings demonstrate the stability and superiority of our approach. Code available at https://github.com/IshiKura-a/AuG-KD ."
https://arxiv.org/abs/2403.07029,2024-03-10,A Model for Assessing Network Asset Vulnerability Using QPSO-LightGBM,"['Xinyu Li', 'Yu Gu', 'Chenwei Wang', 'Peng Zhao']","With the continuous development of computer technology and network technology, the scale of the network continues to expand, the network space tends to be complex, and the application of computers and networks has been deeply into politics, the military, finance, electricity, and other important fields. When security events do not occur, the vulnerability assessment of these high-risk network assets can be actively carried out to prepare for rainy days, to effectively reduce the loss caused by security events. Therefore, this paper proposes a multi-classification prediction model of network asset vulnerability based on quantum particle swarm algorithm-Lightweight Gradient Elevator (QPSO-LightGBM). In this model, based on using the Synthetic minority oversampling technique (SMOTE) to balance the data, quantum particle swarm optimization (QPSO) was used for automatic parameter optimization, and LightGBM was used for modeling. Realize multi-classification prediction of network asset vulnerability. To verify the rationality of the model, the proposed model is compared with the model constructed by other algorithms. The results show that the proposed model is better in various predictive performance indexes."
https://arxiv.org/abs/2403.07028,2024-03-10,An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem,"['Runze Guo', 'Feng Xue', 'Anlong Ming', 'Nicu Sebe']","Recently, neural networks (NN) have made great strides in combinatorial optimization. However, they face challenges when solving the capacitated arc routing problem (CARP) which is to find the minimum-cost tour covering all required edges on a graph, while within capacity constraints. In tackling CARP, NN-based approaches tend to lag behind advanced metaheuristics, since they lack directed arc modeling and efficient learning methods tailored for complex CARP. In this paper, we introduce an NN-based solver to significantly narrow the gap with advanced metaheuristics while exhibiting superior efficiency. First, we propose the direction-aware attention model (DaAM) to incorporate directionality into the embedding process, facilitating more effective one-stage decision-making. Second, we design a supervised reinforcement learning scheme that involves supervised pre-training to establish a robust initial policy for subsequent reinforcement fine-tuning. It proves particularly valuable for solving CARP that has a higher complexity than the node routing problems (NRPs). Finally, a path optimization method is proposed to adjust the depot return positions within the path generated by DaAM. Experiments illustrate that our approach surpasses heuristics and achieves decision quality comparable to state-of-the-art metaheuristics for the first time while maintaining superior efficiency."
https://arxiv.org/abs/2403.07027,2024-03-10,FWin transformer for dengue prediction under climate and ocean influence,"['Nhat Thanh Tran', 'Jack Xin', 'Guofa Zhou']","Dengue fever is one of the most deadly mosquito-born tropical infectious diseases. Detailed long range forecast model is vital in controlling the spread of disease and making mitigation efforts. In this study, we examine methods used to forecast dengue cases for long range predictions. The dataset consists of local climate/weather in addition to global climate indicators of Singapore from 2000 to 2019. We utilize newly developed deep neural networks to learn the intricate relationship between the features. The baseline models in this study are in the class of recent transformers for long sequence forecasting tasks. We found that a Fourier mixed window attention (FWin) based transformer performed the best in terms of both the mean square error and the maximum absolute error on the long range dengue forecast up to 60 weeks."
https://arxiv.org/abs/2403.07026,2024-03-10,Whiteness-based bilevel learning of regularization parameters in imaging,"['Carlo Santambrogio', 'Monica Pragliola', 'Alessandro Lanza', 'Marco Donatelli', 'Luca Calatroni']","We consider an unsupervised bilevel optimization strategy for learning regularization parameters in the context of imaging inverse problems in the presence of additive white Gaussian noise. Compared to supervised and semi-supervised metrics relying either on the prior knowledge of reference data and/or on some (partial) knowledge on the noise statistics, the proposed approach optimizes the whiteness of the residual between the observed data and the observation model with no need of ground-truth data.We validate the approach on standard Total Variation-regularized image deconvolution problems which show that the proposed quality metric provides estimates close to the mean-square error oracle and to discrepancy-based principles."
https://arxiv.org/abs/2403.07025,2024-03-10,Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation via Neural Networks,"['Subhasree Bhattacharjee', 'Soumyadip Sarkar', 'Kunal Das', 'Bikramjit Sarkar']","In the emergent realm of quantum computing, the Variational Quantum Eigensolver (VQE) stands out as a promising algorithm for solving complex quantum problems, especially in the noisy intermediate-scale quantum (NISQ) era. However, the ubiquitous presence of noise in quantum devices often limits the accuracy and reliability of VQE outcomes. This research introduces a novel approach to ameliorate this challenge by utilizing neural networks for zero noise extrapolation (ZNE) in VQE computations. By employing the Qiskit framework, we crafted parameterized quantum circuits using the RY-RZ ansatz and examined their behavior under varying levels of depolarizing noise. Our investigations spanned from determining the expectation values of a Hamiltonian, defined as a tensor product of Z operators, under different noise intensities to extracting the ground state energy. To bridge the observed outcomes under noise with the ideal noise-free scenario, we trained a Feed Forward Neural Network on the error probabilities and their associated expectation values. Remarkably, our model proficiently predicted the VQE outcome under hypothetical noise-free conditions. By juxtaposing the simulation results with real quantum device executions, we unveiled the discrepancies induced by noise and showcased the efficacy of our neural network-based ZNE technique in rectifying them. This integrative approach not only paves the way for enhanced accuracy in VQE computations on NISQ devices but also underlines the immense potential of hybrid quantum-classical paradigms in circumventing the challenges posed by quantum noise. Through this research, we envision a future where quantum algorithms can be reliably executed on noisy devices, bringing us one step closer to realizing the full potential of quantum computing."
https://arxiv.org/abs/2403.07024,2024-03-10,A polynomial chaos approach for uncertainty quantification of Monte Carlo transport codes,"['Gianluca Geraci', 'Kayla Clements', 'Aaron J Olson']","In this contribution, we discuss the construction of Polynomial Chaos surrogates for Monte Carlo radiation transport applications via non-intrusive spectral projection. This contribution focuses on improvements with respect to the approach that we previously introduced in previous work. We focus on understanding the impact of re-sampling cost on the algorithm performance and provide algorithm refinements, which allow to obtain unbiased estimators for the variance, estimate the PC variability due to limited samples, and adapt the expansion. An attenuation-only test case is provided to illustrate and discuss the results."
https://arxiv.org/abs/2403.07023,2024-03-10,Propensity-score matching analysis in COVID-19-related studies: a method and quality systematic review,"['Chunhui Gu', 'Ruosha Li', 'Guoqiang Zhang']",Objectives: To provide an overall quality assessment of the methods used for COVID-19-related studies using propensity score matching (PSM).
https://arxiv.org/abs/2403.07022,2024-03-09,A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units,"['Liyue Chen', 'Jiangyi Fang', 'Tengfei Liu', 'Shaosheng Cao', 'Leye Wang']","Spatio-Temporal (ST) prediction is crucial for making informed decisions in urban location-based applications like ride-sharing. However, existing ST models often require region partition as a prerequisite, resulting in two main pitfalls. Firstly, location-based services necessitate ad-hoc regions for various purposes, requiring multiple ST models with varying scales and zones, which can be costly to support. Secondly, different ST models may produce conflicting outputs, resulting in confusing predictions. In this paper, we propose One4All-ST, a framework that can conduct ST prediction for arbitrary modifiable areal units using only one model. To reduce the cost of getting multi-scale predictions, we design an ST network with hierarchical spatial modeling and scale normalization modules to efficiently and equally learn multi-scale representations. To address prediction inconsistencies across scales, we propose a dynamic programming scheme to solve the formulated optimal combination problem, minimizing predicted error through theoretical analysis. Besides, we suggest using an extended quad-tree to index the optimal combinations for quick response to arbitrary modifiable areal units in practical online scenarios. Extensive experiments on two real-world datasets verify the efficiency and effectiveness of One4All-ST in ST prediction for arbitrary modifiable areal units. The source codes and data of this work are available at https://github.com/uctb/One4All-ST."
https://arxiv.org/abs/2403.07021,2024-03-09,Stochastic Quantum Dynamics Stabilization: A Lyapunov-Based Control Approach with Homodyne-Mediated Filtering,"['Nahid Binandeh Dehaghani', 'A. Pedro Aguiar', 'Rafal Wisniewski']","Efficient control of stochastic dynamics in quantum systems is pivotal for various applications, including quantum information processing and metrology. This paper introduces a Lyapunov-based control approach with homodyne-mediated filtering. We employ a modified extended Kalman filtering method to directly estimate the evolution of the quantum density operator $ρ$, considering sequential homodyne current measurements. Our method explicitly addresses the dynamics of a stochastic master equation with correlated noise, ensuring by construction the quantum properties of the estimated state variable $\hatρ$. Moreover, our proposed switching based Lyapunov control scheme that is fed with $\hatρ$, guarantees noise-to-state practically stable in probability of the desired quantum stationary target set with respect to the estimation error variance. We demonstrate our approach's efficacy in stabilizing a qubit coupled to a leaky cavity under homodyne detection."
https://arxiv.org/abs/2403.07020,2024-03-09,Remarks on the integrabililty of the Lorenz System,['Yiting Yao'],"In this work, we study the integrability, as well as the dynamics of the Lorenz System. This include a very useful identity:\["
https://arxiv.org/abs/2403.07019,2024-03-09,Reasons behind the Water Crisis and its Potential Health Outcomes,"['Md. Galib Ishraq Emran', 'Rhidi Barma', 'Akram Hussain Khan', 'Mrinmoy Roy']","Globally, the water crisis has become a significant problem that affects developing and industrialized nations. Water shortage can harm public health by increasing the chance of contracting water-borne diseases, dehydration, and malnutrition. This study aims to examine the causes of the water problem and its likely effects on human health. The study scrutinizes the reasons behind the water crisis, including population increase, climate change, and inefficient water management techniques. The results of a lack of water on human health, such as the spread of infectious diseases, a higher risk of starvation and dehydration, and psychological stress, are also concealed in the study. The research further suggests several ways to deal with the water situation and lessen its potential outcomes on human health. These remedies include enhanced sanitation and hygiene procedures, water management, and conservation techniques like rainwater gathering and wastewater recycling."
https://arxiv.org/abs/2403.07018,2024-03-09,Contemplating Secure and Optimal Design Practices for Information Infrastructure From a Human Factors Perspective,['Niroop Sugunaraj'],"Designing secure information infrastructure is a function of design and usability. However, security is seldom given priority when systems are being developed. Secure design practices should balance between functionality (i.e., proper design) to meet minimum requirements and user-friendliness. Design recommendations such as those with a user-centric approach (i.e., inclusive of only relevant information, user liberty) and presenting information within its proper context in a clear and engaging manner has been scientifically shown to improve user response and experience."
https://arxiv.org/abs/2403.07017,2024-03-09,Mathematics of multi-agent learning systems at the interface of game theory and artificial intelligence,"['Long Wang', 'Feng Fu', 'Xingru Chen']","Evolutionary Game Theory (EGT) and Artificial Intelligence (AI) are two fields that, at first glance, might seem distinct, but they have notable connections and intersections. The former focuses on the evolution of behaviors (or strategies) in a population, where individuals interact with others and update their strategies based on imitation (or social learning). The more successful a strategy is, the more prevalent it becomes over time. The latter, meanwhile, is centered on machine learning algorithms and (deep) neural networks. It is often from a single-agent perspective but increasingly involves multi-agent environments, in which intelligent agents adjust their strategies based on feedback and experience, somewhat akin to the evolutionary process yet distinct in their self-learning capacities. In light of the key components necessary to address real-world problems, including (i) learning and adaptation, (ii) cooperation and competition, (iii) robustness and stability, and altogether (iv) population dynamics of individual agents whose strategies evolve, the cross-fertilization of ideas between both fields will contribute to the advancement of mathematics of multi-agent learning systems, in particular, to the nascent domain of ``collective cooperative intelligence'' bridging evolutionary dynamics and multi-agent reinforcement learning."
https://arxiv.org/abs/2403.07016,2024-03-09,A Field-Mill Proxy Climatology for the Lightning Launch Commit Criteria at Cape Canaveral Air Force Station and NASA Kennedy Space Center,"['Shane Gardner', 'Edward White', 'Brent Langhals', 'Todd McNamara', 'William Roeder', 'Alfred E. Thal Jr']","The Lightning Launch Commit Criteria (LLCC) are a set of complex rules to avoid natural and rocket-triggered lightning strikes to in-flight space launch vehicles. The LLCC are the leading source of scrubs and delays to space launches from Cape Canaveral Air Force Station (CCAFS) and NASA Kennedy Space Center (KSC). An LLCC climatology would be useful for designing launch concept of operations, mission planning, long-range forecasting, training, and setting LLCC improvement priorities. Unfortunately, an LLCC climatology has not been available for CCAFS/KSC. Attempts have been made to develop such a climatology, but they have not been entirely successful. The main shortfall has been the lack of a long continuous record of LLCC evaluations. Even though CCAFS/KSC is the world's busiest spaceport, the record of LLCC evaluations is not detailed enough to create the climatology. As a potential solution, the research in this study developed a proxy climatology of LLCC violations by using the long continuous record of surface electric field mills at CCAFS/KSC."
https://arxiv.org/abs/2403.07015,2024-03-09,Adaptive Hyperparameter Optimization for Continual Learning Scenarios,"['Rudy Semola', 'Julio Hurtado', 'Vincenzo Lomonaco', 'Davide Bacciu']","Hyperparameter selection in continual learning scenarios is a challenging and underexplored aspect, especially in practical non-stationary environments. Traditional approaches, such as grid searches with held-out validation data from all tasks, are unrealistic for building accurate lifelong learning systems. This paper aims to explore the role of hyperparameter selection in continual learning and the necessity of continually and automatically tuning them according to the complexity of the task at hand. Hence, we propose leveraging the nature of sequence task learning to improve Hyperparameter Optimization efficiency. By using the functional analysis of variance-based techniques, we identify the most crucial hyperparameters that have an impact on performance. We demonstrate empirically that this approach, agnostic to continual scenarios and strategies, allows us to speed up hyperparameters optimization continually across tasks and exhibit robustness even in the face of varying sequential task orders. We believe that our findings can contribute to the advancement of continual learning methodologies towards more efficient, robust and adaptable models for real-world applications."
https://arxiv.org/abs/2403.07014,2024-03-09,Dihedral Tilings of the Sphere by Regular Polygons and Quadrilaterals II: Regular Polygons with High Gonality and Rhombi,"['Ho Man Cheung', 'Hoi Ping Luk']",We classify the dihedral edge-to-edge tilings of the sphere by regular polygons with gonality at least 5 and rhombi.
https://arxiv.org/abs/2403.07013,2024-03-09,AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional Mutual Information,"['Jun Xia', 'Shaorong Chen', 'Jingbo Zhou', 'Tianze Lin', 'Wenjie Du', 'Sizhe Liu', 'Stan Z. Li']","Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the analysis of protein composition in biological samples. Despite the development of various deep learning methods for identifying amino acid sequences (peptides) responsible for observed spectra, challenges persist in \emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify amino acids with post-translational modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in decreased peptide-level identification precision. Secondly, diverse types of noise and missing peaks in mass spectra reduce the reliability of training data (peptide-spectrum matches, PSMs). To address these challenges, we propose AdaNovo, a novel framework that calculates conditional mutual information (CMI) between the spectrum and each amino acid/peptide, using CMI for adaptive model training. Extensive experiments demonstrate AdaNovo's state-of-the-art performance on a 9-species benchmark, where the peptides in the training set are almost completely disjoint from the peptides of the test sets. Moreover, AdaNovo excels in identifying amino acids with PTMs and exhibits robustness against data noise. The supplementary materials contain the official code."
https://arxiv.org/abs/2403.07012,2024-03-09,Non-Intrusive Load Monitoring with Missing Data Imputation Based on Tensor Decomposition,['DengYu Shi'],"With the widespread adoption of Non-Intrusive Load Monitoring (NILM) in building energy management, ensuring the high quality of NILM data has become imperative. However, practical applications of NILM face challenges associated with data loss, significantly impacting accuracy and reliability in energy management. This paper addresses the issue of NILM data loss by introducing an innovative tensor completion(TC) model- Proportional-Integral-Derivative (PID)-incorporated Non-negative Latent Factorization of Tensors (PNLFT) with twofold ideas: 1) To tackle the issue of slow convergence in Latent Factorization of Tensors (LFT) using Stochastic Gradient Descent (SGD), a Proportional-Integral-Derivative controller is introduced during the learning process. The PID controller utilizes historical and current information to control learning residuals. 2) Considering the characteristics of NILM data, non-negative update rules are proposed in the model's learning scheme. Experimental results on three datasets demonstrate that, compared to state-of-the-art models, the proposed model exhibits noteworthy enhancements in both convergence speed and accuracy."
https://arxiv.org/abs/2403.07011,2024-03-09,Automatic Detection and Classification of Corona Infection (COVID-19) from X-ray Images Using Convolution Neural Network,"['Kinjal A Patel', 'Tanvi Goswami']","The novel coronavirus universally known as the COVID-19 outbreak arises at the end of 2019 in one of the East Asian countries and it is subjected to widespread discussion and debate. There are almost 200 countries affected across the globe by COVID-19 and it has ruined many lives and the global economy. The virus is spreading very rapidly at the pace of around 10 fold in less than a month. Also, in the case of COVID- 19 it is critical to detect the infection as it employs various symptoms which may differ from person to person. Hence, diagnosis in starting stage and treatment are very much important for such type of infectious disease. The chest x-ray is one of the primary techniques among blood tests and Computed Tomography contributes a major role in the early diagnosis of COVID-19. There is a rising need for automated and auxiliary diagnostic tools for early diagnosis, as there are no accurate and truthful automated tool kits on hand. In this research study, we have designed a Convolution Neural Network architecture a deep net for the classification of x-ray images of chest among two classes: COVID-19 or Non-COVID- 19 infection. The anticipated model is expected to provide accurate diagnostic results and produced classification accuracy of 99%, 100%, and 100% with 70%-30%,75%-25% and 80%-20% train-test data split respectively, for the binary classification of the x-ray image to be COVID-19 or Non-COVID-19 infection category. We have designed the CNN with optimized parameters with 3 convolution layers and optimized number of filters in each layer."
https://arxiv.org/abs/2403.07010,2024-03-08,On Globular T-Spherical Fuzzy (G-TSF) Sets with Application to G-TSF Multi-Criteria Group Decision-Making,"['Miin-Shen Yang', 'Yasir Akhtar', 'Mehboob Ali']","In this paper, we give the concept of Globular T-Spherical Fuzzy (G-TSF) Sets (G-TSFSs) as an innovative extension of T-Spherical Fuzzy Sets (TSFSs) and Circular Spherical Fuzzy Sets (C-SFSs). G-TSFSs represent membership, indeterminacy, and non-membership degrees using a globular/sphere bound that can offer a more accurate portrayal of vague, ambiguous, and imprecise information. By employing a structured representation of data points on a sphere with a specific center and radius, this model enhances decision-making processes by enabling a more comprehensive evaluation of objects within a flexible region. Following the newly defined G-TSFSs, we establish some basic set operations and introduce fundamental algebraic operations for G-TSF Values (G-TSFVs). These operations expand the evaluative capabilities of decision-makers, facilitating more sensitive decision-making processes in a broader region. To quantify a similarity measure (SM) between GTSFVs, the SM is defined based on the radius of G-TSFSs. Additionally, Hamming distance and Euclidean distance are introduced for G-TSFSs. We also present theorems and examples to elucidate computational mechanisms. Furthermore, we give the G-TSF Weighted Average (G-TSFWA) and G-TSF Weighted Geometric (G-TSFWG) operators. Leveraging our proposed SM, a Multi-Criteria Group Decision-Making (MCGDM) scheme for G-TSFSs, named G-TSF MCGDM (G-TSFMCGDM), is developed to address group decision-making problems. The applicability and effectiveness of the proposed G-TSFMCGDM method are demonstrated by applying it to solve the selection problem of the best venue for professional development training sessions in a firm. The analysis results affirm the suitability and utility of the proposed method for resolving MCGDM problems, establishing its effectiveness in practical decision-making scenarios."
https://arxiv.org/abs/2403.07009,2024-03-08,Solving Functional Equations Dear to W.T. Tutte using the Naive (yet fullly rigorous!) Guess And Check Method,"['Shalosh B. Ekhad', 'Doron Zeilberger']","In his seminal paper ``A census of planar triangulations"", published in 1962, the iconic graph theorist (and code-breaker), W.T. Tutte, spent a few pages to prove that a certain bi-variate generating function that enumerates triangulations, satisfies a certain functional equation. He then used his genius to actually solve it, giving closed-form solutions to the enumerating sequences. While the first part, of deriving the functional equation, still needs human ingenuity, the second part, of solving it, can nowadays be fully automated. Our Maple program, accompanying this paper, Tutte.txt, can not only solve Tutte's original equation in a few seconds, it can also solve many, far more complicated ones, way beyond the scope of even such a giant as W.T. Tutte. We use our favorite method of ``guess and check"" and show how it can always be made fully rigorous (if desired)."
https://arxiv.org/abs/2403.07008,2024-03-08,AutoEval Done Right: Using Synthetic Data for Model Evaluation,"['Pierre Boyeau', 'Anastasios N. Angelopoulos', 'Nir Yosef', 'Jitendra Malik', 'Michael I. Jordan']",The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
https://arxiv.org/abs/2403.07007,2024-03-08,Dynamic Frequency Assignment for Mobile Users in Multibeam Satellite Constellations,"['Guillem Casadesus-Vila', 'Juan Jose Garau-Luis', 'Nils Pachler', 'Edward Crawley', 'Bruce Cameron']","Mobile users such as airplanes or ships will constitute an important segment of the future satellite communications market. Operators are now able to leverage digital payloads that allow flexible resource allocation policies that are robust against dynamic user bases. One of the key problems is managing the frequency spectrum efficiently, which has not been sufficiently explored for mobile users."
https://arxiv.org/abs/2403.07006,2024-03-08,Designing Wearable Augmented Reality Concepts to Support Scalability in Autonomous Vehicle-Pedestrian Interaction,"['Tram Thi Minh Tran', 'Callum Parker', 'Yiyuan Wang', 'Martin Tomitsch']","Wearable augmented reality (AR) offers new ways for supporting the interaction between autonomous vehicles (AVs) and pedestrians due to its ability to integrate timely and contextually relevant data into the user's field of view. This article presents novel wearable AR concepts that assist crossing pedestrians in multi-vehicle scenarios where several AVs frequent the road from both directions. Three concepts with different communication approaches for signaling responses from multiple AVs to a crossing request, as well as a conventional pedestrian push button, were simulated and tested within a virtual reality environment. The results showed that wearable AR is a promising way to reduce crossing pedestrians' cognitive load when the design offers both individual AV responses and a clear signal to cross. The willingness of pedestrians to adopt a wearable AR solution, however, is subject to different factors, including costs, data privacy, technical defects, liability risks, maintenance duties, and form factors. We further found that all participants favored sending a crossing request to AVs rather than waiting for the vehicles to detect their intentions-pointing to an important gap and opportunity in the current AV-pedestrian interaction literature."
https://arxiv.org/abs/2403.07005,2024-03-08,Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines,"['Xuejing Zheng', 'Chao Yu']","In this paper, we study the cooperative Multi-Agent Reinforcement Learning (MARL) problems using Reward Machines (RMs) to specify the reward functions such that the prior knowledge of high-level events in a task can be leveraged to facilitate the learning efficiency. Unlike the existing work that RMs have been incorporated into MARL for task decomposition and policy learning in relatively simple domains or with an assumption of independencies among the agents, we present Multi-Agent Reinforcement Learning with a Hierarchy of RMs (MAHRM) that is capable of dealing with more complex scenarios when the events among agents can occur concurrently and the agents are highly interdependent."
https://arxiv.org/abs/2403.07004,2024-03-07,Convergence of Some Convex Message Passing Algorithms to a Fixed Point,"['Vaclav Voracek', 'Tomas Werner']","A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. Examples of such algorithms are max-sum diffusion and sequential tree-reweighted message passing. Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any single point). We prove a stronger result (which was conjectured before but never proved): the iterates converge to a fixed point of the algorithm. Moreover, we show that they achieve precision $\varepsilon>0$ in $\mathcal{O}(1/\varepsilon)$ iterations."
https://arxiv.org/abs/2403.07003,2024-03-07,Evacuation Management Framework towards Smart City-wide Intelligent Emergency Interactive Response System,"['Anuj Abraham', 'Yi Zhang', 'Shitala Prasad']","A smart city solution toward future 6G network deployment allows small and medium sized enterprises (SMEs), industry, and government entities to connect with the infrastructures and play a crucial role in enhancing emergency preparedness with advanced sensors. The objective of this work is to propose a set of coordinated technological solutions to transform an existing emergency response system into an intelligent interactive system, thereby improving the public services and the quality of life for residents at home, on road, in hospitals, transport hubs, etc. In this context, we consider a city wide view from three different application scenes that are closely related to peoples daily life, to optimize the actions taken at relevant departments. Therefore, using artificial intelligence (AI) and machine learning (ML) techniques to enable the next generation connected vehicle experiences, we specifically focus on accidents happening in indoor households, urban roads, and at large public facilities. This smart interactive response system will benefit from advanced sensor fusion and AI by formulating a real time dynamic model."
https://arxiv.org/abs/2403.06998,2024-03-12,High-speed Low-consumption sEMG-based Transient-state micro-Gesture Recognition,"['Youfang Han', 'Wei Zhao', 'Xiangjin Chen', 'Xin Meng']","Gesture recognition on wearable devices is extensively applied in human-computer interaction. Electromyography (EMG) has been used in many gesture recognition systems for its rapid perception of muscle signals. However, analyzing EMG signals on devices, like smart wristbands, usually needs inference models to have high performances, such as low inference latency, low power consumption, and low memory occupation. Therefore, this paper proposes an improved spiking neural network (SNN) to achieve these goals. We propose an adaptive multi-delta coding as a spiking coding method to improve recognition accuracy. We propose two additive solvers for SNN, which can reduce inference energy consumption and amount of parameters significantly, and improve the robustness of temporal differences. In addition, we propose a linear action detection method TAD-LIF, which is suitable for SNNs. TAD-LIF is an improved LIF neuron that can detect transient-state gestures quickly and accurately. We collected two datasets from 20 subjects including 6 micro gestures. The collection devices are two designed lightweight consumer-level sEMG wristbands (3 and 8 electrode channels respectively). Compared to CNN, FCN, and normal SNN-based methods, the proposed SNN has higher recognition accuracy. The accuracy of the proposed SNN is 83.85% and 93.52% on the two datasets respectively. In addition, the inference latency of the proposed SNN is about 1% of CNN, the power consumption is about 0.1% of CNN, and the memory occupation is about 20% of CNN. The proposed methods can be used for precise, high-speed, and low-power micro-gesture recognition tasks, and are suitable for consumer-level intelligent wearable devices, which is a general way to achieve ubiquitous computing."
https://arxiv.org/abs/2403.06980,2024-03-11,Imaging of I Zw 18 by JWST: I. Strategy and First Results of Dusty Stellar Populations,"['Alec S. Hirschauer', 'Nicolas Crouzet', 'Nolan Habel', 'Laura Lenkić', 'Conor Nally', 'Olivia C. Jones', 'Giacomo Bortolini', 'Martha L. Boyer', 'Kay Justtanont Margaret Meixner', 'Göran Östlin', 'Gillian S. Wright', 'Ruyman Azzollini', 'Joris A. D. L. Blommaert', 'Bernhard Brandl', 'Leen Decin', 'Omnarayani Nayak', 'Pierre Royer', 'B. A. Sargent', 'Paul van der Werf']","We present a James Webb Space Telescope (JWST) imaging survey of I Zw 18, the archetypal extremely metal-poor, star-forming, blue compact dwarf galaxy. With an oxygen abundance of only $\sim$3% $Z_{\odot}$, it is among the lowest-metallicity systems known in the local universe, and is, therefore, an excellent accessible analog for the galactic building blocks which existed at early epochs of ionization and star formation. These JWST data provide a comprehensive infrared (IR) view of I Zw 18 with eight filters utilizing both NIRCam (F115W, F200W, F356W, and F444W) and MIRI (F770W, F1000W, F1500W, and F1800W) photometry, which we have used to identify key stellar populations that are bright in the near- and mid-IR. These data allow for a better understanding of the origins of dust and dust-production mechanisms in metal-poor environments by characterizing the population of massive, evolved stars in the red supergiant (RSG) and asymptotic giant branch (AGB) phases. In addition, it enables the identification of the brightest dust-enshrouded young stellar objects (YSOs), which provide insight into the formation of massive stars at extremely low metallicities typical of the very early universe. This paper provides an overview of the observational strategy and data processing, and presents first science results, including identifications of dusty AGB star, RSG, and bright YSO candidates. These first results assess the scientific quality of JWST data and provide a guide for obtaining and interpreting future observations of the dusty and evolved stars inhabiting compact dwarf star-forming galaxies in the local universe."
https://arxiv.org/abs/2403.06979,2024-03-11,Tidal synchronization trapping in stars and planets with convective envelopes,['Janosz W. Dewberry'],"Tidal torques can alter the spins of tidally interacting stars and planets, usually over shorter timescales than the tidal damping of orbital separations or eccentricities. Simple tidal models predict that in eccentric binary or planetary systems, rotation periods will evolve toward a ""pseudosynchronous"" ratio with the orbital period. However, this prediction does not account for ""inertial"" waves that are present in stars or gaseous planets with (i) convective envelopes, and (ii) even very slow rotation. We demonstrate that tidal driving of inertial oscillations in eccentric systems generically produces a network of stable ""synchronization traps"" at ratios of orbital to rotation period that are simple to predict, but can deviate significantly from pseudosynchronization. The mechanism underlying spin synchronization trapping is similar to tidal resonance locking, involving a balance between torques that is maintained automatically by the scaling of inertial mode frequencies with the rotation rate. In contrast with many resonance locking scenarios, however, the torque balance required for synchronization trapping need not drive mode amplitudes to nonlinearity. Synchronization traps may provide an explanation for low-mass stars and hot Jupiters with observed rotation rates that deviate from pseudosynchronous or synchronous expectations."
https://arxiv.org/abs/2403.06978,2024-03-11,Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling,"['Wele Gedara Chaminda Bandara', 'Vishal M. Patel']","In this paper, we introduce Attention Prompt Tuning (APT) - a computationally efficient variant of prompt tuning for video-based applications such as action recognition. Prompt tuning approaches involve injecting a set of learnable prompts along with data tokens during fine-tuning while keeping the backbone frozen. This approach greatly reduces the number of learnable parameters compared to full tuning. For image-based downstream tasks, normally a couple of learnable prompts achieve results close to those of full tuning. However, videos, which contain more complex spatiotemporal information, require hundreds of tunable prompts to achieve reasonably good results. This reduces the parameter efficiency observed in images and significantly increases latency and the number of floating-point operations (FLOPs) during inference. To tackle these issues, we directly inject the prompts into the keys and values of the non-local attention mechanism within the transformer block. Additionally, we introduce a novel prompt reparameterization technique to make APT more robust against hyperparameter selection. The proposed APT approach greatly reduces the number of FLOPs and latency while achieving a significant performance boost over the existing parameter-efficient tuning methods on UCF101, HMDB51, and SSv2 datasets for action recognition. The code and pre-trained models are available at https://github.com/wgcban/apt"
https://arxiv.org/abs/2403.06977,2024-03-12,VideoMamba: State Space Model for Efficient Video Understanding,"['Kunchang Li', 'Xinhao Li', 'Yi Wang', 'Yinan He', 'Yali Wang', 'Limin Wang', 'Yu Qiao']","Addressing the dual challenges of local redundancy and global dependencies in video understanding, this work innovatively adapts the Mamba to the video domain. The proposed VideoMamba overcomes the limitations of existing 3D convolution neural networks and video transformers. Its linear-complexity operator enables efficient long-term modeling, which is crucial for high-resolution long video understanding. Extensive evaluations reveal VideoMamba's four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel self-distillation technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in multi-modal contexts. Through these distinct advantages, VideoMamba sets a new benchmark for video understanding, offering a scalable and efficient solution for comprehensive video understanding. All the code and models are available at https://github.com/OpenGVLab/VideoMamba."
https://arxiv.org/abs/2403.06976,2024-03-11,BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion,"['Xuan Ju', 'Xian Liu', 'Xintao Wang', 'Yuxuan Bian', 'Ying Shan', 'Qiang Xu']","Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of diffusion models (DMs). Despite these advancements, current DM adaptations for inpainting, which involve modifications to the sampling strategy or the development of inpainting-specific DMs, frequently suffer from semantic inconsistencies and reduced image quality. Addressing these challenges, our work introduces a novel paradigm: the division of masked image features and noisy latent into separate branches. This division dramatically diminishes the model's learning load, facilitating a nuanced incorporation of essential masked image information in a hierarchical fashion. Herein, we present BrushNet, a novel plug-and-play dual-branch model engineered to embed pixel-level masked image features into any pre-trained DM, guaranteeing coherent and enhanced image inpainting outcomes. Additionally, we introduce BrushData and BrushBench to facilitate segmentation-based inpainting training and performance assessment. Our extensive experimental analysis demonstrates BrushNet's superior performance over existing models across seven key metrics, including image quality, mask region preservation, and textual coherence."
https://arxiv.org/abs/2403.06975,2024-03-11,Ehrhart polynomials of partial permutohedra,['Roger E. Behrend'],"For positive integers $m$ and $n$, the partial permutohedron $\mathcal{P}(m,n)$ is a certain integral polytope in $\mathbb{R}^m$, which can be defined as the convex hull of the vectors from $\{0,1,\ldots,n\}^m$ whose nonzero entries are distinct. For $n=m-1$, $\mathcal{P}(m,m-1)$ is (after translation by $(1,\ldots,1)$) the polytope $P_m$ of parking functions of length $m$, and for $n\ge m$, $\mathcal{P}(m,n)$ is combinatorially equivalent to an $m$-stellohedron. The main result of this paper is an explicit expression for the Ehrhart polynomial of $\mathcal{P}(m,n)$ for any $m$ and $n$ with $n\ge m-1$. The result confirms the validity of a conjecture for this Ehrhart polynomial in arXiv:2207.14253, and the $n=m-1$ case also answers a question of Stanley regarding the number of integer points in $P_m$. The proof of the result involves transforming $\mathcal{P}(m,n)$ to a unimodularly equivalent polytope in $\mathbb{R}^{m+1}$, obtaining a decomposition of this lifted version of $\mathcal{P}(m,n)$ with $n\ge m-1$ as a Minkowski sum of dilated coordinate simplices, applying a result of Postnikov for the number of integer points in generalized permutohedra of this form, observing that this gives an expression for the Ehrhart polynomial of $\mathcal{P}(m,n)$ with $n\ge m-1$ as an edge-weighted sum over graphs (with loops and multiple edges permitted) on $m$ labelled vertices in which each connected component contains at most one cycle, and then applying standard techniques for the enumeration of such graphs."
https://arxiv.org/abs/2403.06974,2024-03-11,Memory-based Adapters for Online 3D Scene Perception,"['Xiuwei Xu', 'Chong Xia', 'Ziwei Wang', 'Linqing Zhao', 'Yueqi Duan', 'Jie Zhou', 'Jiwen Lu']","In this paper, we propose a new framework for online 3D scene perception. Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene geometry as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos. To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information. To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability. Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features. Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame. We further propose 3D-to-2D adapter to enhance image features with strong global context. Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply finetuning existing offline models, without any model and task-specific designs. \href{https://xuxw98.github.io/Online3D/}{Project page}."
https://arxiv.org/abs/2403.06973,2024-03-11,Bayesian Diffusion Models for 3D Shape Reconstruction,"['Haiyang Xu', 'Yu Lei', 'Zeyuan Chen', 'Xiang Zhang', 'Yue Zhao', 'Yilin Wang', 'Zhuowen Tu']","We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom-up processes where each itself is a diffusion process. We demonstrate state-of-the-art results on both synthetic and real-world benchmarks for 3D shape reconstruction."
https://arxiv.org/abs/2403.06972,2024-03-11,Investigating model dependencies for obscured Active Galactic Nuclei: a case study of NGC 3982,"['Kristína Kallová', 'Peter G. Boorman', 'Claudio Ricci']","X-ray spectroscopy of heavily obscured Active Galactic Nuclei (AGN) offers a unique opportunity to study the circum-nuclear environment of accreting supermassive black holes (SMBHs). However, individual models describing the obscurer have unique parameter spaces that give distinct parameter posterior distributions when fit to the same data. To assess the impact of model-specific parameter dependencies, we present a case study of the nearby heavily obscured low-luminosity AGN NGC 3982, which has a variety of column density estimations reported in the literature. We fit the same broadband XMM-Newton + NuSTAR spectra of the source with five unique obscuration models and generate posterior parameter distributions for each. By using global parameter exploration, we traverse the full prior-defined parameter space to accurately reproduce complex posterior shapes and inter-parameter degeneracies. The unique model posteriors for the line-of-sight column density are broadly consistent, predicting Compton-thick $N_{\rm H}$ $>1.5\times10^{24}\rm cm^{-2}$ at the 3$σ$ confidence level. The posterior median intrinsic X-ray luminosity in the 2-10 keV band however was found to differ substantially, with values in the range log $L_{ 2-10\,{\rm keV}}$ergs$^{-1}$ = 40.9-42.1 for the individual models. We additionally show that the posterior distributions for each model occupy unique regions of their respective multi-dimensional parameters spaces, and how such differences can propagate into the inferred properties of the central engine. We conclude by showcasing the improvement in parameter inference attainable with the High Energy X-ray Probe (HEX-P) with a uniquely broad simultaneous and high-sensitivity bandpass of 0.2-80 keV."
https://arxiv.org/abs/2403.06971,2024-03-11,A representation-learning game for classes of prediction tasks,"['Neria Uzan', 'Nir Weinberger']","We propose a game-based formulation for learning dimensionality-reducing representations of feature vectors, when only a prior knowledge on future prediction tasks is available. In this game, the first player chooses a representation, and then the second player adversarially chooses a prediction task from a given class, representing the prior knowledge. The first player aims is to minimize, and the second player to maximize, the regret: The minimal prediction loss using the representation, compared to the same loss using the original features. For the canonical setting in which the representation, the response to predict and the predictors are all linear functions, and under the mean squared error loss function, we derive the theoretically optimal representation in pure strategies, which shows the effectiveness of the prior knowledge, and the optimal regret in mixed strategies, which shows the usefulness of randomizing the representation. For general representations and loss functions, we propose an efficient algorithm to optimize a randomized representation. The algorithm only requires the gradients of the loss function, and is based on incrementally adding a representation rule to a mixture of such rules."
https://arxiv.org/abs/2403.06970,2024-03-11,MRL Parsing Without Tears: The Case of Hebrew,"['Shaltiel Shmidman', 'Avi Shmidman', 'Moshe Koppel', 'Reut Tsarfaty']","Syntactic parsing remains a critical tool for relation extraction and information extraction, especially in resource-scarce languages where LLMs are lacking. Yet in morphologically rich languages (MRLs), where parsers need to identify multiple lexical units in each token, existing systems suffer in latency and setup complexity. Some use a pipeline to peel away the layers: first segmentation, then morphology tagging, and then syntax parsing; however, errors in earlier layers are then propagated forward. Others use a joint architecture to evaluate all permutations at once; while this improves accuracy, it is notoriously slow. In contrast, and taking Hebrew as a test case, we present a new ""flipped pipeline"": decisions are made directly on the whole-token units by expert classifiers, each one dedicated to one specific task. The classifiers are independent of one another, and only at the end do we synthesize their predictions. This blazingly fast approach sets a new SOTA in Hebrew POS tagging and dependency parsing, while also reaching near-SOTA performance on other Hebrew NLP tasks. Because our architecture does not rely on any language-specific resources, it can serve as a model to develop similar parsers for other MRLs."
https://arxiv.org/abs/2403.06969,2024-03-11,Non-Abelian R-symmetries in $\mathcal{N}=1$ supersymmetry,"['James Brister', 'Longjie Ran', 'Zheng Sun']","We investigate non-Abelian R-symmetries in $\mathcal{N}=1$ supersymmetric theory, where fields may transform under the R-symmetry in representations with dimension higher than one. While a continuous non-Abelian R-symmetry can always be decomposed to a $U(1)$ R-symmetry and non-R symmetries, there are non-trivial discrete non-Abelian R-symmetries that do not admit such a decomposition, and effective R-charges cannot be defined in such models. Previous results on sufficient conditions for R-symmetric supersymmetric vacua in Wess-Zumino models still hold, and do not depend on fields in representations of dimension greater than one. However, fields in higher-dimensional representations enter the sufficient conditions for supersymmetric vacua that break R-symmetry, but it is difficult to identify the independent variables which can be used to solve the F-flatness equation in this case, unless other conditions are fulfilled. We present examples with discrete non-Abelian R-symmetries of the lowest order in this case."
https://arxiv.org/abs/2403.06968,2024-03-11,Consistency of matrix decomposition factor analysis,['Yoshikazu Terada'],"For factor analysis, many estimators, starting with the maximum likelihood estimator, are developed, and the statistical properties of most estimators are well discussed. In the early 2000s, a new estimator based on matrix factorization, called Matrix Decomposition Factor Analysis (MDFA), was developed. Although the estimator is obtained by minimizing the principal component analysis-like loss function, this estimator empirically behaves like other consistent estimators of factor analysis, not principal component analysis. Since the MDFA estimator cannot be formulated as a classical M-estimator, the statistical properties of the MDFA estimator have not yet been discussed. To explain this unexpected behavior theoretically, we establish the consistency of the MDFA estimator as the factor analysis. That is, we show that the MDFA estimator has the same limit as other consistent estimators of factor analysis."
https://arxiv.org/abs/2403.06967,2024-03-11,POD-ROM methods: from a finite set of snapshots to continuous-in-time approximations,"['Bosco Garcia-Archilla', 'Volker John', 'Julia Novo']","This paper studies discretization of time-dependent partial differential equations (PDEs) by proper orthogonal decomposition reduced order models (POD-ROMs). Most of the analysis in the literature has been performed on fully-discrete methods using first order methods in time, typically the implicit Euler time integrator. Our aim is to show which kind of error bounds can be obtained using any time integrator, both in the full order model (FOM), applied to compute the snapshots, and in the POD-ROM method. To this end, we analyze in this paper the continuous-in-time case for both the FOM and POD-ROM methods, although the POD basis is obtained from snapshots taken at a discrete (i.e., not continuous) set times. Two cases for the set of snapshots are considered: The case in which the snapshots are based on first order divided differences in time and the case in which they are based on temporal derivatives. Optimal pointwise-in-time error bounds {between the FOM and the POD-ROM solutions} are proved for the $L^2(Ω)$ norm of the error for a semilinear reaction-diffusion model problem. The dependency of the errors on the distance in time between two consecutive snapshots and on the tail of the POD eigenvalues is tracked. Our detailed analysis allows to show that, in some situations, a small number of snapshots in a given time interval might be sufficient to accurately approximate the solution in the full interval. Numerical studies support the error analysis."
https://arxiv.org/abs/2403.06966,2024-03-11,Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts,"['Onur Celik', 'Aleksandar Taranovic', 'Gerhard Neumann']","Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose \textbf{Di}verse \textbf{Skil}l \textbf{L}earning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distributions and demonstrate how we can efficiently train them using the standard policy gradient objective. We show on challenging robot simulation tasks that Di-SkilL can learn diverse and performant skills."
https://arxiv.org/abs/2403.06965,2024-03-11,Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena,"['Leonie Weissweiler', 'Abdullatif Köksal', 'Hinrich Schütze']","Argument Structure Constructions (ASCs) are one of the most well-studied construction groups, providing a unique opportunity to demonstrate the usefulness of Construction Grammar (CxG). For example, the caused-motion construction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates that constructions must carry meaning, otherwise the fact that ``sneeze'' in this context causes movement cannot be explained. We form the hypothesis that this remains challenging even for state-of-the-art Large Language Models (LLMs), for which we devise a test based on substituting the verb with a prototypical motion verb. To be able to perform this test at statistically significant scale, in the absence of adequate CxG corpora, we develop a novel pipeline of NLP-assisted collection of linguistically annotated text. We show how dependency parsing and GPT-3.5 can be used to significantly reduce annotation cost and thus enable the annotation of rare phenomena at scale. We then evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of the CMC using the newly collected corpus. We find that all models struggle with understanding the motion component that the CMC adds to a sentence."
https://arxiv.org/abs/2403.06964,2024-03-11,Decorrelation of a leader by the increasing number of followers,"['Satya N. Majumdar', 'Gregory Schehr']","We compute the connected two-time correlator of the maximum $M_N(t)$ of $N$ independent Gaussian stochastic processes (GSP) characterised by a common correlation coefficient $ρ$ that depends on the two times $t_1$ and $t_2$. We show analytically that this correlator, for fixed times $t_1$ and $t_2$, decays for large $N$ as a power law $N^{-γ}$ (with logarithmic corrections) with a decorrelation exponent $γ= (1-ρ)/(1+ ρ)$ that depends only on $ρ$, but otherwise is universal for any GSP. We study several examples of physical processes including the fractional Brownian motion (fBm) with Hurst exponent $H$ and the Ornstein-Uhlenbeck (OU) process. For the fBm, $ρ$ is only a function of $τ= \sqrt{t_1/t_2}$ and we find an interesting ``freezing'' transition at a critical value $τ= τ_c=(3-\sqrt{5})/2$. For $τ< τ_c$, there is an optimal $H^*(τ) > 0$ that maximises the exponent $γ$ and this maximal value freezes to $γ= 1/3$ for $τ>τ_c$. For the OU process, we show that $γ= {\rm tanh}(μ\,|t_1-t_2|/2)$ where $μ$ is the stiffness of the harmonic trap. Numerical simulations confirm our analytical predictions."
https://arxiv.org/abs/2403.06963,2024-03-11,The pitfalls of next-token prediction,"['Gregor Bachmann', 'Vaishnavh Nagarajan']","Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn. We provide preliminary evidence that this failure can be resolved when training to predict multiple tokens in advance. We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm. We make our code available under https://github.com/gregorbachmann/Next-Token-Failures"
https://arxiv.org/abs/2403.06962,2024-03-11,Fully non-Gaussian Scalar-Induced Gravitational Waves,"['Gabriele Perna', 'Chiara Testini', 'Angelo Ricciardone', 'Sabino Matarrese']","Scalar-induced Gravitational Waves (SIGWs) represent a particular class of primordial signals which are sourced at second-order in perturbation theory whenever a scalar fluctuation of the metric is present. They form a guaranteed Stochastic Gravitational Wave Background (SGWB) that, depending on the amplification of primordial scalar fluctuations, can be detected by GW detectors. The amplitude and the frequency shape of the scalar-induced SGWB can be influenced by the statistical properties of the scalar density perturbations. In this work we study the intuitive physics behind SIGWs and we analyze the imprints of local non-Gaussianity of the primordial curvature perturbation on the GW spectrum. We consider all the relevant non-Gaussian contributions up to fifth-order in the scalar seeds without any hierarchy, and we derive the related GW energy density $Ω_{\rm GW}(f)$. We perform a Fisher matrix analysis to understand to which accuracy non-Gaussianity can be constrained with the LISA detector, which will be sensitive in the milli-Hertz frequency band. We find that LISA, neglecting the impact of astrophysical foregrounds, will be able to measure the amplitude, the width and the peak of the spectrum with an accuracy up to $\mathcal{O}(10^{-4})$, while non-Gaussianity can be measured up to $\mathcal{O}(10^{-3})$. Finally, we discuss the implications of our non-Gaussianity expansion on the fraction of Primordial Black Holes."
https://arxiv.org/abs/2403.06961,2024-03-11,Explainable Transformer Prototypes for Medical Diagnoses,"['Ugur Demir', 'Debesh Jha', 'Zheyuan Zhang', 'Elif Keles', 'Bradley Allen', 'Aggelos K. Katsaggelos', 'Ulas Bagci']","Deployments of artificial intelligence in medical diagnostics mandate not just accuracy and efficacy but also trust, emphasizing the need for explainability in machine decisions. The recent trend in automated medical image diagnostics leans towards the deployment of Transformer-based architectures, credited to their impressive capabilities. Since the self-attention feature of transformers contributes towards identifying crucial regions during the classification process, they enhance the trustability of the methods. However, the complex intricacies of these attention mechanisms may fall short of effectively pinpointing the regions of interest directly influencing AI decisions. Our research endeavors to innovate a unique attention block that underscores the correlation between 'regions' rather than 'pixels'. To address this challenge, we introduce an innovative system grounded in prototype learning, featuring an advanced self-attention mechanism that goes beyond conventional ad-hoc visual explanation techniques by offering comprehensible visual insights. A combined quantitative and qualitative methodological approach was used to demonstrate the effectiveness of the proposed method on the large-scale NIH chest X-ray dataset. Experimental results showed that our proposed method offers a promising direction for explainability, which can lead to the development of more trustable systems, which can facilitate easier and rapid adoption of such technology into routine clinics. The code is available at www.github.com/NUBagcilab/r2r_proto."
https://arxiv.org/abs/2403.06960,2024-03-11,The MODEST catalog of depth-dependent spatially coupled inversions of sunspots observed by Hinode/SOT-SP,"['J. S. Castellanos Durán', 'N. Milanovic', 'A. Korpi-Lagg', 'B. Löptien', 'M. van Noort', 'S. K. Solanki']","We present a catalog that we named MODEST containing depth-dependent information on the atmospheric conditions inside sunspot groups of all types. The catalog is currently composed of 942 observations of 117 individual active regions with sunspots that cover all types of features observed in the solar photosphere. We use the SPINOR-2D code to perform spatially coupled inversions of the Stokes profiles observed by Hinode/SOT-SP at high spatial resolution. SPINOR-2D accounts for the unavoidable degradation of the spatial information due to the point spread function of the telescope. The sunspot sample focuses on complex sunspot groups, but simple sunspots are also part of the catalog for completeness. Sunspots were observed from 2006 to 2019, covering parts of solar cycles 23 and 24. The catalog is a living resource, as with time, more sunspot groups will be included."
https://arxiv.org/abs/2403.06959,2024-03-11,Gluon Double-Spin Asymmetry in the Longitudinally Polarized $p+p$ Collisions,"['Yuri V. Kovchegov', 'Ming Li']","We derive the first-ever small-$x$ expression for the inclusive gluon production cross section in the central rapidity region of the longitudinally polarized proton-proton collisions. The cross section depends on the polarizations of both protons, therefore comprising the numerator of the longitudinal double-spin asymmetry $A_{LL}$ for the produced gluons. The cross section is calculated in the shock wave formalism and is expressed in terms of the polarized dipole scattering amplitudes on the projectile and target protons. We show that the small-$x$ evolution corrections are included into our cross section expression if one evolves these polarized dipole amplitudes using the double-logarithmic helicity evolution derived in \cite{Kovchegov:2015pbl, Kovchegov:2016zex, Kovchegov:2018znm, Cougoulic:2022gbk}. Our calculation is performed for the gluon sector only, with the quark contribution left for future work. When that work is complete, the resulting formula will be applicable to longitudinally polarized proton-proton and proton-nucleus collisions, as well as to polarized semi-inclusive deep inelastic scattering (SIDIS) on a proton or a nucleus. Our results should allow one to extend the small-$x$ helicity phenomenology analysis of \cite{Adamiak:2023yhz} to the jet/hadron production data reported for the longitudinally polarized proton-proton collisions at RHIC and to polarized SIDIS measurements at central rapidities to be performed at the EIC."
https://arxiv.org/abs/2403.06958,2024-03-11,Notes on solitary-wave solutions of Rosenau-type equations,"['A. Durán', 'G. M. Muslu']","The present paper is concerned with the existence of solitary wave solutions of Rosenau-type equations. By using two standard theories, Normal Form Theory and Concentration-Compactness Theory, some results of existence of solitary waves of three different forms are derived. The results depend on some conditions on the speed of the waves with respect to the parameters of the equations. They are discussed for several families of Rosenau equations present in the literature. The analysis is illustrated with a numerical study of generation of approximate solitary-wave profiles from a numerical procedure based on the Petviashvili iteration."
https://arxiv.org/abs/2403.06957,2024-03-11,The maximal subsemigroups of the ideals on a monoid of partial injections,"['Apatsara Sareeto', 'Jörg Koppitz']","In the present paper, a submonoid of the well studied monoid $POI_n$ of all order-preserving partial injections on an $n$-element chain is studied. The set $IOF_n^{par}$ of all partial transformations in $POI_n$ which are fence-preserving as well as parity-preserving form a submonoid of $POI_n$. We describe the Green's relations and ideals of $IOF_n^{par}$. For each ideal of $IOF_n^{par}$, we characterize the maximal subsemigroups. We will observe that there are three different types of maximal subsemigroups."
https://arxiv.org/abs/2403.06956,2024-03-11,Ternary Positroids,"['Jeremy Quail', 'Puck Rombach']","A positroid is an ordered matroid realizable by a real matrix with all nonnegative maximal minors. Postnikov gave a map from ordered matroids to Grassmann necklaces, for which there is a unique positroid in each fiber of the map. Here, we characterize the ternary positroids and the set of matroids in their respective fibers, referred to as their positroid envelope classes. We show that a positroid is ternary if and only if it is near-regular, and that all ternary positroids are formed by direct sums and 2-sums of binary positroids and positroid whirls. We fully characterize the envelope classes of ternary positroids; in particular, the envelope class of a positroid whirl of rank-$r$ contains exactly four matroids."
https://arxiv.org/abs/2403.06955,2024-03-11,Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic Perovskites,"['Nima Karimitari', 'William J. Baldwin', 'Evan W. Muller', 'Zachary J. L. Bare', 'W. Joshua Kennedy', 'Gábor Csányi', 'Christopher Sutton']","Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a promising class of electronically active materials for both light absorption and emission. The design space of HOIPs is extremely large, since a diverse space of organic cations can be combined with different inorganic frameworks. This immense design space allows for tunable electronic and mechanical properties, but also necessitates the development of new tools for in silico high throughput analysis of candidate structures. In this work, we present an accurate, efficient, transferable and widely applicable machine learning interatomic potential (MLIP) for predicting the structure of new 2D HOIPs. Using the MACE architecture, an MLIP is trained on 86 diverse experimentally reported HOIP structures. The model is tested on 73 unseen perovskite compositions, and achieves chemical accuracy with respect to the reference electronic structure method. Our model is then combined with a simple random structure search algorithm to predict the structure of hypothetical HOIPs given only the proposed composition. Success is demonstrated by correctly and reliably recovering the crystal structure of a set of experimentally known 2D perovskites. Such a random structure search is impossible with ab initio methods due to the associated computational cost, but is relatively inexpensive with the MACE potential. Finally, the procedure is used to predict the structure formed by a new organic cation with no previously known corresponding perovskite. Laboratory synthesis of the new hybrid perovskite confirms the accuracy of our prediction. This capability, applied at scale, enables efficient screening of thousands of combinations of organic cations and inorganic layers."
https://arxiv.org/abs/2403.06954,2024-03-11,Quadruped-Frog: Rapid Online Optimization of Continuous Quadruped Jumping,"['Guillaume Bellegarda', 'Milad Shafiee', 'Merih Ekin Özberk', 'Auke Ijspeert']","Legged robots are becoming increasingly agile in exhibiting dynamic behaviors such as running and jumping. Usually, such behaviors are either optimized and engineered offline (i.e. the behavior is designed for before it is needed), either through model-based trajectory optimization, or through deep learning-based methods involving millions of timesteps of simulation interactions. Notably, such offline-designed locomotion controllers cannot perfectly model the true dynamics of the system, such as the motor dynamics. In contrast, in this paper, we consider a quadruped jumping task that we rapidly optimize online. We design foot force profiles parameterized by only a few parameters which we optimize for directly on hardware with Bayesian Optimization. The force profiles are tracked at the joint level, and added to Cartesian PD impedance control and Virtual Model Control to stabilize the jumping motions. After optimization, which takes only a handful of jumps, we show that this control architecture is capable of diverse and omnidirectional jumps including forward, lateral, and twist (turning) jumps, even on uneven terrain, enabling the Unitree Go1 quadruped to jump 0.5 m high, 0.5 m forward, and jump-turn over 2 rad. Video results can be found at https://youtu.be/SvfVNQ90k_w."
https://arxiv.org/abs/2403.06953,2024-03-11,Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot Domain Transfer,"['Siddhant Satyanaik', 'Aditya Murali', 'Deepak Alapatt', 'Xin Wang', 'Pietro Mascagni', 'Nicolas Padoy']","Purpose: Advances in deep learning have resulted in effective models for surgical video analysis; however, these models often fail to generalize across medical centers due to domain shift caused by variations in surgical workflow, camera setups, and patient demographics. Recently, object-centric learning has emerged as a promising approach for improved surgical scene understanding, capturing and disentangling visual and semantic properties of surgical tools and anatomy to improve downstream task performance. In this work, we conduct a multi-centric performance benchmark of object-centric approaches, focusing on Critical View of Safety assessment in laparoscopic cholecystectomy, then propose an improved approach for unseen domain generalization."
https://arxiv.org/abs/2403.06952,2024-03-11,SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data,"['Jialu Li', 'Jaemin Cho', 'Yi-Lin Sung', 'Jaehong Yoon', 'Mohit Bansal']","Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models."
https://arxiv.org/abs/2403.06951,2024-03-11,DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations,"['Tianhao Qi', 'Shancheng Fang', 'Yanze Wu', 'Hongtao Xie', 'Jiawei Liu', 'Lang Chen', 'Qian He', 'Yongdong Zhang']","The diffusion-based text-to-image model harbors immense potential in transferring reference style. However, current encoder-based approaches significantly impair the text controllability of text-to-image models while transferring styles. In this paper, we introduce DEADiff to address this issue using the following two strategies: 1) a mechanism to decouple the style and semantics of reference images. The decoupled feature representations are first extracted by Q-Formers which are instructed by different text descriptions. Then they are injected into mutually exclusive subsets of cross-attention layers for better disentanglement. 2) A non-reconstructive learning method. The Q-Formers are trained using paired images rather than the identical target, in which the reference image and the ground-truth image are with the same style or semantics. We show that DEADiff attains the best visual stylization results and optimal balance between the text controllability inherent in the text-to-image model and style similarity to the reference image, as demonstrated both quantitatively and qualitatively. Our project page is https://tianhao-qi.github.io/DEADiff/."
https://arxiv.org/abs/2403.06950,2024-03-11,Applicability of oculomics for individual risk prediction: Repeatability and robustness of retinal Fractal Dimension using DART and AutoMorph,"['Justin Engelmann', 'Diana Moukaddem', 'Lucas Gago', 'Niall Strang', 'Miguel O. Bernabeu']","Purpose: To investigate whether Fractal Dimension (FD)-based oculomics could be used for individual risk prediction by evaluating repeatability and robustness. Methods: We used two datasets: Caledonia, healthy adults imaged multiple times in quick succession for research (26 subjects, 39 eyes, 377 colour fundus images), and GRAPE, glaucoma patients with baseline and follow-up visits (106 subjects, 196 eyes, 392 images). Mean follow-up time was 18.3 months in GRAPE, thus it provides a pessimistic lower-bound as vasculature could change. FD was computed with DART and AutoMorph. Image quality was assessed with QuickQual, but no images were initially excluded. Pearson, Spearman, and Intraclass Correlation (ICC) were used for population-level repeatability. For individual-level repeatability, we introduce measurement noise parameter λ which is within-eye Standard Deviation (SD) of FD measurements in units of between-eyes SD. Results: In Caledonia, ICC was 0.8153 for DART and 0.5779 for AutoMorph, Pearson/Spearman correlation (first and last image) 0.7857/0.7824 for DART, and 0.3933/0.6253 for AutoMorph. In GRAPE, Pearson/Spearman correlation (first and next visit) was 0.7479/0.7474 for DART, and 0.7109/0.7208 for AutoMorph (all p<0.0001). Median λ in Caledonia without exclusions was 3.55\% for DART and 12.65\% for AutoMorph, and improved to up to 1.67\% and 6.64\% with quality-based exclusions, respectively. Quality exclusions primarily mitigated large outliers. Worst quality in an eye correlated strongly with λ (Pearson 0.5350-0.7550, depending on dataset and method, all p<0.0001). Conclusions: Repeatability was sufficient for individual-level predictions in heterogeneous populations. DART performed better on all metrics and might be able to detect small, longitudinal changes, highlighting the potential of robust methods."
https://arxiv.org/abs/2403.06949,2024-03-11,Materials science in the era of large language models: a perspective,"['Ge Lei', 'Ronan Docherty', 'Samuel J. Cooper']","Large Language Models (LLMs) have garnered considerable interest due to their impressive natural language capabilities, which in conjunction with various emergent properties make them versatile tools in workflows ranging from complex code generation to heuristic finding for combinatorial problems. In this paper we offer a perspective on their applicability to materials science research, arguing their ability to handle ambiguous requirements across a range of tasks and disciplines mean they could be a powerful tool to aid researchers. We qualitatively examine basic LLM theory, connecting it to relevant properties and techniques in the literature before providing two case studies that demonstrate their use in task automation and knowledge extraction at-scale. At their current stage of development, we argue LLMs should be viewed less as oracles of novel insight, and more as tireless workers that can accelerate and unify exploration across domains. It is our hope that this paper can familiarise material science researchers with the concepts needed to leverage these tools in their own research."
https://arxiv.org/abs/2403.06948,2024-03-11,On the face stratification of the $m=2$ amplituhedron,['Thomas Lam'],"We define and study the face stratification of the m=2 amplituhedron. We show that the face poset is an upper order ideal in the face poset of the totally nonnegative Grassmannian. Our construction is consistent with earlier work of Lukowski, and we confirm various predictions of Lukowski."
https://arxiv.org/abs/2403.06947,2024-03-11,Advancing Generalizable Remote Physiological Measurement through the Integration of Explicit and Implicit Prior Knowledge,"['Yuting Zhang', 'Hao Lu', 'Xin Liu', 'Yingcong Chen', 'Kaishun Wu']","Remote photoplethysmography (rPPG) is a promising technology that captures physiological signals from face videos, with potential applications in medical health, emotional computing, and biosecurity recognition. The demand for rPPG tasks has expanded from demonstrating good performance on intra-dataset testing to cross-dataset testing (i.e., domain generalization). However, most existing methods have overlooked the prior knowledge of rPPG, resulting in poor generalization ability. In this paper, we propose a novel framework that simultaneously utilizes explicit and implicit prior knowledge in the rPPG task. Specifically, we systematically analyze the causes of noise sources (e.g., different camera, lighting, skin types, and movement) across different domains and incorporate these prior knowledge into the network. Additionally, we leverage a two-branch network to disentangle the physiological feature distribution from noises through implicit label correlation. Our extensive experiments demonstrate that the proposed method not only outperforms state-of-the-art methods on RGB cross-dataset evaluation but also generalizes well from RGB datasets to NIR datasets. The code is available at https://github.com/keke-nice/Greip."
https://arxiv.org/abs/2403.06946,2024-03-11,Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation,"['Xinyao Li', 'Yuke Li', 'Zhekai Du', 'Fengling Li', 'Ke Lu', 'Jingjing Li']","Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task. Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities. In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation. Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP's features into language-associated and vision-associated components. Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances. We align features across domains using a modality discriminator. Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS"
https://arxiv.org/abs/2403.06945,2024-03-11,A note on ideal C$^\ast$-completions and amenability,['Tomasz Kochanek'],"For a discrete group $G$, we consider certain ideals $\mathcal{I}\subset c_0(G)$ of sequences with prescribed rate of convergence to zero. We show that the equality between the full group C$^\ast$-algebra of $G$ and the C$^\ast$-completion $\mathrm{C}_{\mathcal{I}}^\ast(G)$ in the sense of Brown and Guentner implies that $G$ is amenable."
https://arxiv.org/abs/2403.06944,2024-03-11,Sj$\ddot{\text{o}}$qvist quantum geometric tensor of finite-temperature mixed states,"['Zheng Zhou', 'Xu-Yang Hou', 'Xin Wang', 'Jia-Chen Tang', 'Hao Guo', 'Chih-Chun Chien']","The quantum geometric tensor (QGT) reveals local geometric properties and associated topological information of quantum states. Here a generalization of the QGT to mixed quantum states at finite temperatures based on the Sj$\ddot{\text{o}}$qvist distance is developed. The resulting Sj$\ddot{\text{o}}$qvist QGT is invariant under gauge transformations of individual spectrum levels. A Pythagorean-like relation connects the distances and gauge transformations, which clarifies the role of the parallel-transport condition. The real part of the QGT naturally decomposes into a sum of the Fisher-Rao metric and Fubini-Study metrics, allowing a distinction between different contributions to the quantum distance. The imaginary part of the QGT is proportional to the weighted summation of the Berry curvatures, which leads to a geometric phase for mixed states under certain conditions. We present three examples of different dimensions to illustrate the temperature dependence of the QGT and a discussion on possible implications."
https://arxiv.org/abs/2403.06943,2024-03-11,AI as a Child of Mother Earth: Regrounding Human-AI Interaction in Ecological Thinking,"['Chunchen Xu', 'Xiao Ge']","The anthropocentric cultural idea that humans are active agents exerting control over their environments has been largely normalized and inscribed in practices, policies, and products of contemporary industrialized societies. This view underlies a human-ecology relationship based on resource and knowledge extraction. To create a more sustainable and equitable future, it is essential to consider alternative cultural ideas rooted in ecological thinking. This perspective underscores the interconnectedness between humans and more-than-human worlds. We propose a path to reshape the human-ecology relationship by advocating for alternative human-AI interactions. In this paper, we undertake a critical comparison between anthropocentrism and ecological thinking, using storytelling to illustrate various human-AI interactions that embody ecological thinking. We also delineate a set of design principles aimed at guiding AI developments toward fostering a more caring human-ecology relationship."
https://arxiv.org/abs/2403.06942,2024-03-11,Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI,"['Lang Tong', 'Xinyi Wang', 'Qing Zhao']","Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference. Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection."
https://arxiv.org/abs/2403.06941,2024-03-11,Comparison of Static Analysis Architecture Recovery Tools for Microservice Applications,"['Simon Schneider', 'Alexander Bakhtin', 'Xiaozhou Li', 'Jacopo Soldani', 'Antonio Brogi', 'Tomas Cerny', 'Riccardo Scandariato', 'Davide Taibi']","Architecture recovery tools help software engineers obtain an overview of their software systems during all phases of the software development lifecycle. This is especially important for microservice applications because their distributed nature makes it more challenging to oversee the architecture. Various tools and techniques for this task are presented in academic and grey literature sources. Practitioners and researchers can benefit from a comprehensive overview of these tools and their abilities. However, no such overview exists that is based on executing the identified tools and assessing their outputs regarding effectiveness. With the study described in this paper, we plan to first identify static analysis architecture recovery tools for microservice applications via a multi-vocal literature review, and then execute them on a common dataset and compare the measured effectiveness in architecture recovery. We will focus on static approaches because they are also suitable for integration into fast-paced CI/CD pipelines."
https://arxiv.org/abs/2403.06940,2024-03-11,Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction,"['Qing Xiao', 'Siyeop Yoon', 'Hui Ren', 'Matthew Tivnan', 'Lichao Sun', 'Quanzheng Li', 'Tianming Liu', 'Yu Zhang', 'Xiang Li']","Alzheimer's Disease (AD) is a neurodegenerative condition characterized by diverse progression rates among individuals, with changes in cortical thickness (CTh) closely linked to its progression. Accurately forecasting CTh trajectories can significantly enhance early diagnosis and intervention strategies, providing timely care. However, the longitudinal data essential for these studies often suffer from temporal sparsity and incompleteness, presenting substantial challenges in modeling the disease's progression accurately. Existing methods are limited, focusing primarily on datasets without missing entries or requiring predefined assumptions about CTh progression. To overcome these obstacles, we propose a conditional score-based diffusion model specifically designed to generate CTh trajectories with the given baseline information, such as age, sex, and initial diagnosis. Our conditional diffusion model utilizes all available data during the training phase to make predictions based solely on baseline information during inference without needing prior history about CTh progression. The prediction accuracy of the proposed CTh prediction pipeline using a conditional score-based model was compared for sub-groups consisting of cognitively normal, mild cognitive impairment, and AD subjects. The Bland-Altman analysis shows our diffusion-based prediction model has a near-zero bias with narrow 95% confidential interval compared to the ground-truth CTh in 6-36 months. In addition, our conditional diffusion model has a stochastic generative nature, therefore, we demonstrated an uncertainty analysis of patient-specific CTh prediction through multiple realizations."
https://arxiv.org/abs/2403.06939,2024-03-11,Surface lattice resonance lasers with epitaxial InP gain medium,"['Anna Fischer', 'Toby Severs Millard', 'Xiaofei Xiao', 'T. V. Raziman', 'Jakub Dranczewski', 'Ross C. Schofield', 'Heinz Schmid', 'Kirsten Moselund', 'Riccardo Sapienza', 'Rupert Oulton']","Surface lattice resonance (SLR) lasers, where gain is supplied by a thin film active material and the feedback comes from multiple scattering by plasmonic nanoparticles, have shown both low threshold lasing and tunability of the angular and spectral emission. However, typically used materials such as organic dyes and QD films suffer from photo-degradation which hampers practical applications. Here, we demonstrate photo-stable single-mode lasing of SLR modes sustained in an epitaxial solid-state InP slab waveguide. The nanoparticle array is weakly coupled to the optical modes, which decreases the scattering losses and hence the experimental lasing threshold is as low as 90 $μ$J/cm$^{2}$. The nanoparticle periodicity defines the lasing wavelength and enables tuneable emission wavelengths over a 70 nm spectral range. Combining plasmonic nanoparticles with an epitaxial solid-state gain medium paves the way for large-area on-chip integrated SLR lasers for applications including optical communication, optical computing, sensing, and LiDAR."
https://arxiv.org/abs/2403.06938,2024-03-11,TCAM-SSD: A Framework for Search-Based Computing in Solid-State Drives,"['Ryan Wong', 'Nikita Kim', 'Kevin Higgs', 'Sapan Agarwal', 'Engin Ipek', 'Saugata Ghose', 'Ben Feinberg']","As the amount of data produced in society continues to grow at an exponential rate, modern applications are incurring significant performance and energy penalties due to high data movement between the CPU and memory/storage. While processing in main memory can alleviate these penalties, it is becoming increasingly difficult to keep large datasets entirely in main memory. This has led to a recent push for in-storage computation, where processing is performed inside the storage device."
https://arxiv.org/abs/2403.06937,2024-03-11,Distributed computing quantum unitary evolution,"['Hui-hui Miao', 'Yuri Igorevich Ozhigov']","A distributed computing approach to solve the curse of dimensionality, caused by the complex quantum system modeling, is discussed. With the help of Cannon's algorithm, the distributed computing transformation of numerical method for simulating quantum unitary evolution is achieved. Based on the Tavis-Cummings model, a large number of atoms are added into the optical cavity to obtain a high-dimensional quantum closed system, implemented on the supercomputer platform. The comparison of time cost and speedup of different distributed computing strategies is discussed."
https://arxiv.org/abs/2403.06936,2024-03-11,Counterfactual Reasoning with Knowledge Graph Embeddings,"['Lena Zellinger', 'Andreas Stephan', 'Benjamin Roth']","Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories. In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR. We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules. We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained. We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark. Our results indicate that KGEs learn patterns in the graph without explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow these patterns. An evaluation on human-annotated data reveals that KGEs adapted with COULDD are mostly unable to recognize changes to the graph that do not follow learned inference rules. In contrast, ChatGPT mostly outperforms KGEs in detecting plausible changes to the graph but has poor knowledge retention. In summary, CFKGR connects two previously distinct areas, namely KG completion and counterfactual reasoning."
https://arxiv.org/abs/2403.06935,2024-03-13,"Naming, Describing, and Quantifying Visual Objects in Humans and LLMs","['Alberto Testoni', 'Juell Sprott', 'Sandro Pezzelle']","While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision \& Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, humans show clear production preferences for highly context-sensitive expressions, such as the quantifiers `few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the ability of VLLMs to capture human naming preferences, with all models failing in tasks that require high-level reasoning such as assigning quantifiers."
https://arxiv.org/abs/2403.06934,2024-03-11,Impact of spin torques and spin pumping phenomena on magnon-plasmon polaritons in antiferromagnetic insulator-semiconductor heterostructures,"['Vemund Falch', 'Jeroen Danon', 'Alireza Qaiumzadeh', 'Arne Brataas']","We investigate the impact of spin torque and spin pumping on the surface magnon polariton dispersion in a antiferromagnetic insulator-semiconductor heterostructure. In the bilayer system, the surface magnon polaritons conventionally couple to the plasma-oscillations in the semiconductor via electromagnetic fields. Additionally, magnons in the antiferromagnetic insulator layer may interact with the semiconductor layer via spin torques and their reciprocal phenomena of spin pumping. Due to the spin-to-charge conversion from the spin Hall and inverse spin Hall effects in the semiconductor layer with a strong spin-orbit coupling, this can couple the magnons to the plasmons in the semiconductor layer. Our research reveals that modifications in the mode frequency and the hybridization gap induced by these phenomena depend on the thickness of the antiferromagnetic layer. In thick layers, the spin-pumping contribution to the frequency shift and damping is inversely proportional to the wavelength, while in thin layers it is inversely proportional to the thickness. Furthermore, hybridization of the surface magnon polariton and dispersive magnons in the antiferromagnet is shown to depend on both the thickness and wavelength of the modes."
https://arxiv.org/abs/2403.06933,2024-03-11,On the stability of fully nonlinear hydraulic-fall solutions to the forced water-wave problem,"['Jack S. Keeler', 'Mark G. Blyth']","Two-dimensional free-surface flow over localised topography is examined with the emphasis on the stability of hydraulic-fall solutions. A Gaussian topography profile is assumed with a positive or negative amplitude modelling a bump or a dip, respectively. Steady hydraulic-fall solutions to the full incompressible, irrotational Euler equations are computed, and their linear and nonlinear stability is analysed by computing eigenspectra of the pertinent linearised operator and by solving an initial value problem. The computations are carried out numerically using a specially developed computational framework based on the finite element method. The Hamiltonian structure of the problem is demonstrated and stability is determined by computing eigenspectra of the pertinent linearised operator. It is found that a hydraulic-fall flow over a bump is spectrally stable. The corresponding flow over a dip is found to be linearly unstable. In the latter case, time-dependent simulations show that the flow ultimately settles into a time-periodic motion that corresponds to an invariant solution in an appropriately defined phase space. Physically, the solution consists of a localised large amplitude wave that pulsates above the dip while simultaneously emitting nonlinear cnoidal waves in the upstream direction and multi-harmonic linear waves in the downstream direction."
https://arxiv.org/abs/2403.06932,2024-03-11,ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis,"['Yanming Liu', 'Xinyue Peng', 'Tianyu Du', 'Jianwei Yin', 'Weihao Liu', 'Xuhong Zhang']","Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs."
https://arxiv.org/abs/2403.06931,2024-03-11,Optimizing sDTW for AMD GPUs,"['Daniel Latta-Lin', 'Sofia Isadora Padilla Munoz']","Subsequence Dynamic Time Warping (sDTW) is the metric of choice when performing many sequence matching and alignment tasks. While sDTW is flexible and accurate, it is neither simple nor fast to compute; significant research effort has been spent devising parallel implementations on the GPU that leverage efficient memory access and computation patterns, as well as features offered by specific vendors and architectures (notably NVIDIA's). We present an implementation of sDTW on AMD hardware using HIP and ROCm. Our implementation employs well-known parallel patterns, as well as lower-level features offered by ROCm. We use shuffling for intra-wavefront communication and shared memory to transfer data between consecutive wavefronts. By constraining the input data to batches of 512 queries of length 2,000, we optimized for peak performance the width of reference elements operated on by a single thread."
https://arxiv.org/abs/2403.06930,2024-03-11,Heavy Ball Momentum for Non-Strongly Convex Optimization,"['Jean-François Aujol', 'Charles Dossal', 'Hippolyte Labarrière', 'Aude Rondepierre']","When considering the minimization of a quadratic or strongly convex function, it is well known that first-order methods involving an inertial term weighted by a constant-in-time parameter are particularly efficient (see Polyak [32], Nesterov [28], and references therein). By setting the inertial parameter according to the condition number of the objective function, these methods guarantee a fast exponential decay of the error. We prove that this type of schemes (which are later called Heavy Ball schemes) is relevant in a relaxed setting, i.e. for composite functions satisfying a quadratic growth condition. In particular, we adapt V-FISTA, introduced by Beck in [10] for strongly convex functions, to this broader class of functions. To the authors' knowledge, the resulting worst-case convergence rates are faster than any other in the literature, including those of FISTA restart schemes. No assumption on the set of minimizers is required and guarantees are also given in the non-optimal case, i.e. when the condition number is not exactly known. This analysis follows the study of the corresponding continuous-time dynamical system (Heavy Ball with friction system), for which new convergence results of the trajectory are shown."
https://arxiv.org/abs/2403.06929,2024-03-11,NLO+NLL' accurate predictions for three-jet event shapes in hadronic Higgs decays,"['Aude Gehrmann-De Ridder', 'Christian T Preuss', 'Daniel Reichelt', 'Steffen Schumann']","We present resummed predictions at next-to-leading logarithmic accuracy matched to the exact next-to-leading order results for a set of classical event-shape observables in hadronic Higgs decays, i.e., for the channels $H\to gg$ and $H\to b\bar{b}$. We furthermore consider soft-drop grooming of the hadronic final states and derive corresponding $\text{NLO}+\text{NLL}^\prime$ predictions for the groomed thrust observable. Differences in the QCD radiation pattern of gluon- and quark-initiated final states are imprinted in the event-shape distributions, offering separation power for the two decay channels. In particular, we show that ungroomed event shapes in $H\to gg$ decays develop a considerably harder spectrum than in $H\to b\bar b$ decays. We highlight that soft-drop grooming can substantially alter this behaviour, unless rather inclusive grooming parameters are chosen."
https://arxiv.org/abs/2403.06928,2024-03-11,"3D simulations of TRAPPIST-1e with varying CO2, CH4 and haze profiles","['Mei Ting Mak', 'Denis Sergeev', 'Nathan Mayne', 'Nahum Banks', 'Jake Eager-Nash', 'James Manners', 'Giada Arney', 'Eric Hebrard', 'Krisztian Kohary']","Using a 3D General Circulation Model, the Unified Model, we present results from simulations of a tidally-locked TRAPPIST-1e with varying carbon dioxide CO2 and methane CH4 gas concentrations, and their corresponding prescribed spherical haze profiles. Our results show that the presence of CO2 leads to a warmer atmosphere globally due to its greenhouse effect, with the increase of surface temperature on the dayside surface reaching up to ~14.1 K, and on the nightside up to ~21.2 K. Increasing presence of CH4 first elevates the surface temperature on the dayside, followed by a decrease due to the balance of tropospheric warming and stratospheric cooling. A thin layer of haze, formed when the partial pressures of CH4 to CO2 (pCH4/pCO2) = 0.1, leads to a dayside warming of ~4.9K due to a change in the water vapour H2O distribution. The presence of a haze layer that formed beyond the ratio of 0.1 leads to dayside cooling. The haze reaches an optical threshold thickness when pCH4/pCO2 ~0.4 beyond which the dayside mean surface temperature does not vary much. The planet is more favourable to maintaining liquid water on the surface (mean surface temperature above 273.15 K) when pCO2 is high, pCH4 is low and the haze layer is thin. The effect of CO2, CH4 and haze on the dayside is similar to that for a rapidly-rotating planet. On the contrary, their effect on the nightside depends on the wind structure and the wind speed in the simulation."
https://arxiv.org/abs/2403.06927,2024-03-11,Effective multiband synthetic four-wave mixing by cascading quadratic processes,"['Li Chen', 'Zheng Ge', 'Su-Jian Niu', 'Yin-Hai Li', 'Zhao-Qi-Zhi Han', 'Yue-Wei Song', 'Wu-Zhen Li', 'Ren-Hui Chen', 'Ming-Yuan Gao', 'Meng-Yu Xie', 'Zhi-Yuan Zhou', 'Bao-Sen Shi']","Four wave mixing (FWM) is an important way to generate supercontinuum and frequency combs in the mid-infrared band. Here, we obtain simultaneous synthetic FWM in the visible and mid-infrared bands by cascading quadratic nonlinear processes in a periodically poled lithium niobate crystal (PPLN), which has a 110dB(at 3000nm) higher conversion efficiency than the FWM directly generated by third-order susceptibilities in bulk PPLN crystals. A general model of this process is developed that is in full agreement with the experimental verifications. The frequency difference between the new frequency components can be freely tuned by changing the frequency difference of the dual pump lasers. Furthermore, by increasing the conversion bandwidth and efficiency of the cascaded processes, it is feasible to generate frequency combs in three bands the visible, near-infrared and mid-infrared bands simultaneously through high-order cascaded processes. This work opens up a new avenue toward free-tuning multiband frequency comb generation with multi-octaves frequency spanning, which will have significant applications in fields such as mid-infrared gas sensing, lidar and precision spectroscopy."
https://arxiv.org/abs/2403.06926,2024-03-11,"Sheaves of $(\infty, \infty)$-categories",['Zach Goldthorpe'],"We provide a functorial presentation of the $(\infty, 1)$-category of sheaves of $(n, r)$-categories for all $-2 \leq n\leq\infty$ and $0 \leq r\leq n+2$ baed on complete Segal space objects. In this definition, the equivalences of sheaves of $(\infty, \infty)$-categories are defined inductively, so we also provide a localisation at the coinductive equivalences to define the $(\infty, 1)$-category of sheaves of $ω$-categories as well. We prove that the $(\infty, 1)$-category of sheaves of $(\infty, \infty)$-categories and the $(\infty, 1)$-category of sheaves of $ω$-categories both define distributors over the underlying topos of sheaves of spaces. Moreover, we show that these distributors define terminal and initial fixed points with respect to the construction of complete Segal space objects in a distributor. We conclude with a sheafification result: the category of sheaves of $(n, r)$-categories over a site $\mathscr{C}$ can be presented as a strongly reflective localisation of $\mathbf{Fun}(\mathscr{C}^{\mathrm{op}}, \mathbf{Cat}_{(n, r)})$, where the localisation functor preserves fibre products over $\mathbf{Sh}(\mathscr{C})$, with the analogous result also holding for sheaves of $ω$-categories."
https://arxiv.org/abs/2403.06925,2024-03-11,Simplicity Bias of Transformers to Learn Low Sensitivity Functions,"['Bhavya Vasudeva', 'Deqing Fu', 'Tianyi Zhou', 'Elliott Kau', 'Youqi Huang', 'Vatsal Sharan']","Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with improved robustness; furthermore, it can also be used as an efficient intervention to further improve the robustness of transformers."
https://arxiv.org/abs/2403.06924,2024-03-11,A method for accelerating low precision operations by sparse matrix multiplication,['Hongyaoxing Gu'],"In recent years, the fervent demand for computational power across various domains has prompted hardware manufacturers to introduce specialized computing hardware aimed at enhancing computational capabilities. Particularly, the utilization of tensor hardware supporting low precision has gained increasing prominence in scientific research. However, the use of low-precision tensor hardware for computational acceleration often introduces errors, posing a fundamental challenge of simultaneously achieving effective acceleration while maintaining computational accuracy."
https://arxiv.org/abs/2403.06923,2024-03-11,A unified diagrammatic approach in Liouville space to quantum transport for bosonic and fermionic reservoirs,"['L. Magazzù', 'E. Paladino', 'M. Grifoni']","We present a diagrammatic approach to quantum transport based on a master equation formalism in Liouville space. It can be applied to linear and nonlinear transport in generic multi-level junctions coupled to bosonic or fermionic reservoirs and presents a convenient perturbation expansion in the strength of the coupling between the reservoirs and the junction. The Redfield theory is recovered at second order, with the partial and full secular master equations discussed. Analytical, approximate expressions are provided up to fourth order for the steady-state boson transport that generalize to multi-level systems the known formula for the low-temperature thermal conductance in the spin-boson model. The formalism is applied to the problem of heat transport in a qubit-resonator junction modeled by the quantum Rabi model. Nontrivial transport features emerge as a result of the interplay between the qubit-oscillator detuning and coupling strength. For quasi-degenerate spectra, nonvanishing steady-state coherences cause a suppression of the thermal conductance."
https://arxiv.org/abs/2403.06922,2024-03-11,Kantowski-Sachs and Bianchi III dynamics in $f\left(Q\right)$-gravity,"['Alfredo D. Millano', 'K. Dialektopoulos', 'N. Dimakis', 'A. Giacomini', 'H. Shababi', 'Amlan Halder', 'A. Paliathanasis']","We explore the phase-space of homogeneous and anisotropic spacetimes within symmetric teleparallel $f(Q)$-gravity. Specifically, we consider the Kantowski-Sachs and locally rotational Bianchi III geometries to describe the physical space. By analyzing the phase-space, we reconstruct the cosmological history dictated by $f(Q)$-gravity and comment about the theory's viability. Our findings suggest that the free parameters of the connection must be constrained to eliminate nonlinear terms in the field equations. Consequently, new stationary points emerge, rendering the theory cosmologically viable. We identify the existence of anisotropic accelerated universes, which may correspond to the pre-inflationary epoch."
https://arxiv.org/abs/2403.06921,2024-03-11,Synthesis of Robust Optimal Strategies in Weighted Timed Games,"['Benjamin Monmege', 'Julie Parreaux', 'Pierre-Alain Reynier']","Weighted Timed Games (WTG for short) are the most widely used model to describe controller synthesis problems involving real-time issues. The synthesized strategies rely on a perfect measure of time elapse, which is not realistic in practice. In order to produce strategies tolerant to timing imprecisions, we rely on a notion of robustness first introduced for timed automata. More precisely, WTGs are two-player zero-sum games played in a timed automaton equipped with integer weights in which one of the players, that we call Min, wants to reach a target location while minimising the cumulated weight. In this work, we equip the underlying timed automaton with a semantics depending on some parameter (representing the maximal possible perturbation) in which the opponent of Min can in addition perturb delays chosen by Min."
https://arxiv.org/abs/2403.06920,2024-03-11,Distributed Average Consensus via Noisy and Non-Coherent Over-the-Air Aggregation,"['Huiwen Yang', 'Xiaomeng Chen', 'Lingying Huang', 'Subhrakanti Dey', 'Ling Shi']","Over-the-air aggregation has attracted widespread attention for its potential advantages in task-oriented applications, such as distributed sensing, learning, and consensus. In this paper, we develop a communication-efficient distributed average consensus protocol by utilizing over-the-air aggregation, which exploits the superposition property of wireless channels rather than combat it. Noisy channels and non-coherent transmission are taken into account, and only half-duplex transceivers are required. We prove that the system can achieve average consensus in mean square and even almost surely under the proposed protocol. Furthermore, we extend the analysis to the scenarios with time-varying topology. Numerical simulation shows the effectiveness of the proposed protocol."
https://arxiv.org/abs/2403.06919,2024-03-11,Hydrogen Column Density Variability in a Sample of Local Compton-Thin AGN II,"['A. Pizzetti', 'N. Torres-Alba', 'S. Marchesi', 'J. Buchner', 'I. Cox', 'X. Zhao', 'S. Neal', 'D. Sengupta', 'R. Silver', 'M. Ajello']","We present the multi-epoch analysis of 13 variable, nearby (z<0.1), Compton-thin (22<logN_H<24) active galactic nuclei (AGN) selected from the 105-month BAT catalog. Analyzing all available archival soft and hard X-ray observations, we investigate the line-of-sight hydrogen column density (N_H) variability on timescales ranging from a few days to approximately 20 years. Each source is analyzed by simultaneously modeling the data with three physical torus models, providing tight constraints on torus properties, including the covering factor, the cloud dispersion, and the torus average hydrogen column density (N_H,av). For each epoch, we measure the N_H and categorize the source as `N_H Variable', `Non-variable in N_H', or `Undetermined' based on the degree of variability. Our final sample includes 27 variable, Compton-thin AGN after implementing another 14 AGN analyzed in our previous work. We find that all sources require either flux or N_H variability. We classify 37% of them as `N_H Variable', 44% as `Non-variable in N_H', and 19% as `Undetermined'. Noticeably, there is no discernible difference between geometrical and intrinsic properties among the three variability classes, suggesting no intrinsic differences between the N_H-variable and non-variable sources. We measure the median variation in N_H between any observation pair of the same source to be 25% with respect to the lowest N_H measure in the pair. Furthermore, 48% of the analyzed sources require the inclusion of a Compton-thick reflector in the spectral fitting. Among these, the 30% exhibits recorded 22 GHz water megamaser emission, suggesting a potential shared nature between the two structures."
https://arxiv.org/abs/2403.06918,2024-03-11,Memory in cyclically crumpled sheets,"['Amit Dawadi', 'Arshad Kudrolli']","We investigate the crumpling of a sheet as it is repeatedly crushed onto itself by rolling it into a cylinder and twisting it axially while allowing the end-to-end length to evolve freely. As deduced from its plastic deformations, the sheet creases and collapses into structures which repeat and sharpen over hundreds of cycles to a remarkable degree before forming new configurations. The observed metastablilty increases with applied cycles leading to recurrent structures over a significant range of loading, but reconfigurations can continue to occur for large enough loading as the creases develop tears. The evolution of the sheet structure as measured by the mean curvature and the total crease length is found to increase logarithmically with cycle number with a rate which increases with degree of compression. We explain the overall extent of creasing using flat folding models, and show the logarithmic growth as being a consequence of individual creases becoming sharper with number of folding cycles, and due to the bifurcation in the curvature field leading to the formation of new creases and folding pathways. Thus, we show that elastoplastic sheets can follow complex folding pathways to form convergent structures after a sufficiently large number of training cycles provided material fatigue remains unimportant."
https://arxiv.org/abs/2403.06917,2024-03-11,Double Eisenstein series and modular forms of level $4$,['Katsumi Kina'],"We study the $\mthbb{Q}$-vector space generated by the double zeta values with character of conductor $4$. For this purpose, we define associated double Eisenstein series and investigate their relation with modular forms of level $4$."
https://arxiv.org/abs/2403.06916,2024-03-11,Energy dissipation in earthquakes,"['David S. Kammer', 'Gregory C. McLaskey', 'Rachel E. Abercrombie', 'Jean-Paul Ampuero', 'Camilla Cattania', 'Massimo Cocco', 'Luca Dal Zilio', 'Georg Dresen', 'Alice-Agnes Gabriel', 'Chun-Yu Ke', 'Chris Marone', 'Paul A. Selvadurai', 'Elisa Tinti']","Earthquakes are rupture-like processes that propagate along tectonic faults and cause seismic waves. The propagation speed and final area of the rupture, which determine an earthquake's potential impact, are directly related to the nature and quantity of the energy dissipation involved in the rupture process. Here we present the challenges associated with defining and measuring the energy dissipation in laboratory and natural earthquakes across many scales. We discuss the importance and implications of distinguishing between energy dissipation that occurs close to and far behind the rupture tip and we identify open scientific questions related to a consistent modeling framework for earthquake physics that extends beyond classical Linear Elastic Fracture Mechanics."
https://arxiv.org/abs/2403.06915,2024-03-11,Monitoring the Venice Lagoon: an IoT Cloud-Based Sensor Nerwork Approach,"['Filippo Campagnaro', 'Matin Ghalkhani', 'Riccardo Tumiati', 'Federico Marin', 'Matteo Del Grande', 'Alessandro Pozzebon', 'Davide De Battisti', 'Roberto Francescon', 'Michele Zorzi']","Monitoring the coastal area of the Venice Lagoon is of significant importance. While the impact of global warming is felt worldwide, coastal and littoral regions bear the brunt more prominently. These areas not only face the threat of rising sea levels but also contend with the escalating occurrence of seaquakes and floods. Additionally, the intricate ecosystems of rivers, seas, and lakes undergo profound transformations due to climate change and pollutants."
https://arxiv.org/abs/2403.06914,2024-03-12,MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning,"['Yichuan Li', 'Xiyao Ma', 'Sixing Lu', 'Kyumin Lee', 'Xiaohu Liu', 'Chenlei Guo']","Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrations through a two-stage training process, which includes meta-distillation pretraining and fine-tuning. Comprehensive evaluations across seven diverse ICL task partitions using decoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not only matches but often outperforms the Vanilla ICL as well as other state-of-the-art distillation models, while significantly reducing the computational demands. This innovation promises enhanced scalability and efficiency for the practical deployment of large language models"
https://arxiv.org/abs/2403.06913,2024-03-11,Accurate and Interpretable Representation of Correlated Electronic Structure via Tensor Product Selected CI,"['Nicole M. Braunscheidel', 'Arnab Bachhar', 'Nicholas J. Mayhall']","The task of computing wavefunctions that are accurate, yet simple enough mathematical objects to use for reasoning has long been a challenge in quantum chemistry. The difficulty in drawing physical conclusions from a wavefunction is often related to the generally large number of configurations with similar weights. In Tensor Product Selected CI, we use a locally correlated tensor product state basis, which has the effect of concentrating the weight of a state onto a smaller number of physically interpretable degrees of freedom. In this paper, we apply TPSCI to a series of three molecular systems ranging in separability, one of which is the first application of TPSCI to an open-shell bimetallic system. For each of these systems, we obtain accurate solutions to large active spaces, and analyze the resulting wavefunctions through a series of different approaches including (i) direct inspection of the TPS basis coefficients, (ii) construction of Bloch effective Hamiltonians, and (iii) computation of cluster correlation functions."
https://arxiv.org/abs/2403.06912,2024-03-13,DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization,"['Jiahe Li', 'Jiawei Zhang', 'Xiao Bai', 'Jin Zheng', 'Xin Ning', 'Jun Zhou', 'Lin Gu']","Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \times$ reduction in training time, and over $3000 \times$ faster rendering speed."
https://arxiv.org/abs/2403.06911,2024-03-11,Homotopical commutative rings and bispans,"['Bastiaan Cnossen', 'Rune Haugseng', 'Tobias Lenz', 'Sil Linskens']","We prove that commutative semirings in a cartesian closed presentable $\infty$-category, as defined by Groth, Gepner, and Nikolaus, are equivalent to product-preserving functors from the $(2,1)$-category of bispans of finite sets. In other words, we identify the latter as the Lawvere theory for commutative semirings in the $\infty$-categorical context. This implies that connective commutative ring spectra can be described as grouplike product-preserving functors from bispans of finite sets to spaces. A key part of the proof is a localization result for $\infty$-categories of spans, and more generally for $\infty$-categories with factorization systems, that may be of independent interest."
https://arxiv.org/abs/2403.06910,2024-03-11,Responsible Artificial Intelligence: A Structured Literature Review,"['Sabrina Goellner', 'Marina Tropmann-Frick', 'Bostjan Brumen']","Our research endeavors to advance the concept of responsible artificial intelligence (AI), a topic of increasing importance within EU policy discussions. The EU has recently issued several publications emphasizing the necessity of trust in AI, underscoring the dual nature of AI as both a beneficial tool and a potential weapon. This dichotomy highlights the urgent need for international regulation. Concurrently, there is a need for frameworks that guide companies in AI development, ensuring compliance with such regulations. Our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of AI regulation, identifying focal areas for future attention. This paper introduces a comprehensive and, to our knowledge, the first unified definition of responsible AI. Through a structured literature review, we elucidate the current understanding of responsible AI. Drawing from this analysis, we propose an approach for developing a future framework centered around this concept. Our findings advocate for a human-centric approach to Responsible AI. This approach encompasses the implementation of AI methods with a strong emphasis on ethics, model explainability, and the pillars of privacy, security, and trust."
https://arxiv.org/abs/2403.06909,2024-03-11,Heat transport in the quantum Rabi model: Universality and ultrastrong coupling effects,"['L. Magazzù', 'E. Paladino', 'M. Grifoni']","Heat transport in the quantum Rabi model at weak interaction with the heat baths is controlled by the qubit-oscillator coupling. Universality of the linear conductance versus the temperature is found for $T\lesssim T_K$, with $T_K$ a coupling-dependent Kondo-like temperature. At low temperature, coherent heat transfer via virtual processes yields a $\sim T^3$ behavior with destructive interference in the presence of quasi-degeneracies in the spectrum. As the temperature increases, incoherent emission and absorption dominate and a maximum is reached at $T\sim T_K/2$. In the presence of a bias on the qubit, the conductance makes a transition from a resonant to a broad, zero-bias peak regime. Parallels and differences are found compared to the spin-boson model in [K. Saito and T. Kato, Phys. Rev. Lett. \textbf{111}, 214301 (2013)], where the qubit-bath coupling instead of the internal qubit-oscillator coupling rules thermal transport."
https://arxiv.org/abs/2403.06908,2024-03-11,FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization,"['Jiahui Zhang', 'Fangneng Zhan', 'Muyu Xu', 'Shijian Lu', 'Eric Xing']","3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis. However, it often suffers from over-reconstruction during Gaussian densification where high-variance image regions are covered by a few large Gaussians only, leading to blur and artifacts in the rendered images. We design a progressive frequency regularization (FreGS) technique to tackle the over-reconstruction issue within the frequency space. Specifically, FreGS performs coarse-to-fine Gaussian densification by exploiting low-to-high frequency components that can be easily extracted with low-pass and high-pass filters in the Fourier space. By minimizing the discrepancy between the frequency spectrum of the rendered image and the corresponding ground truth, it achieves high-quality Gaussian densification and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments over multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and Deep Blending) show that FreGS achieves superior novel view synthesis and outperforms the state-of-the-art consistently."
https://arxiv.org/abs/2403.06907,2024-03-11,Towards Incident Response Orchestration and Automation for the Advanced Metering Infrastructure,"['Alexios Lekidis', 'Vasileios Mavroeidis', 'Konstantinos Fysarakis']","The threat landscape of industrial infrastructures has expanded exponentially over the last few years. Such infrastructures include services such as the smart meter data exchange that should have real-time availability. Smart meters constitute the main component of the Advanced Metering Infrastructure, and their measurements are also used as historical data for forecasting the energy demand to avoid load peaks that could lead to blackouts within specific areas. Hence, a comprehensive Incident Response plan must be in place to ensure high service availability in case of cyber-attacks or operational errors. Currently, utility operators execute such plans mostly manually, requiring extensive time, effort, and domain expertise, and they are prone to human errors. In this paper, we present a method to provide an orchestrated and highly automated Incident Response plan targeting specific use cases and attack scenarios in the energy sector, including steps for preparedness, detection and analysis, containment, eradication, recovery, and post-incident activity through the use of playbooks. In particular, we use the OASIS Collaborative Automated Course of Action Operations (CACAO) standard to define highly automatable workflows in support of cyber security operations for the Advanced Metering Infrastructure. The proposed method is validated through an Advanced Metering Infrastructure testbed where the most prominent cyber-attacks are emulated, and playbooks are instantiated to ensure rapid response for the containment and eradication of the threat, business continuity on the smart meter data exchange service, and compliance with incident reporting requirements."
https://arxiv.org/abs/2403.06906,2024-03-11,Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints,"['Jean V. Alves', 'Diogo Leitão', 'Sérgio Jesus', 'Marco O. P. Sampaio', 'Javier Liébana', 'Pedro Saleiro', 'Mário A. T. Figueiredo', 'Pedro Bizarro']","Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series of cost-sensitive fraud detection scenarios with different teams of 9 synthetic fraud analysts, with individual work capacity constraints. The results demonstrate that our approach performs significantly better than the baselines in a wide array of scenarios, achieving an average 8.4% reduction in the misclassification cost."
https://arxiv.org/abs/2403.06905,2024-03-11,Biphoton State Reconstruction via Phase Retrieval Methods,"['Nazanin Dehghan', ""Alessio D'Errico"", 'Francesco Di Colandrea', 'Ebrahim Karimi']","The complete measurement of the quantum state of two correlated photons requires reconstructing the amplitude and phase of the biphoton wavefunction. We show how, by means of spatially resolved single photon detection, one can infer the spatial structure of bi-photons generated by spontaneous parametric down conversion. In particular, a spatially resolved analysis of the second-order correlations allows us to isolate the moduli of the pump and phasematching contributions to the two-photon states. When carrying this analysis on different propagation planes, the free space propagation of pump and phasematching is observed. This result allows, in principle, to gain enough information to reconstruct also the phase of pump and phasematching, and thus the full biphoton wavefunction. We show this in different examples where the pump is shaped as a superposition of orbital angular momentum modes or as a smooth amplitude with a phase structure with no singularities. The corresponding phase structure is retrieved employing maximum likelihood or genetic algorithms. These findings have potential applications in fast, efficient quantum state characterisation that does not require any control over the source."
https://arxiv.org/abs/2403.06904,2024-03-11,FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in Human-Centric Tasks,"['Muhammad Saif Ullah Khan', 'Muhammad Ferjad Naeem', 'Federico Tombari', 'Luc Van Gool', 'Didier Stricker', 'Muhammad Zeshan Afzal']","We propose FocusCLIP, integrating subject-level guidance--a specialized mechanism for target-specific supervision--into the CLIP framework for improved zero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP on both the vision and text sides. On the vision side, we incorporate ROI heatmaps emulating human visual attention mechanisms to emphasize subject-relevant image regions. On the text side, we introduce human pose descriptions to provide rich contextual information. For human-centric tasks, FocusCLIP is trained with images from the MPII Human Pose dataset. The proposed approach surpassed CLIP by an average of 8.61% across five previously unseen datasets covering three human-centric tasks. FocusCLIP achieved an average accuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement in activity recognition, a 14.78% improvement in age classification, and a 7.06% improvement in emotion recognition. Moreover, using our proposed single-shot LLM prompting strategy, we release a high-quality MPII Pose Descriptions dataset to encourage further research in multimodal learning for human-centric tasks. Furthermore, we also demonstrate the effectiveness of our subject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47% improvement over CLIP in zero-shot bird classification using the CUB dataset. Our findings emphasize the potential of integrating subject-level guidance with general pretraining methods for enhanced downstream performance."
https://arxiv.org/abs/2403.06903,2024-03-11,Benign overfitting in leaky ReLU networks with moderate input dimension,"['Kedar Karhadkar', 'Erin George', 'Michael Murray', 'Guido Montúfar', 'Deanna Needell']","The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property. In contrast to prior work we do not require near orthogonality conditions on the training data: notably, for input dimension $d$ and training sample size $n$, while prior work shows asymptotically optimal error when $d = Ω(n^2 \log n)$, here we require only $d = Ω\left(n \log \frac{1}ε\right)$ to obtain error within $ε$ of optimal."
https://arxiv.org/abs/2403.06902,2024-03-11,Deep adaptative spectral zoom for improved remote heart rate estimation,"['Joaquim Comas', 'Adria Ruiz', 'Federico Sukno']","Recent advances in remote heart rate measurement, motivated by data-driven approaches, have notably enhanced accuracy. However, these improvements primarily focus on recovering the rPPG signal, overlooking the implicit challenges of estimating the heart rate (HR) from the derived signal. While many methods employ the Fast Fourier Transform (FFT) for HR estimation, the performance of the FFT is inherently affected by a limited frequency resolution. In contrast, the Chirp-Z Transform (CZT), a generalization form of FFT, can refine the spectrum to the narrow-band range of interest for heart rate, providing improved frequential resolution and, consequently, more accurate estimation. This paper presents the advantages of employing the CZT for remote HR estimation and introduces a novel data-driven adaptive CZT estimator. The objective of our proposed model is to tailor the CZT to match the characteristics of each specific dataset sensor, facilitating a more optimal and accurate estimation of HR from the rPPG signal without compromising generalization across diverse datasets. This is achieved through a Sparse Matrix Optimization (SMO). We validate the effectiveness of our model through exhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE, and UBFC-rPPG employing both intra- and cross-database performance metrics. The results reveal outstanding heart rate estimation capabilities, establishing the proposed approach as a robust and versatile estimator for any rPPG method."
https://arxiv.org/abs/2403.06901,2024-03-11,LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration,"['Dingrong Wang', 'Soheil Azadvar', 'Jon Heiselman', 'Xiajun Jiang', 'Michael Miga', 'Linwei Wang']","The surgical environment imposes unique challenges to the intraoperative registration of organ shapes to their preoperatively-imaged geometry. Biomechanical model-based registration remains popular, while deep learning solutions remain limited due to the sparsity and variability of intraoperative measurements and the limited ground-truth deformation of an organ that can be obtained during the surgery. In this paper, we propose a novel \textit{hybrid} registration approach that leverage a linearized iterative boundary reconstruction (LIBR) method based on linear elastic biomechanics, and use deep neural networks to learn its residual to the ground-truth deformation (LIBR+). We further formulate a dual-branch spline-residual graph convolutional neural network (SR-GCN) to assimilate information from sparse and variable intraoperative measurements and effectively propagate it through the geometry of the 3D organ. Experiments on a large intraoperative liver registration dataset demonstrated the consistent improvements achieved by LIBR+ in comparison to existing rigid, biomechnical model-based non-rigid, and deep-learning based non-rigid approaches to intraoperative liver registration."
https://arxiv.org/abs/2403.06900,2024-03-11,"Dynamic Client Clustering, Bandwidth Allocation, and Workload Optimization for Semi-synchronous Federated Learning","['Liangkun Yu', 'Xiang Sun', 'Rana Albelaihi', 'Chaeeun Park', 'Sihua Shao']","Federated Learning (FL) revolutionizes collaborative machine learning among Internet of Things (IoT) devices by enabling them to train models collectively while preserving data privacy. FL algorithms fall into two primary categories: synchronous and asynchronous. While synchronous FL efficiently handles straggler devices, it can compromise convergence speed and model accuracy. In contrast, asynchronous FL allows all devices to participate but incurs high communication overhead and potential model staleness. To overcome these limitations, the semi-synchronous FL framework introduces client tiering based on computing and communication latencies. Clients in different tiers upload their local models at distinct frequencies, striking a balance between straggler mitigation and communication costs. Enter the DecantFed algorithm (Dynamic client clustering, bandwidth allocation, and local training for semi-synchronous Federated learning), a dynamic solution that optimizes client clustering, bandwidth allocation, and local training workloads to maximize data sample processing rates. Additionally, DecantFed adapts client learning rates according to their tiers, addressing the model staleness problem. The algorithm's performance shines in extensive simulations using benchmark datasets, including MNIST and CIFAR-10, under independent and identically distributed (IID) and non-IID scenarios. DecantFed outpaces FedAvg and FedProx in terms of convergence speed and delivers a remarkable minimum 28% boost in model accuracy compared to FedProx."
https://arxiv.org/abs/2403.06899,2024-03-11,Multiobject Tracking for Thresholded Cell Measurements,"['Thomas Kropfreiter', 'Jason L. Williams', 'Florian Meyer']","In many multiobject tracking applications, including radar and sonar tracking, after prefiltering the received signal, measurement data is typically structured in cells. The cells, e.g., represent different range and bearing values. However, conventional multiobject tracking methods use so-called point measurements. Point measurements are provided by a preprocessing stage that applies a threshold or detector and breaks up the cell's structure by converting cell indexes into, e.g., range and bearing measurements. We here propose a Bayesian multiobject tracking method that processes measurements that have been thresholded but are still cell-structured. We first derive a likelihood function that systematically incorporates an adjustable detection threshold which makes it possible to control the number of cell measurements. We then propose a Poisson Multi-Bernoulli (PMB) filter based on the likelihood function for cell measurements. Furthermore, we establish a link to the conventional point measurement model by deriving the likelihood function for point measurements with amplitude information (AM) and discuss the PMB filter that uses point measurements with AM. Our numerical results demonstrate the advantages of the proposed method that relies on thresholded cell measurements compared to the conventional multiobject tracking based on point measurements with and without AM."
https://arxiv.org/abs/2403.06898,2024-03-11,"SFVInt: Simple, Fast and Generic Variable-Length Integer Decoding using Bit Manipulation Instructions","['Gang Liao', 'Ye Liu', 'Yonghua Ding', 'Le Cai', 'Jianjun Chen']","The ubiquity of variable-length integers in data storage and communication necessitates efficient decoding techniques. In this paper, we present SFVInt, a simple and fast approach to decode the prevalent Little Endian Base-128 (LEB128) varints. Our approach, distilled into a mere 500 lines of code, effectively utilizes the Bit Manipulation Instruction Set 2 (BMI2) in modern Intel and AMD processors, achieving significant performance improvement while maintaining simplicity and avoiding overengineering. SFVInt, with its generic design, effectively processes both 32-bit and 64-bit unsigned integers using a unified code template, marking a significant leap forward in varint decoding efficiency. We thoroughly evaluate SFVInt's performance across various datasets and scenarios, demonstrating that it achieves up to a 2x increase in decoding speed when compared to varint decoding methods used in established frameworks like Facebook Folly and Google Protobuf."
https://arxiv.org/abs/2403.06897,2024-03-11,4-torsion classes in the integral cohomology of oriented Grassmannians,"['Ákos K. Matszangosz', 'Matthias Wendt']","We investigate the existence of 4-torsion in the integral cohomology of oriented Grassmannians. We prove a general criterion for the appearance of 4-torsion classes based on (twisted) Steenrod squares and show that there are many cases where this criterion is satisfied for minimal-degree anomalous classes, assuming a conjecture on the characteristic rank. We also establish the upper bound in the characteristic rank conjecture for oriented Grassmannians $\tilde{Gr}_k(n)$, and prove the equality in the cases $k=5, n=2^t-1,2^t$ and $k=6, n=2^t$. This provides infinitely many examples of oriented Grassmannians having 4-torsion in their integral cohomology. On the way, we clarify the relation between minimal-degree anomalous classes and results of Stong on the height of the first Stiefel-Whitney class $w_1$ in the mod 2 cohomology of real Grassmannians, for which we give an independent proof. We also establish some bounds on torsion exponents for the integral cohomology of oriented flag manifolds. Based on these findings and further computational evidence, we formulate a conjectural relationship between the torsion exponent in the integral cohomology of homogeneous spaces and their deficiency."
https://arxiv.org/abs/2403.06896,2024-03-11,The Contextual Fraction as a Measure of Entanglement,"['Tim Chan', 'Andrei Constantin']","The contextual fraction introduced by Abramsky and Brandenburger defines a quantitative measure of contextuality associated with empirical models, i.e. tables of probabilities of measurement outcomes in experimental scenarios. In this paper we define a measure of entanglement relying on the contextual fraction. We first show that any separable state is necessarily non-contextual, regardless of the measurement scenario. Then, for bipartite states we associate a distinguished empirical model and show that the corresponding contextual fraction is positively correlated with the entanglement entropy of the state, suggesting that contextuality may be regarded as a refinement of entanglement."
https://arxiv.org/abs/2403.06895,2024-03-11,GRITv2: Efficient and Light-weight Social Relation Recognition,"['N K Sagar Reddy', 'Neeraj Kasera', 'Avinash Thakur']","Our research focuses on the analysis and improvement of the Graph-based Relation Inference Transformer (GRIT), which serves as an important benchmark in the field. We conduct a comprehensive ablation study using the PISC-fine dataset, to find and explore improvement in efficiency and performance of GRITv2. Our research has provided a new state-of-the-art relation recognition model on the PISC relation dataset. We introduce several features in the GRIT model and analyse our new benchmarks in two versions: GRITv2-L (large) and GRITv2-S (small). Our proposed GRITv2-L surpasses existing methods on relation recognition and the GRITv2-S is within 2% performance gap of GRITv2-L, which has only 0.0625x the model size and parameters of GRITv2-L. Furthermore, we also address the need for model compression, an area crucial for deploying efficient models on resource-constrained platforms. By applying quantization techniques, we efficiently reduced the GRITv2-S size to 22MB and deployed it on the flagship OnePlus 12 mobile which still surpasses the PISC-fine benchmarks in performance, highlighting the practical viability and improved efficiency of our model on mobile devices."
https://arxiv.org/abs/2403.06894,2024-03-11,Scalable multi-qubit intrinsic gates in quantum dot arrays,"['Jiaan Qi', 'Zhi-Hai Liu', 'Hongqi Xu']","We study the multi-qubit quantum gates intrinsic to a general array of semiconductor quantum dots and investigate how they can be implemented in a scalable way. The intrinsic quantum gates refer to the class of natural-forming transformations in the qubit rotating-frame under direct exchange coupling, and can be recognized as the instruction set of a spin-qubit chip. Adopting an perturbative treatment, we can model intrinsic gates by first-order dynamics in the coupling strength. A general formalism is developed for identifying the multi-qubit intrinsic gates under arbitrary array connectivity. Factors influencing the fidelities of the multi-qubit intrinsic gates are discussed. The advantageous applications of intrinsic gates in quantum computing and quantum error correction are explored. We also propose a theoretical scheme to overcome the problem of inhomogeneous coupling using dynamical calibration of the connecting bonds. This scheme can be further combined with periodic dynamical decoupling for robust implementations of multi-qubit gates in large-scale quantum computers."
https://arxiv.org/abs/2403.06893,2024-03-11,Partner selection and evolution of out-group avoidance,['Hirofumi Takesue'],"The preferential treatment of in-group members is widely observed. This study examines this phenomenon in the domain of cooperation in social dilemmas using evolutionary agent-based models that consider the role of partner selection. The model considers a repeated prisoner's dilemma game, in which agents belong to one of two groups that are distinguished by the continuation probability of pair interactions. On the basis of the behavior in the last round and the group affiliation of the partner, each individual selects to cooperate, defect, or stop interactions and search for a different partner. The results of simulation demonstrated that agents adopt cooperative strategies, including tit-for-tat and out-for-tat, toward in-group members. By contrast, agents stop interactions immediately after pair formation without observing their partner's behavior when they are paired with an out-group individual. Higher continuation probability with in-group partners hinders interaction with out-group individuals. Our results imply the importance of avoidance in intergroup interactions in social dilemmas and might explain in-group favoritism without enmity toward out-groups."
https://arxiv.org/abs/2403.06892,2024-03-11,Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head,"['Tiancheng Zhao', 'Peng Liu', 'Xuan He', 'Lu Zhang', 'Kyusong Lee']","End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO. Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied. Notably, in zero-shot scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art supervised models. Furthermore, it establishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an AP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of OmDet-Turbo in industrial applications is underscored by its exceptional performance on benchmark datasets and superior inference speed, positioning it as a compelling choice for real-time object detection tasks. Code: \url{https://github.com/om-ai-lab/OmDet}"
https://arxiv.org/abs/2403.06891,2024-03-11,Data Cubes in Hand: A Design Space of Tangible Cubes for Visualizing 3D Spatio-Temporal Data in Mixed Reality,"['Shuqi He', 'Haonan Yao', 'Luyan Jiang', 'Kaiwen Li', 'Nan Xiang', 'Yue Li', 'Hai-Ning Liang', 'Lingyun Yu']","Tangible interfaces in mixed reality (MR) environments allow for intuitive data interactions. Tangible cubes, with their rich interaction affordances, high maneuverability, and stable structure, are particularly well-suited for exploring multi-dimensional data types. However, the design potential of these cubes is underexplored. This study introduces a design space for tangible cubes in MR, focusing on interaction space, visualization space, sizes, and multiplicity. Using spatio-temporal data, we explored the interaction affordances of these cubes in a workshop (N=24). We identified unique interactions like rotating, tapping, and stacking, which are linked to augmented reality (AR) visualization commands. Integrating user-identified interactions, we created a design space for tangible-cube interactions and visualization. A prototype visualizing global health spending with small cubes was developed and evaluated, supporting both individual and combined cube manipulation. This research enhances our grasp of tangible interaction in MR, offering insights for future design and application in diverse data contexts."
https://arxiv.org/abs/2403.06890,2024-03-11,Application of Quantum Tensor Networks for Protein Classification,"['Debarshi Kundu', 'Archisman Ghosh', 'Srinivasan Ekambaram', 'Jian Wang', 'Nikolay Dokholyan', 'Swaroop Ghosh']","We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems. We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences. We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results. We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve the binary classification task mentioned above. Our top-performing quantum model has achieved a 94% accuracy rate, which is comparable to the performance of a classical model that uses the ESM2 protein language model embeddings. It's noteworthy that the ESM2 model is extremely large, containing 8 million parameters in its smallest configuration, whereas our best quantum model requires only around 800 parameters. We demonstrate that these hybrid models exhibit promising performance, showcasing their potential to compete with classical models of similar complexity."
https://arxiv.org/abs/2403.06889,2024-03-11,Numerical simulation of individual coil placement -- A proof-of-concept study for the prediction of recurrence after aneurysm coiling,"['Julian Schwarting', 'Fabian Holzberger', 'Markus Muhr', 'Martin Renz', 'Tobias Boeckh-Behrens', 'Barbara Wohlmuth', 'Jan Kirschke']","Rupture of intracranial aneurysms results in severe subarachnoidal hemorrhage, which is associated with high morbidity and mortality. Neurointerventional occlusion of the aneurysm through coiling has evolved to a therapeutical standard. The choice of the specific coil has an important influence on secondary regrowth requiring retreatment. Aneurysm occlusion was simulated either through virtual implantation of a preshaped 3D coil or with a porous media approach. In this study, we used a recently developed numerical approach to simulate aneurysm shapes in specific challenging aneurysm anatomies and correlated these with aneurysm recurrence 6 months after treatment. The simulation showed a great variety of coil shapes depending on the variability in possible microcatheter positions. Aneurysms with a later recurrence showed a tendency for more successful coiling attempts. Results revealed further trends suggesting lower simulated packing densities in aneurysms with reoccurrence. Simulated packing densities did not correlate with those calculated by conventional software, indicating the potential for our approach to offer additional predictive value. Our study, therefore, pioneers a comprehensive numerical model for simulating aneurysm coiling, providing insights into individualized treatment strategies and outcome prediction. Future directions involve expanding the model's capabilities to simulate intraprocedural outcomes and long-term predictions, aiming to refine occlusion quality criteria and validate prediction parameters in larger patient cohorts. This simulation framework holds promise for enhancing clinical decision-making and optimizing patient outcomes in endovascular aneurysm treatment."
https://arxiv.org/abs/2403.06888,2024-03-12,Process signature-driven high spatio-temporal resolution alignment of multimodal data,"['Abhishek Hanchate', 'Himanshu Balhara', 'Vishal S. Chindepalli', 'Satish T. S. Bukkapatnam']","We present HiRA-Pro, a novel procedure to align, at high spatio-temporal resolutions, multimodal signals from real-world processes and systems that exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing machines. It is based on discerning and synchronizing the process signatures of salient kinematic and dynamic events in these disparate signals. HiRA-Pro addresses the challenge of aligning data with sub-millisecond phenomena, where traditional timestamp, external trigger, or clock-based alignment methods fall short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing context, where it aligns data from 13+ channels acquired during 3D-printing and milling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data is then voxelized to generate 0.25 second aligned data chunks that correspond to physical voxels on the produced part. The superiority of HiRA-Pro is further showcased through case studies in additive manufacturing, demonstrating improved machine learning-based predictive performance due to precise multimodal data alignment. Specifically, testing classification accuracies improved by almost 35% with the application of HiRA-Pro, even with limited data, allowing for precise localization of artifacts. The paper also provides a comprehensive discussion on the proposed method, its applications, and comparative qualitative analysis with a few other alignment methods. HiRA-Pro achieves temporal-spatial resolutions of 10-1000 us and 100 um in order to generate datasets that register with physical voxels on the 3D-printed and milled part. These resolutions are at least an order of magnitude finer than the existing alignment methods that employ individual timestamps, statistical correlations, or common clocks, which achieve precision of hundreds of milliseconds."
https://arxiv.org/abs/2403.06887,2024-03-11,Admissibility of the Structural Rules in the Sequent Calculus with Equality,"['Franco Parlamento', 'Flavio Previale']","On the ground of a general theorem concerning the admissibility of the structural rules in sequent calculi with additional atomic rules, we develop a proof theoretic analysis for several extensions of the ${\bf G3[mic]}$ sequent calculi with rules for equality, including the one originally proposed by H.Wang. In the classical case we relate our results with the semantic tableau method for first order logic with equality. In particular we establish that, for languages without function symbols, in Fitting's alternative semantic tableau method, strictness (which does not allow the repetition of equalities which are modified) can be imposed together with the orientation of the replacement of equals. A significant progress is made toward extending that result to languages with function symbols although whether that is possible or not remains to be settled. We also briefly consider systems that, in the classical case, are related to the semantic tableau method in which one can expand branches by adding identities at will, obtaining that also in that case strictness can be imposed. Furthermore we discuss to what extent the strengthened form of the nonlengthening property of Orevkov known to hold for the sequent calculi with the structural rules applies also to the present context."
https://arxiv.org/abs/2403.06886,2024-03-11,QED Effects on Kerr-Newman Black Hole Shadows,"['Shaobing Yuan', 'Changkai Luo', 'Zezhou Hu', 'Zhenyu Zhang', 'Bin Chen']","Incorporating first-order QED effects, we explore the shadows of Kerr-Newman black holes with a magnetic charge through the numerical backward ray-tracing method. Our investigation accounts for both the direct influence of the electromagnetic field on light rays and the distortion of the background spacetime metric due to QED corrections. We notice that the area of the shadow increases with the QED effect, mainly due to the fact that the photons move more slowly in the effective medium and become easier to be trapped by the black hole."
https://arxiv.org/abs/2403.06885,2024-03-11,Model Predictive Control Strategies for Electric Endurance Race Cars Accounting for Competitors Interactions,"['Jorn van Kampen', 'Mauro Moriggi', 'Francesco Braghin', 'Mauro Salazar']","This paper presents model predictive control strategies for battery electric endurance race cars accounting for interactions with the competitors. In particular, we devise an optimization framework capturing the impact of the actions of the ego vehicle when interacting with competitors in a probabilistic fashion, jointly accounting for the optimal pit stop decision making, the charge times and the driving style in the course of the race. We showcase our method for a simulated 1h endurance race at the Zandvoort circuit, using real-life data of internal combustion engine race cars from a previous event. Our results show that optimizing both the race strategy as well as the decision making during the race is very important, resulting in a significant 21s advantage over an always overtake approach, whilst revealing the competitiveness of e-race cars w.r.t. conventional ones."
https://arxiv.org/abs/2403.06884,2024-03-11,A Holistic Framework Towards Vision-based Traffic Signal Control with Microscopic Simulation,"['Pan He', 'Quanyi Li', 'Xiaoyong Yuan', 'Bolei Zhou']","Traffic signal control (TSC) is crucial for reducing traffic congestion that leads to smoother traffic flow, reduced idling time, and mitigated CO2 emissions. In this study, we explore the computer vision approach for TSC that modulates on-road traffic flows through visual observation. Unlike traditional feature-based approaches, vision-based methods depend much less on heuristics and predefined features, bringing promising potentials for end-to-end learning and optimization of traffic signals. Thus, we introduce a holistic traffic simulation framework called TrafficDojo towards vision-based TSC and its benchmarking by integrating the microscopic traffic flow provided in SUMO into the driving simulator MetaDrive. This proposed framework offers a versatile traffic environment for in-depth analysis and comprehensive evaluation of traffic signal controllers across diverse traffic conditions and scenarios. We establish and compare baseline algorithms including both traditional and Reinforecment Learning (RL) approaches. This work sheds insights into the design and development of vision-based TSC approaches and open up new research opportunities. All the code and baselines will be made publicly available."
https://arxiv.org/abs/2403.06883,2024-03-11,Rates of convergence for holomorphic semigroups of finite shift,"['Maria Kourou', 'Eleftherios K. Theodosiadis', 'Konstantinos Zarvalis']","We study parabolic semigroups of finite shift in the unit disk with regard to the rate of convergence of their orbits to the Denjoy--Wolff point. We examine this rate in terms of Euclidean distance, hyperbolic distance and harmonic measure. In each case, we provide explicit examples to display the sharpness of the results. We further discuss the corresponding rates of convergence for parabolic semigroups of positive hyperbolic step and infinite shift."
https://arxiv.org/abs/2403.06882,2024-03-11,Algebraic Bethe ansatz approach to the correlation functions of the one-dimensional bosons with attraction,['N. A. Slavnov'],"We consider a model of a one-dimensional Bose gas with attraction. We study ground state equal-time correlation functions in this model using the algebraic Bethe ansatz. In cases of strong interaction or/and large-volume systems, we obtain very simple explicit formulas for correlations."
https://arxiv.org/abs/2403.06881,2024-03-11,Linear independence for $C_\ell^{(1)}$ by using $C_{2\ell}^{(1)}$,"['Mirko Primc', 'Goran Trupčević']",In this note we prove linear independence of the combinatorial spanning set for standard $C_\ell^{(1)}$-module $L(kΛ_0)$ by establishing a connection with the combinatorial basis of Feigin-Stoyanovsky's type subspace $W(kΛ_0)$ of $C_{2\ell}^{(1)}$-module $L(kΛ_0)$. It should be noted that the proof of linear independence for the basis of $W(kΛ_0)$ is obtained by using simple currents and intertwining operators in the vertex operator algebra $L(kΛ_0)$.
https://arxiv.org/abs/2403.06880,2024-03-11,Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning,"['Junseok Park', 'Yoonsung Kim', 'Hee Bin Yoo', 'Min Whoo Lee', 'Kibeom Kim', 'Won-Seok Choi', 'Minsu Lee', 'Byoung-Tak Zhang']","Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting wide minima that enhance generalization in RL models."
https://arxiv.org/abs/2403.06879,2024-03-11,Partially identified heteroskedastic SVARs,"['Emanuele Bacchiocchi', 'Andrea Bastianin', 'Toru Kitagawa', 'Elisabetta Mirto']","This paper studies the identification of Structural Vector Autoregressions (SVARs) exploiting a break in the variances of the structural shocks. Point-identification for this class of models relies on an eigen-decomposition involving the covariance matrices of reduced-form errors and requires that all the eigenvalues are distinct. This point-identification, however, fails in the presence of multiplicity of eigenvalues. This occurs in an empirically relevant scenario where, for instance, only a subset of structural shocks had the break in their variances, or where a group of variables shows a variance shift of the same amount. Together with zero or sign restrictions on the structural parameters and impulse responses, we derive the identified sets for impulse responses and show how to compute them. We perform inference on the impulse response functions, building on the robust Bayesian approach developed for set identified SVARs. To illustrate our proposal, we present an empirical example based on the literature on the global crude oil market where the identification is expected to fail due to multiplicity of eigenvalues."
https://arxiv.org/abs/2403.06878,2024-03-11,Anderson-Higgs amplitude mode in Josephson junctions,"['Pierre Vallet', 'Jérôme Cayssol']","The Anderson-Higgs mode in a superconductor corresponds to a collective and coherent oscillation of the order parameter amplitude. We propose to detect this mode in a tunnel Josephson junction between two singlet s-wave diffusive superconductors. We find a strong enhancement of the tunneling current when the junction is pumped at the equilibrium gap frequency, corresponding to the activation of the Anderson-Higgs mode. By solving the Keldysh-Usadel equations, we obtain current peaks at specific bias voltages that can serve as signatures of the Anderson-Higgs mode."
https://arxiv.org/abs/2403.06877,2024-03-11,SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields for Robotic Inspection,"['Yifu Tao', 'Yash Bhalgat', 'Lanke Frank Tarimo Fu', 'Matias Mattamala', 'Nived Chebrolu', 'Maurice Fallon']","We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures. This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals. We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss. We use submapping to scale the system to large-scale environments captured over long trajectories. We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building. Website: https://ori-drs.github.io/projects/silvr/"
https://arxiv.org/abs/2403.06876,2024-03-11,Hierarchical Cutting of Complex Networks Performed by Random Walks,"['Alexandre Benatti', 'Luciano da F. Costa']","Several interesting approaches have been reported in the literature on complex networks, random walks, and hierarchy of graphs. While many of these works perform random walks on stable, fixed networks, in the present work we address the situation in which the connections traversed by each step of a uniformly random walks are progressively removed, yielding a successively less interconnected structure that may break into two components, therefore establishing a respective hierarchy. The sizes of each of these pairs of sliced networks, as well as the permanence of each connected component, are studied in the present work. Several interesting results are reported, including the tendency of geometrical networks sometimes to be broken into two components with comparable large sizes."
https://arxiv.org/abs/2403.06875,2024-03-11,Ising model with non-reciprocal interactions,"['Agney K. Rajeev', 'A. V. Anil Kumar']","Effective interactions that violate Newton's third law of action-reaction symmetry are common in systems where interactions are mediated by a non-equilibrium environment. Extensive Monte Carlo simulations are carried out on a two-dimensional Ising model, where the interactions are modified non-reciprocally. We demonstrate that the critical temperature decreases as the non-reciprocity increases and this decrease depends only on the magnitude of non-reciprocity. Further, travelling spin waves due to the local fluctuations in magnetisation are observed and these spin waves travel opposite to the non-reciprocity vector."
https://arxiv.org/abs/2403.06874,2024-03-11,COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification,"['L. E. Hogeweg', 'R. Gangireddy', 'D. Brunink', 'V. J. Kalkman', 'L. Cornelissen', 'J. W. Kamminga']","High-performing out-of-distribution (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models. In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality. We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a supervised model. The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind. COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection. We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority of experiments, e.g., improving detecting ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset. SHAP (feature contribution) analysis shows that different individual OOD measures are essential for various tasks, indicating that multiple OOD measures and combinations are needed to generalize. Additionally, we show that explicitly considering ID images that are incorrectly classified for the original (species) recognition task is important for constructing high-performing OOD detection methods and for practical applicability. The framework can easily be extended or adapted to other tasks and media modalities."
https://arxiv.org/abs/2403.06873,2024-03-11,Last Iterate Convergence of Incremental Methods and Applications in Continual Learning,"['Xufeng Cai', 'Jelena Diakonikolas']","Incremental gradient methods and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature. Yet, when it comes to their convergence guarantees, nonasymptotic (first-order or proximal) oracle complexity bounds have been obtained fairly recently, almost exclusively applying to the average iterate. Motivated by applications in continual learning, we obtain the first convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings. Our oracle complexity bounds for the last iterate nearly match (i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods. We further obtain generalizations of our results to weighted averaging of the iterates with increasing weights, which can be seen as interpolating between the last iterate and the average iterate guarantees. Additionally, we discuss how our results can be generalized to variants of studied incremental methods with permuted ordering of updates. Our results generalize last iterate guarantees for incremental methods compared to state of the art, as such results were previously known only for overparameterized linear models, which correspond to convex quadratic problems with infinitely many solutions."
https://arxiv.org/abs/2403.06872,2024-03-11,Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents,"['Nishchal Prasad', 'Mohand Boughanem', 'Taoufiq Dkaki']","Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation. We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; ""Multi-stage Encoder-based Supervised with-clustering""; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the hierarchical framework of MESc and compare them with their standalone performance on legal texts. We also study their intra-domain(legal) transfer learning capability and the impact of combining embeddings from their last layers in MESc. We test these methods and their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset. Our approach achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art methods."
https://arxiv.org/abs/2403.06871,2024-03-11,On the Generalization Ability of Unsupervised Pretraining,"['Yuyang Deng', 'Junyuan Hong', 'Jiayu Zhou', 'Mehrdad Mahdavi']","Recent advances in unsupervised learning have shown that unsupervised pre-training, followed by fine-tuning, can improve model generalization. However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the fine-tuned model is lacking. Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and fine-tuning stage. To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during unsupervised pre-training to the subsequent fine-tuning phase, ultimately affecting the generalization capabilities of the fine-tuned model on downstream tasks. We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked Autoencoder pre-training with deep transformers, followed by fine-tuning on a binary classification task. Finally, inspired by our findings, we propose a novel regularization method during pre-training to further enhances the generalization of fine-tuned model. Overall, our results contribute to a better understanding of unsupervised pre-training and fine-tuning paradigm, and can shed light on the design of more effective pre-training algorithms."
https://arxiv.org/abs/2403.06870,2024-03-11,Semantic Residual Prompts for Continual Learning,"['Martin Menabue', 'Emanuele Frascaroli', 'Matteo Boschini', 'Enver Sangineto', 'Lorenzo Bonicelli', 'Angelo Porrello', 'Simone Calderara']","Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a second pool. The retrieved prompts serve to adapt a pre-trained ViT, granting plasticity. In doing so, we also propose a novel residual mechanism to transfer CLIP semantics to the ViT layers. Through extensive analysis on established CL benchmarks, we show that our method significantly outperforms both state-of-the-art CL approaches and the zero-shot CLIP test. Notably, our findings hold true even for datasets with a substantial domain gap w.r.t. the pre-training knowledge of the backbone model, as showcased by experiments on satellite imagery and medical datasets."
https://arxiv.org/abs/2403.06869,2024-03-11,Learning with Noisy Foundation Models,"['Hao Chen', 'Jindong Wang', 'Zihan Wang', 'Ran Tao', 'Hongxin Wei', 'Xing Xie', 'Masashi Sugiyama', 'Bhiksha Raj']","Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning."
https://arxiv.org/abs/2403.06868,2024-03-11,Exact Multi-Point Correlations in the Stochastic Heat Equation for Strictly Sublinear Coordinates,"['Pierre Yves Gaudreau Lamarre', 'Yier Lin']","We consider the Stochastic Heat Equation (SHE) in $(1+1)$ dimensions with delta Dirac initial data and spacetime white noise. We prove exact large-time asymptotics for multi-point correlations of the SHE for strictly sublinear space coordinates. The sublinear condition is optimal, in the sense that different asymptotics are known to occur when the space coordinates grow linearly [Lin 2023, Theorem 1.1]. Lastly, a notable feature of our result is that it confirms the connection between multi-point correlations in the SHE and the ground state of the Hamiltonian of the delta-Bose gas."
https://arxiv.org/abs/2403.06867,2024-03-11,Exploring Unique Quasinormal Modes of a Massive Scalar Field in Brane-World Scenarios,['Antonina F. Zinhailo'],"We compute precise values of quasinormal modes of a massive scalar field in the background of the Schwarzschild-like brane-localised black holes. It is shown that the quasinormal spectrum of the massive field differs qualitatively from that previously known for other black hole models, due to the presence of two kinds of modes: those whose damping rate vanishes as the mass of the field $μ$ increases up to some critical value, and those whose real oscillation frequency vanishes at a certain value of $μ$. While the first type of modes, which are arbitrarily long-lived, are recognized in various four-dimensional backgrounds as quasi-resonances, the second type is a novel feature for asymptotically flat black holes. When $Re (ω)$ reaches zero, the fundamental mode disappears from the spectrum and the first overtone becomes the fundamental mode. We also demonstrate that quasi-resonances may not exist for brane-localised black holes immersed in $D\geq 6$ - dimensional bulk."
https://arxiv.org/abs/2403.06866,2024-03-12,QUASAR: QUality and Aesthetics Scoring with Advanced Representations,"['Sergey Kastryulin', 'Denis Prokopenko', 'Artem Babenko', 'Dmitry V. Dylov']","This paper introduces a new data-driven, non-parametric method for image quality and aesthetics assessment, surpassing existing approaches and requiring no prompt engineering or fine-tuning. We eliminate the need for expressive textual embeddings by proposing efficient image anchors in the data. Through extensive evaluations of 7 state-of-the-art self-supervised models, our method demonstrates superior performance and robustness across various datasets and benchmarks. Notably, it achieves high agreement with human assessments even with limited data and shows high robustness to the nature of data and their pre-processing pipeline. Our contributions offer a streamlined solution for assessment of images while providing insights into the perception of visual information."
https://arxiv.org/abs/2403.06865,2024-03-13,On the Preservation of Africa's Cultural Heritage in the Age of Artificial Intelligence,['Mohamed El Louadi'],"In this paper we delve into the historical evolution of data as a fundamental element in communication and knowledge transmission. The paper traces the stages of knowledge dissemination from oral traditions to the digital era, highlighting the significance of languages and cultural diversity in this progression. It also explores the impact of digital technologies on memory, communication, and cultural preservation, emphasizing the need for promoting a culture of the digital (rather than a digital culture) in Africa and beyond. Additionally, it discusses the challenges and opportunities presented by data biases in AI development, underscoring the importance of creating diverse datasets for equitable representation. We advocate for investing in data as a crucial raw material for fostering digital literacy, economic development, and, above all, cultural preservation in the digital age."
https://arxiv.org/abs/2403.06864,2024-03-11,Rigid Poisson suspensions without roots,['Valery V. Ryzhikov'],"Examples of rigid Poisson suspensions without roots are presented. The discrete rational component in spectrum of an ergodic automorphism S prevents some roots from existing. If S is tensorly multiplied by an ergodic automorphism of the space with a sigma-finite measure, discrete spectrum disappears in this product, but the memory of it can remain in the form of the absence of roots. In additional conditions, this effect is inherited by the Poisson suspension over the product."
https://arxiv.org/abs/2403.06863,2024-03-11,Single sided multiplier Hopf algebras,['Alfons Van Daele'],"Let $A$ be a non-degenerate algebra over the complex numbers and $Δ$ a homomorphism from $A$ to the multiplier algebra $M(A\otimes A)$. Consider the linear maps $T_1$ and $T_2$ from $A\otimes A$ to $M(A\otimes A)$ defined by \begin{equation*} T_1(a\otimes b)=Δ(a)(1\otimes b) \qquad\text{and}\qquad T_2(c\otimes a)=(c\otimes 1)Δ(a). \end{equation*} The pair $(A,Δ)$ is a multiplier Hopf algebra if these two maps have range in $A\otimes A$ and are bijections from $A\otimes A$ to itself. In our recent paper on the Larson-Sweedler theorem, single sided multiplier Hopf algebras emerge in a natural way. For this case, instead of requiring the above for the maps $T_1$ and $T_2$, we now have this property for the maps $T_1$ and $T_4$ or for $T_2$ and $T_3$ where \begin{equation*} T_3(a\otimes b)=(1\otimes b)Δ(a) \qquad\text{and}\qquad T_4(c\otimes a)=Δ(a)(c\otimes 1). \end{equation*} As it turns out, also for these single sided multiplier Hopf algebras, the existence of a unique counit and antipode can be proven. In fact, rather surprisingly, using the properties of the antipode, one can actually show that for a single sided multiplier Hopf algebra all four canonical maps are bijections from $A\otimes A$ to itself. In other words, $(A,Δ)$ is automatically a regular multiplier Hopf algebra. We take the advantage of this approach to reconsider some of the known results for a regular multiplier Hopf algebra."
https://arxiv.org/abs/2403.06862,2024-03-11,Real-Time Simulated Avatar from Head-Mounted Sensors,"['Zhengyi Luo', 'Jinkun Cao', 'Rawal Khirodkar', 'Alexander Winkler', 'Kris Kitani', 'Weipeng Xu']","We present SimXR, a method for controlling a simulated avatar from information (headset pose and cameras) obtained from AR / VR headsets. Due to the challenging viewpoint of head-mounted cameras, the human body is often clipped out of view, making traditional image-based egocentric pose estimation challenging. On the other hand, headset poses provide valuable information about overall body motion, but lack fine-grained details about the hands and feet. To synergize headset poses with cameras, we control a humanoid to track headset movement while analyzing input images to decide body movement. When body parts are seen, the movements of hands and feet will be guided by the images; when unseen, the laws of physics guide the controller to generate plausible motion. We design an end-to-end method that does not rely on any intermediate representations and learns to directly map from images and headset poses to humanoid control signals. To train our method, we also propose a large-scale synthetic dataset created using camera configurations compatible with a commercially available VR headset (Quest 2) and show promising results on real-world captures. To demonstrate the applicability of our framework, we also test it on an AR headset with a forward-facing camera."
https://arxiv.org/abs/2403.06861,2024-03-11,Quantum thermodynamics of driven-dissipative condensates,"['Luisa Toledo Tude', 'Paul R. Eastham']","Polariton condensates occur away from thermal equilibrium, in an open system where heat and particles are continually exchanged with reservoirs. These phenomena have been extensively analyzed in terms of kinetic equations. Based on the collection of knowledge about polariton kinetics provided by these simulations and by experimental works, we constructed a few-level model that captures the main processes involved in the buildup of a ground-state population of polaritons. This allows condensation to be understood as the output of a heat engine and exposes the thermodynamic constraints on its occurrence. The model consists of a three-level system interacting with a field and connected to a hot and a cold thermal reservoir that represent a non-resonant pump and the lattice phonons. This subsystem can drive a condensate, through polariton-polariton scattering, which produces work in the form of coherent light emission from the microcavity. We obtain a phase diagram as a function of the temperatures of the two baths and investigate the possible types of phase transition that lead to the condensate phase."
https://arxiv.org/abs/2403.06860,2024-03-11,A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa,"['Ibrahim Salihu Yusuf', 'Mukhtar Opeyemi Yusuf', 'Kobby Panford-Quainoo', 'Arnu Pretorius']","Desert locust swarms present a major threat to agriculture and food security. Addressing this challenge, our study develops an operationally-ready model for predicting locust breeding grounds, which has the potential to enhance early warning systems and targeted control measures. We curated a dataset from the United Nations Food and Agriculture Organization's (UN-FAO) locust observation records and analyzed it using two types of spatio-temporal input features: remotely-sensed environmental and climate data as well as multi-spectral earth observation images. Our approach employed custom deep learning models (three-dimensional and LSTM-based recurrent convolutional networks), along with the geospatial foundational model Prithvi recently released by Jakubik et al., 2023. These models notably outperformed existing baselines, with the Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding from our research is that multi-spectral earth observation images alone are sufficient for effective locust breeding ground prediction without the need to explicitly incorporate climatic or environmental features."
https://arxiv.org/abs/2403.06859,2024-03-11,Radial Tully-Fisher relation and the local variance of Hubble parameter,"['Balakrishna S. Haridasu', 'Paolo Salucci', 'Gauri Sharma']","Utilizing the well-established Radial Tully-Fisher (RTF) relation observed in a `large' (843) sample of local galaxies, we report the maximum allowed variance in the Hubble parameter, $H_0$. We estimate the total intrinsic scatter in the magnitude of the RTF relation(s) implementing a cosmological model-independent cosmographic expansion. We find that the maximum allowed local variation in our baseline analysis, using 4 RTF relations in the galaxy sample is $ΔH_0/H_0 \lesssim 3 \%$ at a $95\%$ C.L. significance. Which is implied form a constraint of $ΔH_0/H_0 = 0.54^{+1.32}_{-1.37} \%$ estimated at $D_{\rm{L}}\sim 10\, [\rm{Mpc}]$. Using only one `best-constrained' radial bin we report a conservative $95\%$ C.L. limit of $ΔH_0/H_0 \lesssim 4 \%$. Through our estimate of maximum variation, we propose a novel method to validate several late-time/local modifications put forth to alleviate the $H_0$ tension. We find that within the range of the current galaxy sample redshift distribution $10 \, [\rm{Mpc}] \le D_{\rm{L}} \le 140\, [\rm{Mpc}]$, it is highly unlikely to obtain a variation of $ΔH_0/H_0 \sim 9\%$, necessary to alleviate the $H_0$-tension. However, we also elaborate on the possible alternative inferences when the innermost radial bin is included in the analysis. Alongside the primary analysis of fitting the individual RTF relations independently, we propose and perform a joint analysis of the RTF relations useful to create a pseudo-standardizable sample of galaxies. We also test for the spatial variation of $H_0$, finding that the current samples' galaxies distributed only in the southern hemisphere support the null hypothesis of isotropy, within the allowed noise levels."
https://arxiv.org/abs/2403.06858,2024-03-11,Estimation of parameters and local times in a discretely observed threshold diffusion model,"['Sara Mazzonetto', 'Paolo Pigato']","We consider a simple mean reverting diffusion process, with piecewise constant drift and diffusion coefficients, discontinuous at a fixed threshold. We discuss estimation of drift and diffusion parameters from discrete observations of the process, with a generalized moment estimator and a maximum likelihood estimator. We develop the asymptotic theory of the estimators when the time horizon of the observations goes to infinity, considering both cases of a fixed time lag (low frequency) and a vanishing time lag (high frequency) between consecutive observations. In the setting of low frequency observations and infinite time horizon we also study the convergence of three local time estimators, that are already known to converge to the local time in the setting of high frequency observations and fixed time horizon. We find that these estimators can behave differently, depending on the assumptions on the time lag between observations."
https://arxiv.org/abs/2403.06857,2024-03-11,Development of a Reliable and Accessible Caregiving Language Model (CaLM),"['Bambang Parmanto', 'Bayu Aryoyudanta', 'Wilbert Soekinto', 'I Made Agus Setiawan', 'Yuhan Wang', 'Haomin Hu', 'Andi Saptono', 'Yong K. Choi']","Unlike professional caregivers, family caregivers often assume this role without formal preparation or training. Because of this, there is an urgent need to enhance the capacity of family caregivers to provide quality care. Large language models can potentially be used as a foundation technology for supporting caregivers as educational tools or as adjunct to care. This study aimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a caregiving knowledge base, develop an accessible CaLM using a small FM that requires fewer computing resources, and evaluate the performance of the model compared to a large FM. We developed CaLM using the Retrieval Augmented Generation (RAG) framework combined with FM fine-tuning for improving the quality of FM answers by grounding the model on a caregiving knowledge base. We used two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B parameters) and larger FM GPT-3.5 as a benchmark. We developed the caregiving knowledge base by gathering various types of documents from the Internet. In this study, we focused on caregivers of individuals with Alzheimer's Disease Related Dementias. We evaluated the models' performance using the benchmark metrics commonly used in evaluating language models and their reliability to provide accurate references with the answers. The RAG framework improved the performance of all FMs used in this study across all measures. As expected, the large FM performed better than small FMs across all metrics. The most interesting result is that small fine-tuned FMs with RAG performed significantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2 small FM performed better than GPT 3.5 (even with RAG) in returning references with the answers. The study shows that reliable and accessible CaLM can be developed by using small FMs with a knowledge base specific to the caregiving domain."
https://arxiv.org/abs/2403.06856,2024-03-11,Concurrent Speaker Detection: A multi-microphone Transformer-Based Approach,"['Amit Eliav', 'Sharon Gannot']","We present a deep-learning approach for the task of Concurrent Speaker Detection (CSD) using a modified transformer model. Our model is designed to handle multi-microphone data but can also work in the single-microphone case. The method can classify audio segments into one of three classes: 1) no speech activity (noise only), 2) only a single speaker is active, and 3) more than one speaker is active. We incorporate a Cost-Sensitive (CS) loss and a confidence calibration to the training procedure. The approach is evaluated using three real-world databases: AMI, AliMeeting, and CHiME 5, demonstrating an improvement over existing approaches."
https://arxiv.org/abs/2403.06855,2024-03-11,Surface-aware Mesh Texture Synthesis with Pre-trained 2D CNNs,"['Áron Samuel Kovács', 'Pedro Hermosilla', 'Renata G. Raidou']","Mesh texture synthesis is a key component in the automatic generation of 3D content. Existing learning-based methods have drawbacks -- either by disregarding the shape manifold during texture generation or by requiring a large number of different views to mitigate occlusion-related inconsistencies. In this paper, we present a novel surface-aware approach for mesh texture synthesis that overcomes these drawbacks by leveraging the pre-trained weights of 2D Convolutional Neural Networks (CNNs) with the same architecture, but with convolutions designed for 3D meshes. Our proposed network keeps track of the oriented patches surrounding each texel, enabling seamless texture synthesis and retaining local similarity to classical 2D convolutions with square kernels. Our approach allows us to synthesize textures that account for the geometric content of mesh surfaces, eliminating discontinuities and achieving comparable quality to 2D image synthesis algorithms. We compare our approach with state-of-the-art methods where, through qualitative and quantitative evaluations, we demonstrate that our approach is more effective for a variety of meshes and styles, while also producing visually appealing and consistent textures on meshes."
https://arxiv.org/abs/2403.06854,2024-03-11,Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification,"['Joar Skalse', 'Alessandro Abate']","Inverse reinforcement learning (IRL) aims to infer an agent's preferences (represented as a reward function $R$) from their behaviour (represented as a policy $π$). To do this, we need a behavioural model of how $π$ relates to $R$. In the current literature, the most common behavioural models are optimality, Boltzmann-rationality, and causal entropy maximisation. However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models. This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data. In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model. Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error above a given threshold. In addition to this, we also characterise the conditions under which a behavioural model is robust to small perturbations of the observed policy, and we analyse how robust many behavioural models are to misspecification of their parameter values (such as e.g.\ the discount rate). Our analysis suggests that the IRL problem is highly sensitive to misspecification, in the sense that very mild misspecification can lead to very large errors in the inferred reward function."
https://arxiv.org/abs/2403.06853,2024-03-11,Vibrational Dynamics and Spectroscopy of Water at Porous g-C$_{3}$N$_{4}$ and C$_{2}$N Materials,"['Deepak Ojha', 'Christopher Penschke', 'Peter Saalfrank']","In this work, the vibrational dynamics and spectroscopy of deuterated water molecules (D$_{2}$O) mimicking dense water layers at room temperature on the surfaces of two different C/N based materials with different N content and pore size, namely graphitic C$_{3}$N$_{4}$ (g-C$_{3}$N$_{4}$) and C$_{2}$N are studied using Ab Initio Molecular Dynamics (AIMD). In particular, Time-Dependent vibrational Sum-Frequency Generation spectra (TD-vSFG) of the OD modes and also time-averaged vSFG spectra and OD frequency distributions are computed."
https://arxiv.org/abs/2403.06852,2024-03-11,Suppressing Correlated Noise in Quantum Computers via Context-Aware Compiling,"['Alireza Seif', 'Haoran Liao', 'Vinay Tripathi', 'Kevin Krsulich', 'Moein Malekakhlagh', 'Mirko Amico', 'Petar Jurcevic', 'Ali Javadi-Abhari']","Coherent errors, and especially those that occur in correlation among a set of qubits, are detrimental for large-scale quantum computing. Correlations in noise can occur as a result of spatial and temporal configurations of instructions executing on the quantum processor. In this paper, we perform a detailed experimental characterization of many of these error sources, and theoretically connect them to the physics of superconducting qubits and gate operations. Equipped with this knowledge, we devise compiler strategies to suppress these errors using dynamical decoupling or error compensation into the rest of the circuit. Importantly, these strategies are successful when the context at each layer of computation is taken into account: how qubits are connected, what crosstalk terms exist on the device, and what gates or idle periods occur in that layer. Our context-aware compiler thus suppresses some dominant sources of error, making further error mitigation or error correction substantially less expensive. For example, our experiments show an increase of 18.5\% in layer fidelity for a candidate 10-qubit circuit layer compared to context-unaware suppression. Owing to the exponential nature of error mitigation, these improvements due to error suppression translate to several orders of magnitude reduction of sampling overhead for a circuit consisting of a moderate number of layers."
https://arxiv.org/abs/2403.06851,2024-03-11,Human-Exoskeleton Interaction Portrait,"['Mohammad Shushtari', 'Julia Foellmer', 'Arash Arami']","Human-robot physical interaction contains crucial information for optimizing user experience, enhancing robot performance, and objectively assessing user adaptation. This study introduces a new method to evaluate human-robot co-adaptation in lower limb exoskeletons by analyzing muscle activity and interaction torque as a two-dimensional random variable. We introduce the Interaction Portrait (IP), which visualizes this variable's distribution in polar coordinates. We applied this metric to compare a recent torque controller (HTC) based on kinematic state feedback and a novel feedforward controller (AMTC) with online learning, proposed herein, against a time-based controller (TBC) during treadmill walking at varying speeds. Compared to TBC, both HTC and AMTC significantly lower users' normalized oxygen uptake, suggesting enhanced user-exoskeleton coordination. IP analysis reveals this improvement stems from two distinct co-adaptation strategies, unidentifiable by traditional muscle activity or interaction torque analyses alone. HTC encourages users to yield control to the exoskeleton, decreasing muscular effort but increasing interaction torque, as the exoskeleton compensates for user dynamics. Conversely, AMTC promotes user engagement through increased muscular effort and reduced interaction torques, aligning it more closely with rehabilitation and gait training applications. IP phase evolution provides insight into each user's interaction strategy development, showcasing IP analysis's potential in comparing and designing novel controllers to optimize human-robot interaction in wearable robots."
https://arxiv.org/abs/2403.06850,2024-03-11,The Berge-Füredi conjecture on the chromatic index of hypergraphs with large hyperedges,"['Alain Bretto', 'Alain Faisant', 'Francois Hennecart']","This paper is concerned with two conjectures which are intimately related. The first is a generalization to hypergraphs of Vizing's Theorem on the chromatic index of a graph and the second is the well-known conjecture of Erdős, Faber and Lovász which deals with the problem of coloring a family of cliques intersecting in at most one vertex. We are led to study a special class of uniform and linear hypergraphs for which a number of properties are established."
https://arxiv.org/abs/2403.06849,2024-03-11,On equivariant embeddings of hyperbolic surfaces into hyperbolic 3-manifolds,['Bruno P. Zimmermann'],"We consider the problem of when a closed hyperbolic surface admits a totally geodesic embedding into a closed hyperbolic 3-manifold, and in particular equivariant versions of such embeddings. In a previous paper we considered orientation-preserving actions on orientable surfaces, in the present paper actions of maximal possible order on nonorientable surfaces, and maximal orientation-reversing actions on orientable surfaces."
https://arxiv.org/abs/2403.06848,2024-03-11,Structure and dielectric properties of BFN-KN solid solutions synthesized through solution combustion route,"['Vijay Khopkar', 'Balaram Sahoo']","We demonstrate that the solution combustion reaction (SCR) route is suitable for the synthesis of phase pure (x)BaFe0.5Nb0.5O3-(1-x)KNbO3 (x = 0, 0.2, 0.4, 0.6, 0.8, 1) (BFN-KN) solid solutions due to atomic level of mixing of precursors than that of the solid-state reaction (SSR) route. A variation in composition 'x' of our double perovskite samples leads to a gradual variation in the structural disorder associated with the disorder in the distribution in the cationic positions, the lattice strain, and the defects at the grain boundaries. With the increase in the BFN content in the solid solutions, a decrease in bandgap and a corresponding change in the color of the samples are observed. Furthermore, three distinct characteristic features in the frequency-dependent polarization behavior in our samples are observed at different frequencies. These features observed to be sustaining up to gradually increasing frequencies are assigned to (1) the interfacial polarization originating due to the structural disorder present at the grain boundaries, (2) the cationic positions leading to tilting/expansion of oxygen octahedra, and (3) the ionic size mismatch leading to strain distribution. The results are well supported by the results obtained for the microstructural, structural, and optical properties of our samples. Hence, our work provides a better understanding of the dielectric polarization behavior of double perovskites."
https://arxiv.org/abs/2403.06847,2024-03-11,SonoTraceLab -- A Raytracing-Based Acoustic Modelling System for Simulating Echolocation Behavior of Bats,"['Wouter Jansen', 'Jan Steckel']","Echolocation is the prime sensing modality for many species of bats, who show the intricate ability to perform a plethora of tasks in complex and unstructured environments. Understanding this exceptional feat of sensorimotor interaction is a key aspect into building more robust and performant man-made sonar sensors. In order to better understand the underlying perception mechanisms it is important to get a good insight into the nature of the reflected signals that the bat perceives. While ensonification experiments are in important way to better understand the nature of these signals, they are as time-consuming to perform as they are informative. In this paper we present SonoTraceLab, an open-source software package for simulating both technical as well as biological sonar systems in complex scenes. Using simulation approaches can drastically increase insights into the nature of biological echolocation systems, while reducing the time- and material complexity of performing them."
https://arxiv.org/abs/2403.06846,2024-03-11,DiaLoc: An Iterative Approach to Embodied Dialog Localization,"['Chao Zhang', 'Mohan Li', 'Ignas Budvytis', 'Stephan Liwicki']","Multimodal learning has advanced the performance for many vision-language tasks. However, most existing works in embodied dialog research focus on navigation and leave the localization task understudied. The few existing dialog-based localization approaches assume the availability of entire dialog prior to localizaiton, which is impractical for deployed dialog-based localization. In this paper, we propose DiaLoc, a new dialog-based localization framework which aligns with a real human operator behavior. Specifically, we produce an iterative refinement of location predictions which can visualize current pose believes after each dialog turn. DiaLoc effectively utilizes the multimodal data for multi-shot localization, where a fusion encoder fuses vision and dialog information iteratively. We achieve state-of-the-art results on embodied dialog-based localization task, in single-shot (+7.08% in Acc5@valUnseen) and multi-shot settings (+10.85% in Acc5@valUnseen). DiaLoc narrows the gap between simulation and real-world applications, opening doors for future research on collaborative localization and navigation."
https://arxiv.org/abs/2403.06845,2024-03-11,DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation,"['Guosheng Zhao', 'Xiaofeng Wang', 'Zheng Zhu', 'Xinze Chen', 'Guan Huang', 'Xiaoyi Bao', 'Xingang Wang']","World models have demonstrated superiority in autonomous driving, particularly in the generation of multi-view driving videos. However, significant challenges still exist in generating customized driving videos. In this paper, we propose DriveDreamer-2, which builds upon the framework of DriveDreamer and incorporates a Large Language Model (LLM) to generate user-defined driving videos. Specifically, an LLM interface is initially incorporated to convert a user's query into agent trajectories. Subsequently, a HDMap, adhering to traffic regulations, is generated based on the trajectories. Ultimately, we propose the Unified Multi-View Model to enhance temporal and spatial coherence in the generated driving videos. DriveDreamer-2 is the first world model to generate customized driving videos, it can generate uncommon driving videos (e.g., vehicles abruptly cut in) in a user-friendly manner. Besides, experimental results demonstrate that the generated videos enhance the training of driving perception methods (e.g., 3D detection and tracking). Furthermore, video generation quality of DriveDreamer-2 surpasses other state-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7, representing relative improvements of 30% and 50%."
https://arxiv.org/abs/2403.06844,2024-03-11,ExoCubed: A Riemann-Solver based Cubed-Sphere Dynamic Core for Planetary Atmospheres,"['Sihe Chen', 'Cheng Li']","The computational fluid dynamics on a sphere is relevant to global simulations of geophysical fluid dynamics. Using the conventional spherical-polar (or lat-lon) grid results in a singularity at the poles, with orders of magnitude smaller cell sizes at the poles in comparison to the equator. To address this problem, we developed a general circulation model (dynamic core) with a gnomonic equiangular cubed-sphere configuration. This model is developed based on the Simulating Nonhydrostatic Atmospheres on Planets (SNAP) model, using a finite volume numerical scheme with a Riemann-solver-based dynamic core and the vertical implicit correction (VIC) scheme. This change of the horizontal configuration gives a 20-time acceleration of global simulations compared to the lat-lon grid with a similar number of cells at medium resolution. We presented standard tests ranging from 2D shallow-water models to 3D general circulation tests, including earth-like planets and shallow hot Jupiters, to validate the accuracy of the model. The method described in this article is generic to transform any existing finite-volume hydrodynamic model in the Cartesian geometry to the spherical geometry."
https://arxiv.org/abs/2403.06843,2024-03-11,Towards an educational tool for supporting neonatologists in the delivery room,"['Giorgio Leonardi', 'Clara Maldarizzi', 'Stefania Montani', 'Manuel Striani', 'Mariachiara Martina Strozzi']","Nowadays, there is evidence that several factors may increase the risk, for an infant, to require stabilisation or resuscitation manoeuvres at birth. However, this risk factors are not completely known, and a universally applicable model for predicting high-risk situations is not available yet. Considering both these limitations and the fact that the need for resuscitation at birth is a rare event, periodic training of the healthcare personnel responsible for newborn caring in the delivery room is mandatory."
https://arxiv.org/abs/2403.06842,2024-03-11,Hybrid optimal control with mixed-integer Lagrangian methods,"['Viktoriya Nikitina', 'Alberto De Marchi', 'Matthias Gerdts']","Models involving hybrid systems are versatile in their application, but difficult to handle and optimize efficiently due to their combinatorial nature. This work presents a method to cope with hybrid optimal control problems which, in contrast to decomposition techniques, does not require relaxing the integrality constraints. Based on the discretize-then-optimize approach, our scheme addresses mixed-integer nonlinear problems under mild assumptions. The proposed numerical algorithm builds upon the augmented Lagrangian framework, whose subproblems are handled using successive mixed-integer linearizations with trust regions. We validate the performance of the numerical routine with extensive investigations using several hybrid optimal control problems from different fields of application. Promising preliminary results are presented for a motion planning task with hysteresis and drag, a Lotka-Volterra fishing problem, and a facility location design problem."
https://arxiv.org/abs/2403.06841,2024-03-11,Inverse Garment and Pattern Modeling with a Differentiable Simulator,"['Boyang Yu', 'Frederic Cordier', 'Hyewon Seo']","The capability to generate simulation-ready garment models from 3D shapes of clothed humans will significantly enhance the interpretability of captured geometry of real garments, as well as their faithful reproduction in the virtual world. This will have notable impact on fields like shape capture in social VR, and virtual try-on in the fashion industry. To align with the garment modeling process standardized by the fashion industry as well as cloth simulation softwares, it is required to recover 2D patterns. This involves an inverse garment design problem, which is the focus of our work here: Starting with an arbitrary target garment geometry, our system estimates an animatable garment model by automatically adjusting its corresponding 2D template pattern, along with the material parameters of the physics-based simulation (PBS). Built upon a differentiable cloth simulator, the optimization process is directed towards minimizing the deviation of the simulated garment shape from the target geometry. Moreover, our produced patterns meet manufacturing requirements such as left-to-right-symmetry, making them suited for reverse garment fabrication. We validate our approach on examples of different garment types, and show that our method faithfully reproduces both the draped garment shape and the sewing pattern."
https://arxiv.org/abs/2403.06840,2024-03-11,RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback,"['Yanming Liu', 'Xinyue Peng', 'Xuhong Zhang', 'Weihao Liu', 'Jianwei Yin', 'Jiannan Cao', 'Tianyu Du']","Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities and reducing hallucinations."
https://arxiv.org/abs/2403.06839,2024-03-11,Advanced Channel Coding Designs for Index-Modulated Fluid Antenna Systems,"['Elio Faddoul', 'Ghassan M. Kraidy', 'Constantinos Psomas', 'Ioannis Krikidis']","The concept of fluid antennas (FAs) has emerged as a promising solution to enhance the spectral efficiency of wireless networks, achieved by introducing additional degrees of freedom, including reconfigurability and flexibility. In this paper, we investigate the use of index-modulated (IM) transmissions within the framework of FA systems, where an FA position is activated during each transmission interval. This approach is motivated by the common characteristics exhibited by FAs and IM transmissions, which entails the use of a single radio-frequency chain. From this perspective, we derive a closed-form expression for the bit error rate of IM-FAs considering spatial correlation, and demonstrating superior performance compared to conventional IM systems. To enhance the performance of IM-FAs in correlated conditions, channel coding techniques are applied. We first analyze a set partition coding (SPC) scheme for IM-FAs to spatially separate the FA ports, and provide a tight performance bound over correlated channels. Furthermore, the spatial SPC scheme is extended to turbo-coded modulation where the performance is analyzed for low and high signal-to-noise ratios. Our results reveal that through the implementation of channel coding techniques designed for FAs and IM transmission, the performance of coded IM-FAs exhibits notable enhancements, particularly in high correlation scenarios."
https://arxiv.org/abs/2403.06838,2024-03-11,ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts,"['Lyuye Zhang', 'Kaixuan Li', 'Kairan Sun', 'Daoyuan Wu', 'Ye Liu', 'Haoye Tian', 'Yang Liu']","Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical. While existing research has proposed multiple detection tools, the automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge. Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of AC lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence."
https://arxiv.org/abs/2403.06837,2024-03-11,Stochastic Cortical Self-Reconstruction,"['Christian Wachinger', 'Dennis Hedderich', 'Fabian Bongratz']","Magnetic resonance imaging (MRI) is critical for diagnosing neurodegenerative diseases, yet accurately assessing mild cortical atrophy remains a challenge due to its subtlety. Automated cortex reconstruction, paired with healthy reference ranges, aids in pinpointing pathological atrophy, yet their generalization is limited by biases from image acquisition and processing. We introduce the concept of stochastic cortical self-reconstruction (SCSR) that creates a subject-specific healthy reference by taking MRI-derived thicknesses as input and, therefore, implicitly accounting for potential confounders. SCSR randomly corrupts parts of the cortex and self-reconstructs them from the remaining information. Trained exclusively on healthy individuals, repeated self-reconstruction generates a stochastic reference cortex for assessing deviations from the norm. We present three implementations of this concept: XGBoost applied on parcels, and two autoencoders on vertex level -- one based on a multilayer perceptron and the other using a spherical U-Net. These models were trained on healthy subjects from the UK Biobank and subsequently evaluated across four public Alzheimer's datasets. Finally, we deploy the model on clinical in-house data, where deviation maps' high spatial resolution aids in discriminating between four types of dementia."
https://arxiv.org/abs/2403.06836,2024-03-11,The metallicity gradients of star-forming regions store information of the assembly history of galaxies,"['F. Jara-Ferreira', 'P. B. Tissera', 'E. Sillero', 'Y. Rosas-Guevara', 'S. E. Pedrosa', 'M. E. De Rossi', 'T. Theuns', 'L. Bignone']","The variations in metallicity and spatial patterns within star-forming regions of galaxies result from diverse physical processes unfolding throughout their evolutionary history, with a particular emphasis in recent events. Analysing MaNGA and \textsc{eagle} galaxies, we discovered an additional dependence of the mass-metallicity relation (MZR) on metallicity gradients ($\nabla_{\rm (O/H)}$). Two regimes emerged for low and high stellar mass galaxies, distinctly separated at approximately ${\rm M_{\star}} >10^{9.75}$."
https://arxiv.org/abs/2403.06835,2024-03-11,Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting,"['Wenting Chen', 'Pengyu Wang', 'Hui Ren', 'Lichao Sun', 'Quanzheng Li', 'Yixuan Yuan', 'Xiang Li']","Data scarcity and privacy concerns limit the availability of high-quality medical images for public use, which can be mitigated through medical image synthesis. However, current medical image synthesis methods often struggle to accurately capture the complexity of detailed anatomical structures and pathological conditions. To address these challenges, we propose a novel medical image synthesis model that leverages fine-grained image-text alignment and anatomy-pathology prompts to generate highly detailed and accurate synthetic medical images. Our method integrates advanced natural language processing techniques with image generative modeling, enabling precise alignment between descriptive text prompts and the synthesized images' anatomical and pathological details. The proposed approach consists of two key components: an anatomy-pathology prompting module and a fine-grained alignment-based synthesis module. The anatomy-pathology prompting module automatically generates descriptive prompts for high-quality medical images. To further synthesize high-quality medical images from the generated prompts, the fine-grained alignment-based synthesis module pre-defines a visual codebook for the radiology dataset and performs fine-grained alignment between the codebook and generated prompts to obtain key patches as visual clues, facilitating accurate image synthesis. We validate the superiority of our method through experiments on public chest X-ray datasets and demonstrate that our synthetic images preserve accurate semantic information, making them valuable for various medical applications."
https://arxiv.org/abs/2403.06834,2024-03-11,Relativistic Roche problem for stars in precessing orbits around a spinning black hole,"['Matteo Stockinger', 'Masaru Shibata']","Tidal disruptions of stars on the equatorial plane orbiting Kerr black holes have been widely studied. However thus far, there have been fewer studies of stars in inclined precessing orbits around a Kerr black hole. In this paper, by using tensor virial equations, we show the presence of possible resonances in these systems for typical physical parameters of black hole-neutron star binaries in close orbits or of a white dwarf/an ordinary star orbiting a supermassive black hole. This suggests the presence of a new instability before the tidal disruption limit is encountered in such systems."
https://arxiv.org/abs/2403.06833,2024-03-11,Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?,"['Egor Zverev', 'Sahar Abdelnabi', 'Mario Fritz', 'Christoph H. Lampert']","Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating the measure, and we report results on several state-of-the-art open-source and closed LLMs. Finally, we quantitatively demonstrate that all evaluated LLMs fail to achieve a high amount of separation, according to our measure. The source code and SEP dataset are openly accessible at https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed."
https://arxiv.org/abs/2403.06832,2024-03-11,The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework,"['Zhuo Chen', 'Yin Fang', 'Yichi Zhang', 'Lingbing Guo', 'Jiaoyan Chen', 'Huajun Chen', 'Wen Zhang']","The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework. This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations. In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and seven for MEMA), demonstrating its robustness and versatility. Besides, SNAG can not only function as a standalone model but also enhance other existing methods, providing stable performance improvements. Our code and data are available at: https://github.com/zjukg/SNAG."
https://arxiv.org/abs/2403.06831,2024-03-11,HDRTransDC: High Dynamic Range Image Reconstruction with Transformer Deformation Convolution,"['Shuaikang Shang', 'Xuejing Kang', 'Anlong Ming']","High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image with realistic details by fusing multi-exposure Low Dynamic Range (LDR) images. Caused by large motion and severe under-/over-exposure among input LDR images, HDR imaging suffers from ghosting artifacts and fusion distortions. To address these critical issues, we propose an HDR Transformer Deformation Convolution (HDRTransDC) network to generate high-quality HDR images, which consists of the Transformer Deformable Convolution Alignment Module (TDCAM) and the Dynamic Weight Fusion Block (DWFB). To solve the ghosting artifacts, the proposed TDCAM extracts long-distance content similar to the reference feature in the entire non-reference features, which can accurately remove misalignment and fill the content occluded by moving objects. For the purpose of eliminating fusion distortions, we propose DWFB to spatially adaptively select useful information across frames to effectively fuse multi-exposed features. Extensive experiments show that our method quantitatively and qualitatively achieves state-of-the-art performance."
https://arxiv.org/abs/2403.06830,2024-03-11,Reduction of Quantum Principal Bundles over non affine bases,"['Rita Fioresi', 'Emanuele Latini', 'Chiara Pagani']",In this paper we develop the theory of reduction of quantum principal bundles over projective bases. We show how the sheaf theoretic approach can be effectively applied to certain relevant examples as the Klein model for the projective spaces; in particular we study in the algebraic setting the reduction of the principal bundle $\mathrm{GL}(n) \to \mathrm{GL}(n)/P= \mathbf{P}^{n-1}(\mathbb{C})$ to the Levi subgroup $G_0$ inside the maximal parabolic subgroup $P$ of $\mathrm{GL}(n)$. We characterize reductions in the sheaf theoretic setting.
https://arxiv.org/abs/2403.06829,2024-03-13,Constructing Variables Using Classifiers as an Aid to Regression: An Empirical Assessment,"['Colin Troisemaine', 'Vincent Lemaire']",This paper proposes a method for the automatic creation of variables (in the case of regression) that complement the information contained in the initial input vector. The method works as a pre-processing step in which the continuous values of the variable to be regressed are discretized into a set of intervals which are then used to define value thresholds. Then classifiers are trained to predict whether the value to be regressed is less than or equal to each of these thresholds. The different outputs of the classifiers are then concatenated in the form of an additional vector of variables that enriches the initial vector of the regression problem. The implemented system can thus be considered as a generic pre-processing tool. We tested the proposed enrichment method with 5 types of regressors and evaluated it in 33 regression datasets. Our experimental results confirm the interest of the approach.
https://arxiv.org/abs/2403.06828,2024-03-11,NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning,"['Ruihua Han', 'Shuai Wang', 'Shuaijun Wang', 'Zeqing Zhang', 'Jianjun Chen', 'Shijie Lin', 'Chengyang Li', 'Chengzhong Xu', 'Yonina C. Eldar', 'Qi Hao', 'Jia Pan']","Navigating a nonholonomic robot in a cluttered environment requires extremely accurate perception and locomotion for collision avoidance. This paper presents NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and environment-invariant robot navigation solution. Leveraging a tightly-coupled perception-locomotion framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw points to a learned multi-frame distance space, avoiding error propagation from perception to control; 2) it is interpretable from an end-to-end model-based learning perspective, enabling provable convergence. The crux of NeuPAN is to solve a high-dimensional end-to-end mathematical model with various point-level constraints using the plug-and-play (PnP) proximal alternating-minimization network (PAN) with neurons in the loop. This allows NeuPAN to generate real-time, end-to-end, physically-interpretable motions directly from point clouds, which seamlessly integrates data- and knowledge-engines, where its network parameters are adjusted via back propagation. We evaluate NeuPAN on car-like robot, wheel-legged robot, and passenger autonomous vehicle, in both simulated and real-world environments. Experiments demonstrate that NeuPAN outperforms various benchmarks, in terms of accuracy, efficiency, robustness, and generalization capability across various environments, including the cluttered sandbox, office, corridor, and parking lot. We show that NeuPAN works well in unstructured environments with arbitrary-shape undetectable objects, making impassable ways passable."
https://arxiv.org/abs/2403.06827,2024-03-11,Orbital relaxation length from first-principles scattering calculations,"['Max Rang', 'Paul J. Kelly']","The orbital Hall effect generates a current of orbital angular momentum perpendicular to a charge current. Experiments suggest that this orbital current decays on a long length scale that is of the order of the spin flip diffusion length or longer. We examine this suggestion using first-principles quantum mechanical scattering calculations to study the decay of orbital currents injected from an orbitally-polarized lead into thermally disordered bulk systems of selected transition metals. We find that the decay occurs over only a few atomic layers. On this length scale the orbital current may be converted into a spin current if the spin Hall angle is sufficiently large, as for Pt. In Cu, Cr and V with small spin Hall angles, the conversion into a spin current is negligible in the bulk and significant conversion only occurs at interfaces."
https://arxiv.org/abs/2403.06826,2024-03-11,In-context Exploration-Exploitation for Reinforcement Learning,"['Zhenwen Dai', 'Federico Tomasi', 'Sina Ghiassian']","In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method."
https://arxiv.org/abs/2403.06825,2024-03-11,Entanglement and logarithmic spirals in a quantum spin-1 many-body system with competing dimer and trimer interactions,"['Huan-Qiang Zhou', 'Qian-Qian Shi', 'Ian P. McCulloch', 'Murray T. Batchelor']","Spontaneous symmetry breaking (SSB) with type-B Goldstone modes is investigated in the macroscopically degenerate phase for a quantum spin-1 many-body system with competing dimer and trimer interactions. The SSB involves three distinct patterns. The first occurs at the dimer point, with the pattern from staggered ${\rm SU}(3)$ to ${\rm U}(1)\times{\rm U}(1)$. The second occurs at the trimer point, with the pattern from uniform ${\rm SU}(3)$ to ${\rm U}(1)\times{\rm U}(1)$. The third occurs in the dimer-trimer regime, with the pattern from uniform ${\rm SU}(2)$ to ${\rm U}(1)$. The number of type-B Goldstone modes is thus two, two and one for the three patterns, respectively. The ground state degeneracies arising from the three patterns are exponential with the system size, which may be recognized as sequences of integers relevant to self-similar logarithmic spirals. This in turn is attributed to the presence of an emergent symmetry operation tailored to a specific degenerate ground state. As a consequence, the residual entropy is non-zero, which measures the disorder present in a unit cell of highly degenerate ground state generated from a generalized highest weight state. An exact Schmidt decomposition exists for the highly degenerate ground states, thus exposing the self-similarities underlying an abstract fractal, described by the fractal dimension. The latter is extracted from performing a universal finite system-size scaling analysis of the entanglement entropy, which is identical to the number of type-B Goldstone modes. The model under investigation thus accommodates an exotic scale invariant quantum state of matter."
https://arxiv.org/abs/2403.06824,2024-03-11,Noise-induced transitions past the onset of a steady symmetry-breaking bifurcation: the case of the sudden expansion,"['Yves-Marie Ducimetière', 'Edouard Boujo', 'François Gallaire']","We consider fluid flows, governed by the Navier-Stokes equations, subject to a steady symmetry-breaking bifurcation and forced by a weak noise acting on a slow time scale. By generalizing the multiple-scale weakly nonlinear expansion technique employed in the literature for the response of the Duffing oscillator, we rigorously derive a stochastically forced Stuart-Landau equation for the dominant symmetry-breaking mode. The probability density function of the solution, and of the escape time from one attractor to the other, are then determined by solving the associated Fokker-Planck equation. The validity of this reduced order model is tested on the flow past a sudden expansion, for a given Reynolds number and different noise amplitudes. At a very low numerical cost, the statistics obtained from the amplitude equation accurately reproduce those of long-time direct numerical simulations."
https://arxiv.org/abs/2403.06823,2024-03-13,"Transparent AI Disclosure Obligations: Who, What, When, Where, Why, How","['Abdallah El Ali', 'Karthikeya Puttur Venkatraj', 'Sophie Morosoli', 'Laurens Naudts', 'Natali Helberger', 'Pablo Cesar']","Advances in Generative Artificial Intelligence (AI) are resulting in AI-generated media output that is (nearly) indistinguishable from human-created content. This can drastically impact users and the media sector, especially given global risks of misinformation. While the currently discussed European AI Act aims at addressing these risks through Article 52's AI transparency obligations, its interpretation and implications remain unclear. In this early work, we adopt a participatory AI approach to derive key questions based on Article 52's disclosure obligations. We ran two workshops with researchers, designers, and engineers across disciplines (N=16), where participants deconstructed Article 52's relevant clauses using the 5W1H framework. We contribute a set of 149 questions clustered into five themes and 18 sub-themes. We believe these can not only help inform future legal developments and interpretations of Article 52, but also provide a starting point for Human-Computer Interaction research to (re-)examine disclosure transparency from a human-centered AI lens."
https://arxiv.org/abs/2403.06822,2024-03-11,Dynamics of Polyalkylfluorene Conjugated Polymers: Insights from Neutron Spectroscopy and Molecular Dynamics Simulations,"['Mohamed Zbiri', 'Anne A. Y. Guilbert']","Dynamics of the conjugated polymers poly(9,9-dioctylfluorene) (PF8) and poly(9,9-didodecylfluorene) (PF12), differing by the length of their side chains, is investigated in the amorphous phase using the quasielastic neutron scattering (QENS) technique. The neutron spectroscopy measurements are synergistically underpinned by molecular dynamics (MD) simulations. The probe is focused on the picosecond time scale where the structural dynamics of both PF8 and PF12 would mainly be dominated by the motions of their side chains. The measurements highlighted temperature-induced dynamics reflected in the broadening of the QENS spectra upon heating. The MD simulations reproduced well the observations, hence the neutron measurements validate the MD force fields, the adopted amorphous model structures and the numerical procedure. As the QENS spectra are dominated by the signal from the hydrogens on the backbones and side chains of PF8 and PF12, extensive analysis of the MD simulations allowed: (i) tagging these hydrogens, (ii) estimating their contributions to the self part of the van Hove functions and hence to the QENS spectra, and (iii) determining the activation energies of the different motions involving the tagged hydrogens. PF12 is found to exhibit broader QENS spectra than PF8, signature of a more pronounced motions of the didodecyl chains as compared to dioctyl chains. This is in agreement with the outcome of our MD analysis: (i) confirming a lower glass transition temperature of PF12 compared to PF8, (ii) showing PF12 having a lower density than PF8, and (iii) highlighting lower activation energies of the motions of PF12 in comparison with PF8. This study helped gaining insights into the temperature-induced side chains dynamics of the PF8 and PF12 conjugated polymers influencing their stability, and thus could practically impact the performance of the associated optoelectronic active layer."
https://arxiv.org/abs/2403.06821,2024-03-11,On discrete-time arrival processes and related random motions,"[""Giuseppe D'Onofrio"", 'Thomas M. Michelitsch', 'Federico Polito', 'Alejandro P. Riascos']","We consider three kinds of discrete-time arrival processes: transient, intermediate and recurrent, characterized by a finite, possibly finite and infinite number of events, respectively. In this context, we study renewal processes which are stopped at the first event of a further independent renewal process whose inter-arrival time distribution can be defective. If this is the case, the resulting arrival process is of an intermediate nature. For non-defective absorbing times, the resulting arrival process is transient, i.e.\ stopped almost surely. For these processes we derive finite time and asymptotic properties. We apply these results to biased and unbiased random walks on the d-dimensional infinite lattice and as a special case on the two-dimensional triangular lattice. We study the spatial propagator of the walker and its large time asymptotics. In particular, we observe the emergence of a superdiffusive (ballistic) behavior in the case of biased walks. For geometrically distributed stopping times, the propagator converges to a stationary non-equilibrium steady state (NESS), which is universal in the sense that it is independent of the stopped process. In dimension one, for both light- and heavy-tailed step distributions, the NESS has an integral representation involving alpha-stable distributions."
https://arxiv.org/abs/2403.06820,2024-03-11,A quasilinear Keller-Segel model with saturated discontinuous advection,"['Maria Gualdani', 'Mikel Ispizua', 'Nicola Zamponi']","We consider the singular limit of a chemotaxis model of bacterial collective motion recently introduced in arXiv:2009.11048 [math.AP]. The equation models aggregation-diffusion phenomena with advection that is discontinuous and depends sharply on the gradient of the density itself. The quasi-linearity of the problem poses major challenges in the construction of the solution and complications arise in the proof of regularity. Our method overcomes these obstacle by relying solely on entropy inequalities and the theory of monotone operators. We provide existence, uniqueness and smoothing estimates in any dimensional space."
https://arxiv.org/abs/2403.06819,2024-03-11,Geometric and topological corrections to Schwarzschild black hole,"[""Rocco D'Agostino"", 'Orlando Luongo', 'Stefano Mancini']","In this paper, we compute departures in the black hole thermodynamics induced by either geometric or topological corrections to general relativity. Specifically, we analyze the spherically symmetric spacetime solutions of two modified gravity scenarios with Lagrangians $\mathcal{L}\sim R^{1+ε}$ and $\mathcal{L}\sim R+ε\, \mathcal{G}^2$, where $\mathcal{G}$ is the Euler density in four dimensions, while $ 0<ε\ll 1$ measures the perturbation around the Hilbert-Einstein action. Accordingly, we find the expressions of the Bekenstein-Hawking entropy by the Penrose formula, and the black hole temperature and horizon of the obtained solutions. We then investigate the heat capacities in terms of the free parameters of the theories under study. In doing so, we show that healing the problem of negative heat capacities can be possible under particular choices of the free constants, albeit with limitations on the masses allowed for the black hole solutions."
https://arxiv.org/abs/2403.06818,2024-03-11,User Tracking and Direction Estimation Codebook Design for IRS-Assisted mmWave Communication,"['Moritz Garkisch', 'Sebastian Lotter', 'Gui Zhou', 'Vahid Jamali', 'Robert Schober']","Future communication systems are envisioned to employ intelligent reflecting surfaces (IRSs) and the millimeter wave (mmWave) frequency band to provide reliable high-rate services. For mobile users, the time-varying channel state information (CSI) requires adequate adjustment of the reflection pattern of the IRS. We propose a novel codebook-based user tracking (UT) algorithm for IRS-assisted mmWave communication, allowing suitable reconfiguration of the IRS unit cell phase shifts, resulting in a high reflection gain. The presented algorithm acquires the direction information of the user based on a peak likelihood-based direction estimation. Using the direction information, the user's trajectory is extrapolated to proactively update the adopted codeword and adjust the IRS phase shift configuration accordingly. Furthermore, we conduct a theoretical analysis of the direction estimation error and utilize the obtained insights to design a codebook specifically optimized for direction estimation. Our numerical results reveal a lower direction estimation error of the proposed UT algorithm when employing our designed codebook compared to codebooks from the literature. Furthermore, the average achieved signal-to-noise ratio (SNR) as well as the average effective rate of the proposed UT algorithm are analyzed. The proposed UT algorithm requires only a low overhead for direction and channel estimation and avoids outdated IRS phase shifts. Furthermore, it is shown to outperform two benchmark schemes based on direct phase shift optimization and hierarchical codebook search, respectively, via computer simulations."
https://arxiv.org/abs/2403.06817,2024-03-11,Are Targeted Messages More Effective?,"['Martin Grohe', 'Eran Rosenbluth']","Graph neural networks (GNN) are deep learning architectures for graphs. Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm."
https://arxiv.org/abs/2403.06816,2024-03-11,"Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science","['Gabriel P. Langlois', 'Jatan Buch', 'Jérôme Darbon']","Maximum entropy (Maxent) models are a class of statistical models that use the maximum entropy principle to estimate probability distributions from data. Due to the size of modern data sets, Maxent models need efficient optimization algorithms to scale well for big data applications. State-of-the-art algorithms for Maxent models, however, were not originally designed to handle big data sets; these algorithms either rely on technical devices that may yield unreliable numerical results, scale poorly, or require smoothness assumptions that many practical Maxent models lack. In this paper, we present novel optimization algorithms that overcome the shortcomings of state-of-the-art algorithms for training large-scale, non-smooth Maxent models. Our proposed first-order algorithms leverage the Kullback-Leibler divergence to train large-scale and non-smooth Maxent models efficiently. For Maxent models with discrete probability distribution of $n$ elements built from samples, each containing $m$ features, the stepsize parameters estimation and iterations in our algorithms scale on the order of $O(mn)$ operations and can be trivially parallelized. Moreover, the strong $\ell_{1}$ convexity of the Kullback--Leibler divergence allows for larger stepsize parameters, thereby speeding up the convergence rate of our algorithms. To illustrate the efficiency of our novel algorithms, we consider the problem of estimating probabilities of fire occurrences as a function of ecological features in the Western US MTBS-Interagency wildfire data set. Our numerical results show that our algorithms outperform the state of the arts by one order of magnitude and yield results that agree with physical models of wildfire occurrence and previous statistical analyses of wildfire drivers."
https://arxiv.org/abs/2403.06815,2024-03-11,Nested traveling wave structures in elastoinertial turbulence,"['Manish Kumar', 'Michael D. Graham']","Elastoinertial turbulence (EIT) is a chaotic flow resulting from the interplay between inertia and viscoelasticity in wall bounded shear flows. Understanding EIT is important because it is thought to set a limit on the effectiveness of turbulent drag reduction in polymer solutions. In the present study, we analyze simulations of two dimensional EIT in channel flow using Spectral Proper Orthogonal Decomposition (SPOD), discovering a family of traveling wave structures that capture the sheetlike stress fluctuations that characterize EIT. The frequency dependence of the leading SPOD mode contains several distinct peaks and the mode structures corresponding to these peaks exhibit well defined traveling structures. The structure of the dominant traveling mode exhibits shift reflect symmetry similar to the viscoelasticity modified Tollmien Schlichting (TS) wave, where the velocity fluctuation in the traveling mode is characterized by the formation of large scale regular structures spanning the channel and the polymeric stress field is characterized by the formation of thin, inclined sheets of high polymeric stress localized at the critical layers near the channel walls. The structures corresponding to the higher frequency modes have a very similar structure, but nested in a region roughly bounded by the critical layer positions of the next lower frequency mode. A simple theory based on the idea that the critical layers of mode $k$ form the walls for the structure of mode $k+1$ yields quantitative agreement with the observed wave speeds and critical layer positions. The physical idea behind this theory is that the sheetlike localized stress fluctuations in the critical layer prevent velocity fluctuations from penetrating them."
https://arxiv.org/abs/2403.06814,2024-03-11,ε-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment,"['Hao-Lun Hsu', 'Qitong Gao', 'Miroslav Pajic']","Deep Brain Stimulation (DBS) stands as an effective intervention for alleviating the motor symptoms of Parkinson's disease (PD). Traditional commercial DBS devices are only able to deliver fixed-frequency periodic pulses to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS). However, they in general suffer from energy inefficiency and side effects, such as speech impairment. Recent research has focused on adaptive DBS (aDBS) to resolve the limitations of cDBS. Specifically, reinforcement learning (RL) based approaches have been developed to adapt the frequencies of the stimuli in order to achieve both energy efficiency and treatment efficacy. However, RL approaches in general require significant amount of training data and computational resources, making it intractable to integrate RL policies into real-time embedded systems as needed in aDBS. In contrast, contextual multi-armed bandits (CMAB) in general lead to better sample efficiency compared to RL. In this study, we propose a CMAB solution for aDBS. Specifically, we define the context as the signals capturing irregular neuronal firing activities in the BG regions (i.e., beta-band power spectral density), while each arm signifies the (discretized) pulse frequency of the stimulation. Moreover, an ε-exploring strategy is introduced on top of the classic Thompson sampling method, leading to an algorithm called ε-Neural Thompson sampling (ε-NeuralTS), such that the learned CMAB policy can better balance exploration and exploitation of the BG environment. The ε-NeuralTS algorithm is evaluated using a computation BG model that captures the neuronal activities in PD patients' brains. The results show that our method outperforms both existing cDBS methods and CMAB baselines."
https://arxiv.org/abs/2403.06813,2024-03-11,LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations,"['Mohammad Alkhalefi', 'Georgios Leontidis', 'Mingjun Zhong']","Contrastive instance discrimination outperforms supervised learning in downstream tasks like image classification and object detection. However, this approach heavily relies on data augmentation during representation learning, which may result in inferior results if not properly implemented. Random cropping followed by resizing is a common form of data augmentation used in contrastive learning, but it can lead to degraded representation learning if the two random crops contain distinct semantic content. To address this issue, this paper introduces LeOCLR (Leveraging Original Images for Contrastive Learning of Visual Representations), a framework that employs a new instance discrimination approach and an adapted loss function that ensures the shared region between positive pairs is semantically correct. The experimental results show that our approach consistently improves representation learning across different datasets compared to baseline models. For example, our approach outperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and several other methods on transfer learning tasks."
https://arxiv.org/abs/2403.06812,2024-03-11,Monotone Individual Fairness,['Yahav Bechavod'],"We revisit the problem of online learning with individual fairness, where an online learner strives to maximize predictive accuracy while ensuring that similar individuals are treated similarly. We first extend the frameworks of Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human auditors regarding fairness violations, as we consider auditing schemes that are capable of aggregating feedback from any number of auditors, using a rich class we term monotone aggregation functions. We then prove a characterization for such auditing schemes, practically reducing the analysis of auditing for individual fairness by multiple auditors to that of auditing by (instance-specific) single auditors. Using our generalized framework, we present an oracle-efficient algorithm achieving an upper bound frontier of $(\mathcal{O}(T^{1/2+2b}),\mathcal{O}(T^{3/4-b}))$ respectively for regret, number of fairness violations, for $0\leq b \leq 1/4$. We then study an online classification setting where label feedback is available for positively-predicted individuals only, and present an oracle-efficient algorithm achieving an upper bound frontier of $(\mathcal{O}(T^{2/3+2b}),\mathcal{O}(T^{5/6-b}))$ for regret, number of fairness violations, for $0\leq b \leq 1/6$. In both settings, our algorithms improve on the best known bounds for oracle-efficient algorithms. Furthermore, our algorithms offer significant improvements in computational efficiency, greatly reducing the number of required calls to an (offline) optimization oracle per round, to $\tilde{\mathcal{O}}(α^{-2})$ in the full information setting, and $\tilde{\mathcal{O}}(α^{-2} + k^2T^{1/3})$ in the partial information setting, where $α$ is the sensitivity for reporting fairness violations, and $k$ is the number of individuals in a round."
https://arxiv.org/abs/2403.06811,2024-03-12,Weak form Shallow Ice Approximation models with an improved time step restriction,"['Igor Tominec', 'Josefin Ahlkrona']","The Shallow Ice Approximation (SIA) model on strong form is commonly used for inferring the flow dynamics of grounded ice sheets. The solution to the SIA model is a closed-form expression for the velocity field. When that velocity field is used to advance the ice surface in time, the time steps have to take small values due to quadratic scaling in terms of the horizontal mesh size. In this paper we write the SIA model on weak form, and add in the Free Surface Stabilization Algorithm (FSSA) terms. We find numerically that the time step restriction scaling is improved from quadratic to linear, but only for large horizontal mesh sizes. We then extend the weak form by adding the initially neglected normal stress terms. This allows for a linear time step restriction across the whole range of the horizontal mesh sizes, leading to an improved efficiency. Theoretical analysis demonstrates that the inclusion of FSSA stabilization terms transitions the explicit time stepping treatment of second derivative surface terms to an implicit approach. Moreover, a computational cost analysis, combined with numerical results on stability and accuracy, advocates for preferring the SIA models written on weak form over the standard SIA model."
https://arxiv.org/abs/2403.06810,2024-03-11,Deep Learning Approaches for Human Action Recognition in Video Data,['Yufei Xie'],"Human action recognition in videos is a critical task with significant implications for numerous applications, including surveillance, sports analytics, and healthcare. The challenge lies in creating models that are both precise in their recognition capabilities and efficient enough for practical use. This study conducts an in-depth analysis of various deep learning models to address this challenge. Utilizing a subset of the UCF101 Videos dataset, we focus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Two-Stream ConvNets. The research reveals that while CNNs effectively capture spatial features and RNNs encode temporal sequences, Two-Stream ConvNets exhibit superior performance by integrating spatial and temporal dimensions. These insights are distilled from the evaluation metrics of accuracy, precision, recall, and F1-score. The results of this study underscore the potential of composite models in achieving robust human action recognition and suggest avenues for future research in optimizing these models for real-world deployment."
https://arxiv.org/abs/2403.06809,2024-03-11,Data-driven sparse modeling of oscillations in plasma space propulsion,"['B. Bayón-Buján', 'M. Merino']","An algorithm to obtain data-driven models of oscillatory phenomena in plasma space propulsion systems is presented, based on sparse regression (SINDy) and Pareto front analysis. The algorithm can incorporate physical constraints, use data bootstrapping for additional robustness, and fine-tuning to different metrics. Standard, weak and integral SINDy formulations are discussed and compared. The scheme is benchmarked in the case of breathing-mode oscillations in Hall effect thrusters, using PIC/fluid simulation data. Models of varying complexity are obtained for the average plasma properties, and shown to have a clear physical interpretability and agreement with existing 0D models in the literature. Lastly, the algorithm applied is also shown to enable the identification of physical subdomains with qualitatively different plasma dynamics, providing valuable information for more advanced modeling approaches."
https://arxiv.org/abs/2403.06808,2024-03-11,Height Filtrations and Base Loci on Flag Bundles over a Curve,"['Yangyu Fan', 'Wenbin Luo', 'Binggang Qu']",Let $k$ be an algebraically closed field of characteristic zero. Let $C/k$ be a projective smooth curve with function field $K=k(C)$ and $G/k$ be a connected reductive group. Let $F$ be a principal $G$-bundle on $C$. Let $P \subseteq G$ be a parabolic subgroup and $λ: P \longrightarrow G$ be a strictly anti-dominant character. Then $F/P \longrightarrow C$ is a flag bundle and $\mathcal{L}_λ=F \times_P k_λ$ on $F/P$ is a relatively ample line bundle.
https://arxiv.org/abs/2403.06807,2024-03-11,Multistep Consistency Models,"['Jonathan Heek', 'Emiel Hoogeboom', 'Tim Salimans']","Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step."
https://arxiv.org/abs/2403.06806,2024-03-11,On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes,"['Navdeep Kumar', 'Yashaswini Murthy', 'Itai Shufaro', 'Kfir Y. Levy', 'R. Srikant', 'Shie Mannor']","We present the first finite time global convergence analysis of policy gradient in the context of infinite horizon average reward Markov decision processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite state and action spaces. Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O\left({\frac{1}{T}}\right),$ which translates to $O\left({\log(T)}\right)$ regret, where $T$ represents the number of iterations. Prior work on performance bounds for discounted reward MDPs cannot be extended to average reward MDPs because the bounds grow proportional to the fifth power of the effective horizon. Thus, our primary contribution is in proving that the policy gradient algorithm converges for average-reward MDPs and in obtaining finite-time performance guarantees. In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit dependence on constants that capture the complexity of the underlying MDP. Motivated by this observation, we reexamine and improve the existing performance bounds for discounted reward MDPs. We also present simulations to empirically evaluate the performance of average reward policy gradient algorithm."
https://arxiv.org/abs/2403.06805,2024-03-11,On the Robustness of Lexicase Selection to Contradictory Objectives,"['Shakiba Shahbandegan', 'Emily Dolson']","Lexicase and epsilon-lexicase selection are state of the art parent selection techniques for problems featuring multiple selection criteria. Originally, lexicase selection was developed for cases where these selection criteria are unlikely to be in conflict with each other, but preliminary work suggests it is also a highly effective many-objective optimization algorithm. However, to predict whether these results generalize, we must understand lexicase selection's performance on contradictory objectives. Prior work has shown mixed results on this question. Here, we develop theory identifying circumstances under which lexicase selection will succeed or fail to find a Pareto-optimal solution. To make this analysis tractable, we restrict our investigation to a theoretical problem with maximally contradictory objectives. Ultimately, we find that lexicase and epsilon-lexicase selection each have a region of parameter space where they are incapable of optimizing contradictory objectives. Outside of this region, however, they perform well despite the presence of contradictory objectives. Based on these findings, we propose theoretically-backed guidelines for parameter choice. Additionally, we identify other properties that may affect whether a many-objective optimization problem is a good fit for lexicase or epsilon-lexicase selection."
https://arxiv.org/abs/2403.06804,2024-03-11,Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction,"['Souhaib Attaiki', 'Maks Ovsjanikov']","We present Shape Non-rigid Kinematics (SNK), a novel zero-shot method for non-rigid shape matching that eliminates the need for extensive training or ground truth data. SNK operates on a single pair of shapes, and employs a reconstruction-based strategy using an encoder-decoder architecture, which deforms the source shape to closely match the target shape. During the process, an unsupervised functional map is predicted and converted into a point-to-point map, serving as a supervisory mechanism for the reconstruction. To aid in training, we have designed a new decoder architecture that generates smooth, realistic deformations. SNK demonstrates competitive results on traditional benchmarks, simplifying the shape-matching process without compromising accuracy. Our code can be found online: https://github.com/pvnieo/SNK"
https://arxiv.org/abs/2403.06803,2024-03-11,Data-Independent Operator: A Training-Free Artifact Representation Extractor for Generalizable Deepfake Detection,"['Chuangchuang Tan', 'Ping Liu', 'RenShuai Tao', 'Huan Liu', 'Yao Zhao', 'Baoyuan Wu', 'Yunchao Wei']","Recently, the proliferation of increasingly realistic synthetic images generated by various generative adversarial networks has increased the risk of misuse. Consequently, there is a pressing need to develop a generalizable detector for accurately recognizing fake images. The conventional methods rely on generating diverse training sources or large pretrained models. In this work, we show that, on the contrary, the small and training-free filter is sufficient to capture more general artifact representations. Due to its unbias towards both the training and test sources, we define it as Data-Independent Operator (DIO) to achieve appealing improvements on unseen sources. In our framework, handcrafted filters and the randomly-initialized convolutional layer can be used as the training-free artifact representations extractor with excellent results. With the data-independent operator of a popular classifier, such as Resnet50, one could already reach a new state-of-the-art without bells and whistles. We evaluate the effectiveness of the DIO on 33 generation models, even DALLE and Midjourney. Our detector achieves a remarkable improvement of $13.3\%$, establishing a new state-of-the-art performance. The DIO and its extension can serve as strong baselines for future methods. The code is available at \url{https://github.com/chuangchuangtan/Data-Independent-Operator}."
https://arxiv.org/abs/2403.06802,2024-03-11,Joint Source-and-Channel Coding for Small Satellite Applications,"['Olga Kondrateva', 'Stefan Dietzel', 'Björn Scheuermann']","Small satellites are widely used today as cost effective means to perform Earth observation and other tasks that generate large amounts of high-dimensional data, such as multi-spectral imagery. These satellites typically operate in low earth orbit, which poses significant challenges for data transmission due to short contact times with ground stations, low bandwidth, and high packet loss probabilities. In this paper, we introduce JSCC-Sat, which applies joint source-and-channel coding using neural networks to provide efficient and robust transmission of compressed image data for satellite applications. We evaluate our mechanism against traditional transmission schemes with separate source and channel coding and demonstrate that it outperforms the existing approaches when applied to Earth observation data of the Sentinel-2 mission."
https://arxiv.org/abs/2403.06801,2024-03-11,CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging,"['Ibrahim Ethem Hamamci', 'Sezgin Er', 'Bjoern Menze']","Medical imaging plays a crucial role in diagnosis, with radiology reports serving as vital documentation. Automating report generation has emerged as a critical need to alleviate the workload of radiologists. While machine learning has facilitated report generation for 2D medical imaging, extending this to 3D has been unexplored due to computational complexity and data scarcity. We introduce the first method to generate radiology reports for 3D medical imaging, specifically targeting chest CT volumes. Given the absence of comparable methods, we establish a baseline using an advanced 3D vision encoder in medical imaging to demonstrate our method's effectiveness, which leverages a novel auto-regressive causal transformer. Furthermore, recognizing the benefits of leveraging information from previous visits, we augment CT2Rep with a cross-attention-based multi-modal fusion module and hierarchical memory, enabling the incorporation of longitudinal multimodal data. Access our code at: https://github.com/ibrahimethemhamamci/CT2Rep"
https://arxiv.org/abs/2403.06800,2024-03-11,MambaMIL: Enhancing Long Sequence Modeling with Sequence Reordering in Computational Pathology,"['Shu Yang', 'Yihui Wang', 'Hao Chen']","Multiple Instance Learning (MIL) has emerged as a dominant paradigm to extract discriminative feature representations within Whole Slide Images (WSIs) in computational pathology. Despite driving notable progress, existing MIL approaches suffer from limitations in facilitating comprehensive and efficient interactions among instances, as well as challenges related to time-consuming computations and overfitting. In this paper, we incorporate the Selective Scan Space State Sequential Model (Mamba) in Multiple Instance Learning (MIL) for long sequence modeling with linear complexity, termed as MambaMIL. By inheriting the capability of vanilla Mamba, MambaMIL demonstrates the ability to comprehensively understand and perceive long sequences of instances. Furthermore, we propose the Sequence Reordering Mamba (SR-Mamba) aware of the order and distribution of instances, which exploits the inherent valuable information embedded within the long sequences. With the SR-Mamba as the core component, MambaMIL can effectively capture more discriminative features and mitigate the challenges associated with overfitting and high computational overhead. Extensive experiments on two public challenging tasks across nine diverse datasets demonstrate that our proposed framework performs favorably against state-of-the-art MIL methods. The code is released at https://github.com/isyangshu/MambaMIL."
https://arxiv.org/abs/2403.06799,2024-03-11,A perspective on active glassy dynamics in biological systems,"['Souvik Sadhukhan', 'Subhodeep Dey', 'Smarajit Karmakar', 'Saroj Kumar Nandi']","Dynamics is central to living systems. In the last two decades, experiments have revealed that the dynamics in diverse biological systems - from intracellular cytoplasm to cellular and organismal aggregates - are remarkably similar to that in dense systems of inanimate particles in equilibrium. They show a glass transition from a solid-like jammed state to a fluid-like flowing state, where a moderate change in control parameter leads to an enormous variation in relaxation time. However, biological systems have crucial differences from the equilibrium systems: the former have activity that drives them out of equilibrium, novel control parameters, and enormous levels of complexity. These active systems showing glassy dynamics are known as active glasses. The field is at the interface of physics and biology, freely borrowing tools from both disciplines and promising novel, fascinating discoveries. We review the experiments that started this field, simulations that have been instrumental for insights, and theories that have helped unify diverse phenomena, reveal correlations, and make novel quantitative predictions. We discuss the primary characteristics that define a glassy system. For most concepts, we first discuss the known equilibrium scenario and then present the key aspects when activity is introduced. We end the article with a discussion of the challenges in the field and possible future directions."
https://arxiv.org/abs/2403.06798,2024-03-11,Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification,"['Shuai Li', 'Xiaoguang Ma', 'Shancheng Jiang', 'Lu Meng']","Remarkable successes were made in Medical Image Classification (MIC) recently, mainly due to wide applications of convolutional neural networks (CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity with raw data, raising serious concerns on network robustness. Although adversarial training (AT), in responding to malevolent AEs, was recognized as an effective approach to improve robustness, it was challenging to overcome generalization decline of networks caused by the AT. In this paper, in order to reserve high generalization while improving robustness, we proposed a dynamic perturbation-adaptive adversarial training (DPAAT) method, which placed AT in a dynamic learning environment to generate adaptive data-level perturbations and provided a dynamically updated criterion by loss information collections to handle the disadvantage of fixed perturbation sizes in conventional AT methods and the dependence on external transference. Comprehensive testing on dermatology HAM10000 dataset showed that the DPAAT not only achieved better robustness improvement and generalization preservation but also significantly enhanced mean average precision and interpretability on various CNNs, indicating its great potential as a generic adversarial training method on the MIC."
https://arxiv.org/abs/2403.06797,2024-03-11,Leveraging Internal Representations of Model for Magnetic Image Classification,"['Adarsh N L', 'Arun P V', 'Alok Porwal', 'Malcolm Aranha']","Data generated by edge devices has the potential to train intelligent autonomous systems across various domains. Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations. This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available. We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity. Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results. This methodology presents a promising avenue for training machine learning models with minimal data."
https://arxiv.org/abs/2403.06796,2024-03-11,Defaults: a double-edged sword in governing common resources,"['Eladio Montero-Porras', 'Rémi Suchon', 'Tom Lenaerts', 'Elias Fernández Domingos']","Extracting from shared resources requires making choices to balance personal profit and sustainability. We present the results of a behavioural experiment wherein we manipulate the default extraction from a finite resource. Participants were exposed to two treatments -- pro-social or self-serving extraction defaults -- and a control without defaults. We examined the persistence of these nudges by removing the default after five rounds. Results reveal that a self-serving default increased the average extraction while present, whereas a pro-social default only decreased extraction for the first two rounds. Notably, the influence of defaults depended on individual inclinations, with cooperative individuals extracting more under a self-serving default, and selfish individuals less under a pro-social default. After the removal of the default, we observed no significant differences with the control treatment. Our research highlights the potential of defaults as cost-effective tools for promoting sustainability, while also advocating for a careful use to avoid adverse effects."
https://arxiv.org/abs/2403.06795,2024-03-11,Jeffery Orbits with Noise Revisited,"['Julian Talbot', 'Charles Antoine', 'Philippe Claudin', 'Ellák Somfai', 'Tamás Börzsönyi']","The behavior of non-spherical particles in a shear-flow is of significant practical and theoretical interest. These systems have been the object of numerous investigations since the pioneering work of Jeffery a century ago. His eponymous orbits describe the deterministic motion of an isolated, rod-like particle in a shear flow. Subsequently, the effect of adding noise was investigated. The theory has been applied to colloidal particles, macromolecules, anisometric granular particles and most recently to microswimmers, for example bacteria. We study the Jeffery orbits of elongated particles subject to noise using Langevin simulations and a Fokker-Planck equation. We extend the analytical solution for infinitely thin needles ($β=1$) obtained by Doi and Edwards to particles with arbitrary shape factor ($0\le β\le 1$) and validate the theory by comparing it with simulations. We examine the rotation of the particle around the vorticity axis and study the orientational order matrix. We use the latter to obtain scalar order parameters $s$ and $r$ describing nematic ordering and biaxiality from the orientational distribution function. The value of $s$ (nematic ordering) increases monotonically with increasing Péclet number, while $r$ (measure of biaxiality) displays a maximum value. From perturbation theory we obtain simple expressions that provide accurate descriptions at low noise (or large Péclet numbers). We also examine the orientational distribution in the v-grad v plane and in the perpendicular direction. Finally we present the solution of the Fokker-Planck equation for a strictly two-dimensional (2D) system. For the same noise amplitude the average rotation speed of the particle in 3D is larger than in 2D."
https://arxiv.org/abs/2403.06794,2024-03-11,Closed-loop control of gamma oscillations in the brain connections through the transcranial stimulations,"['Xuan Zhang', 'Duoyu Feng', 'Djibrina Barry', 'Jiajia Li']","The reconstruction of brain neural network connections occurs not only during the infancy and early childhood stages of brain development, but also in patients with cognitive impairment in middle and old age under the therapy with stimulated external interference, such as the non-invasive repetitive transcranial magnetic stimulation (rTMS) and the transcranial direct current stimulation(tDCS). However, until now, it is not clear how brain stimulation triggers and controls the reconstruction of neural network connections in the brain. This paper combines the EEG data analysis and the cortical neuronal network modeling methods. On one hand, an E-I balanced cortical neural network model was constructed under a long-lasting external stimulation of sinusoidal-exponential form TMS or square-wave tDCS was introduced into the network model for simulate the treatment process for the brain connections. On the other hand, by combining Butterworth filter and functional connectivity algorithm, the paper analyzes the relations between the attentional gamma oscillation responses and the brain connection based on the publicly available EEGs during the pre-tDCS and post-tDCS treatment phases. Firstly, the simulation results indicate that, during long-lasting external stimulations of tDCS/rTMS, The sustained gamma oscillation was found to trigger more release of BDNF from astrocytes to participate in the positively reshaping the excitatory neuronal network connection."
https://arxiv.org/abs/2403.06793,2024-03-11,Boosting Image Restoration via Priors from Pre-trained Models,"['Xiaogang Xu', 'Shu Kong', 'Tao Hu', 'Zhe Liu', 'Hujun Bao']","Pre-trained models with large-scale training data, such as CLIP and Stable Diffusion, have demonstrated remarkable performance in various high-level computer vision tasks such as image understanding and generation from language descriptions. Yet, their potential for low-level tasks such as image restoration remains relatively unexplored. In this paper, we explore such models to enhance image restoration. As off-the-shelf features (OSF) from pre-trained models do not directly serve image restoration, we propose to learn an additional lightweight module called Pre-Train-Guided Refinement Module (PTG-RM) to refine restoration results of a target restoration network with OSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying Enhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention (PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations, while PTG-CSA enhances spatial-channel attention for restoration-related learning. Extensive experiments demonstrate that PTG-RM, with its compact size ($<$1M parameters), effectively enhances restoration performance of various models across different tasks, including low-light enhancement, deraining, deblurring, and denoising."
https://arxiv.org/abs/2403.06792,2024-03-11,"Study of the mechanism of electroacupuncture regulating ferroptosis, inhibiting bladder neck fibrosis, and improving bladder urination function after suprasacral spinal cord injury using proteomics","['Jin-Can Liu', 'Li-Ya Tang', 'Xiao-Ying Sun', 'Qi-Rui Qu', 'Qiong Liu', 'Lu Zhou', 'Hong Zhang', 'Bruce Song', 'Ming Xu', 'Kun Ai']","Purpose The aim of this study was to explore whether electroacupuncture regulates phenotypic transformation of smooth muscle cells by inhibiting ferroptosis and inhibiting fibrosis, thereby improving bladder urination function after suprasacral spinal cord injury (SSCI). Methods The experiment was divided into sham, model, and electroacupuncture group. After 10 days of electroacupuncture intervention, urodynamic examination was performed, and bladder neck was taken for HE staining, tandem mass tag (TMT)-based quantitative proteomics analysis, Western blot(WB) detection, ferrous ion concentration detection and Masson staining. Conclusion Electroacupuncture may prevent the phenotype of bladder neck smooth muscle cells from transforming from contraction type to synthesis type by inhibiting ferroptosis, inhibit bladder neck fibrosis, improve bladder neck compliance, and thus improve bladder urination function after SSCI."
https://arxiv.org/abs/2403.06791,2024-03-11,"Dirichlet heat kernel estimates of subordinate diffusions with continuous components in $C^{1, α}$ open sets",['Jie-Ming Wang'],"In this paper, we derive explicit sharp two-sided estimates for the Dirichlet heat kernels for a class of subordinate diffusions with continuous components when the scaling order of purely discontinuous part of the subordinator is between $0$ and $1$ including $1$ in $C^{1, α}(α\in (0, 1])$ open sets. As a corollary, we obtain the sharp two-sided estimates for Green functions of these processes in bounded $C^{1, α}$ open sets."
https://arxiv.org/abs/2403.06790,2024-03-11,Next4: Snapshots in Ext4 File System,"['Aditya Dani', 'Shardul Mangade', 'Piyush Nimbalkar', 'Harshad Shirwadkar']","The growing value of data as a strategic asset has given rise to the necessity of implementing reliable backup and recovery solutions in the most efficient and cost-effective manner. The data backup methods available today on linux are not effective enough, because while running, most of them block I/Os to guarantee data integrity. We propose and implement Next4 - file system based snapshot feature in Ext4 which creates an instant image of the file system, to provide incremental versions of data, enabling reliable backup and data recovery. In our design, the snapshot feature is implemented by efficiently infusing the copy-on-write strategy in the write-in-place, extent based Ext4 file system, without affecting its basic structure. Each snapshot is an incremental backup of the data within the system. What distinguishes Next4 is the way that the data is backed up, improving both space utilization as well as performance."
https://arxiv.org/abs/2403.06789,2024-03-11,SPLADE-v3: New baselines for SPLADE,"['Carlos Lassance', 'Hervé Déjean', 'Thibault Formal', 'Stéphane Clinchant']","A companion to the release of the latest version of the SPLADE library. We describe changes to the training structure and present our latest series of models -- SPLADE-v3. We compare this new version to BM25, SPLADE++, as well as re-rankers, and showcase its effectiveness via a meta-analysis over more than 40 query sets. SPLADE-v3 further pushes the limit of SPLADE models: it is statistically significantly more effective than both BM25 and SPLADE++, while comparing well to cross-encoder re-rankers. Specifically, it gets more than 40 MRR@10 on the MS MARCO dev set, and improves by 2% the out-of-domain results on the BEIR benchmark."
https://arxiv.org/abs/2403.06788,2024-03-11,Efficient dual-scale generalized Radon-Fourier transform detector family for long time coherent integration,"['Suqi Li', 'Yihan Wang', 'Bailu Wang', 'Giorgio Battistelli', 'Luigi Chisci', 'Guolong Cui']","Long Time Coherent Integration (LTCI) aims to accumulate target energy through long time integration, which is an effective method for the detection of a weak target. However, for a moving target, defocusing can occur due to range migration (RM) and Doppler frequency migration (DFM). To address this issue, RM and DFM corrections are required in order to achieve a well-focused image for the subsequent detection. Since RM and DFM are induced by the same motion parameters, existing approaches such as the generalized Radon-Fourier transform (GRFT) or the keystone transform (KT)-matching filter process (MFP) adopt the same search space for the motion parameters in order to eliminate both effects, thus leading to large redundancy in computation. To this end, this paper first proposes a dual-scale decomposition of the target motion parameters, consisting of well designed coarse and fine motion parameters. Then, utilizing this decomposition, the joint correction of the RM and DFM effects is decoupled into a cascade procedure, first RM correction on the coarse search space and then DFM correction on the fine search spaces. As such, step size of the search space can be tailored to RM and DFM corrections, respectively, thus avoiding large redundant computation effectively. The resulting algorithms are called dual-scale GRFT (DS-GRFT) or dual-scale GRFT (DS-KTMFP) which provide comparable performance while achieving significant improvement in computational efficiency compared to standard GRFT (KT-MFP). Simulation experiments verify their effectiveness and efficiency."
https://arxiv.org/abs/2403.06787,2024-03-11,Bjorken and threshold limit of a space-like structure function in the 2D $U(N)$ Gross-Neveu model,['Yizhuang Liu'],"In this note, we investigate a simple coordinate-space structure function $f_a(z^2m^2,λ)$ in the 2D $U(N)$ Gross-Neveu model to next-to-leading order in the large $N$ expansion. We analytically perform the twist expansion in the Bjorken limit through double Mellin-representations. Hard and non-perturbative scaling functions at leading and next-to-leading powers are naturally generated in their Borel representations. At leading power (LP), the factorization formula is explicitly verified, and the issue of ``scale-dependency'' of perturbative and non-perturbative functions is explained naturally. At NLP, there are three series of non-perturbative functions and the related hard functions. The renormalon cancellation pattern between the hard function at LP and the non-perturbative function at NLP is verified explicitly. We also investigate the threshold expansion of the structure function and its relation to the twist expansion."
https://arxiv.org/abs/2403.06786,2024-03-11,Genetic Learning for Designing Sim-to-Real Data Augmentations,"['Bram Vanherle', 'Nick Michiels', 'Frank Van Reeth']","Data augmentations are useful in closing the sim-to-real domain gap when training on synthetic data. This is because they widen the training data distribution, thus encouraging the model to generalize better to other domains. Many image augmentation techniques exist, parametrized by different settings, such as strength and probability. This leads to a large space of different possible augmentation policies. Some policies work better than others for overcoming the sim-to-real gap for specific datasets, and it is unclear why. This paper presents two different interpretable metrics that can be combined to predict how well a certain augmentation policy will work for a specific sim-to-real setting, focusing on object detection. We validate our metrics by training many models with different augmentation policies and showing a strong correlation with performance on real data. Additionally, we introduce GeneticAugment, a genetic programming method that can leverage these metrics to automatically design an augmentation policy for a specific dataset without needing to train a model."
https://arxiv.org/abs/2403.06785,2024-03-11,An ergodic and isotropic zero-conductance model with arbitrarily strong local connectivity,"['Martin Heida', 'Benedikt Jahnel', 'Anh Duc Vu']",We exhibit a percolating ergodic and isotropic lattice model in all but at least two dimensions that has zero effective conductivity in all spatial directions and for all non-trivial choices of the connectivity parameter. The model is based on the so-called randomly stretched lattice where we additionally elongate layers containing few open edges.
https://arxiv.org/abs/2403.06784,2024-03-11,Uniqueness of the critical points of solutions to two kinds of semilinear elliptic equations in higher dimensional domains,"['Haiyun Deng', 'Jingwen Ji', 'Feida Jiang', 'Jiabin Yin']","In this paper, we provide an affirmative answer to the conjecture A for bounded simple rotationally symmetric domains $Ω\subset \mathbb{R}^n(n\geq 3)$ along $x_n$ axis. Precisely, we use a new simple argument to study the symmetry of positive solutions for two kinds of semilinear elliptic equations. To do this, when $f(\cdot,s)$ is strictly convex with respect to $s$, we show that the nonnegativity of the first eigenvalue of the corresponding linearized operator in somehow symmetric domains is a sufficient condition for the symmetry of $u$. Moreover, we prove the uniqueness of critical points of a positive solution to semilinear elliptic equation $-\triangle u=f(\cdot,u)$ with zero Dirichlet boundary condition for simple rotationally symmetric domains in $\mathbb{R}^n$ by continuity method and a variety of maximum principles."
https://arxiv.org/abs/2403.06783,2024-03-11,A doubly robust estimator for the Mann Whitney Wilcoxon Rank Sum Test when applied for causal inference in observational studies,"['Ruohui Chen', 'Tuo Lin', 'Lin Liu', 'Jinyuan Liu', 'Ruifeng Chen', 'Jingjing Zou', 'Chenyu Liu', 'Loki Natarajan', 'Tang Wang', 'Xinlian Zhang', 'Xin Tu']","The Mann-Whitney-Wilcoxon rank sum test (MWWRST) is a widely used method for comparing two treatment groups in randomized control trials, particularly when dealing with highly skewed data. However, when applied to observational study data, the MWWRST often yields invalid results for causal inference. To address this limitation, Wu et al. (2014) introduced an approach that incorporates inverse probability weighting (IPW) into this rank-based statistics to mitigate confounding effects. Subsequently, Mao (2018), Zhang et al. (2019), and Ai et al. (2020) extended this IPW estimator to develop doubly robust estimators."
https://arxiv.org/abs/2403.06782,2024-03-11,Mass from an extrinsic point of view,"['Alexandre de Sousa', 'Frederico Girão']","We express the $q$-th Gauss--Bonnet--Chern mass of an immersed submanifold of Euclidean space as a linear combination of two terms: the total $(2q)$-th mean curvature and the integral, over the entire manifold, of the inner product between the $(2q+1)$-th mean curvature vector and the position vector of the immersion. As a consequence, we obtain, for each $q$, a geometric inequality that holds whenever the positive mass theorem (for the $q$-th Gauss--Bonnet--Chern mass) holds."
https://arxiv.org/abs/2403.06781,2024-03-11,Weak multiset sequenceability and weak BHR conjecture,['Simone Costa'],"A subset $S$ of a group $(G,+)$ is $t$-weakly sequenceable if there is an ordering $(y_1, \ldots, y_k)$ of its elements such that the partial sums~$s_0, s_1, \ldots, s_k$, given by $s_0 = 0$ and $s_i = \sum_{j=1}^i y_j$ for $1 \leq i \leq k$, satisfy $s_i \neq s_j$ whenever and $1 \leq |i-j|\leq t$. In this paper, we consider the weak sequenceability problem on multisets. In particular, we are able to prove that a multiset $M=[a_1^{λ_1},a_2^{λ_2},\dots,a_n^{λ_n}]$ of non-identity elements of a generic group $G$ is $t$-weakly sequenceable whenever the underlying set $\{a_1,a_2,\dots,a_n\}$ is sufficiently large (with respect to $t$) and the smallest prime divisor $p$ of $|G|$ is larger than $t$."
https://arxiv.org/abs/2403.06780,2024-03-11,Domain-Independent Dynamic Programming and Constraint Programming Approaches for Assembly Line Balancing Problems with Setups,"['Jiachen Zhang', 'J. Christopher Beck']","We propose domain-independent dynamic programming (DIDP) and constraint programming (CP) models to exactly solve type-1 and type-2 assembly line balancing problem with sequence-dependent setup times (SUALBP). The goal is to assign tasks to assembly stations and to sequence these tasks within each station, while satisfying precedence relations specified between a subset of task pairs. Each task has a given processing time and a setup time dependent on the previous task on the station to which the task is assigned. The sum of the processing and setup times of tasks assigned to each station constitute the station time and the maximum station time is called the cycle time. For type-1 SUALBP, the objective is to minimize the number of stations, given a maximum cycle time. For type-2 SUALBP, the objective is to minimize the cycle time, given the number of stations. On a set of diverse SUALBP instances, experimental results show that our approaches significantly outperform the state-of-the-art mixed integer programming models for SUALBP-1. For SUALBP-2, the DIDP model outperforms the state-of-the-art exact approach based on logic-based Benders decomposition. By closing 76 open instances for SUALBP-2, our results demonstrate the promise of DIDP for solving complex planning and scheduling problems."
https://arxiv.org/abs/2403.06779,2024-03-11,From Factor Models to Deep Learning: Machine Learning in Reshaping Empirical Asset Pricing,"['Junyi Ye', 'Bhaskar Goswami', 'Jingyi Gu', 'Ajim Uddin', 'Guiling Wang']","This paper comprehensively reviews the application of machine learning (ML) and AI in finance, specifically in the context of asset pricing. It starts by summarizing the traditional asset pricing models and examining their limitations in capturing the complexities of financial markets. It explores how 1) ML models, including supervised, unsupervised, semi-supervised, and reinforcement learning, provide versatile frameworks to address these complexities, and 2) the incorporation of advanced ML algorithms into traditional financial models enhances return prediction and portfolio optimization. These methods can adapt to changing market dynamics by modeling structural changes and incorporating heterogeneous data sources, such as text and images. In addition, this paper explores challenges in applying ML in asset pricing, addressing the growing demand for explainability in decision-making and mitigating overfitting in complex models. This paper aims to provide insights into novel methodologies showcasing the potential of ML to reshape the future of quantitative finance."
https://arxiv.org/abs/2403.06778,2024-03-11,Topological solitons stabilized by a background gauge field and soliton-anti-soliton asymmetry,"['Yuki Amari', 'Minoru Eto', 'Muneto Nitta']","We study topological lumps supported by the second homotopy group $π_2(S^2) \simeq {\mathbb Z}$ in a gauged $O(3)$ model without any potential term coupled with a (non)dynamical $U(1)$ gauge field. It is known that gauged-lumps are stable with an easy-plane potential term but are unstable to expand if the model has no potential term. In this paper, we find that these gauged lumps without a potential term can be made stable by putting them in a uniform magnetic field, irrespective of whether the gauge field is dynamical or not. In the case of the non-dynamical gauge field, only either of lumps or anti-lumps stably exists depending on the sign of the background magnetic field, and the other is unstable to shrink to be singular. We also construct coaxial multiple lumps whose size and mass exhibit a behaviour of droplets. In the case of the dynamical gauge field, both the lumps and anti-lumps stably exist with different masses; the lighter (heavier) one corresponds to the (un)stable one in the case of the nondynamical gauge field. We find that a lump behaves as a superconducting ring and traps magnetic field in its inside, with the total magnetic field reduced from the background magnetic field."
https://arxiv.org/abs/2403.06777,2024-03-11,Fast classical simulation of quantum circuits via parametric rewriting in the ZX-calculus,"['Matthew Sutcliffe', 'Aleks Kissinger']","The ZX-calculus is an algebraic formalism that allows quantum computations to be simplified via a small number of simple graphical rewrite rules. Recently, it was shown that, when combined with a family of ""sum-over-Cliffords"" techniques, the ZX-calculus provides a powerful tool for classical simulation of quantum circuits. However, for several important classical simulation tasks, such as computing the probabilities associated with many measurement outcomes of a single quantum circuit, this technique results in reductions over many very similar diagrams, where much of the same computational work is repeated. In this paper, we show that the majority of this work can be shared across branches, by developing reduction strategies that can be run parametrically on diagrams with boolean free parameters. As parameters only need to be fixed after the bulk of the simplification work is already done, we show that it is possible to perform the final stage of classical simulation quickly utilising a high degree of GPU parallelism. Using these methods, we demonstrate speedups upwards of 100x for certain classical simulation tasks vs. the non-parametric approach."
https://arxiv.org/abs/2403.06776,2024-03-11,"Born to Run, Programmed to Play: Mapping the Extended Reality Exergames Landscape","['Sukran Karaosmanoglu', 'Sebastian Cmentowski', 'Lennart E. Nacke', 'Frank Steinicke']","Many people struggle to exercise regularly, raising the risk of serious health-related issues. Extended reality (XR) exergames address these hurdles by combining physical exercises with enjoyable, immersive gameplay. While a growing body of research explores XR exergames, no previous review has structured this rapidly expanding research landscape. We conducted a scoping review of the current state of XR exergame research to (i) provide a structured overview, (ii) highlight trends, and (iii) uncover knowledge gaps. After identifying 1318 papers in human-computer interaction and medical databases, we ultimately included 186 papers in our analysis. We provide a quantitative and qualitative summary of XR exergame research, showing current trends and potential future considerations. Finally, we provide a taxonomy of XR exergames to help future design and methodological investigation and reporting."
https://arxiv.org/abs/2403.06775,2024-03-11,FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation,"['Pengchong Qiao', 'Lei Shang', 'Chang Liu', 'Baigui Sun', 'Xiangyang Ji', 'Jie Chen']","Subject-driven generation has garnered significant interest recently due to its ability to personalize text-to-image generation. Typical works focus on learning the new subject's private attributes. However, an important fact has not been taken seriously that a subject is not an isolated new concept but should be a specialization of a certain category in the pre-trained model. This results in the subject failing to comprehensively inherit the attributes in its category, causing poor attribute-related generations. In this paper, motivated by object-oriented programming, we model the subject as a derived class whose base class is its semantic category. This modeling enables the subject to inherit public attributes from its category while learning its private attributes from the user-provided example. Specifically, we propose a plug-and-play method, Subject-Derived regularization (SuDe). It constructs the base-derived class modeling by constraining the subject-driven generated images to semantically belong to the subject's category. Extensive experiments under three baselines and two backbones on various subjects show that our SuDe enables imaginative attribute-related generations while maintaining subject fidelity. Codes will be open sourced soon at FaceChain (https://github.com/modelscope/facechain)."
https://arxiv.org/abs/2403.06773,2024-03-11,Real Nullstellensatz for 2-step nilpotent Lie algebras,"['Philipp Schmitt', 'Matthias Schötz']","We prove a noncommutative real Nullstellensatz for 2-step nilpotent Lie algebras that extends the classical, commutative real Nullstellensatz as follows: Instead of the real polynomial algebra $\mathbb R[x_1, \dots, x_d]$ we consider the universal enveloping *-algebra of a 2-step nilpotent real Lie algebra (i.e. the universal enveloping algebra of its complexification with the canonical *-involution). Evaluation at points of $\mathbb R^d$ is then generalized to evaluation through integrable *-representations, which in this case are equivalent to filtered *-algebra morphisms from the universal enveloping *-algebra to a Weyl algebra. Our Nullstellensatz characterizes the common kernels of a set of such *-algebra morphisms as the real ideals of the universal enveloping *-algebra."
https://arxiv.org/abs/2403.06772,2024-03-11,Local Intuitionistic Modal Logics and Their Calculi,"['Philippe Balbiani', 'Han Gao', 'Çiğdem Gencer', 'Nicola Olivetti']","We investigate intuitionistic modal logics with locally interpreted $\square$ and $\lozenge$. The basic logic LIK is stronger than constructive modal logic WK and incomparable with intuitionistic modal logic IK. We propose an axiomatization of LIK and some of its extensions. We propose bi-nested calculi for LIK and these extensions, thus providing both a decision procedure and a procedure of finite countermodel extraction."
https://arxiv.org/abs/2403.06771,2024-03-11,Redefining Event Types and Group Evolution in Temporal Data,"['Andrea Failla', 'Rémy Cazabet', 'Giulio Rossetti', 'Salvatore Citraro']","Groups -- such as clusters of points or communities of nodes -- are fundamental when addressing various data mining tasks. In temporal data, the predominant approach for characterizing group evolution has been through the identification of ``events"". However, the events usually described in the literature, e.g., shrinks/growths, splits/merges, are often arbitrarily defined, creating a gap between such theoretical/predefined types and real-data group observations. Moving beyond existing taxonomies, we think of events as ``archetypes"" characterized by a unique combination of quantitative dimensions that we call ``facets"". Group dynamics are defined by their position within the facet space, where archetypal events occupy extremities. Thus, rather than enforcing strict event types, our approach can allow for hybrid descriptions of dynamics involving group proximity to multiple archetypes. We apply our framework to evolving groups from several face-to-face interaction datasets, showing it enables richer, more reliable characterization of group dynamics with respect to state-of-the-art methods, especially when the groups are subject to complex relationships. Our approach also offers intuitive solutions to common tasks related to dynamic group analysis, such as choosing an appropriate aggregation scale, quantifying partition stability, and evaluating event quality."
https://arxiv.org/abs/2403.06770,2024-03-11,Estimates on the convergence of expansions at finite baryon chemical potentials,"['Rui Wen', 'Shi Yin', 'Wei-jie Fu']","Convergence of three different expansion schemes at finite baryon chemical potentials, including the conventional Taylor expansion, the Padé approximants, and the $T'$ expansion proposed recently in lattice QCD simulations, have been investigated in a low energy effective theory within the fRG approach. It is found that the $T'$ expansion or the Padé approximants would hardly improve the convergence of expansion in comparison to the conventional Taylor expansion, within the expansion orders considered in this work. Furthermore, we find that the consistent regions of the three different expansions are in agreement with the convergence radius of the Lee-Yang edge singularities."
https://arxiv.org/abs/2403.06769,2024-03-11,Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning,"['Tong Zhang', 'Chen Huang', 'Yang Deng', 'Hongru Liang', 'Jia Liu', 'Zujie Wen', 'Wenqiang Lei', 'Tat-Seng Chua']","We investigate non-collaborative dialogue agents that must engage in tailored strategic planning for diverse users to secure a favorable agreement. This poses challenges for existing dialogue agents due to two main reasons: their inability to integrate user-specific characteristics into their strategic planning and their training paradigm's failure to produce strategic planners that can generalize to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users."
https://arxiv.org/abs/2403.06768,2024-03-11,XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage,"['Jae-Jun Lee', 'Sung Whan Yoon']","Meta-learning, which pursues an effective initialization model, has emerged as a promising approach to handling unseen tasks. However, a limitation remains to be evident when a meta-learner tries to encompass a wide range of task distribution, e.g., learning across distinctive datasets or domains. Recently, a group of works has attempted to employ multiple model initializations to cover widely-ranging tasks, but they are limited in adaptively expanding initializations. We introduce XB-MAML, which learns expandable basis parameters, where they are linearly combined to form an effective initialization to a given task. XB-MAML observes the discrepancy between the vector space spanned by the basis and fine-tuned parameters to decide whether to expand the basis. Our method surpasses the existing works in the multi-domain meta-learning benchmarks and opens up new chances of meta-learning for obtaining the diverse inductive bias that can be combined to stretch toward the effective initialization for diverse unseen tasks."
https://arxiv.org/abs/2403.06767,2024-03-11,Continuity and equivariant dimension,"['Alexandru Chirvasitu', 'Benjamin Passer']","We study the local-triviality dimensions of actions on $C^*$-algebras, which are invariants developed for noncommutative Borsuk-Ulam theory. While finiteness of the local-triviality dimensions is known to guarantee freeness of an action, we show that free actions need not have finite weak local-triviality dimension. Moreover, the local-triviality dimensions of a continuous field may be greater than those of its individual fibers, and the dimensions may fail to vary continuously across the fibers. However, in certain circumstances upper semicontinuity of the weak local-triviality dimension is guaranteed. We examine these results and counterexamples with a focus on noncommutative tori and noncommutative spheres, both in terms of computation and theory."
https://arxiv.org/abs/2403.06766,2024-03-11,Determination of the number of $ψ(3686)$ events taken at BESIII,"[' BESIII Collaboration', 'M. Ablikim', 'M. N. Achasov', 'P. Adlarson', 'O. Afedulidis', 'X. C. Ai', 'R. Aliberti', 'A. Amoroso', 'Q. An', 'Y. Bai', 'O. Bakina', 'I. Balossino', 'Y. Ban', 'H. -R. Bao', 'V. Batozskaya', 'K. Begzsuren', 'N. Berger', 'M. Berlowski', 'M. Bertani', 'D. Bettoni', 'F. Bianchi', 'E. Bianco', 'A. Bortone', 'I. Boyko', 'R. A. Briere']","The number of $ψ(3686)$ events collected by the BESIII detector during the 2021 run period is determined to be $(2259.3\pm 11.1)\times 10^6$ by counting inclusive $ψ(3686)$ hadronic events. The uncertainty is systematic and the statistical uncertainty is negligible. Meanwhile, the numbers of $ψ(3686)$ events collected during the 2009 and 2012 run periods are updated to be $(107.7\pm0.6)\times 10^6$ and $(345.4\pm 2.6)\times 10^6$, respectively. Both numbers are consistent with the previous measurements within one standard deviation. The total number of $ψ(3686)$ events in the three data samples is $(2712.4\pm14.3)\times10^6$."
https://arxiv.org/abs/2403.06765,2024-03-11,ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model,"['Zhiwei Liu', 'Boyang Liu', 'Paul Thompson', 'Kailai Yang', 'Raghav Jain', 'Sophia Ananiadou']","The internet has brought both benefits and harms to society. A prime example of the latter is misinformation, including conspiracy theories, which flood the web. Recent advances in natural language processing, particularly the emergence of large language models (LLMs), have improved the prospects of accurate misinformation detection. However, most LLM-based approaches to conspiracy theory detection focus only on binary classification and fail to account for the important relationship between misinformation and affective features (i.e., sentiment and emotions). Driven by a comprehensive analysis of conspiracy text that reveals its distinctive affective features, we propose ConspEmoLLM, the first open-source LLM that integrates affective information and is able to perform diverse tasks relating to conspiracy theories. These tasks include not only conspiracy theory detection, but also classification of theory type and detection of related discussion (e.g., opinions towards theories). ConspEmoLLM is fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset, which includes five tasks to support LLM instruction tuning and evaluation. We demonstrate that when applied to these tasks, ConspEmoLLM largely outperforms several open-source general domain LLMs and ChatGPT, as well as an LLM that has been fine-tuned using ConDID, but which does not use affective features. This project will be released on https://github.com/lzw108/ConspEmoLLM/."
https://arxiv.org/abs/2403.06764,2024-03-11,An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models,"['Liang Chen', 'Haozhe Zhao', 'Tianyu Liu', 'Shuai Bai', 'Junyang Lin', 'Chang Zhou', 'Baobao Chang']","In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve a lower budget than that of a 7B-parameter model, while still maintaining superior performance. We believe FastV has practical values for deployment of LVLMs in edge devices and commercial models. Code is released at https://github.com/pkunlp-icler/FastV."
https://arxiv.org/abs/2403.06763,2024-03-11,Coupled geophysical and thermal constraints link between Mars basal molten layer and the planet viscosity profile,"['Alex Guinard', 'Agnes Fienga', 'Anthony Memin', 'Clement Ganino']","Computing the tidal deformations of Mars, we explored various Mars internal structures by examining profiles that include or exclude a basal molten layer within the mantle and a solid inner core. By assessing their compatibility with a diverse set of geophysical observations we show that despite the very short periods of excitation, tidal deformation is very efficient to constraint the Mars interior. We calculated densities and thicknesses for Martian lithosphere, mantle, {core-mantle boundary} layer and core and {found them} coherent with preexisting results from other methods. We also estimated new viscosities for these layers. We demonstrated that the geodetic record associated with thermal constraint is very sensitive to the {presence} of a basal molten layer in deep Martian mantle, and less sensitive to the solid core. Our results also indicate that the existence of the basal molten layer necessarily comes together with an inversion of viscosity between the lithosphere and the mantle. In this case, we could attribute this reverse viscosity contrast to the poor hydration state of Martian mantle and we underlined that this result prevents a strict Earth-like plate tectonics on Mars. The existence of the basal molten layer is also associated with a non-inversion of viscosity between the core-mantle boundary layer and the liquid core. Finally, in our results, a basal molten layer is incompatible with the existence of a solid inner core. Efforts to detect basal molten layer are then of prime importance to decipher the Martian interior. {Inversely, viscosity profiles appear to be very good tools for probing the existence of such molten layer at the base of the Mars mantle."
https://arxiv.org/abs/2403.06762,2024-03-11,A dark-field setup for the measurement of light-by-light scattering with high-intensity lasers,"['Fabian Schütze', 'Leonard Doyle', 'Jörg Schreiber', 'Matt Zepf', 'Felix Karbstein']","We put forward a concrete experimental setup allowing to measure light-by-light scattering in the collision of two optical high-intensity laser beams at state-of-the-art high-field facilities operating petawatt class laser systems. Our setup uses the same focusing optics for both laser beams to be collided and employs a dark-field approach for the detection of the single-photon-level nonlinear quantum vacuum response in the presence of a large background. Based on an advanced modeling of the colliding laser fields, we in particular provide reliable estimates for the prospective numbers of signal photons scattered into the dark-field for various laser polarizations."
https://arxiv.org/abs/2403.06761,2024-03-11,Magnetic billiards and the Hofer-Zehnder capacity of disk tangent bundles of lens spaces,"['Johanna Bimmermann', 'Levin Maier']","We compute the Hofer-Zehnder capacity of disk tangent bundles of certain lens spaces with respect to the round metric. Interestingly we find that the Hofer-Zehnder capacity does not see the covering, i.e. the capacity of the disk tangent bundle of the lens space coincides with the capacity of the disk tangent bundle of the 3-sphere covering it. In particular, this gives a first example, where Gromov width and Hofer-Zehnder capacity of a disk tangent bundle disagree. Techniques we use include for the lower bound magnetic billiards and for the upper bound Gromov-Witten invariants."
https://arxiv.org/abs/2403.06760,2024-03-13,Performance of SK-Gd's Upgraded Real-time Supernova Monitoring System,"['Y. Kashiwagi', 'K. Abe', 'C. Bronner', 'Y. Hayato', 'K. Hiraide', 'K. Hosokawa', 'K. Ieki', 'M. Ikeda', 'J. Kameda', 'Y. Kanemura', 'R. Kaneshima', 'Y. Kataoka', 'S. Miki', 'S. Mine', 'M. Miura', 'S. Moriyama', 'Y. Nakano', 'M. Nakahata', 'S. Nakayama', 'Y. Noguchi', 'K. Sato', 'H. Sekiya', 'H. Shiba', 'K. Shimizu', 'M. Shiozawa']","Among multi-messenger observations of the next galactic core-collapse supernova, Super-Kamiokande (SK) plays a critical role in detecting the emitted supernova neutrinos, determining the direction to the supernova (SN), and notifying the astronomical community of these observations in advance of the optical signal. On 2022, SK has increased the gadolinium dissolved in its water target (SK-Gd) and has achieved a Gd concentration of 0.033%, resulting in enhanced neutron detection capability, which in turn enables more accurate determination of the supernova direction. Accordingly, SK-Gd's real-time supernova monitoring system (Abe te al. 2016b) has been upgraded. SK_SN Notice, a warning system that works together with this monitoring system, was released on December 13, 2021, and is available through GCN Notices (Barthelmy et al. 2000). When the monitoring system detects an SN-like burst of events, SK_SN Notice will automatically distribute an alarm with the reconstructed direction to the supernova candidate within a few minutes. In this paper, we present a systematic study of SK-Gd's response to a simulated galactic SN. Assuming a supernova situated at 10 kpc, neutrino fluxes from six supernova models are used to characterize SK-Gd's pointing accuracy using the same tools as the online monitoring system. The pointing accuracy is found to vary from 3-7$^\circ$ depending on the models. However, if the supernova is closer than 10 kpc, SK_SN Notice can issue an alarm with three-degree accuracy, which will benefit follow-up observations by optical telescopes with large fields of view."
https://arxiv.org/abs/2403.06759,2024-03-11,Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation,"['Theodore Barfoot', 'Luis Garcia-Peraza-Herrera', 'Ben Glocker', 'Tom Vercauteren']","Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS"
https://arxiv.org/abs/2403.06758,2024-03-11,EarthLoc: Astronaut Photography Localization by Indexing Earth from Space,"['Gabriele Berton', 'Alex Stoken', 'Barbara Caputo', 'Carlo Masone']","Astronaut photography, spanning six decades of human spaceflight, presents a unique Earth observations dataset with immense value for both scientific research and disaster response. Despite its significance, accurately localizing the geographical extent of these images, crucial for effective utilization, poses substantial challenges. Current manual localization efforts are time-consuming, motivating the need for automated solutions. We propose a novel approach - leveraging image retrieval - to address this challenge efficiently. We introduce innovative training techniques, including Year-Wise Data Augmentation and a Neutral-Aware Multi-Similarity Loss, which contribute to the development of a high-performance model, EarthLoc. We develop six evaluation datasets and perform a comprehensive benchmark comparing EarthLoc to existing methods, showcasing its superior efficiency and accuracy. Our approach marks a significant advancement in automating the localization of astronaut photography, which will help bridge a critical gap in Earth observations data. Code and datasets are available at https://github.com/gmberton/EarthLoc"
https://arxiv.org/abs/2403.06757,2024-03-13,Koopman Ensembles for Probabilistic Time Series Forecasting,"['Anthony Frion', 'Lucas Drumetz', 'Guillaume Tochon', 'Mauro Dalla Mura', 'Albdeldjalil Aïssa El Bey']","In the context of an increasing popularity of data-driven models to represent dynamical systems, many machine learning-based implementations of the Koopman operator have recently been proposed. However, the vast majority of those works are limited to deterministic predictions, while the knowledge of uncertainty is critical in fields like meteorology and climatology. In this work, we investigate the training of ensembles of models to produce stochastic outputs. We show through experiments on real remote sensing image time series that ensembles of independently trained models are highly overconfident and that using a training criterion that explicitly encourages the members to produce predictions with high inter-model variances greatly improves the uncertainty quantification of the ensembles."
https://arxiv.org/abs/2403.06756,2024-03-11,One-Bit Target Detection in Collocated MIMO Radar with Colored Background Noise,"['Yu-Hang Xiao', 'David Ramírez', 'Lei Huang', 'Xiao Peng Li', 'Hing Cheung So']","One-bit sampling has emerged as a promising technique in multiple-input multiple-output (MIMO) radar systems due to its ability to significantly reduce data volume and processing requirements. Nevertheless, current detection methods have not adequately addressed the impact of colored noise, which is frequently encountered in real scenarios. In this paper, we present a novel detection method that accounts for colored noise in MIMO radar systems. Specifically, we derive Rao's test by computing the derivative of the likelihood function with respect to the target reflectivity parameter and the Fisher information matrix, resulting in a detector that takes the form of a weighted matched filter. To ensure the constant false alarm rate (CFAR) property, we also consider noise covariance uncertainty and examine its effect on the probability of false alarm. The detection probability is also studied analytically. Simulation results demonstrate that the proposed detector provides considerable performance gains in the presence of colored noise."
https://arxiv.org/abs/2403.06755,2024-03-11,SMC-Last Extracted Photometry,"['T. A. Kuchar', 'G. C. Sloan', 'D. R. Mizuno', 'Kathleen E. Kraemer', 'M. L. Boyer', 'Martin A. T. Groenewegen', 'O. C. Jones', 'F. Kemper', 'Iain McDonald', 'Joana M. Oliveira', 'Marta Sewiło', 'Sundar Srinivasan', 'Jacco Th. van Loon', 'Albert Zijlstra']","We present point-source photometry from the Spitzer Space Telescope's final survey of the Small Magellanic Cloud (SMC). We mapped 30 square degrees in two epochs in 2017, with the second extending to early 2018 at 3.6 and 4.5 microns using the Infrared Array Camera. This survey duplicates the footprint from the SAGE-SMC program in 2008. Together, these surveys cover a nearly 10 yr temporal baseline in the SMC. We performed aperture photometry on the mosaicked maps produced from the new data. We did not use any prior catalogs as inputs for the extractor in order to be sensitive to any moving objects (e.g., foreground brown dwarfs) and other transient phenomena (e.g., cataclysmic variables or FU Ori-type eruptions). We produced a point-source catalog with high-confidence sources for each epoch as well as combined-epoch catalog. For each epoch and the combined-epoch data, we also produced a more complete archive with lower-confidence sources. All of these data products will be available to the community at the Infrared Science Archive."
https://arxiv.org/abs/2403.06754,2024-03-11,ALaRM: Align Language Models via Hierarchical Rewards Modeling,"['Yuhang Lai', 'Siyuan Wang', 'Shujun Liu', 'Xuanjing Huang', 'Zhongyu Wei']","We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment. We release our code at https://ALaRM-fdu.github.io."
https://arxiv.org/abs/2403.06753,2024-03-11,Complementing cell taxonomies with a multicellular functional analysis of tissues,"['Ricardo Omar Ramirez Flores', 'Philipp Sven Lars Schäfer', 'Leonie Küchenhoff', 'Julio Saez-Rodriguez']","The application of single-cell molecular profiling coupled with spatial technologies has enabled charting cellular heterogeneity in reference tissues and in disease. This new wave of molecular data has highlighted the expected diversity of single-cell dynamics upon shared external queues and spatial organizations. However, little is known about the relationship between single cell heterogeneity and the emergence and maintenance of robust multicellular processes in developed tissues and its role in (patho)physiology. Here, we present emerging computational modeling strategies that use increasingly available large-scale cross-condition single cell and spatial datasets, to study multicellular organization in tissues and complement cell taxonomies. This perspective should enable us to better understand how cells within tissues collectively process information and adapt synchronized responses in disease contexts and to bridge the gap between structural changes and functions in tissues."
https://arxiv.org/abs/2403.06752,2024-03-13,"The $K_{1,2}$-structure-connectivity of graphs","['Xiao Zhao', 'Haojie Zheng', 'Hengzhe Li']","In this paper, we mainly investigate $K_{1,2}$-structure-connectivity for any connected graph. Let $G$ be a connected graph with $n$ vertices, we show that $κ(G; K_{1,2})$ is well-defined if $diam(G)\geq 4$, or $n\equiv 1\pmod 3$, or $G\notin \{C_{5},K_{n}\}$ when $n\equiv 2\pmod 3$, or there exist three vertices $u,v,w$ such that $N_{G}(u)\cap (N_{G}(v,w)\cup\{v,w\})=\emptyset$ when $n\equiv 0\pmod 3$. Furthermore, if $G$ has $K_{1,2}$-structure-cut, we prove $κ(G)/3\leqκ(G; K_{1,2})\leqκ(G)$."
https://arxiv.org/abs/2403.06751,2024-03-11,Sharp restricted weak-type estimates for sparse operators,"['Irina Holmes Fay', 'Guillermo Rey', 'Kristina Ana Škreb']",We find the exact Bellman function associated to the level-sets of sparse operators acting on characteristic functions.
https://arxiv.org/abs/2403.06750,2024-03-11,Generalising Multi-Agent Cooperation through Task-Agnostic Communication,"['Dulhan Jayalath', 'Steven Morad', 'Amanda Prorok']","Existing communication methods for multi-agent reinforcement learning (MARL) in cooperative multi-robot problems are almost exclusively task-specific, training new communication strategies for each unique task. We address this inefficiency by introducing a communication strategy applicable to any task within a given environment. We pre-train the communication strategy without task-specific reward guidance in a self-supervised manner using a set autoencoder. Our objective is to learn a fixed-size latent Markov state from a variable number of agent observations. Under mild assumptions, we prove that policies using our latent representations are guaranteed to converge, and upper bound the value error introduced by our Markov state approximation. Our method enables seamless adaptation to novel tasks without fine-tuning the communication strategy, gracefully supports scaling to more agents than present during training, and detects out-of-distribution events in an environment. Empirical results on diverse MARL scenarios validate the effectiveness of our approach, surpassing task-specific communication strategies in unseen tasks. Our implementation of this work is available at https://github.com/proroklab/task-agnostic-comms."
https://arxiv.org/abs/2403.06749,2024-03-11,"Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, Evaluation Strategies, and Future Challenges","['Alessandro Berti', 'Humam Kourani', 'Hannes Hafke', 'Chiao-Yun Li', 'Daniel Schuster']","Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results. However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks. This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks? The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms."
https://arxiv.org/abs/2403.06748,2024-03-11,Shortcut Learning in Medical Image Segmentation,"['Manxi Lin', 'Nina Weng', 'Kamil Mikolaj', 'Zahra Bashir', 'Morten Bo Søndergaard Svendsen', 'Martin Tolsgaard', 'Anders Nymark Christensen', 'Aasa Feragen']","Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights and methodologies for evaluating and overcoming this pervasive challenge and call for attention in the community for shortcuts in segmentation."
https://arxiv.org/abs/2403.06747,2024-03-12,MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation,"['Wenhao Wu', 'Jialiang Zhou', 'Ailong He', 'Shuguang Han', 'Jufeng Chen', 'Bo Zheng']","Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSNet) to split user history sequence regarding to the volume of stock for each product, and adopt differentiated modeling approaches for different sequences. As for the limited-stock products, a meta-learning approach is applied to address the problem of inconvergence, which is achieved by designing meta scaling and shifting networks with ID and side information. In addition, traditional approach can hardly update item embedding once the product is consumed. Thereby, we propose an auxiliary loss that makes the parameters updatable even when the product is no longer in distribution. To the best of our knowledge, this is the first solution addressing the recommendation of limited-stock product. Experimental results on the production dataset and online A/B testing demonstrate the effectiveness of our proposed method."
https://arxiv.org/abs/2403.06746,2024-03-11,Integration of Physics-Derived Memristor Models with Machine Learning Frameworks,"['Zhenming Yu', 'Stephan Menzel', 'John Paul Strachan', 'Emre Neftci']","Simulation frameworks such MemTorch, DNN+NeuroSim, and aihwkit are commonly used to facilitate the end-to-end co-design of memristive machine learning (ML) accelerators. These simulators can take device nonidealities into account and are integrated with modern ML frameworks. However, memristors in these simulators are modeled with either lookup tables or simple analytic models with basic nonlinearities. These simple models are unable to capture certain performance-critical aspects of device nonidealities. For example, they ignore the physical cause of switching, which induces errors in switching timings and thus incorrect estimations of conductance states. This work aims at bringing physical dynamics into consideration to model nonidealities while being compatible with GPU accelerators. We focus on Valence Change Memory (VCM) cells, where the switching nonlinearity and SET/RESET asymmetry relate tightly with the thermal resistance, ion mobility, Schottky barrier height, parasitic resistance, and other effects. The resulting dynamics require solving an ODE that captures changes in oxygen vacancies. We modified a physics-derived SPICE-level VCM model, integrated it with the aihwkit simulator and tested the performance with the MNIST dataset. Results show that noise that disrupts the SET/RESET matching affects network performance the most. This work serves as a tool for evaluating how physical dynamics in memristive devices affect neural network accuracy and can be used to guide the development of future integrated devices."
https://arxiv.org/abs/2403.06745,2024-03-11,ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation,"['Shaojie Dai', 'Xin Liu', 'Ping Luo', 'Yue Yu']","Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning. However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation. For this issue, this paper introduces an \textbf{\underline{A}}uto-\textbf{\underline{C}}onstriction \textbf{\underline{T}}urning mechanism for \textbf{\underline{M}}ultilingual \textbf{\underline{N}}eural \textbf{\underline{M}}achine \textbf{\underline{T}}ranslation (\model), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods. In this method, \model automatically constructs a constrained template in the target side by adding trigger tokens ahead of the ground truth. Furthermore, trigger tokens can be arranged and combined freely to represent different task semantics, and they can be iteratively updated to maximize the label likelihood. Experiments are performed on WMT test sets with multiple metrics, and the experimental results demonstrate that \model achieves substantially improved performance across multiple translation directions and reduce the off-target phenomena in the translation."
https://arxiv.org/abs/2403.06744,2024-03-11,Design and Performance Comparison of FuzzyPID and Non-linear Model Predictive Controller for 4-Wheel Omni-drive Robot,['Love Panta'],"Trajectory tracking for an Omni-drive robot presents a challenging task that demands an efficient controller design. To address the limitations of manual tuning, we introduce a self-optimizing controller named fuzzyPID, leveraging the analysis of responses from various dynamic and static systems. The rule-based controller design is implemented using Matlab/Simulink, and trajectory tracking simulations are conducted within the CoppeliaSim environment. Similarly, a non-linear model predictive controller(NMPC) is proposed to compare tracking performance with fuzzyPID. We also assess the impact of tunable parameters of NMPC on its tracking accuracy. Simulation results validate the precision and effectiveness of NMPC over fuzzyPID controller while trading computational complexity."
https://arxiv.org/abs/2403.06743,2024-03-11,PolyominoIdeals: a package for Macaulay2 to work with the inner $2$-minor ideals of collections of cells,"['Cisto Carmelo', 'Rizwan Jahangir', 'Francesco Navarra']","Let $\mathcal{P}$ be a collection of cells and $I_\mathcal{P}$ be the associated ideal of inner $2$-minors as defined by A. A. Qureshi in 2012. In this paper, we provide a description of the package $\texttt{PolyominoIdeals}$ for the computer algebra software $\texttt{Macaulay2}$. More precisely, this package provides some functions that allow to define the ideal $I_{\mathcal{P}}$ in $\texttt{Macaulay2}$ and to compute its algebraic invariants or verifying its algebraic properties. We explain the usage of these functions and also give some examples."
https://arxiv.org/abs/2403.06742,2024-03-11,Search for charged-lepton-flavour violating $μτqt$ interactions in top-quark production and decay in $pp$ collisions at $\sqrt{s}= 13$ TeV with the ATLAS detector at the LHC,[' ATLAS Collaboration'],"A search for charged-lepton-flavour violating $μτqt$ ($q=u,c$) interactions is presented, considering both top-quark production and decay. The data analysed correspond to 140 $\textrm{fb}^{-1}$ of proton-proton collisions at a centre-of-mass energy of $\sqrt{s}= $13 TeV recorded with the ATLAS detector at the Large Hadron Collider. The analysis targets events containing two muons with the same electric charge, a hadronically decaying $τ$-lepton and at least one jet, with exactly one $b$-tagged jet, produced by a $μτqt$ interaction. Agreement with the Standard Model expectation within $1.6σ$ is observed, and limits are set at the 95% CL on the charged-lepton-flavour violation branching ratio of $\mathcal{B}(t \to μτq) < 8.7 \times 10^{-7}$. An Effective Field Theory interpretation is performed yielding 95% CL limits on Wilson coefficients, dependent on the flavour of the associated light quark and the Lorentz structure of the coupling. These range from $|c_{\mathsf{lequ}}^{3(2313)}| / Λ^{2} < 0.10\textrm{ TeV}^{-2}$ for $μτut$ to $|c_{\mathsf{ lequ}}^{1(2323)}| / Λ^{2} < 1.8\textrm{ TeV}^{-2}$ for $μτct$. An additional interpretation is performed for scalar leptoquark production inducing charged lepton flavour violation, with fixed inter-generational couplings. Upper limits on leptoquark coupling strengths are set at the 95% CL, ranging from $λ^{\textrm{LQ}} = $1.3 to $λ^{\textrm{LQ}} = $3.7 for leptoquark masses between 0.5 and 2.0 TeV."
https://arxiv.org/abs/2403.06741,2024-03-11,Distribution-Aware Data Expansion with Diffusion Models,"['Haowei Zhu', 'Ling Yang', 'Jun-Hai Yong', 'Wentao Zhang', 'Bin Wang']","The scale and quality of a dataset significantly impact the performance of deep models. However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor. To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models. Current data expansion methods encompass image transformation-based and synthesis-based methods. The transformation-based methods introduce only local variations, resulting in poor diversity. While image synthesis-based methods can create entirely new content, significantly enhancing informativeness. However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples. In this paper, we propose DistDiff, an effective data expansion framework based on the distribution-aware diffusion model. DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance. We demonstrate its ability to generate distribution-consistent samples, achieving substantial improvements in data expansion tasks. Specifically, without additional training, DistDiff achieves a 30.7% improvement in accuracy across six image datasets compared to the model trained on original datasets and a 9.8% improvement compared to the state-of-the-art diffusion-based method. Our code is available at https://github.com/haoweiz23/DistDiff"
https://arxiv.org/abs/2403.06740,2024-03-11,"A Two-Field-Scan Harmonic Hall Voltage Analysis For Fast, Accurate Quantification Of Spin-Orbit Torques In Magnetic Heterostructures","['Xin Lin', 'Lijun Zhu']","The efficiencies of the spin-orbit torques (SOTs) play a key role in the determination of the power consumption, integration density, and endurance of SOT-driven devices. Accurate and time-efficient determination of the SOT efficiencies is of great importance not only for evaluating the practical potential of SOT devices but also for developing new mechanisms for enhancing the SOT efficiencies. Here, we develop a ""two-field-scan"" harmonic Hall voltage (HHV) analysis that collects the second HHV as a function of a swept in-plane magnetic field at 45° and 0° relative to the excitation current. We demonstrate that this two-field-scan analysis is as accurate as the well-established but time-consuming angle-scan HHV analysis even in the presence of considerable thermoelectric effects but takes more than a factor of 7 less measurement time. We also show that the 3-parameter fit of the HHV data from a single field scan at 0°, which is commonly employed in the literature, is not reliable because the employment of too many free parameters in the fitting of the very slowly varying HHV signal allows unrealistic pseudo-solution and thus erroneous conclusion about the SOT efficiencies."
https://arxiv.org/abs/2403.06739,2024-03-11,Energy loss of a heavy fermion in a collisional QED plasma,"['Yun Guo', 'Luhua Qiu', 'Ruizhe Zhao', 'Michael Strickland']","We compute the energy loss of heavy fermions moving in a plasma, taking into account the modification of the photon collective modes induced by collisions using a Bhatnagar-Gross-Krook collisional kernel. We include contributions from both hard and soft scatterings of the heavy fermion using a collisionally modified hard-thermal-loop resummed propagator. Using this method, one does not need to introduce a separation scale between hard- and soft-momentum exchanges. To place our calculation in context, we review other theoretical approaches to computing the collisional energy loss of fermions and discuss the systematics and results obtained in each approach compared to using a resummed propagator for both hard and soft momentum exchanges. Our final results indicate that self-consistently including the effect of collisions in the self-energies of the resummed propagator results in an increased energy loss compared to using collisionless hard-thermal-loop propagators. The effect becomes larger as the magnitude of the coupling constant and the velocity of the fermion increase."
https://arxiv.org/abs/2403.06738,2024-03-11,V3D: Video Diffusion Models are Effective 3D Generators,"['Zilong Chen', 'Yikai Wang', 'Feng Wang', 'Zhengyi Wang', 'Huaping Liu']","Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at https://github.com/heheyas/V3D"
https://arxiv.org/abs/2403.06737,2024-03-11,Post-Training Attribute Unlearning in Recommender Systems,"['Chaochao Chen', 'Yizhao Zhang', 'Yuyuan Li', 'Dan Meng', 'Jun Wang', 'Xiaoli Zheng', 'Jianwei Yin']","With the growing privacy concerns in recommender systems, recommendation unlearning is getting increasing attention. Existing studies predominantly use training data, i.e., model inputs, as unlearning target. However, attackers can extract private information from the model even if it has not been explicitly encountered during training. We name this unseen information as \textit{attribute} and treat it as unlearning target. To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to make target attributes indistinguishable. In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed. To address the PoT-AU problem in recommender systems, we propose a two-component loss function. The first component is distinguishability loss, where we design a distribution-based measurement to make attribute labels indistinguishable from attackers. We further extend this measurement to handle multi-class attribute cases with efficient computational overhead. The second component is regularization loss, where we explore a function-space measurement that effectively maintains recommendation performance compared to parameter-space regularization. We use stochastic gradient descent algorithm to optimize our proposed loss. Extensive experiments on four real-world datasets demonstrate the effectiveness of our proposed methods."
https://arxiv.org/abs/2403.06736,2024-03-11,Towards $21$-cm intensity mapping at $z=2.28$ with uGMRT using the tapered gridded estimator -- IV. Wideband analysis,"['Khandakar Md Asif Elahi', 'Somnath Bharadwaj', 'Srijita Pal', 'Abhik Ghosh', 'Sk. Saiyad Ali', 'Samir Choudhuri', 'Arnab Chakraborty', 'Abhirup Datta', 'Nirupam Roy', 'Madhurima Choudhury', 'Prasun Dutta']","We present a Wideband Tapered Gridded Estimator (TGE), which incorporates baseline migration and variation of the primary beam pattern for neutral hydrogen (${\rm H\hspace{0.5mm}}{\scriptsize {\rm I}}$) 21-cm intensity mapping (IM) with large frequency bandwidth radio-interferometric observations. Here we have analysed $394-494 \, {\rm MHz}$ $(z = 1.9 - 2.6)$ uGMRT data to estimate the Multi-frequency Angular Power Spectrum (MAPS) $C_\ell(Δν)$ from which we have removed the foregrounds using the polynomial fitting (PF) and Gaussian Process Regression (GPR) methods developed in our earlier work. Using the residual $C_\ell(Δν)$ to estimate the mean squared 21-cm brightness temperature fluctuation $Δ^2(k)$, we find that this is consistent with $0 \pm 2 σ$ in several $k$ bins. The resulting $2σ$ upper limit $Δ^2(k) < (4.68)^2 \, \rm{mK^2}$ at $k=0.219\,\rm{Mpc^{-1}}$ is nearly $15$ times tighter than earlier limits obtained from a smaller bandwidth ($24.4 \, {\rm MHz}$) of the same data. The $2σ$ upper limit $[Ω_{\rm H\hspace{0.5mm}{\scriptsize {\rm I}}} b_{\rm H\hspace{0.5mm}{\scriptsize {\rm I}}}] < 1.01 \times 10^{-2}$ is within an order of magnitude of the value expected from independent estimates of the ${\rm H\hspace{0.5mm}}{\scriptsize {\rm I}}$ mass density $Ω_{\rm H\hspace{0.5mm}{\scriptsize {\rm I}}}$ and the ${\rm H\hspace{0.5mm}}{\scriptsize {\rm I}}$ bias $b_{\rm H\hspace{0.5mm}{\scriptsize {\rm I}}}$. The techniques used here can be applied to other telescopes and frequencies, including $\sim 150 \, {\rm MHz}$ Epoch of Reionization observations."
https://arxiv.org/abs/2403.06735,2024-03-11,Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback,"['Adarsh N L', 'Arun P V', 'Aravindh N L']","Research on generative models to produce human-aligned / human-preferred outputs has seen significant recent contributions. Between text and image-generative models, we narrowed our focus to text-based generative models, particularly to produce captions for images that align with human preferences. In this research, we explored a potential method to amplify the performance of the Deep Neural Network Model to generate captions that are preferred by humans. This was achieved by integrating Supervised Learning and Reinforcement Learning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel loss function that is capable of optimizing the model based on human feedback is introduced. In this paper, we provide a concise sketch of our approach and results, hoping to contribute to the ongoing advances in the field of human-aligned generative AI models."
https://arxiv.org/abs/2403.06734,2024-03-11,Real-Time Multimodal Cognitive Assistant for Emergency Medical Services,"['Keshara Weerasinghe', 'Saahith Janapati', 'Xueren Ge', 'Sion Kim', 'Sneha Iyer', 'John A. Stankovic', 'Homa Alemzadeh']","Emergency Medical Services (EMS) responders often operate under time-sensitive conditions, facing cognitive overload and inherent risks, requiring essential skills in critical thinking and rapid decision-making. This paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system that can act as a collaborative virtual partner engaging in the real-time acquisition and analysis of multimodal data from an emergency scene and interacting with EMS responders through Augmented Reality (AR) smart glasses. CognitiveEMS processes the continuous streams of data in real-time and leverages edge computing to provide assistance in EMS protocol selection and intervention recognition. We address key technical challenges in real-time cognitive assistance by introducing three novel components: (i) a Speech Recognition model that is fine-tuned for real-world medical emergency conversations using simulated EMS audio recordings, augmented with synthetic data generated by large language models (LLMs); (ii) an EMS Protocol Prediction model that combines state-of-the-art (SOTA) tiny language models with EMS domain knowledge using graph-based attention mechanisms; (iii) an EMS Action Recognition module which leverages multimodal audio and video data and protocol predictions to infer the intervention/treatment actions taken by the responders at the incident scene. Our results show that for speech recognition we achieve superior performance compared to SOTA (WER of 0.290 vs. 0.618) on conversational data. Our protocol prediction component also significantly outperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition achieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s for protocol prediction on the edge and 0.31s on the server."
https://arxiv.org/abs/2403.06733,2024-03-11,On the construction of a quantum channel corresponding to non-commutative graph for a qubit interacting with quantum oscillator,"['G. G. Amosov', 'A. S. Mokeev', 'A. N. Pechen']","We consider error correction, based on the theory of non-commutative graphs, for a model of a qubit interacting with quantum oscillator. The dynamics of the composite system is governed by the Schrödinger equation which generates positive operator-valued measure (POVM) for the system dynamics. We construct a quantum channel generating the non-commutative graph as a linear envelope of the POVM. The idea is based on applying a generalized version of a quantum channel using the apparatus of von Neumann algebras. The results are analyzes for a non-commutative graph generated by a qubit interacting with quantum oscillator. For this model the quantum anticlique which determines the error correcting subspace has an explicit expression."
https://arxiv.org/abs/2403.06732,2024-03-11,Greedy construction of quadratic manifolds for nonlinear dimensionality reduction and nonlinear model reduction,"['Paul Schwerdtner', 'Benjamin Peherstorfer']","Dimensionality reduction on quadratic manifolds augments linear approximations with quadratic correction terms. Previous works rely on linear approximations given by projections onto the first few leading principal components of the training data; however, linear approximations in subspaces spanned by the leading principal components alone can miss information that are necessary for the quadratic correction terms to be efficient. In this work, we propose a greedy method that constructs subspaces from leading as well as later principal components so that the corresponding linear approximations can be corrected most efficiently with quadratic terms."
https://arxiv.org/abs/2403.06731,2024-03-11,On the Approximation of Kernel functions,"['Paul Dommel', 'Alois Pichler']","Various methods in statistical learning build on kernels considered in reproducing kernel Hilbert spaces. In applications, the kernel is often selected based on characteristics of the problem and the data. This kernel is then employed to infer response variables at points, where no explanatory data were observed. The data considered here are located in compact sets in higher dimensions and the paper addresses approximations of the kernel itself. The new approach considers Taylor series approximations of radial kernel functions. For the Gauss kernel on the unit cube, the paper establishes an upper bound of the associated eigenfunctions, which grows only polynomially with respect to the index. The novel approach substantiates smaller regularization parameters than considered in the literature, overall leading to better approximations. This improvement confirms low rank approximation methods such as the Nyström method."
https://arxiv.org/abs/2403.06730,2024-03-11,An invariance principle for the 2d weakly self-repelling Brownian polymer,"['Giuseppe Cannizzaro', 'Harry Giles']","We investigate the large-scale behaviour of the Self-Repelling Brownian Polymer (SRBP) in the critical dimension $d=2$. The SRBP is a model of self-repelling motion, which is formally given by the solution a stochastic differential equation driven by a standard Brownian motion and with a drift given by the negative gradient of its own local time. As with its discrete counterpart, the ""true"" self-avoiding walk (TSAW) of [D.J. Amit, G. Parisi, & L. Peliti, Asymptotic behaviour of the ""true"" self-avoiding walk, Phys. Rev. B, 1983], it is conjectured to be logarithmically superdiffusive, i.e. to be such that its mean-square displacement grows as $t(\log t)^β$ for $t$ large and some currently unknown $β\in(0,1)$. The main result of the paper is an invariance principle for the SRBP under the weak coupling scaling, which corresponds to scaling the SRBP diffusively and simultaneously tuning down the strength of the self-interaction in a scale-dependent way. The diffusivity for the limiting Brownian motion is explicit and its expression provides compelling evidence that the $β$ above should be $1/2$. Further, we derive the scaling limit of the so-called environment seen by the particle process, which formally solves a non-linear singular stochastic PDE of transport-type, and prove this is given by the solution of a stochastic linear transport equation with enhanced diffusivity."
https://arxiv.org/abs/2403.06729,2024-03-11,Galaxy Morphologies Revealed with Subaru HSC and Super-Resolution Techniques II: Environmental Dependence of Galaxy Mergers at z~2-5,"['Takatoshi Shibuya', 'Yohito Ito', 'Kenta Asai', 'Takanobu Kirihara', 'Seiji Fujimoto', 'Yoshiki Toba', 'Noriaki Miura', 'Takuya Umayahara', 'Kenji Iwadate', 'Sadman S. Ali', 'Tadayuki Kodama']","We super-resolve the seeing-limited Subaru Hyper Suprime-Cam (HSC) images for 32,187 galaxies at z~2-5 in three techniques, namely, the classical Richardson-Lucy (RL) point spread function (PSF) deconvolution, sparse modeling, and generative adversarial networks to investigate the environmental dependence of galaxy mergers. These three techniques generate overall similar high spatial resolution images but with some slight differences in galaxy structures, for example, more residual noises are seen in the classical RL PSF deconvolution. To alleviate disadvantages of each technique, we create combined images by averaging over the three types of super-resolution images, which result in galaxy sub-structures resembling those seen in the Hubble Space Telescope images. Using the combined super-resolution images, we measure the relative galaxy major merger fraction corrected for the chance projection effect, f_merg, for galaxies in the ~300 deg^2-area data of the HSC Strategic Survey Program and the CFHT Large Area U-band Survey. Our f_merg measurements at z~3 validate previous findings showing that f_merg is higher in regions with a higher galaxy overdensity delta at z~2-3. Thanks to the large galaxy sample, we identify a nearly linear increase in f_merg with increasing delta at z~4-5, providing the highest-z observational evidence that galaxy mergers are related to delta. In addition to our f_merg measurements, we find that the galaxy merger fractions in the literature also broadly align with the linear f_merg-delta relation across a wide redshift range of z~2-5. This alignment suggests that the linear f_merg-delta relation can serve as a valuable tool for quantitatively estimating the contributions of galaxy mergers to various environmental dependences. This super-resolution analysis can be readily applied to datasets from wide field-of-view space telescopes such as Euclid and Roman."
https://arxiv.org/abs/2403.06728,2024-03-11,Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning,"['Zijian Zhou', 'Miaojing Shi', 'Meng Wei', 'Oluwatosin Alabi', 'Zijie Yue', 'Tom Vercauteren']","Radiology report generation (RRG) has attracted significant attention due to its potential to reduce the workload of radiologists. Current RRG approaches are still unsatisfactory against clinical standards. This paper introduces a novel RRG method, \textbf{LM-RRG}, that integrates large models (LMs) with clinical quality reinforcement learning to generate accurate and comprehensive chest X-ray radiology reports. Our method first designs a large language model driven feature extractor to analyze and interpret different regions of the chest X-ray image, emphasizing specific regions with medical significance. Next, based on the large model's decoder, we develop a multimodal report generator that leverages multimodal prompts from visual features and textual instruction to produce the radiology report in an auto-regressive way. Finally, to better reflect the clinical significant and insignificant errors that radiologists would normally assign in the report, we introduce a novel clinical quality reinforcement learning strategy. It utilizes the radiology report clinical quality (RadCliQ) metric as a reward function in the learning process. Extensive experiments on the MIMIC-CXR and IU-Xray datasets demonstrate the superiority of our method over the state of the art."
https://arxiv.org/abs/2403.06727,2024-03-11,Minimisers of supremal functionals and mass-minimising 1-currents,"['Nikos Katzourakis', 'Roger Moser']","We study vector-valued functions that minimise the $L^\infty$-norm of their derivatives for prescribed boundary data. We construct a vector-valued, mass minimising $1$-current (i.e., a generalised geodesic) in the domain such that all solutions of the problem coincide on its support. Furthermore, this current can be interpreted as a streamline of the solutions. It also has maximal support among all $1$-currents with certain properties."
https://arxiv.org/abs/2403.06726,2024-03-11,Probabilistic Contrastive Learning for Long-Tailed Visual Recognition,"['Chaoqun Du', 'Yulin Wang', 'Shiji Song', 'Gao Huang']","Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples. Such imbalance issue considerably impairs the performance of standard supervised learning algorithms, which are mainly designed for balanced training sets. Recent investigations have revealed that supervised contrastive learning exhibits promising potential in alleviating the data imbalance. However, the performance of supervised contrastive learning is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct contrastive pairs that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data. To overcome this obstacle, we propose a novel probabilistic contrastive (ProCo) learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples contrastive pairs accordingly. In fact, estimating the distributions of all classes using features in a small batch, particularly for imbalanced data, is not feasible. Our key idea is to introduce a reasonable and simple assumption that the normalized features in contrastive learning follow a mixture of von Mises-Fisher (vMF) distributions on unit space, which brings two-fold benefits. First, the distribution parameters can be estimated using only the first sample moment, which can be efficiently computed in an online manner across different batches. Second, based on the estimated distribution, the vMF distribution allows us to sample an infinite number of contrastive pairs and derive a closed form of the expected contrastive loss for efficient optimization. Our code is available at https://github.com/LeapLabTHU/ProCo."
https://arxiv.org/abs/2403.06725,2024-03-11,Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning,"['Hengyuan Zhang', 'Zitao Liu', 'Shuyan Huang', 'Chenming Shang', 'Bojun Zhan', 'Yong Jiang']","Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent ""pre-training and fine-tuning"" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subsequently facilitate effective adaptation to low-resource KT datasets. Specifically, we simplify existing sophisticated DLKT model architectures with purely a stack of transformer decoders. We design an encoding mechanism to incorporate student interactions from multiple KT data sources and develop an importance mechanism to prioritize updating parameters with high importance while constraining less important ones during the fine-tuning stage. We evaluate LoReKT on six public KT datasets and experimental results demonstrate the superiority of our approach in terms of AUC and Accuracy. To encourage reproducible research, we make our data and code publicly available at https://anonymous.4open.science/r/LoReKT-C619."
https://arxiv.org/abs/2403.06724,2024-03-11,A note on the Segal conjecture for large objects,"['Robert Burklund', 'Vignesh Subramanian']","The Segal conjecture for $C_p$ (as proved by Lin and Gunawardena) asserts that the canonical map from the $p$-complete sphere spectrum to the Tate construction for the trivial action of $C_p$ on the $p$-complete sphere spectrum is an isomorphism. In this article we extend the collection of spectra for which the canonical map $X \to X^{tC_p}$ is known to be an isomorphism to include any $p$-complete, bounded below spectrum whose mod $p$ homology, viewed a module over the Steenrod algebra, is complete with respect to the maximal ideal $I \subseteq \mathcal{A}$."
https://arxiv.org/abs/2403.06723,2024-03-11,A SysML Profile for the Standardized Description of Processes during System Development,"['Lasse Beers', 'Hamied Nabizada', 'Maximilian Weigand', 'Felix Gehlhoff', 'Alexander Fay']","A key aspect in creating models of production systems with the use of model-based systems engineering (MBSE) lies in the description of system functions. These functions shouldbe described in a clear and standardized manner.The VDI/VDE 3682 standard for Formalised Process De-scription (FPD) provides a simple and easily understandable representation of processes. These processes can be conceptualized as functions within the system model, making the FPD particularly well-suited for the standardized representation ofthe required functions. Hence, this contribution focuses on thedevelopment of a Domain-Specific Modeling Language(DSML) that facilitates the integration of VDI/VDE 3682 into the Systems Modeling Language (SysML). The presented approach not onlyextends classical SysML with domain-specific requirements but also facilitates model verification through constraints modeled in Object Constraint Language (OCL). Additionally, it enables automatic serialization of process descriptions into the Extensible Markup Language (XML) using the Velocity Template Language (VTL). This serialization enables the use of process modeling in applications outside of MBSE. The approach was validated using an collar screwing use case in the major component assembly in aircraft production."
https://arxiv.org/abs/2403.06722,2024-03-11,Asymptotics of the finite-temperature sine kernel determinant,['Shuai-Xia Xu'],"In the present paper, we study the asymptotics of the Fredholm determinant $D(x,s)$ of the finite-temperature deformation of the sine kernel, which represents the probability that there is no particles on the interval $(-x/π,x/π)$ in the bulk scaling limit of the finite-temperature fermion system. The variable $s$ in $D(x,s)$ is related to the temperature. The determinant also corresponds to the finite-temperature correlation function of one dimensional Bose gas. We derive the asymptotics of $D(x,s)$ in several different regimes in the $(x,s)$-plane. A third-order phase transition is observed in the asymptotic expansions as both $x$ and $s$ tend to positive infinity at certain related speed. The phase transition is then shown to be described by the integral involving the Hastings-McLeod solution of the second Painlevé equation."
https://arxiv.org/abs/2403.06721,2024-03-11,Fundamental Theorems for Timelike Surfaces in the Minkowski 4-Space,"['Victoria Bencheva', 'Velichka Milousheva']","In the present paper, we study timelike surfaces free of minimal points in the four-dimensional Minkowski space. For each such surface we introduce a geometrically determined pseudo-orthonormal frame field and writing the derivative formulas with respect to this moving frame field and using the integrability conditions, we obtain a system of six functions satisfying some natural conditions. In the general case, we prove a Fundamental Bonnet-type theorem (existence and uniqueness theorem) stating that these six functions, satisfying the natural conditions, determine the surface up to a motion. In some particular cases, we reduce the number of functions and give the fundamental theorems."
https://arxiv.org/abs/2403.06720,2024-03-11,On the Secrecy Rate of In-Band Full-duplex Two-way Wiretap Channel,"['Navneet Garg', 'Haifeng Luo', 'Tharmalingam Ratnarajah']","In this paper, we consider a two-way wiretap Multi-Input Multi-Output Multi-antenna Eve (MIMOME) channel, where both nodes (Alice and Bob) transmit and receive in an in-band full-duplex (IBFD) manner. For this system with keyless security, we provide a novel artificial noise (AN) based signal design, where the AN is injected in both signal and null spaces. We present an ergodic secrecy rate approximation to derive the power allocation algorithm. We consider scenarios where AN is known and unknown to legitimate users and include imperfect channel information effects. To maximize secrecy rates subject to the transmit power constraint, a two-step power allocation solution is proposed, where the first step is known at Eve, and the second step helps to improve the secrecy further. We also consider scenarios where partial information is known by Eve and the effects of non-ideal self-interference cancellation. The usefulness and limitations of the resulting power allocation solution are analyzed and verified via simulations. Results show that secrecy rates are less when AN is unknown to receivers or Eve has more information about legitimate users. Since the ergodic approximation only considers Eves distance, the resulting power allocation provides secrecy rates close to the actual ones."
https://arxiv.org/abs/2403.06719,2024-03-11,"Injection spectra of different species of cosmic rays from AMS-02, ACE-CRIS and Voyager-1","['Xu Pan', 'Qiang Yuan']","Precise measurements of energy spectra of different cosmic ray species were obtained in recent years, by particularly the AMS-02 experiment on the International Space Station. It has been shown that apparent differences exist in different groups of the primary cosmic rays. However, it is not straightforward to conclude that the source spectra of different particle groups are different since they will experience different propagation processes (e.g., energy losses and fragmentations) either. In this work, we study the injection spectra of different nuclear species using the measurements from Voyager-1 outside the solar system, and ACR-CRIS and AMS-02 on top of atmosphere, in a physical framework of cosmic ray transportation. Two types of injection spectra are assumed, the broken power-law and the non-parametric spline interpolation form. The non-parametric form fits the data better than the broken power-law form, implying that potential structures beyond the constrained spectral shape of broken power-law may exist. For different nuclei the injection spectra are overall similar in shape but do show some differences among each other. For the non-parametric spectral form, the helium injection spectrum is the softest at low energies and the hardest at high energies. For both spectral shapes, the low-energy injection spectrum of neon is the hardest among all these species, and the carbon and oxygen spectra have more prominent bumps in 1-10 GV in the R2dN/dR presentation. Such differences suggest the existence of differences in the sources or acceleration processes of various nuclei of cosmic rays."
https://arxiv.org/abs/2403.06718,2024-03-11,Bayesian prediction regions and density estimation with type-2 censored data,"['Akbar Asgharzadeh', 'Éric Marchand', 'Ali Saadati Nik']","For exponentially distributed lifetimes, we consider the prediction of future order statistics based on having observed the first $m$ order statistics. We focus on the previously less explored aspects of predicting: (i) an arbitrary pair of future order statistics such as the next and last ones, as well as (ii) the next $N$ future order statistics. We provide explicit and exact Bayesian credible regions associated with Gamma priors, and constructed by identifying a region with a given credibility $1-λ$ under the Bayesian predictive density. For (ii), the HPD region is obtained, while a two-step algorithm is given for (i). The predictive distributions are represented as mixtures of bivariate Pareto distributions, as well as multivariate Pareto distributions. For the non-informative prior density choice, we demonstrate that a resulting Bayesian credible region has matching frequentist coverage probability, and that the resulting predictive density possesses the optimality properties of best invariance and minimaxity."
https://arxiv.org/abs/2403.06717,2024-03-11,Unprotected 4G/5G Control Procedures at Low Layers Considered Dangerous,"['Norbert Ludant', 'Marinos Vomvas', 'Guevara Noubir']","Over the years, several security vulnerabilities in the 3GPP cellular systems have been demonstrated in the literature. Most studies focus on higher layers of the cellular radio stack, such as the RRC and NAS, which are cryptographically protected. However, lower layers of the stack, such as PHY and MAC, are not as thoroughly studied, even though they are neither encrypted nor integrity protected. Furthermore, the latest releases of 5G significantly increased the number of low-layer control messages and procedures. The complexity of the cellular standards and the high degree of cross-layer operations, makes reasoning about security non-trivial, and requires a systematic analysis. We study the control procedures carried by each physical channel, and find that current cellular systems are susceptible to several new passive attacks due to information leakage, and active attacks by injecting MAC and PHY messages. For instance, we find that beamforming information leakage enables fingerprinting-based localization and tracking of users. We identify active attacks that reduce the users' throughput by disabling RF front ends at the UE, disrupt user communications by tricking other connected UEs into acting as jammers, or stealthily disconnect an active user. We evaluate our attacks against COTS UEs in various scenarios and demonstrate their practicality by measuring current operators' configurations across three countries. Our results show that an attacker can, among other things, localize users with an accuracy of 20 meters 96% of the time, track users' moving paths with a probability of 90%, reduce throughput by more than 95% within 2 seconds (by spoofing a 39 bits DCI), and disconnect users."
https://arxiv.org/abs/2403.06716,2024-03-11,Emergency Response Inference Mapping (ERIMap): A Bayesian Network-based Method for Dynamic Observation Processing in Spatially Distributed Emergencies,"['Moritz Schneider', 'Lukas Halekotte', 'Tina Comes', 'Daniel Lichte', 'Frank Fiedrich']","In emergencies, high stake decisions often have to be made under time pressure and strain. In order to support such decisions, information from various sources needs to be collected and processed rapidly. The information available tends to be temporally and spatially variable, uncertain, and sometimes conflicting, leading to potential biases in decisions. Currently, there is a lack of systematic approaches for information processing and situation assessment which meet the particular demands of emergency situations. To address this gap, we present a Bayesian network-based method called ERIMap that is tailored to the complex information-scape during emergencies. The method enables the systematic and rapid processing of heterogeneous and potentially uncertain observations and draws inferences about key variables of an emergency. It thereby reduces complexity and cognitive load for decision makers. The output of the ERIMap method is a dynamically evolving and spatially resolved map of beliefs about key variables of an emergency that is updated each time a new observation becomes available. The method is illustrated in a case study in which an emergency response is triggered by an accident causing a gas leakage on a chemical plant site."
https://arxiv.org/abs/2403.06715,2024-03-11,Dynamic minimisation of the commute time for a one-dimensional diffusion,"['Ma. Elena Hernández-Hernández', 'Saul Jacka']","Motivated in part by a problem in simulated tempering (a form of Markov chain Monte Carlo) we seek to minimise, in a suitable sense, the time it takes a (regular) diffusion with instantaneous reflection at 0 and 1 to travel to $1$ and then return to the origin (the so-called commute time from 0 to 1). Substantially extending results in a previous paper, we consider a dynamic version of this problem where the control mechanism is related to the diffusion's drift via the corresponding scale function. We are only able to choose the drift at each point at the time of first visiting that point and the drift is constrained on a set of the form $[0,\ell)\cup(i,1]$. This leads to a type of stochastic control problem with infinite dimensional state."
https://arxiv.org/abs/2403.06714,2024-03-11,Societal and scientific impact of policy research: A large-scale empirical study of some explanatory factors using Altmetric and Overton,"['Pablo Dorta-González', 'Alejandro Rodríguez-Caro', 'María Isabel Dorta-González']","This study investigates how scientific research influences policymaking by analyzing citations of research articles in policy documents (policy impact) for nearly 125,000 articles across 434 public policy journals. We reveal distinct citation patterns between policymakers and other stakeholders like researchers, journalists, and the public. News and blog mentions, social media engagement, and open access publications (excluding fully open access) significantly increase the likelihood of a research article being cited in policy documents. Conversely, articles locked behind paywalls and those published under the full open access model (based on Altmetric data) have a lower chance of being policy-cited. Publication year and policy type show no significant influence. Our findings emphasize the crucial role of science communication channels like news media and social media in bridging the gap between research and policy. Interestingly, academic citations hold a weaker influence on policy citations compared to news mentions, suggesting a potential disconnect between how researchers reference research and how policymakers utilize it. This highlights the need for improved communication strategies to ensure research informs policy decisions more effectively. This study provides valuable insights for researchers, policymakers, and science communicators. Researchers can tailor their dissemination efforts to reach policymakers through media channels. Policymakers can leverage these findings to identify research with higher policy relevance. Science communicators can play a critical role in translating research for policymakers and fostering dialogue between the scientific and policymaking communities."
https://arxiv.org/abs/2403.06713,2024-03-11,"Influence of Li-stoichiometry on electrical and acoustic properties and temperature stability of Li(Nb,Ta)O$_{3}$ solid solutions up to 900 °C","['Éva Tichy-Rács', 'Stepan Hurskyy', 'Uliana Yakhnevych', 'Piotr Gaczyński', 'Steffen Ganschow', 'Holger Fritze', 'Yuriy Suhak']","The current work is focused on the impact of the lithium stoichiometry on electrical conductivity, acoustic properties and high-temperature stability of single crystalline Li(Nb,Ta)O$_{3}$ at high temperatures. The crystals grown from Li-deficient melts were treated by the vapor transport equilibration (VTE) method, achieving near stoichiometric Li-content. It is shown, that the VTE-treated specimens generally exhibit lower conductivity at temperatures below 800 °C, which is attributed to the reduced number of Li-vacancies in near stoichiometric Li(Nb,Ta)O$_{3}$, provided that the Li-ion migration dominates the conductivity in this temperature range. Further, it is shown, that above 600-650 °C different mechanism increasingly contributes to the conductivity, which is consequently attributed to the electronic conduction. Further, it is shown that losses in LNT strongly increase above about 500 °C, which is interpreted to originate from conductivity-related relaxation mechanism. Finally, the thermal stability of Li(Nb,Ta)O$_{3}$ is evaluated by the measurement of the conductivity and resonance frequency as a function of time. It is found that during annealing at 700 °C for 350 hours, the resonance frequency of LiNbO$_{3}$ remains in a {\textpm} 100 ppm range of the initial value of 3.5 MHz."
https://arxiv.org/abs/2403.06712,2024-03-11,The Ouroboros of Memristors: Neural Networks Facilitating Memristor Programming,"['Zhenming Yu', 'Ming-Jay Yang', 'Jan Finkbeiner', 'Sebastian Siegel', 'John Paul Strachan', 'Emre Neftci']","Memristive devices hold promise to improve the scale and efficiency of machine learning and neuromorphic hardware, thanks to their compact size, low power consumption, and the ability to perform matrix multiplications in constant time. However, on-chip training with memristor arrays still faces challenges, including device-to-device and cycle-to-cycle variations, switching non-linearity, and especially SET and RESET asymmetry. To combat device non-linearity and asymmetry, we propose to program memristors by harnessing neural networks that map desired conductance updates to the required pulse times. With our method, approximately 95% of devices can be programmed within a relative percentage difference of +-50% from the target conductance after just one attempt. Our approach substantially reduces memristor programming delays compared to traditional write-and-verify methods, presenting an advantageous solution for on-chip training scenarios. Furthermore, our proposed neural network can be accelerated by memristor arrays upon deployment, providing assistance while reducing hardware overhead compared with previous works."
https://arxiv.org/abs/2403.06711,2024-03-11,Identifying plasma fractionation processes in the chromosphere using IRIS,"['David M. Long', 'Deborah Baker', 'Andy S. H. To', 'Lidia van Driel-Gesztelyi', 'David H. Brooks', 'Marco Stangalini', 'Mariarita Murabito', 'Alexander W. James', 'Mihalis Mathioudakis', 'Paola Testa']","The composition of the solar corona differs from that of the photosphere, with the plasma thought to fractionate in the solar chromosphere according to the First Ionisation Potential (FIP) of the different elements. This produces a FIP bias, wherein elements with a low FIP are preferentially enhanced in the corona compared to their photospheric abundance, but direct observations of this process remain elusive. Here we use a series of spectroscopic observations of Active Region AR 12759 as it transited the solar disc over a period of 6 days from 2-7 April 2020 taken using the Hinode Extreme ultraviolet Imaging Spectrometer (EIS) and Interface Region Imaging Spectrograph (IRIS) instruments to look for signatures of plasma fractionation in the solar chromosphere. Using the Si X/S X and Ca XIV/Ar XIV diagnostics, we find distinct differences between the FIP bias of the leading and following polarities of the active region. The widths of the IRIS Si IV lines exhibited clear differences between the leading and following polarity regions, indicating increased unresolved wave activity in the following polarity region compared to the leading polarity region, with the chromospheric velocities derived using the Mg II lines exhibiting comparable, albeit much weaker, behaviour. These results are consistent with plasma fractionation via resonant/non-resonant waves at different locations in the solar chromosphere following the ponderomotive force model, and indicate that IRIS could be used to further study this fundamental physical process."
https://arxiv.org/abs/2403.06710,2024-03-11,HILL: A Hallucination Identifier for Large Language Models,"['Florian Leiser', 'Sven Eckhardt', 'Valentin Leuthe', 'Merlin Knaeble', 'Alexander Maedche', 'Gerhard Schwabe', 'Ali Sunyaev']","Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the ""Hallucination Identifier for Large Language Models"". First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILL's interface design by surveying 17 participants. Further, we investigated HILL's functionality to identify hallucinations based on an existing question-answering dataset and five user interviews. We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution. With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts."
https://arxiv.org/abs/2403.06709,2024-03-12,Validation of the GreenX library time-frequency component for efficient GW and RPA calculations,"['Maryam Azizi', 'Jan Wilhelm', 'Dorothea Golze', 'Francisco A. Delesma', 'Ramón L. Panadés-Barrueta', 'Patrick Rinke', 'Matteo Giantomassi', 'Xavier Gonze']","Electronic structure calculations based on many-body perturbation theory (e.g. GW or the random-phase approximation (RPA)) require function evaluations in the complex time and frequency domain, for example inhomogeneous Fourier transforms or analytic continuation from the imaginary axis to the real axis. For inhomogeneous Fourier transforms, the time-frequency component of the GreenX library provides time-frequency grids that can be utilized in low-scaling RPA and GW implementations. In addition, the adoption of the compact frequency grids provided by our library also reduces the computational overhead in RPA implementations with conventional scaling. In this work, we present low-scaling GW and conventional RPA benchmark calculations using the GreenX grids with different codes (FHI-aims, CP2K and ABINIT) for molecules, two-dimensional materials and solids. Very small integration errors are observed when using 30 time-frequency points for our test cases, namely $<10^{-8}$ eV/electron for the RPA correlation energies, and 10 meV for the GW quasiparticle energies."
https://arxiv.org/abs/2403.06708,2024-03-11,Tikhonov Regularization for Stochastic Non-Smooth Convex Optimization in Hilbert Spaces,"['Rodrigo Maulen-Soto', 'Jalal Fadili', 'Hedy Attouch']","To solve non-smooth convex optimization problems with a noisy gradient input, we analyze the global behavior of subgradient-like flows under stochastic errors. The objective function is composite, being equal to the sum of two convex functions, one being differentiable and the other potentially non-smooth. We then use stochastic differential inclusions where the drift term is minus the subgradient of the objective function, and the diffusion term is either bounded or square-integrable. In this context, under Lipschitz's continuity of the differentiable term and a growth condition of the non-smooth term, our first main result shows almost sure weak convergence of the trajectory process towards a minimizer of the objective function. Then, using Tikhonov regularization with a properly tuned vanishing parameter, we can obtain almost sure strong convergence of the trajectory towards the minimum norm solution. We find an explicit tuning of this parameter when our objective function satisfies a local error-bound inequality. We also provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex and strongly convex case."
https://arxiv.org/abs/2403.06707,2024-03-11,Deriving Dependently-Typed OOP from First Principles -- Extended Version with Additional Appendices,"['David Binder', 'Ingo Skupin', 'Tim Süberkrüb', 'Klaus Ostermann']","The expression problem describes how most types can easily be extended with new ways to produce the type or new ways to consume the type, but not both. When abstract syntax trees are defined as an algebraic data type, for example, they can easily be extended with new consumers, such as print or eval, but adding a new constructor requires the modification of all existing pattern matches. The expression problem is one way to elucidate the difference between functional or data-oriented programs (easily extendable by new consumers) and object-oriented programs (easily extendable by new producers). This difference between programs which are extensible by new producers or new consumers also exists for dependently typed programming, but with one core difference: Dependently-typed programming almost exclusively follows the functional programming model and not the object-oriented model, which leaves an interesting space in the programming language landscape unexplored. In this paper, we explore the field of dependently-typed object-oriented programming by deriving it from first principles using the principle of duality. That is, we do not extend an existing object-oriented formalism with dependent types in an ad-hoc fashion, but instead start from a familiar data-oriented language and derive its dual fragment by the systematic use of defunctionalization and refunctionalization. Our central contribution is a dependently typed calculus which contains two dual language fragments. We provide type- and semantics-preserving transformations between these two language fragments: defunctionalization and refunctionalization. We have implemented this language and these transformations and use this implementation to explain the various ways in which constructions in dependently typed programming can be explained as special instances of the phenomenon of duality."
https://arxiv.org/abs/2403.06706,2024-03-11,Propagation of Solar Energetic Particles in 3D MHD Simulations of the Solar Wind,"['Houeibib Ahmed', 'Pantellini Filippo', 'Griton Léa']","We propagate relativistic test particles in the field of a steady 3D MHD simulations of the solar wind. We use the MPI-AMRVAC code for the wind simulations and integrate the relativistic guiding center equations using a new third-order accurate time integration scheme to solve the particle trajectories. Diffusion in velocity space, given a particle-turbulence mean free path $λ_\parallel$ along the magnetic field, is also included. Preliminary results for $81\:{\rm keV}$ electrons injected at 0.139 AU heliocentric distance and mean free path $λ_\parallel =0.5\:{\rm AU}$ are in a good qualitative agreement with measurements at 1 AU."
https://arxiv.org/abs/2403.06705,2024-03-11,Multimodal Transformers for Real-Time Surgical Activity Prediction,"['Keshara Weerasinghe', 'Seyed Hamid Reza Roodabeh', 'Kay Hutchinson', 'Homa Alemzadeh']",Real-time recognition and prediction of surgical activities are fundamental to advancing safety and autonomy in robot-assisted surgery. This paper presents a multimodal transformer architecture for real-time recognition and prediction of surgical gestures and trajectories based on short segments of kinematic and video data. We conduct an ablation study to evaluate the impact of fusing different input modalities and their representations on gesture recognition and prediction performance. We perform an end-to-end assessment of the proposed architecture using the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) dataset. Our model outperforms the state-of-the-art (SOTA) with 89.5\% accuracy for gesture prediction through effective fusion of kinematic features with spatial and contextual video features. It achieves the real-time performance of 1.1-1.3ms for processing a 1-second input window by relying on a computationally efficient model.
https://arxiv.org/abs/2403.06704,2024-03-11,Proof complexity of universal algebra in a CSP dichotomy proof,['Azza Gaysin'],"The constraint satisfaction problem (CSP) can be formulated as a homomorphism problem between relational structures: given a structure $\mathcal{A}$, for any structure $\mathcal{X}$, whether there exists a homomorphism from $\mathcal{X}$ to $\mathcal{A}$. For years, it has been conjectured that all problems of this type are divided into polynomial-time and NP-complete problems, and the conjecture was proved in 2017 separately by Zhuk (2017) and Bulatov (2017). Zhuk's algorithm solves tractable CSPs in polynomial time. The algorithm is partly based on universal algebra theorems: informally, they state that after reducing some domain of an instance to its strong subuniverses, a satisfiable instance maintains a solution."
https://arxiv.org/abs/2403.06703,2024-03-11,A viscoelastic model for skin via homogenization theory,['Juan Casado-Díaz'],We carry out the homogenization of a fluid-structure interaction problem consisting in the periodic inclusions of a viscous fluid in an elastic body. We get a macrostructure model where the body behaves as a viscoelastic material with a long-range memory term. Our aim is not only to get this limit problem but also to study its main properties. Using the micro-structure variables it is simple to check that it satisfies an energy conservation law assuring in particular the existence and uniqueness of solution. The difficulty is to characterize these properties using only the macroscopic system. We prove that the nonlocal term is given through a convolution kernel which exponentially decreases to zero and satisfies some positive conditions which we write in terms of the Laplace transform. These conditions can be used to directly prove the existence and uniqueness of solution. The results apply to modeling the mechanical behavior of skin as an indicator of what kind of models we should use. In a naive interpretation the fluid inclusions represent the cells and the elastic medium the extracellular matrix.
https://arxiv.org/abs/2403.06702,2024-03-11,Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization,"['Jinlu Zhang', 'Yiyi Zhou', 'Qiancheng Zheng', 'Xiaoxiong Du', 'Gen Luo', 'Jun Peng', 'Xiaoshuai Sun', 'Rongrong Ji']","Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging research hot spot in machine learning, which still suffers from low efficiency and poor quality. In this paper, we propose an End-to-End Efficient and Effective network for fast and accurate T3D face generation and manipulation, termed $E^3$-FaceNet. Different from existing complex generation paradigms, $E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware visual space. We introduce a novel Style Code Enhancer to enhance cross-modal semantic alignment, alongside an innovative Geometric Regularization objective to maintain consistency across multi-view generations. Extensive experiments on three benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve picture-like 3D face generation and manipulation, but also improve inference speed by orders of magnitudes. For instance, compared with Latent3D, $E^3$-FaceNet speeds up the five-view generations by almost 470 times, while still exceeding in generation quality. Our code are released at https://github.com/Aria-Zhangjl/E3-FaceNet."
https://arxiv.org/abs/2403.06701,2024-03-11,Exceptional or half-integral chirally cosmetic surgeries,"['Kazuhiro Ichihara', 'Toshio Saito']","A pair of Dehn surgeries on a knot is called chirally cosmetic if they yield orientation-reversingly homeomorphic 3-manifolds. In this paper, we consider exceptional or half-integral chirally cosmetic surgeries, and obtain several restrictions."
https://arxiv.org/abs/2403.06700,2024-03-11,Enhancing Adversarial Training with Prior Knowledge Distillation for Robust Image Compression,"['Cao Zhi', 'Bao Youneng', 'Meng Fanyang', 'Li Chao', 'Tan Wen', 'Wang Genhong', 'Liang Yongsheng']","Deep neural network-based image compression (NIC) has achieved excellent performance, but NIC method models have been shown to be susceptible to backdoor attacks. Adversarial training has been validated in image compression models as a common method to enhance model robustness. However, the improvement effect of adversarial training on model robustness is limited. In this paper, we propose a prior knowledge-guided adversarial training framework for image compression models. Specifically, first, we propose a gradient regularization constraint for training robust teacher models. Subsequently, we design a knowledge distillation based strategy to generate a priori knowledge from the teacher model to the student model for guiding adversarial training. Experimental results show that our method improves the reconstruction quality by about 9dB when the Kodak dataset is elected as the backdoor attack object for psnr attack. Compared with Ma2023, our method has a 5dB higher PSNR output at high bitrate points."
https://arxiv.org/abs/2403.06699,2024-03-11,Solving Distributed Flexible Job Shop Scheduling Problems in the Wool Textile Industry with Quantum Annealing,"['Lilia Toma', 'Markus Zajac', 'Uta Störl']","Many modern manufacturing companies have evolved from a single production site to a multi-factory production environment that must handle both geographically dispersed production orders and their multi-site production steps. The availability of a range of machines in different locations capable of performing the same operation and shipping times between factories have transformed planning systems from the classic Job Shop Scheduling Problem (JSSP) to Distributed Flexible Job Shop Scheduling Problem (DFJSP). As a result, the complexity of production planning has increased significantly. In our work, we use Quantum Annealing (QA) to solve the DFJSP. In addition to the assignment of production orders to production sites, the assignment of production steps to production sites also takes place. This requirement is based on a real use case of a wool textile manufacturer. To investigate the applicability of this method to large problem instances, problems ranging from 50 variables up to 250 variables, the largest problem that could be embedded into a D-Wave quantum annealer Quantum Processing Unit (QPU), are formulated and solved. Special attention is dedicated to the determination of the Lagrange parameters of the Quadratic Unconstrained Binary Optimization (QUBO) model and the QPU configuration parameters, as these factors can significantly impact solution quality. The obtained solutions are compared to solutions obtained by Simulated Annealing (SA), both in terms of solution quality and calculation time. The results demonstrate that QA has the potential to solve large problem instances specific to the industry."
https://arxiv.org/abs/2403.06698,2024-03-11,PCLD: Point Cloud Layerwise Diffusion for Adversarial Purification,"['Mert Gulsen', 'Batuhan Cengiz', 'Yusuf H. Sahin', 'Gozde Unal']","Point clouds are extensively employed in a variety of real-world applications such as robotics, autonomous driving and augmented reality. Despite the recent success of point cloud neural networks, especially for safety-critical tasks, it is essential to also ensure the robustness of the model. A typical way to assess a model's robustness is through adversarial attacks, where test-time examples are generated based on gradients to deceive the model. While many different defense mechanisms are studied in 2D, studies on 3D point clouds have been relatively limited in the academic field. Inspired from PointDP, which denoises the network inputs by diffusion, we propose Point Cloud Layerwise Diffusion (PCLD), a layerwise diffusion based 3D point cloud defense strategy. Unlike PointDP, we propagated the diffusion denoising after each layer to incrementally enhance the results. We apply our defense method to different types of commonly used point cloud models and adversarial attacks to evaluate its robustness. Our experiments demonstrate that the proposed defense method achieved results that are comparable to or surpass those of existing methodologies, establishing robustness through a novel technique. Code is available at https://github.com/batuceng/diffusion-layer-robustness-pc."
https://arxiv.org/abs/2403.06697,2024-03-11,Additive kinematic formulas for convex functions,"['Daniel Hug', 'Fabian Mussnig', 'Jacopo Ulivelli']","We prove a functional version of the additive kinematic formula as an application of the Hadwiger theorem on convex functions together with a Kubota-type formula for mixed Monge-Ampère measures. As an application, we give a new explanation for the equivalence of the representations of functional intrinsic volumes as singular Hessian valuations and as integrals with respect to mixed Monge-Ampère measures. In addition, we obtain a new integral geometric formula for mixed area measures of convex bodies, where integration on $\operatorname{SO}(n-1)\times \operatorname{O}(1)$ is considered."
https://arxiv.org/abs/2403.06696,2024-03-11,On the smallness of mean oscillations on metric-measure spaces and applications,['Dung Le'],It will be established that the mean oscillation of a function on a metric-measure space $X\times Y$ will be small if its mean oscillation on $X$ is small and some simple information on its (partial $Y$) upper-gradient is given. Applications to the regularity and global existence of bounded solutions to strongly coupled elliptic/parabolic systems on thin domains are also considered.
https://arxiv.org/abs/2403.06695,2024-03-11,A Minority of C++ Objects Account for the Majority of Allocation CPU Time,"['Eugene Darashkevich', 'Roman Rusyaev', 'Roman Korostinskiy', 'Yegor Bugayenko']","In C++, an object can be allocated in static memory, on the stack, or on the heap, where the latter is by the order of magnitude more expensive operation, performance wise, than the first two. However, it is not clear how much overall performance loss may be attributed to the use of on-heap objects in C++ applications. This study aims to fill this gap by analyzing object allocation practices in open-source C++ code, investigating the frequency of stack and heap allocations using real-time dynamic analysis with tools such as DynamoRIO and Valgrind. We found out that the majority of objects (97.2%) are allocated on the stack, with only a small portion (2.8%) allocated on the heap. However, when considering the computational cost of each allocation method, we find that heap allocations account for a substantial 85% of the total CPU cycles consumed by object allocations. These findings underscore the importance of optimization of on-heap object allocations, in C++ programming."
https://arxiv.org/abs/2403.06694,2024-03-11,$C_{2k+1}$-coloring of bounded-diameter graphs,['Marta Piecyk'],"For a fixed graph $H$, in the graph homomorphism problem, denoted by $Hom(H)$, we are given a graph $G$ and we have to determine whether there exists an edge-preserving mapping $\varphi: V(G) \to V(H)$. Note that $Hom(C_3)$, where $C_3$ is the cycle of length $3$, is equivalent to $3$-Coloring. The question whether $3$-Coloring is polynomial-time solvable on diameter-$2$ graphs is a well-known open problem. In this paper we study the $Hom(C_{2k+1})$ problem on bounded-diameter graphs for $k\geq 2$, so we consider all other odd cycles than $C_3$. We prove that for $k\geq 2$, the $Hom(C_{2k+1})$ problem is polynomial-time solvable on diameter-$(k+1)$ graphs -- note that such a result for $k=1$ would be precisely a polynomial-time algorithm for $3$-Coloring of diameter-$2$ graphs."
https://arxiv.org/abs/2403.06693,2024-03-11,Chart4Blind: An Intelligent Interface for Chart Accessibility Conversion,"['Omar Moured', 'Morris Baumgarten-Egemole', 'Alina Roitberg', 'Karin Muller', 'Thorsten Schwarz', 'Rainer Stiefelhagen']","In a world driven by data visualization, ensuring the inclusive accessibility of charts for Blind and Visually Impaired (BVI) individuals remains a significant challenge. Charts are usually presented as raster graphics without textual and visual metadata needed for an equivalent exploration experience for BVI people. Additionally, converting these charts into accessible formats requires considerable effort from sighted individuals. Digitizing charts with metadata extraction is just one aspect of the issue; transforming it into accessible modalities, such as tactile graphics, presents another difficulty. To address these disparities, we propose Chart4Blind, an intelligent user interface that converts bitmap image representations of line charts into universally accessible formats. Chart4Blind achieves this transformation by generating Scalable Vector Graphics (SVG), Comma-Separated Values (CSV), and alternative text exports, all comply with established accessibility standards. Through interviews and a formal user study, we demonstrate that even inexperienced sighted users can make charts accessible in an average of 4 minutes using Chart4Blind, achieving a System Usability Scale rating of 90%. In comparison to existing approaches, Chart4Blind provides a comprehensive solution, generating end-to-end accessible SVGs suitable for assistive technologies such as embossed prints (papers and laser cut), 2D tactile displays, and screen readers. For additional information, including open-source codes and demos, please visit our project page https://moured.github.io/chart4blind/."
https://arxiv.org/abs/2403.06692,2024-03-11,Log motivic exceptional direct image functors,['Doosung Park'],"In this paper, we construct the motivic exceptional direct image functors for fs log schemes. This construction is a part of the motivic six-functor formalism for fs log schemes."
https://arxiv.org/abs/2403.06691,2024-03-11,Approximating Maximum Edge 2-Coloring by Normalizing Graphs,"['Tobias Mömke', 'Alexandru Popa', 'Aida Roshany-Tabrizi', 'Michael Ruderer', 'Roland Vincze']","In a simple, undirected graph G, an edge 2-coloring is a coloring of the edges such that no vertex is incident to edges with more than 2 distinct colors. The problem maximum edge 2-coloring (ME2C) is to find an edge 2-coloring in a graph G with the goal to maximize the number of colors. For a relevant graph class, ME2C models anti-Ramsey numbers and it was considered in network applications. For the problem a 2-approximation algorithm is known, and if the input graph has a perfect matching, the same algorithm has been shown to have a performance guarantee of 5/3. It is known that ME2C is APX-hard and that it is UG-hard to obtain an approximation ratio better than 1.5. We show that if the input graph has a perfect matching, there is a polynomial time 1.625-approximation and if the graph is claw-free or if the maximum degree of the input graph is at most three (i.e., the graph is subcubic), there is a polynomial time 1.5-approximation algorithm for ME2C"
https://arxiv.org/abs/2403.06690,2024-03-11,From S-matrix theory to strings: Scattering data and the commitment to non-arbitrariness,['Robert van Leeuwen'],"The early history of string theory is marked by a shift from strong interaction physics to quantum gravity. The first string models and associated theoretical framework were formulated in the late 1960s and early 1970s in the context of the S-matrix program for the strong interactions. In the mid-1970s, the models were reinterpreted as a potential theory unifying the four fundamental forces. This paper provides a historical analysis of how string theory was developed out of S-matrix physics, aiming to clarify how modern string theory, as a theory detached from experimental data, grew out of an S-matrix program that was strongly dependent upon observable quantities. Surprisingly, the theoretical practice of physicists already turned away from experiment before string theory was recast as a potential unified quantum gravity theory. With the formulation of dual resonance models (the ""hadronic string theory""), physicists were able to determine almost all of the models' parameters on the basis of theoretical reasoning. It was this commitment to ""non-arbitrariness"", i.e., a lack of free parameters in the theory, that initially drove string theorists away from experimental input, and not the practical inaccessibility of experimental data in the context of quantum gravity physics. This is an important observation when assessing the role of experimental data in string theory."
https://arxiv.org/abs/2403.06689,2024-03-11,Dynamic of frustrated Kuramoto oscillators with modular connections,"['Guilherme S. Costa', 'Marcus A. M. de Aguiar']","Synchronization and collective movement are phenomena of a highly interdisciplinary nature, with examples ranging from neuronal activation to walking pedestrians. As of today, the Kuramoto model stands as the quintessential framework for investigating synchronization phenomena, displaying a second order phase transition from disordered motion to synchronization as the coupling between oscillators increases. The model was recently extended to higher dimensions allowing for the coupling parameter to be promoted to a matrix, leading to generalized frustration and new synchronized states. This model was previously investigated in the case of all-to-all and homogeneous interactions. Here, we extend the analysis to modular graphs, which mimic the community structure presented in many real systems. We investigated, both numerically and analytically, the matrix coupled Kuramoto model with oscillators divided into two groups with distinct coupling parameters to understand in which conditions they synchronize independently or globally. We discovered a very rich and complex dynamic, including an extended region in the parameter space in which the interactions between modules were destructive, leading to a global disordered motion even tough the uncoupled dynamic presented higher levels of synchronization. Additional simulations considering synthetic modular networks were performed to assess the robustness of our findings."
https://arxiv.org/abs/2403.06688,2024-03-11,Ground-state chiral current via periodic modulation,"['Shuyue Wang', 'Wuji Zhang', 'Chunfang Sun', 'Chunfeng Wu', 'X. Q. Shao', 'Gangcheng Wang']","In this study, we engineer the Dzyaloshinskii-Moriya interaction mediated by photons to emulate ground-state chiral current based on three-level atoms driven by quantum and classical fields. We employ adiabatic elimination techniques to derive an effective Dzyaloshinskii-Moriya interaction Hamiltonian of two-level systems, which can address the challenges arising from the finite lifetime of excited states. Furthermore, we can ensure to achieve desired dynamics through the implementation of periodic modulation on the atomic ground states. Besides, three-state and multi-state chiral current can be obtained by choosing appropriate driving frequencies and phases. We also design the Dzyaloshinskii-Moriya interaction for the other components based on a toggling frame. The numerical simulation results further indicate that our proposal can generate a perfectly reliable ground-state chiral current and open up possibilities for quantum state transfer and the development of future quantum networks."
https://arxiv.org/abs/2403.06687,2024-03-11,Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data,"['Jinghan Huang', 'Qiufeng Chen', 'Yijun Bian', 'Pengli Zhu', 'Nanguang Chen', 'Moo K. Chung', 'Anqi Qiu']","Graph neural networks (GNNs) have proven effective in capturing relationships among nodes in a graph. This study introduces a novel perspective by considering a graph as a simplicial complex, encompassing nodes, edges, triangles, and $k$-simplices, enabling the definition of graph-structured data on any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous graph attention network (HL-HGAT), designed to learn heterogeneous signal representations across $k$-simplices. The HL-HGAT incorporates three key components: HL convolutional filters (HL-filters), simplicial projection (SP), and simplicial attention pooling (SAP) operators, applied to $k$-simplices. HL-filters leverage the unique topology of $k$-simplices encoded by the Hodge-Laplacian (HL) operator, operating within the spectral domain of the $k$-th HL operator. To address computation challenges, we introduce a polynomial approximation for HL-filters, exhibiting spatial localization properties. Additionally, we propose a pooling operator to coarsen $k$-simplices, combining features through simplicial attention mechanisms of self-attention and cross-attention via transformers and SP operators, capturing topological interconnections across multiple dimensions of simplices. The HL-HGAT is comprehensively evaluated across diverse graph applications, including NP-hard problems, graph multi-label and classification challenges, and graph regression tasks in logistics, computer vision, biology, chemistry, and neuroscience. The results demonstrate the model's efficacy and versatility in handling a wide range of graph-based scenarios."
